[
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "e45e032b8d90435de2f338f77a732c88f8cca66e",
        "url": "https://github.com/apache/kafka/commit/e45e032b8d90435de2f338f77a732c88f8cca66e",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public maybeThrowAsyncError() : void extracted from private deliverMessages() : void in class org.apache.kafka.connect.runtime.WorkerSinkTask & moved to class org.apache.kafka.connect.runtime.errors.WorkerErrantRecordReporter",
            "leftSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 595,
                    "endLine": 634,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private deliverMessages() : void"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 605,
                    "endLine": 606,
                    "startColumn": 17,
                    "endColumn": 57,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 604,
                    "endLine": 607,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 604,
                    "endLine": 607,
                    "startColumn": 109,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java",
                    "startLine": 162,
                    "endLine": 166,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public maybeThrowAsyncError() : void"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java",
                    "startLine": 164,
                    "endLine": 164,
                    "startColumn": 13,
                    "endColumn": 103,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java",
                    "startLine": 163,
                    "endLine": 165,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java",
                    "startLine": 163,
                    "endLine": 165,
                    "startColumn": 100,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 595,
                    "endLine": 633,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private deliverMessages() : void"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 605,
                    "endLine": 605,
                    "startColumn": 17,
                    "endColumn": 66,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "workerErrantRecordReporter.maybeThrowAsyncError()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 567,
        "extraction_results": {
            "success": true,
            "newCommitHash": "886af689929a02781bb9b4d7dd6b3305235a6f6b",
            "newBranchName": "extract-maybeThrowAsyncError-deliverMessages-72b7028"
        },
        "telemetry": {
            "id": "bd0057d5-56ad-433b-a425-4032d7ec96f9",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 864,
                "lineStart": 72,
                "lineEnd": 935,
                "bodyLineStart": 72,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                "sourceCode": "/**\n * {@link WorkerTask} that uses a {@link SinkTask} to export data from Kafka.\n */\nclass WorkerSinkTask extends WorkerTask {\n    private static final Logger log = LoggerFactory.getLogger(WorkerSinkTask.class);\n\n    private final WorkerConfig workerConfig;\n    private final SinkTask task;\n    private final ClusterConfigState configState;\n    private Map<String, String> taskConfig;\n    private final Converter keyConverter;\n    private final Converter valueConverter;\n    private final HeaderConverter headerConverter;\n    private final TransformationChain<SinkRecord> transformationChain;\n    private final SinkTaskMetricsGroup sinkTaskMetricsGroup;\n    private final boolean isTopicTrackingEnabled;\n    private final Consumer<byte[], byte[]> consumer;\n    private WorkerSinkTaskContext context;\n    private final List<SinkRecord> messageBatch;\n    private final Map<TopicPartition, OffsetAndMetadata> lastCommittedOffsets;\n    private final Map<TopicPartition, OffsetAndMetadata> currentOffsets;\n    private final Map<TopicPartition, OffsetAndMetadata> origOffsets;\n    private RuntimeException rebalanceException;\n    private long nextCommit;\n    private int commitSeqno;\n    private long commitStarted;\n    private int commitFailures;\n    private boolean pausedForRedelivery;\n    private boolean committing;\n    private boolean taskStopped;\n    private final WorkerErrantRecordReporter workerErrantRecordReporter;\n    private final Supplier<List<ErrorReporter>> errorReportersSupplier;\n\n    public WorkerSinkTask(ConnectorTaskId id,\n                          SinkTask task,\n                          TaskStatus.Listener statusListener,\n                          TargetState initialState,\n                          WorkerConfig workerConfig,\n                          ClusterConfigState configState,\n                          ConnectMetrics connectMetrics,\n                          Converter keyConverter,\n                          Converter valueConverter,\n                          ErrorHandlingMetrics errorMetrics,\n                          HeaderConverter headerConverter,\n                          TransformationChain<SinkRecord> transformationChain,\n                          Consumer<byte[], byte[]> consumer,\n                          ClassLoader loader,\n                          Time time,\n                          RetryWithToleranceOperator retryWithToleranceOperator,\n                          WorkerErrantRecordReporter workerErrantRecordReporter,\n                          StatusBackingStore statusBackingStore,\n                          Supplier<List<ErrorReporter>> errorReportersSupplier) {\n        super(id, statusListener, initialState, loader, connectMetrics, errorMetrics,\n                retryWithToleranceOperator, time, statusBackingStore);\n\n        this.workerConfig = workerConfig;\n        this.task = task;\n        this.configState = configState;\n        this.keyConverter = keyConverter;\n        this.valueConverter = valueConverter;\n        this.headerConverter = headerConverter;\n        this.transformationChain = transformationChain;\n        this.messageBatch = new ArrayList<>();\n        this.lastCommittedOffsets = new HashMap<>();\n        this.currentOffsets = new HashMap<>();\n        this.origOffsets = new HashMap<>();\n        this.pausedForRedelivery = false;\n        this.rebalanceException = null;\n        this.nextCommit = time.milliseconds() +\n                workerConfig.getLong(WorkerConfig.OFFSET_COMMIT_INTERVAL_MS_CONFIG);\n        this.committing = false;\n        this.commitSeqno = 0;\n        this.commitStarted = -1;\n        this.commitFailures = 0;\n        this.sinkTaskMetricsGroup = new SinkTaskMetricsGroup(id, connectMetrics);\n        this.sinkTaskMetricsGroup.recordOffsetSequenceNumber(commitSeqno);\n        this.consumer = consumer;\n        this.isTopicTrackingEnabled = workerConfig.getBoolean(TOPIC_TRACKING_ENABLE_CONFIG);\n        this.taskStopped = false;\n        this.workerErrantRecordReporter = workerErrantRecordReporter;\n        this.errorReportersSupplier = errorReportersSupplier;\n    }\n\n    @Override\n    public void initialize(TaskConfig taskConfig) {\n        try {\n            this.taskConfig = taskConfig.originalsStrings();\n            this.context = new WorkerSinkTaskContext(consumer, this, configState);\n        } catch (Throwable t) {\n            log.error(\"{} Task failed initialization and will not be started.\", this, t);\n            onFailure(t);\n        }\n    }\n\n    @Override\n    public void stop() {\n        // Offset commit is handled upon exit in work thread\n        super.stop();\n        consumer.wakeup();\n    }\n\n    @Override\n    protected void close() {\n        // FIXME Kafka needs to add a timeout parameter here for us to properly obey the timeout\n        // passed in\n        try {\n            task.stop();\n        } catch (Throwable t) {\n            log.warn(\"Could not stop task\", t);\n        }\n        taskStopped = true;\n        Utils.closeQuietly(consumer, \"consumer\");\n        Utils.closeQuietly(transformationChain, \"transformation chain\");\n        Utils.closeQuietly(retryWithToleranceOperator, \"retry operator\");\n        Utils.closeQuietly(headerConverter, \"header converter\");\n        /*\n            Setting partition count explicitly to 0 to handle the case,\n            when the task fails, which would cause its consumer to leave the group.\n            This would cause onPartitionsRevoked to be invoked in the rebalance listener, but not onPartitionsAssigned,\n            so the metrics for the task (which are still available for failed tasks until they are explicitly revoked\n            from the worker) would become inaccurate.\n        */\n        sinkTaskMetricsGroup.recordPartitionCount(0);\n    }\n\n    @Override\n    public void removeMetrics() {\n        try {\n            sinkTaskMetricsGroup.close();\n        } finally {\n            super.removeMetrics();\n        }\n    }\n\n    @Override\n    public void transitionTo(TargetState state) {\n        super.transitionTo(state);\n        consumer.wakeup();\n    }\n\n    @Override\n    public void execute() {\n        log.info(\"{} Executing sink task\", this);\n        // Make sure any uncommitted data has been committed and the task has\n        // a chance to clean up its state\n        try (UncheckedCloseable suppressible = this::closeAllPartitions) {\n            while (!isStopping())\n                iteration();\n        } catch (WakeupException e) {\n            log.trace(\"Consumer woken up during initial offset commit attempt, \" \n                + \"but succeeded during a later attempt\");\n        }\n    }\n\n    protected void iteration() {\n        final long offsetCommitIntervalMs = workerConfig.getLong(WorkerConfig.OFFSET_COMMIT_INTERVAL_MS_CONFIG);\n\n        try {\n            long now = time.milliseconds();\n\n            // Maybe commit\n            if (!committing && (context.isCommitRequested() || now >= nextCommit)) {\n                commitOffsets(now, false);\n                nextCommit = now + offsetCommitIntervalMs;\n                context.clearCommitRequest();\n            }\n\n            final long commitTimeoutMs = commitStarted + workerConfig.getLong(WorkerConfig.OFFSET_COMMIT_TIMEOUT_MS_CONFIG);\n\n            // Check for timed out commits\n            if (committing && now >= commitTimeoutMs) {\n                log.warn(\"{} Commit of offsets timed out\", this);\n                commitFailures++;\n                committing = false;\n            }\n\n            // And process messages\n            long timeoutMs = Math.max(nextCommit - now, 0);\n            poll(timeoutMs);\n        } catch (WakeupException we) {\n            log.trace(\"{} Consumer woken up\", this);\n\n            if (isStopping())\n                return;\n\n            if (shouldPause()) {\n                pauseAll();\n                onPause();\n                context.requestCommit();\n            } else if (!pausedForRedelivery) {\n                resumeAll();\n                onResume();\n            }\n        }\n    }\n\n    /**\n     * Respond to a previous commit attempt that may or may not have succeeded. Note that due to our use of async commits,\n     * these invocations may come out of order and thus the need for the commit sequence number.\n     *\n     * @param error            the error resulting from the commit, or null if the commit succeeded without error\n     * @param seqno            the sequence number at the time the commit was requested\n     * @param committedOffsets the offsets that were committed; may be null if the commit did not complete successfully\n     *                         or if no new offsets were committed\n     */\n    private void onCommitCompleted(Throwable error, long seqno, Map<TopicPartition, OffsetAndMetadata> committedOffsets) {\n        if (commitSeqno != seqno) {\n            log.debug(\"{} Received out of order commit callback for sequence number {}, but most recent sequence number is {}\",\n                    this, seqno, commitSeqno);\n            sinkTaskMetricsGroup.recordOffsetCommitSkip();\n        } else {\n            long durationMillis = time.milliseconds() - commitStarted;\n            if (error != null) {\n                log.error(\"{} Commit of offsets threw an unexpected exception for sequence number {}: {}\",\n                        this, seqno, committedOffsets, error);\n                commitFailures++;\n                recordCommitFailure(durationMillis, error);\n            } else {\n                log.debug(\"{} Finished offset commit successfully in {} ms for sequence number {}: {}\",\n                        this, durationMillis, seqno, committedOffsets);\n                if (committedOffsets != null) {\n                    log.trace(\"{} Adding to last committed offsets: {}\", this, committedOffsets);\n                    lastCommittedOffsets.putAll(committedOffsets);\n                    log.debug(\"{} Last committed offsets are now {}\", this, committedOffsets);\n                    sinkTaskMetricsGroup.recordCommittedOffsets(committedOffsets);\n                }\n                commitFailures = 0;\n                recordCommitSuccess(durationMillis);\n            }\n            committing = false;\n        }\n    }\n\n    public int commitFailures() {\n        return commitFailures;\n    }\n\n    /**\n     * Initializes and starts the SinkTask.\n     */\n    @Override\n    protected void initializeAndStart() {\n        SinkConnectorConfig.validate(taskConfig);\n        retryWithToleranceOperator.reporters(errorReportersSupplier.get());\n\n        if (SinkConnectorConfig.hasTopicsConfig(taskConfig)) {\n            List<String> topics = SinkConnectorConfig.parseTopicsList(taskConfig);\n            consumer.subscribe(topics, new HandleRebalance());\n            log.debug(\"{} Initializing and starting task for topics {}\", this, Utils.join(topics, \", \"));\n        } else {\n            String topicsRegexStr = taskConfig.get(SinkTask.TOPICS_REGEX_CONFIG);\n            Pattern pattern = Pattern.compile(topicsRegexStr);\n            consumer.subscribe(pattern, new HandleRebalance());\n            log.debug(\"{} Initializing and starting task for topics regex {}\", this, topicsRegexStr);\n        }\n\n        task.initialize(context);\n        task.start(taskConfig);\n        log.info(\"{} Sink task finished initialization and start\", this);\n    }\n\n    /**\n     * Poll for new messages with the given timeout. Should only be invoked by the worker thread.\n     */\n    protected void poll(long timeoutMs) {\n        rewind();\n        long retryTimeout = context.timeout();\n        if (retryTimeout > 0) {\n            timeoutMs = Math.min(timeoutMs, retryTimeout);\n            context.timeout(-1L);\n        }\n\n        log.trace(\"{} Polling consumer with timeout {} ms\", this, timeoutMs);\n        ConsumerRecords<byte[], byte[]> msgs = pollConsumer(timeoutMs);\n        assert messageBatch.isEmpty() || msgs.isEmpty();\n        log.trace(\"{} Polling returned {} messages\", this, msgs.count());\n\n        convertMessages(msgs);\n        deliverMessages();\n    }\n\n    // Visible for testing\n    boolean isCommitting() {\n        return committing;\n    }\n\n    private void doCommitSync(Map<TopicPartition, OffsetAndMetadata> offsets, int seqno) {\n        log.debug(\"{} Committing offsets synchronously using sequence number {}: {}\", this, seqno, offsets);\n        try {\n            consumer.commitSync(offsets);\n            onCommitCompleted(null, seqno, offsets);\n        } catch (WakeupException e) {\n            // retry the commit to ensure offsets get pushed, then propagate the wakeup up to poll\n            doCommitSync(offsets, seqno);\n            throw e;\n        } catch (KafkaException e) {\n            onCommitCompleted(e, seqno, offsets);\n        }\n    }\n\n    private void doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno) {\n        log.debug(\"{} Committing offsets asynchronously using sequence number {}: {}\", this, seqno, offsets);\n        OffsetCommitCallback cb = (tpOffsets, error) -> onCommitCompleted(error, seqno, tpOffsets);\n        consumer.commitAsync(offsets, cb);\n    }\n\n    /**\n     * Starts an offset commit by flushing outstanding messages from the task and then starting\n     * the write commit.\n     */\n    private void doCommit(Map<TopicPartition, OffsetAndMetadata> offsets, boolean closing, int seqno) {\n        if (isCancelled()) {\n            log.debug(\"Skipping final offset commit as task has been cancelled\");\n            return;\n        }\n        if (closing) {\n            doCommitSync(offsets, seqno);\n        } else {\n            doCommitAsync(offsets, seqno);\n        }\n    }\n\n    private void commitOffsets(long now, boolean closing) {\n        commitOffsets(now, closing, consumer.assignment());\n    }\n\n    private void commitOffsets(long now, boolean closing, Collection<TopicPartition> topicPartitions) {\n        log.trace(\"Committing offsets for partitions {}\", topicPartitions);\n        if (workerErrantRecordReporter != null) {\n            log.trace(\"Awaiting reported errors to be completed\");\n            workerErrantRecordReporter.awaitFutures(topicPartitions);\n            log.trace(\"Completed reported errors\");\n        }\n\n        Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = currentOffsets.entrySet().stream()\n            .filter(e -> topicPartitions.contains(e.getKey()))\n            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n        if (offsetsToCommit.isEmpty())\n            return;\n\n        committing = true;\n        commitSeqno += 1;\n        commitStarted = now;\n        sinkTaskMetricsGroup.recordOffsetSequenceNumber(commitSeqno);\n\n        Map<TopicPartition, OffsetAndMetadata> lastCommittedOffsetsForPartitions = this.lastCommittedOffsets.entrySet().stream()\n            .filter(e -> offsetsToCommit.containsKey(e.getKey()))\n            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n        final Map<TopicPartition, OffsetAndMetadata> taskProvidedOffsets;\n        try {\n            log.trace(\"{} Calling task.preCommit with current offsets: {}\", this, offsetsToCommit);\n            taskProvidedOffsets = task.preCommit(new HashMap<>(offsetsToCommit));\n        } catch (Throwable t) {\n            if (closing) {\n                log.warn(\"{} Offset commit failed during close\", this);\n            } else {\n                log.error(\"{} Offset commit failed, rewinding to last committed offsets\", this, t);\n                for (Map.Entry<TopicPartition, OffsetAndMetadata> entry : lastCommittedOffsetsForPartitions.entrySet()) {\n                    log.debug(\"{} Rewinding topic partition {} to offset {}\", this, entry.getKey(), entry.getValue().offset());\n                    consumer.seek(entry.getKey(), entry.getValue().offset());\n                }\n                currentOffsets.putAll(lastCommittedOffsetsForPartitions);\n            }\n            onCommitCompleted(t, commitSeqno, null);\n            return;\n        } finally {\n            if (closing) {\n                log.trace(\"{} Closing the task before committing the offsets: {}\", this, offsetsToCommit);\n                task.close(topicPartitions);\n            }\n        }\n\n        if (taskProvidedOffsets.isEmpty()) {\n            log.debug(\"{} Skipping offset commit, task opted-out by returning no offsets from preCommit\", this);\n            onCommitCompleted(null, commitSeqno, null);\n            return;\n        }\n\n        Collection<TopicPartition> allAssignedTopicPartitions = consumer.assignment();\n        final Map<TopicPartition, OffsetAndMetadata> committableOffsets = new HashMap<>(lastCommittedOffsetsForPartitions);\n        for (Map.Entry<TopicPartition, OffsetAndMetadata> taskProvidedOffsetEntry : taskProvidedOffsets.entrySet()) {\n            final TopicPartition partition = taskProvidedOffsetEntry.getKey();\n            final OffsetAndMetadata taskProvidedOffset = taskProvidedOffsetEntry.getValue();\n            if (committableOffsets.containsKey(partition)) {\n                long taskOffset = taskProvidedOffset.offset();\n                long currentOffset = offsetsToCommit.get(partition).offset();\n                if (taskOffset <= currentOffset) {\n                    committableOffsets.put(partition, taskProvidedOffset);\n                } else {\n                    log.warn(\"{} Ignoring invalid task provided offset {}/{} -- not yet consumed, taskOffset={} currentOffset={}\",\n                        this, partition, taskProvidedOffset, taskOffset, currentOffset);\n                }\n            } else if (!allAssignedTopicPartitions.contains(partition)) {\n                log.warn(\"{} Ignoring invalid task provided offset {}/{} -- partition not assigned, assignment={}\",\n                        this, partition, taskProvidedOffset, allAssignedTopicPartitions);\n            } else {\n                log.debug(\"{} Ignoring task provided offset {}/{} -- partition not requested, requested={}\",\n                        this, partition, taskProvidedOffset, committableOffsets.keySet());\n            }\n        }\n\n        if (committableOffsets.equals(lastCommittedOffsetsForPartitions)) {\n            log.debug(\"{} Skipping offset commit, no change since last commit\", this);\n            onCommitCompleted(null, commitSeqno, null);\n            return;\n        }\n\n        doCommit(committableOffsets, closing, commitSeqno);\n    }\n\n\n    @Override\n    public String toString() {\n        return \"WorkerSinkTask{\" +\n                \"id=\" + id +\n                '}';\n    }\n\n    private ConsumerRecords<byte[], byte[]> pollConsumer(long timeoutMs) {\n        ConsumerRecords<byte[], byte[]> msgs = consumer.poll(Duration.ofMillis(timeoutMs));\n\n        // Exceptions raised from the task during a rebalance should be rethrown to stop the task and mark it as failed\n        if (rebalanceException != null) {\n            RuntimeException e = rebalanceException;\n            rebalanceException = null;\n            throw e;\n        }\n\n        sinkTaskMetricsGroup.recordRead(msgs.count());\n        return msgs;\n    }\n\n    private void convertMessages(ConsumerRecords<byte[], byte[]> msgs) {\n        for (ConsumerRecord<byte[], byte[]> msg : msgs) {\n            log.trace(\"{} Consuming and converting message in topic '{}' partition {} at offset {} and timestamp {}\",\n                    this, msg.topic(), msg.partition(), msg.offset(), msg.timestamp());\n\n            retryWithToleranceOperator.consumerRecord(msg);\n\n            SinkRecord transRecord = convertAndTransformRecord(msg);\n\n            origOffsets.put(\n                    new TopicPartition(msg.topic(), msg.partition()),\n                    new OffsetAndMetadata(msg.offset() + 1)\n            );\n            if (transRecord != null) {\n                messageBatch.add(transRecord);\n            } else {\n                log.trace(\n                        \"{} Converters and transformations returned null, possibly because of too many retries, so \" +\n                                \"dropping record in topic '{}' partition {} at offset {}\",\n                        this, msg.topic(), msg.partition(), msg.offset()\n                );\n            }\n        }\n        sinkTaskMetricsGroup.recordConsumedOffsets(origOffsets);\n    }\n\n    private SinkRecord convertAndTransformRecord(final ConsumerRecord<byte[], byte[]> msg) {\n        SchemaAndValue keyAndSchema = retryWithToleranceOperator.execute(() -> keyConverter.toConnectData(msg.topic(), msg.headers(), msg.key()),\n                Stage.KEY_CONVERTER, keyConverter.getClass());\n\n        SchemaAndValue valueAndSchema = retryWithToleranceOperator.execute(() -> valueConverter.toConnectData(msg.topic(), msg.headers(), msg.value()),\n                Stage.VALUE_CONVERTER, valueConverter.getClass());\n\n        Headers headers = retryWithToleranceOperator.execute(() -> convertHeadersFor(msg), Stage.HEADER_CONVERTER, headerConverter.getClass());\n\n        if (retryWithToleranceOperator.failed()) {\n            return null;\n        }\n\n        Long timestamp = ConnectUtils.checkAndConvertTimestamp(msg.timestamp());\n        SinkRecord origRecord = new SinkRecord(msg.topic(), msg.partition(),\n                keyAndSchema.schema(), keyAndSchema.value(),\n                valueAndSchema.schema(), valueAndSchema.value(),\n                msg.offset(),\n                timestamp,\n                msg.timestampType(),\n                headers);\n        log.trace(\"{} Applying transformations to record in topic '{}' partition {} at offset {} and timestamp {} with key {} and value {}\",\n                this, msg.topic(), msg.partition(), msg.offset(), timestamp, keyAndSchema.value(), valueAndSchema.value());\n        if (isTopicTrackingEnabled) {\n            recordActiveTopic(origRecord.topic());\n        }\n\n        // Apply the transformations\n        SinkRecord transformedRecord = transformationChain.apply(origRecord);\n        if (transformedRecord == null) {\n            return null;\n        }\n        // Error reporting will need to correlate each sink record with the original consumer record\n        return new InternalSinkRecord(msg, transformedRecord);\n    }\n\n    private Headers convertHeadersFor(ConsumerRecord<byte[], byte[]> record) {\n        Headers result = new ConnectHeaders();\n        org.apache.kafka.common.header.Headers recordHeaders = record.headers();\n        if (recordHeaders != null) {\n            String topic = record.topic();\n            for (org.apache.kafka.common.header.Header recordHeader : recordHeaders) {\n                SchemaAndValue schemaAndValue = headerConverter.toConnectHeader(topic, recordHeader.key(), recordHeader.value());\n                result.add(recordHeader.key(), schemaAndValue);\n            }\n        }\n        return result;\n    }\n\n    protected WorkerErrantRecordReporter workerErrantRecordReporter() {\n        return workerErrantRecordReporter;\n    }\n\n    private void resumeAll() {\n        for (TopicPartition tp : consumer.assignment())\n            if (!context.pausedPartitions().contains(tp))\n                consumer.resume(singleton(tp));\n    }\n\n    private void pauseAll() {\n        consumer.pause(consumer.assignment());\n    }\n\n    private void deliverMessages() {\n        // Finally, deliver this batch to the sink\n        try {\n            // Since we reuse the messageBatch buffer, ensure we give the task its own copy\n            log.trace(\"{} Delivering batch of {} messages to task\", this, messageBatch.size());\n            long start = time.milliseconds();\n            task.put(new ArrayList<>(messageBatch));\n            // if errors raised from the operator were swallowed by the task implementation, an\n            // exception needs to be thrown to kill the task indicating the tolerance was exceeded\n            maybeThrowAsyncError();\n            recordBatch(messageBatch.size());\n            sinkTaskMetricsGroup.recordPut(time.milliseconds() - start);\n            currentOffsets.putAll(origOffsets);\n            origOffsets.clear();\n            messageBatch.clear();\n            // If we had paused all consumer topic partitions to try to redeliver data, then we should resume any that\n            // the task had not explicitly paused\n            if (pausedForRedelivery) {\n                if (!shouldPause())\n                    resumeAll();\n                pausedForRedelivery = false;\n            }\n        } catch (RetriableException e) {\n            log.error(\"{} RetriableException from SinkTask:\", this, e);\n            if (!pausedForRedelivery) {\n                // If we're retrying a previous batch, make sure we've paused all topic partitions so we don't get new data,\n                // but will still be able to poll in order to handle user-requested timeouts, keep group membership, etc.\n                pausedForRedelivery = true;\n                pauseAll();\n            }\n            // Let this exit normally, the batch will be reprocessed on the next loop.\n        } catch (Throwable t) {\n            log.error(\"{} Task threw an uncaught and unrecoverable exception. Task is being killed and will not \"\n                    + \"recover until manually restarted. Error: {}\", this, t.getMessage(), t);\n            throw new ConnectException(\"Exiting WorkerSinkTask due to unrecoverable exception.\", t);\n        }\n    }\n\n    private void maybeThrowAsyncError() {\n        RetryWithToleranceOperator retryWithToleranceOperator = getRetryWithToleranceOperator();\n        if (workerErrantRecordReporter.getTaskPutException().get()!=null\n                && retryWithToleranceOperator.failed() && !retryWithToleranceOperator.withinToleranceLimits()) {\n            throw new ConnectException(\"Tolerance exceeded in error handler\",\n                workerErrantRecordReporter.getTaskPutException().get());\n        }\n    }\n\n    private void rewind() {\n        Map<TopicPartition, Long> offsets = context.offsets();\n        if (offsets.isEmpty()) {\n            return;\n        }\n        for (Map.Entry<TopicPartition, Long> entry: offsets.entrySet()) {\n            TopicPartition tp = entry.getKey();\n            Long offset = entry.getValue();\n            if (offset != null) {\n                log.trace(\"{} Rewind {} to offset {}\", this, tp, offset);\n                consumer.seek(tp, offset);\n                lastCommittedOffsets.put(tp, new OffsetAndMetadata(offset));\n                currentOffsets.put(tp, new OffsetAndMetadata(offset));\n            } else {\n                log.warn(\"{} Cannot rewind {} to null offset\", this, tp);\n            }\n        }\n        context.clearOffsets();\n    }\n\n    private void openPartitions(Collection<TopicPartition> partitions) {\n        task.open(partitions);\n    }\n\n    private void closeAllPartitions() {\n        closePartitions(currentOffsets.keySet(), false);\n    }\n\n    private void closePartitions(Collection<TopicPartition> topicPartitions, boolean lost) {\n        if (!lost) {\n            commitOffsets(time.milliseconds(), true, topicPartitions);\n        } else {\n            log.trace(\"{} Closing the task as partitions have been lost: {}\", this, topicPartitions);\n            task.close(topicPartitions);\n            if (workerErrantRecordReporter != null) {\n                log.trace(\"Cancelling reported errors for {}\", topicPartitions);\n                workerErrantRecordReporter.cancelFutures(topicPartitions);\n                log.trace(\"Cancelled all reported errors for {}\", topicPartitions);\n            }\n            origOffsets.keySet().removeAll(topicPartitions);\n            currentOffsets.keySet().removeAll(topicPartitions);\n        }\n        lastCommittedOffsets.keySet().removeAll(topicPartitions);\n    }\n\n    private void updatePartitionCount() {\n        sinkTaskMetricsGroup.recordPartitionCount(consumer.assignment().size());\n    }\n\n    @Override\n    protected void recordBatch(int size) {\n        super.recordBatch(size);\n        sinkTaskMetricsGroup.recordSend(size);\n    }\n\n    @Override\n    protected void recordCommitSuccess(long duration) {\n        super.recordCommitSuccess(duration);\n        sinkTaskMetricsGroup.recordOffsetCommitSuccess();\n    }\n\n    SinkTaskMetricsGroup sinkTaskMetricsGroup() {\n        return sinkTaskMetricsGroup;\n    }\n\n    // Visible for testing\n    long getNextCommit() {\n        return nextCommit;\n    }\n\n    private class HandleRebalance implements ConsumerRebalanceListener {\n        @Override\n        public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n            log.debug(\"{} Partitions assigned {}\", WorkerSinkTask.this, partitions);\n\n            for (TopicPartition tp : partitions) {\n                long pos = consumer.position(tp);\n                lastCommittedOffsets.put(tp, new OffsetAndMetadata(pos));\n                currentOffsets.put(tp, new OffsetAndMetadata(pos));\n                log.debug(\"{} Assigned topic partition {} with offset {}\", WorkerSinkTask.this, tp, pos);\n            }\n            sinkTaskMetricsGroup.assignedOffsets(currentOffsets);\n\n            boolean wasPausedForRedelivery = pausedForRedelivery;\n            pausedForRedelivery = wasPausedForRedelivery && !messageBatch.isEmpty();\n            if (pausedForRedelivery) {\n                // Re-pause here in case we picked up new partitions in the rebalance\n                pauseAll();\n            } else {\n                // If we paused everything for redelivery and all partitions for the failed deliveries have been revoked, make\n                // sure anything we paused that the task didn't request to be paused *and* which we still own is resumed.\n                // Also make sure our tracking of paused partitions is updated to remove any partitions we no longer own.\n                if (wasPausedForRedelivery) {\n                    resumeAll();\n                }\n                // Ensure that the paused partitions contains only assigned partitions and repause as necessary\n                context.pausedPartitions().retainAll(consumer.assignment());\n                if (shouldPause())\n                    pauseAll();\n                else if (!context.pausedPartitions().isEmpty())\n                    consumer.pause(context.pausedPartitions());\n            }\n            updatePartitionCount();\n            if (partitions.isEmpty()) {\n                return;\n            }\n\n            // Instead of invoking the assignment callback on initialization, we guarantee the consumer is ready upon\n            // task start. Since this callback gets invoked during that initial setup before we've started the task, we\n            // need to guard against invoking the user's callback method during that period.\n            if (rebalanceException == null || rebalanceException instanceof WakeupException) {\n                try {\n                    openPartitions(partitions);\n                    // Rewind should be applied only if openPartitions succeeds.\n                    rewind();\n                } catch (RuntimeException e) {\n                    // The consumer swallows exceptions raised in the rebalance listener, so we need to store\n                    // exceptions and rethrow when poll() returns.\n                    rebalanceException = e;\n                }\n            }\n        }\n\n        @Override\n        public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n            onPartitionsRemoved(partitions, false);\n        }\n\n        @Override\n        public void onPartitionsLost(Collection<TopicPartition> partitions) {\n            onPartitionsRemoved(partitions, true);\n        }\n\n        private void onPartitionsRemoved(Collection<TopicPartition> partitions, boolean lost) {\n            if (taskStopped) {\n                log.trace(\"Skipping partition revocation callback as task has already been stopped\");\n                return;\n            }\n            log.debug(\"{} Partitions {}: {}\", WorkerSinkTask.this, lost ? \"lost\" : \"revoked\", partitions);\n\n            if (partitions.isEmpty())\n                return;\n\n            try {\n                closePartitions(partitions, lost);\n                sinkTaskMetricsGroup.clearOffsets(partitions);\n            } catch (RuntimeException e) {\n                // The consumer swallows exceptions raised in the rebalance listener, so we need to store\n                // exceptions and rethrow when poll() returns.\n                rebalanceException = e;\n            }\n\n            // Make sure we don't have any leftover data since offsets for these partitions will be reset to committed positions\n            messageBatch.removeIf(record -> partitions.contains(new TopicPartition(record.topic(), record.kafkaPartition())));\n        }\n    }\n\n    static class SinkTaskMetricsGroup {\n        private final ConnectorTaskId id;\n        private final ConnectMetrics metrics;\n        private final MetricGroup metricGroup;\n        private final Sensor sinkRecordRead;\n        private final Sensor sinkRecordSend;\n        private final Sensor partitionCount;\n        private final Sensor offsetSeqNum;\n        private final Sensor offsetCompletion;\n        private final Sensor offsetCompletionSkip;\n        private final Sensor putBatchTime;\n        private final Sensor sinkRecordActiveCount;\n        private long activeRecords;\n        private Map<TopicPartition, OffsetAndMetadata> consumedOffsets = new HashMap<>();\n        private Map<TopicPartition, OffsetAndMetadata> committedOffsets = new HashMap<>();\n\n        public SinkTaskMetricsGroup(ConnectorTaskId id, ConnectMetrics connectMetrics) {\n            this.metrics = connectMetrics;\n            this.id = id;\n\n            ConnectMetricsRegistry registry = connectMetrics.registry();\n            metricGroup = connectMetrics\n                                  .group(registry.sinkTaskGroupName(), registry.connectorTagName(), id.connector(), registry.taskTagName(),\n                                         Integer.toString(id.task()));\n            // prevent collisions by removing any previously created metrics in this group.\n            metricGroup.close();\n\n            sinkRecordRead = metricGroup.sensor(\"sink-record-read\");\n            sinkRecordRead.add(metricGroup.metricName(registry.sinkRecordReadRate), new Rate());\n            sinkRecordRead.add(metricGroup.metricName(registry.sinkRecordReadTotal), new CumulativeSum());\n\n            sinkRecordSend = metricGroup.sensor(\"sink-record-send\");\n            sinkRecordSend.add(metricGroup.metricName(registry.sinkRecordSendRate), new Rate());\n            sinkRecordSend.add(metricGroup.metricName(registry.sinkRecordSendTotal), new CumulativeSum());\n\n            sinkRecordActiveCount = metricGroup.sensor(\"sink-record-active-count\");\n            sinkRecordActiveCount.add(metricGroup.metricName(registry.sinkRecordActiveCount), new Value());\n            sinkRecordActiveCount.add(metricGroup.metricName(registry.sinkRecordActiveCountMax), new Max());\n            sinkRecordActiveCount.add(metricGroup.metricName(registry.sinkRecordActiveCountAvg), new Avg());\n\n            partitionCount = metricGroup.sensor(\"partition-count\");\n            partitionCount.add(metricGroup.metricName(registry.sinkRecordPartitionCount), new Value());\n\n            offsetSeqNum = metricGroup.sensor(\"offset-seq-number\");\n            offsetSeqNum.add(metricGroup.metricName(registry.sinkRecordOffsetCommitSeqNum), new Value());\n\n            offsetCompletion = metricGroup.sensor(\"offset-commit-completion\");\n            offsetCompletion.add(metricGroup.metricName(registry.sinkRecordOffsetCommitCompletionRate), new Rate());\n            offsetCompletion.add(metricGroup.metricName(registry.sinkRecordOffsetCommitCompletionTotal), new CumulativeSum());\n\n            offsetCompletionSkip = metricGroup.sensor(\"offset-commit-completion-skip\");\n            offsetCompletionSkip.add(metricGroup.metricName(registry.sinkRecordOffsetCommitSkipRate), new Rate());\n            offsetCompletionSkip.add(metricGroup.metricName(registry.sinkRecordOffsetCommitSkipTotal), new CumulativeSum());\n\n            putBatchTime = metricGroup.sensor(\"put-batch-time\");\n            putBatchTime.add(metricGroup.metricName(registry.sinkRecordPutBatchTimeMax), new Max());\n            putBatchTime.add(metricGroup.metricName(registry.sinkRecordPutBatchTimeAvg), new Avg());\n        }\n\n        void computeSinkRecordLag() {\n            Map<TopicPartition, OffsetAndMetadata> consumed = this.consumedOffsets;\n            Map<TopicPartition, OffsetAndMetadata> committed = this.committedOffsets;\n            activeRecords = 0L;\n            for (Map.Entry<TopicPartition, OffsetAndMetadata> committedOffsetEntry : committed.entrySet()) {\n                final TopicPartition partition = committedOffsetEntry.getKey();\n                final OffsetAndMetadata consumedOffsetMeta = consumed.get(partition);\n                if (consumedOffsetMeta != null) {\n                    final OffsetAndMetadata committedOffsetMeta = committedOffsetEntry.getValue();\n                    long consumedOffset = consumedOffsetMeta.offset();\n                    long committedOffset = committedOffsetMeta.offset();\n                    long diff = consumedOffset - committedOffset;\n                    // Connector tasks can return offsets, so make sure nothing wonky happens\n                    activeRecords += Math.max(diff, 0L);\n                }\n            }\n            sinkRecordActiveCount.record(activeRecords);\n        }\n\n        void close() {\n            metricGroup.close();\n        }\n\n        void recordRead(int batchSize) {\n            sinkRecordRead.record(batchSize);\n        }\n\n        void recordSend(int batchSize) {\n            sinkRecordSend.record(batchSize);\n        }\n\n        void recordPut(long duration) {\n            putBatchTime.record(duration);\n        }\n\n        void recordPartitionCount(int assignedPartitionCount) {\n            partitionCount.record(assignedPartitionCount);\n        }\n\n        void recordOffsetSequenceNumber(int seqNum) {\n            offsetSeqNum.record(seqNum);\n        }\n\n        void recordConsumedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets) {\n            consumedOffsets.putAll(offsets);\n            computeSinkRecordLag();\n        }\n\n        void recordCommittedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets) {\n            committedOffsets = offsets;\n            computeSinkRecordLag();\n        }\n\n        void assignedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets) {\n            consumedOffsets = new HashMap<>(offsets);\n            committedOffsets = offsets;\n            computeSinkRecordLag();\n        }\n\n        void clearOffsets(Collection<TopicPartition> topicPartitions) {\n            consumedOffsets.keySet().removeAll(topicPartitions);\n            committedOffsets.keySet().removeAll(topicPartitions);\n            computeSinkRecordLag();\n        }\n\n        void recordOffsetCommitSuccess() {\n            offsetCompletion.record(1.0);\n        }\n\n        void recordOffsetCommitSkip() {\n            offsetCompletionSkip.record(1.0);\n        }\n\n        protected MetricGroup metricGroup() {\n            return metricGroup;\n        }\n    }\n}",
                "methodCount": 56
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 15,
                "candidates": [
                    {
                        "lineStart": 371,
                        "lineEnd": 375,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method doCommitAsync to class SinkTaskMetricsGroup",
                        "description": "Move method doCommitAsync to org.apache.kafka.connect.runtime.WorkerSinkTask.SinkTaskMetricsGroup\nRationale: The method doCommitAsync() is explicitly dealing with offsets, which are directly mentioned in the context of SinkTaskMetricsGroup (e.g., in methods like recordCommittedOffsets and computeSinkRecordLag). The SinkTaskMetricsGroup class is concerned with metrics and keeping track of offsets, making it a natural fit. Furthermore, the usage of the sequence number within the method aligns with the offset tracking and metrics calculation responsibilities of this class. The Converter interface is more focused on data serialization/deserialization and thus does not align well with the asynchronous commitment of offsets.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 393,
                        "lineEnd": 395,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffsets to class SinkTaskMetricsGroup",
                        "description": "Move method commitOffsets to org.apache.kafka.connect.runtime.WorkerSinkTask.SinkTaskMetricsGroup\nRationale: The method commitOffsets deals with offset management which is highly relevant to metrics tracking of Sink Tasks. Given the context provided by the SinkTaskMetricsGroup class, which includes various sensors for offset tracking and lag computations, it is a more appropriate fit. The extensive use of offset-related methods and Map objects for consumed and committed offsets in the SinkTaskMetricsGroup indicates that this class is responsible for handling offset data, making it a suitable and logical place for the commitOffsets method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 892,
                        "lineEnd": 894,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method recordPartitionCount to class Sensor",
                        "description": "Move method recordPartitionCount to org.apache.kafka.common.metrics.Sensor\nRationale: The method 'recordPartitionCount' is responsible for recording a numerical value, which aligns with the core functionality of the 'Sensor' class that deals with recording and maintaining metrics. The Sensor class includes similar methods that record values (`record(double)`, `record(double, long)`), making it a natural fit for the `recordPartitionCount` method. Moreover, moving the method to the Sensor class would enhance cohesion as all recording functionalities will be consolidated within a single class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 896,
                        "lineEnd": 898,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method recordOffsetSequenceNumber to class Sensor",
                        "description": "Move method recordOffsetSequenceNumber to org.apache.kafka.common.metrics.Sensor\nRationale: The method recordOffsetSequenceNumber(int seqNum) appears to be recording a sequence number, which aligns with the responsibilities of the Sensor class. The Sensor class already has methods related to recording values (such as record(double) and record(double, long)), making it the most relevant target for handling sequence numbers as well. This ensures that all recording operations are centralized within the Sensor class, promoting cohesion and maintainability.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 584,
                        "lineEnd": 588,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method resumeAll to class SinkTaskMetricsGroup",
                        "description": "Move method resumeAll to org.apache.kafka.connect.runtime.WorkerSinkTask.SinkTaskMetricsGroup\nRationale: The method `resumeAll` interacts with consumer assignments and utilizes paused partitions, indicating it is concerned with managing consumer behavior. The `SinkTaskMetricsGroup` class already involves consumer metrics and active record management, making it a more appropriate place for this method. The `Converter` interface focuses on serializing and deserializing data, and does not align with the consumer-related operations that `resumeAll` performs.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 880,
                        "lineEnd": 882,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method recordRead to class Sensor",
                        "description": "Move method recordRead to org.apache.kafka.common.metrics.Sensor\nRationale: The recordRead(int batchSize) method is closely related to the recording operations that the Sensor class performs. The Sensor class contains multiple overloaded record methods designed to log occurrences and values, making it a natural fit for the batch recording functionality. Furthermore, integrating recordRead into Sensor centralizes all recording logic pertaining to metrics and data logging, ensuring consistency and coherence within the codebase. By moving recordRead to the Sensor class, it enhances maintainability and observability of the data flow and metrics within the system.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 884,
                        "lineEnd": 886,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method recordSend to class Sensor",
                        "description": "Move method recordSend to org.apache.kafka.common.metrics.Sensor\nRationale: The method `recordSend(int batchSize)` involves recording a batch size, which is a metric that falls within the domain of the `Sensor` class. The `Sensor` class is responsible for recording and maintaining a set of metrics, making it an appropriate home for this method. Placing `recordSend` in this class allows direct interaction with existing methods for recording metrics without unnecessary inter-class communication. Additionally, by moving it to `Sensor`, we maintain a single responsibility principle and enhance the cohesion of the class methods.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 888,
                        "lineEnd": 890,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method recordPut to class Sensor",
                        "description": "Move method recordPut to org.apache.kafka.common.metrics.Sensor\nRationale: The method recordPut is closely related to recording metrics based on its definition and usage of putBatchTime. The Sensor class is designed to handle the recording and measurement of various metrics using different recording methods. Given that the method involves recording a duration metric, it would naturally fit within the Sensor class, which already contains methods for recording values. Moving the method to the Sensor class will provide a centralized and cohesive handling of recording metrics and ensure consistency.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 922,
                        "lineEnd": 924,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method recordOffsetCommitSuccess to class Sensor",
                        "description": "Move method recordOffsetCommitSuccess to org.apache.kafka.common.metrics.Sensor\nRationale: The method 'recordOffsetCommitSuccess()' utilizes the 'record(double)' method to record numerical values related to offset commit success. The 'Sensor' class is responsible for recording various metrics using its 'record(double)' method and managing associated statistics. Therefore, 'Sensor' is a highly relevant class for this method, allowing it to fit seamlessly into the existing framework for recording metric values.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 926,
                        "lineEnd": 928,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method recordOffsetCommitSkip to class Sensor",
                        "description": "Move method recordOffsetCommitSkip to org.apache.kafka.common.metrics.Sensor\nRationale: The method `recordOffsetCommitSkip` involves recording a metric value which directly aligns with the purpose of the `Sensor` class. The `Sensor` class is designed to handle recording various types of metrics, as evidenced by its multiple `record` methods. Moving the method to `Sensor` ensures that metric recording functionality is centralized, making it more logical and organized. This improves maintainability by consolidating related functionalities in one place, avoiding code scattering, and enhancing cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 332,
                        "lineEnd": 350,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method poll to class SinkTaskMetricsGroup",
                        "description": "Move method poll to org.apache.kafka.connect.runtime.WorkerSinkTask.SinkTaskMetricsGroup\nRationale: The 'poll' method entails operations that cater to consumer polling, message conversion, and delivery mechanisms which are tightly coupled with task management aspects found in 'SinkTaskMetricsGroup'. This class already handles metrics related to Kafka Connect tasks, which includes reading and sending records. Relocating the 'poll' method here aligns consumer operations with task metrics and performance tracking, thus ensuring a coherent management of sink tasks and their associated activities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 876,
                        "lineEnd": 878,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method close to class MetricGroup",
                        "description": "Move method close to org.apache.kafka.connect.runtime.ConnectMetrics.MetricGroup\nRationale: The close() method directly interacts with and modifies the state of MetricGroup by calling close() on its metricGroup property. As MetricGroup already implements the close() method as part of AutoCloseable, it is logically consistent to encapsulate all close related functionalities within this class. Moving the method to MetricGroup maintains cohesion, ensuring that the closing logic remains centralized to a class whose purpose is to manage a group of metrics.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 661,
                        "lineEnd": 663,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method openPartitions to class SinkTask",
                        "description": "Move method openPartitions to org.apache.kafka.connect.sink.SinkTask\nRationale: The 'openPartitions' method interacts directly with the task, specifically calling 'task.open(partitions)'. It clearly performs an operation related to partition management, which aligns with the responsibilities of the SinkTask class. SinkTask already has methods like 'open(Collection<TopicPartition>)' that handles partition assignments, indicating that partition management is a core concern of this class. Therefore, it is logical and cohesive to move the 'openPartitions' method to the SinkTask class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 267,
                        "lineEnd": 302,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method onCommitCompleted to class Converter",
                        "description": "Move method onCommitCompleted to org.apache.kafka.connect.storage.Converter\nRationale: The `onCommitCompleted` method is primarily concerned with responding to commit attempts, which appears closely related to the commit process and data conversion typical in message-oriented middleware environments like Kafka. The `Converter` class deals with the translation of Kafka Connect's runtime data formats, which could naturally involve handling commit results and offsets. `Converter` also seems more relevant to the data processing, serialization and deserialization workflow, making it a more suitable class for this method rather than `HeaderConverter` which specifically deals with headers.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 505,
                        "lineEnd": 529,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method convertMessages to class Converter",
                        "description": "Move method convertMessages to org.apache.kafka.connect.storage.Converter\nRationale: The method `convertMessages` heavily revolves around converting consumer records into SinkRecords, which directly aligns with the core functionality of the Converter interface. Placing this method inside the Converter class will ensure it is positioned within a context explicitly designed for data transformation and serialization tasks. Additionally, the Converter class is designed to deal with the conversion logic between byte arrays and structured Kafka Connect data, making it the most appropriate home for the `convertMessages` method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [],
                    "llm_response_time": 7517
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "onCommitCompleted",
                            "method_signature": "private void onCommitCompleted(Throwable error, long seqno, Map<TopicPartition, OffsetAndMetadata> committedOffsets)",
                            "target_class": "ConsumerRebalanceListener",
                            "rationale": "The onCommitCompleted method directly deals with the completion and handling of commit attempts. As it is significant for consumer behavior during rebalances, it is more appropriate to be part of ConsumerRebalanceListener."
                        },
                        {
                            "method_name": "doCommitSync",
                            "method_signature": "private void doCommitSync(Map<TopicPartition, OffsetAndMetadata> offsets, int seqno)",
                            "target_class": "WorkerTaskCommitter",
                            "rationale": "Commit methods such as doCommitSync deal specifically with committing offsets and can be logically grouped into a separate utility or class responsible for committing, such as WorkerTaskCommitter."
                        },
                        {
                            "method_name": "doCommitAsync",
                            "method_signature": "private void doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)",
                            "target_class": "WorkerTaskCommitter",
                            "rationale": "Methods dealing with asynchronous commits, like doCommitAsync, are best grouped into a class specifically designed for commit operations, such as WorkerTaskCommitter."
                        },
                        {
                            "method_name": "doCommit",
                            "method_signature": "private void doCommit(Map<TopicPartition, OffsetAndMetadata> offsets, boolean closing, int seqno)",
                            "target_class": "WorkerTaskCommitter",
                            "rationale": "This method handles the commit operation logic and should be part of a dedicated Committer class to encapsulate this functionality and keep WorkerSinkTask cleaner."
                        },
                        {
                            "method_name": "commitOffsets",
                            "method_signature": "private void commitOffsets(long now, boolean closing)",
                            "target_class": "WorkerTaskCommitter",
                            "rationale": "Given that this method's main responsibility is committing offsets, it should be segregated into a class whose main concern is handling the commit operation."
                        },
                        {
                            "method_name": "commitOffsets",
                            "method_signature": "private void commitOffsets(long now, boolean closing, Collection<TopicPartition> topicPartitions)",
                            "target_class": "WorkerTaskCommitter",
                            "rationale": "Similar to the above suggestion, this overload of commitOffsets should also be moved to a commit-specific class to adhere to the Single Responsibility Principle."
                        }
                    ],
                    "llm_response_time": 6462
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "onCommitCompleted",
                            "method_signature": "private void onCommitCompleted(Throwable error, long seqno, Map<TopicPartition, OffsetAndMetadata> committedOffsets)",
                            "target_class": "CommitHandler",
                            "rationale": "The onCommitCompleted method seems to be responsible for handling and processing commit completion notifications. This logic appears to be more relevant to a class that specializes in offset commit handling."
                        },
                        {
                            "method_name": "commitOffsets",
                            "method_signature": "private void commitOffsets(long now, boolean closing)",
                            "target_class": "CommitHandler",
                            "rationale": "The commitOffsets method handles the task of committing offsets, which is a specific responsibility that can be encapsulated within a CommitHandler class."
                        },
                        {
                            "method_name": "doCommitSync",
                            "method_signature": "private void doCommitSync(Map<TopicPartition, OffsetAndMetadata> offsets, int seqno)",
                            "target_class": "CommitHandler",
                            "rationale": "This method performs synchronous commits and is a lower-level operation pertaining to offset management. It aligns better with a commit-specific class."
                        },
                        {
                            "method_name": "doCommitAsync",
                            "method_signature": "private void doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)",
                            "target_class": "CommitHandler",
                            "rationale": "The async commit functionality is also highly specialized and should reside in a class dedicated to handling offset commits."
                        },
                        {
                            "method_name": "doCommit",
                            "method_signature": "private void doCommit(Map<TopicPartition, OffsetAndMetadata> offsets, boolean closing, int seqno)",
                            "target_class": "CommitHandler",
                            "rationale": "The doCommit method encapsulates the logic for committing offsets, which should be housed within a class focused on commit operations."
                        },
                        {
                            "method_name": "poll",
                            "method_signature": "protected void poll(long timeoutMs)",
                            "target_class": "PollingHandler",
                            "rationale": "The poll method is responsible for polling messages from Kafka, a task that is distinct from the core functionality of WorkerSinkTask. It would be better suited in a PollingHandler class."
                        },
                        {
                            "method_name": "pollConsumer",
                            "method_signature": "private ConsumerRecords<byte[], byte[]> pollConsumer(long timeoutMs)",
                            "target_class": "PollingHandler",
                            "rationale": "This method is focused on polling the consumer, which aligns well with the responsibilities of a PollingHandler class."
                        },
                        {
                            "method_name": "convertMessages",
                            "method_signature": "private void convertMessages(ConsumerRecords<byte[], byte[]> msgs)",
                            "target_class": "MessageConverter",
                            "rationale": "The convertMessages method deals with message conversion, which is a specialized responsibility that warrants its own MessageConverter class."
                        },
                        {
                            "method_name": "convertAndTransformRecord",
                            "method_signature": "private SinkRecord convertAndTransformRecord(final ConsumerRecord<byte[], byte[]> msg)",
                            "target_class": "MessageConverter",
                            "rationale": "This method performs conversion and transformation of records, making it highly suitable for inclusion in a MessageConverter class."
                        },
                        {
                            "method_name": "convertHeadersFor",
                            "method_signature": "private Headers convertHeadersFor(ConsumerRecord<byte[], byte[]> record)",
                            "target_class": "MessageConverter",
                            "rationale": "Header conversion is a distinct responsibility that should be encapsulated within the MessageConverter class."
                        },
                        {
                            "method_name": "deliverMessages",
                            "method_signature": "protected void deliverMessages()",
                            "target_class": "MessageDeliveryHandler",
                            "rationale": "The deliverMessages method deals with delivering batches of messages to the sink task, an operation specialized enough to warrant a MessageDeliveryHandler class."
                        }
                    ],
                    "llm_response_time": 9760
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "iteration",
                            "method_signature": "protected iteration()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onCommitCompleted",
                            "method_signature": "private onCommitCompleted(Throwable error, long seqno, Map<TopicPartition, OffsetAndMetadata> committedOffsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "poll",
                            "method_signature": "protected poll(long timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doCommitSync",
                            "method_signature": "private doCommitSync(Map<TopicPartition, OffsetAndMetadata> offsets, int seqno)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doCommitAsync",
                            "method_signature": "private doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffsets",
                            "method_signature": "private commitOffsets(long now, boolean closing)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffsets",
                            "method_signature": "private commitOffsets(long now, boolean closing, Collection<TopicPartition> topicPartitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollConsumer",
                            "method_signature": "private pollConsumer(long timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "convertMessages",
                            "method_signature": "private convertMessages(ConsumerRecords<byte[], byte[]> msgs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "convertAndTransformRecord",
                            "method_signature": "private convertAndTransformRecord(final ConsumerRecord<byte[], byte[]> msg)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "convertHeadersFor",
                            "method_signature": "private convertHeadersFor(ConsumerRecord<byte[], byte[]> record)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resumeAll",
                            "method_signature": "private resumeAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pauseAll",
                            "method_signature": "private pauseAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deliverMessages",
                            "method_signature": "private deliverMessages()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeThrowAsyncError",
                            "method_signature": "private maybeThrowAsyncError()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rewind",
                            "method_signature": "private rewind()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "openPartitions",
                            "method_signature": "private openPartitions(Collection<TopicPartition> partitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closePartitions",
                            "method_signature": "private closePartitions(Collection<TopicPartition> topicPartitions, boolean lost)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updatePartitionCount",
                            "method_signature": "private updatePartitionCount()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onPartitionsRemoved",
                            "method_signature": "private onPartitionsRemoved(Collection<TopicPartition> partitions, boolean lost)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "computeSinkRecordLag",
                            "method_signature": " computeSinkRecordLag()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": " close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordRead",
                            "method_signature": " recordRead(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordSend",
                            "method_signature": " recordSend(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordPut",
                            "method_signature": " recordPut(long duration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordPartitionCount",
                            "method_signature": " recordPartitionCount(int assignedPartitionCount)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetSequenceNumber",
                            "method_signature": " recordOffsetSequenceNumber(int seqNum)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetCommitSuccess",
                            "method_signature": " recordOffsetCommitSuccess()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetCommitSkip",
                            "method_signature": " recordOffsetCommitSkip()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "doCommitAsync",
                            "method_signature": "private doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffsets",
                            "method_signature": "private commitOffsets(long now, boolean closing)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordPartitionCount",
                            "method_signature": " recordPartitionCount(int assignedPartitionCount)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetSequenceNumber",
                            "method_signature": " recordOffsetSequenceNumber(int seqNum)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resumeAll",
                            "method_signature": "private resumeAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordRead",
                            "method_signature": " recordRead(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordSend",
                            "method_signature": " recordSend(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordPut",
                            "method_signature": " recordPut(long duration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetCommitSuccess",
                            "method_signature": " recordOffsetCommitSuccess()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetCommitSkip",
                            "method_signature": " recordOffsetCommitSkip()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "poll",
                            "method_signature": "protected poll(long timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": " close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "openPartitions",
                            "method_signature": "private openPartitions(Collection<TopicPartition> partitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onCommitCompleted",
                            "method_signature": "private onCommitCompleted(Throwable error, long seqno, Map<TopicPartition, OffsetAndMetadata> committedOffsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "convertMessages",
                            "method_signature": "private convertMessages(ConsumerRecords<byte[], byte[]> msgs)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)": {
                        "first": {
                            "method_name": "doCommitAsync",
                            "method_signature": "private doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.406091538496737
                    },
                    "private commitOffsets(long now, boolean closing)": {
                        "first": {
                            "method_name": "commitOffsets",
                            "method_signature": "private commitOffsets(long now, boolean closing)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.42248189795497776
                    },
                    " recordPartitionCount(int assignedPartitionCount)": {
                        "first": {
                            "method_name": "recordPartitionCount",
                            "method_signature": " recordPartitionCount(int assignedPartitionCount)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.45845643569688205
                    },
                    " recordOffsetSequenceNumber(int seqNum)": {
                        "first": {
                            "method_name": "recordOffsetSequenceNumber",
                            "method_signature": " recordOffsetSequenceNumber(int seqNum)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.45845643569688205
                    },
                    "private resumeAll()": {
                        "first": {
                            "method_name": "resumeAll",
                            "method_signature": "private resumeAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4587345269423742
                    },
                    " recordRead(int batchSize)": {
                        "first": {
                            "method_name": "recordRead",
                            "method_signature": " recordRead(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4599084421995814
                    },
                    " recordSend(int batchSize)": {
                        "first": {
                            "method_name": "recordSend",
                            "method_signature": " recordSend(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4599084421995814
                    },
                    " recordPut(long duration)": {
                        "first": {
                            "method_name": "recordPut",
                            "method_signature": " recordPut(long duration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4599084421995814
                    },
                    " recordOffsetCommitSuccess()": {
                        "first": {
                            "method_name": "recordOffsetCommitSuccess",
                            "method_signature": " recordOffsetCommitSuccess()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.502210711674642
                    },
                    " recordOffsetCommitSkip()": {
                        "first": {
                            "method_name": "recordOffsetCommitSkip",
                            "method_signature": " recordOffsetCommitSkip()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.502210711674642
                    },
                    "protected poll(long timeoutMs)": {
                        "first": {
                            "method_name": "poll",
                            "method_signature": "protected poll(long timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5041224919749608
                    },
                    " close()": {
                        "first": {
                            "method_name": "close",
                            "method_signature": " close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5056242196043148
                    },
                    "private openPartitions(Collection<TopicPartition> partitions)": {
                        "first": {
                            "method_name": "openPartitions",
                            "method_signature": "private openPartitions(Collection<TopicPartition> partitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5119010990528992
                    },
                    "private onCommitCompleted(Throwable error, long seqno, Map<TopicPartition, OffsetAndMetadata> committedOffsets)": {
                        "first": {
                            "method_name": "onCommitCompleted",
                            "method_signature": "private onCommitCompleted(Throwable error, long seqno, Map<TopicPartition, OffsetAndMetadata> committedOffsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5214968635466863
                    },
                    "private convertMessages(ConsumerRecords<byte[], byte[]> msgs)": {
                        "first": {
                            "method_name": "convertMessages",
                            "method_signature": "private convertMessages(ConsumerRecords<byte[], byte[]> msgs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5278228191109035
                    }
                },
                "voyage": {
                    "private pauseAll()": {
                        "first": {
                            "method_name": "pauseAll",
                            "method_signature": "private pauseAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3034905923555618
                    },
                    " recordPut(long duration)": {
                        "first": {
                            "method_name": "recordPut",
                            "method_signature": " recordPut(long duration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3151296941709663
                    },
                    " recordOffsetSequenceNumber(int seqNum)": {
                        "first": {
                            "method_name": "recordOffsetSequenceNumber",
                            "method_signature": " recordOffsetSequenceNumber(int seqNum)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.32475407177362325
                    },
                    " close()": {
                        "first": {
                            "method_name": "close",
                            "method_signature": " close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.32507132793481397
                    },
                    "private convertHeadersFor(ConsumerRecord<byte[], byte[]> record)": {
                        "first": {
                            "method_name": "convertHeadersFor",
                            "method_signature": "private convertHeadersFor(ConsumerRecord<byte[], byte[]> record)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3260073327555455
                    },
                    "private resumeAll()": {
                        "first": {
                            "method_name": "resumeAll",
                            "method_signature": "private resumeAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.37652010904576017
                    },
                    "private commitOffsets(long now, boolean closing)": {
                        "first": {
                            "method_name": "commitOffsets",
                            "method_signature": "private commitOffsets(long now, boolean closing)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4042639834033087
                    },
                    " recordPartitionCount(int assignedPartitionCount)": {
                        "first": {
                            "method_name": "recordPartitionCount",
                            "method_signature": " recordPartitionCount(int assignedPartitionCount)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4091636092092239
                    },
                    "private maybeThrowAsyncError()": {
                        "first": {
                            "method_name": "maybeThrowAsyncError",
                            "method_signature": "private maybeThrowAsyncError()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.44100983712322883
                    },
                    " recordOffsetCommitSuccess()": {
                        "first": {
                            "method_name": "recordOffsetCommitSuccess",
                            "method_signature": " recordOffsetCommitSuccess()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4431344518004918
                    },
                    " recordRead(int batchSize)": {
                        "first": {
                            "method_name": "recordRead",
                            "method_signature": " recordRead(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.44724402058703494
                    },
                    " recordSend(int batchSize)": {
                        "first": {
                            "method_name": "recordSend",
                            "method_signature": " recordSend(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.45005512355090194
                    },
                    " recordOffsetCommitSkip()": {
                        "first": {
                            "method_name": "recordOffsetCommitSkip",
                            "method_signature": " recordOffsetCommitSkip()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.467222858335552
                    },
                    "private rewind()": {
                        "first": {
                            "method_name": "rewind",
                            "method_signature": "private rewind()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4674973948205494
                    },
                    "protected poll(long timeoutMs)": {
                        "first": {
                            "method_name": "poll",
                            "method_signature": "protected poll(long timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4792897723663226
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private onCommitCompleted(Throwable error, long seqno, Map<TopicPartition, OffsetAndMetadata> committedOffsets)",
                        "private commitOffsets(long now, boolean closing)",
                        "private doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)",
                        "protected poll(long timeoutMs)",
                        "private convertMessages(ConsumerRecords<byte[], byte[]> msgs)",
                        "private openPartitions(Collection<TopicPartition> partitions)",
                        "private resumeAll()",
                        " recordPartitionCount(int assignedPartitionCount)",
                        " recordOffsetSequenceNumber(int seqNum)",
                        " recordRead(int batchSize)",
                        " recordSend(int batchSize)",
                        " recordPut(long duration)",
                        " recordOffsetCommitSuccess()",
                        " recordOffsetCommitSkip()"
                    ],
                    "llm_response_time": 3930
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private commitOffsets(long now, boolean closing)",
                        "private doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)",
                        "private resumeAll()"
                    ],
                    "llm_response_time": 6170
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private commitOffsets(long now, boolean closing)",
                        "private doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)"
                    ],
                    "llm_response_time": 4154
                },
                "voyage": {
                    "priority_method_names": [],
                    "llm_response_time": 2544
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private convertHeadersFor(ConsumerRecord<byte[], byte[]> record)",
                        "private pauseAll()"
                    ],
                    "llm_response_time": 4015
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private pauseAll()"
                    ],
                    "llm_response_time": 3218
                }
            },
            "targetClassMap": {
                "doCommitAsync": {
                    "target_classes": [
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.028617427596496936
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.028617427596496936
                        },
                        {
                            "class_name": "HeaderConverter",
                            "similarity_score": 0.016359952408967662
                        },
                        {
                            "class_name": "SinkTaskMetricsGroup",
                            "similarity_score": 0.3670651741928988
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SinkTaskMetricsGroup",
                        "Converter",
                        "Converter"
                    ],
                    "llm_response_time": 3149,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "commitOffsets": {
                    "target_classes": [
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.03926674661733455
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.03926674661733455
                        },
                        {
                            "class_name": "HeaderConverter",
                            "similarity_score": 0.014591156645246276
                        },
                        {
                            "class_name": "SinkTaskMetricsGroup",
                            "similarity_score": 0.36830141598201066
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SinkTaskMetricsGroup",
                        "Converter",
                        "Converter"
                    ],
                    "llm_response_time": 3084,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "recordPartitionCount": {
                    "target_classes": [
                        {
                            "class_name": "Sensor",
                            "similarity_score": 0.328982844601181
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Sensor"
                    ],
                    "llm_response_time": 2214,
                    "similarity_computation_time": 3,
                    "similarity_metric": "cosine"
                },
                "recordOffsetSequenceNumber": {
                    "target_classes": [
                        {
                            "class_name": "Sensor",
                            "similarity_score": 0.328982844601181
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Sensor"
                    ],
                    "llm_response_time": 2472,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "resumeAll": {
                    "target_classes": [
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.07224030685898176
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.07224030685898176
                        },
                        {
                            "class_name": "HeaderConverter",
                            "similarity_score": 0.06039860389867787
                        },
                        {
                            "class_name": "SinkTaskMetricsGroup",
                            "similarity_score": 0.35492079126761683
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SinkTaskMetricsGroup",
                        "Converter",
                        "Converter"
                    ],
                    "llm_response_time": 3434,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "recordRead": {
                    "target_classes": [
                        {
                            "class_name": "Sensor",
                            "similarity_score": 0.328982844601181
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Sensor"
                    ],
                    "llm_response_time": 2398,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "recordSend": {
                    "target_classes": [
                        {
                            "class_name": "Sensor",
                            "similarity_score": 0.328982844601181
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Sensor"
                    ],
                    "llm_response_time": 2566,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "recordPut": {
                    "target_classes": [
                        {
                            "class_name": "Sensor",
                            "similarity_score": 0.328982844601181
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Sensor"
                    ],
                    "llm_response_time": 2313,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "recordOffsetCommitSuccess": {
                    "target_classes": [
                        {
                            "class_name": "Sensor",
                            "similarity_score": 0.36038265004056697
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Sensor"
                    ],
                    "llm_response_time": 1892,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "recordOffsetCommitSkip": {
                    "target_classes": [
                        {
                            "class_name": "Sensor",
                            "similarity_score": 0.36038265004056697
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Sensor"
                    ],
                    "llm_response_time": 2448,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "poll": {
                    "target_classes": [
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.25808208409099265
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.25808208409099265
                        },
                        {
                            "class_name": "HeaderConverter",
                            "similarity_score": 0.24740390281223112
                        },
                        {
                            "class_name": "SinkTaskMetricsGroup",
                            "similarity_score": 0.38627778387552847
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SinkTaskMetricsGroup",
                        "Converter",
                        "Converter"
                    ],
                    "llm_response_time": 3144,
                    "similarity_computation_time": 3,
                    "similarity_metric": "cosine"
                },
                "close": {
                    "target_classes": [
                        {
                            "class_name": "MetricGroup",
                            "similarity_score": 0.14953401637408878
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "MetricGroup"
                    ],
                    "llm_response_time": 1802,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "openPartitions": {
                    "target_classes": [
                        {
                            "class_name": "SinkTask",
                            "similarity_score": 0.06903392319921543
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.04430112469403924
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.04430112469403924
                        },
                        {
                            "class_name": "HeaderConverter",
                            "similarity_score": 0.018290982847556567
                        },
                        {
                            "class_name": "SinkTaskMetricsGroup",
                            "similarity_score": 0.4616902584383193
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SinkTask",
                        "SinkTaskMetricsGroup",
                        "Converter"
                    ],
                    "llm_response_time": 3813,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "onCommitCompleted": {
                    "target_classes": [
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.4674096746446506
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.4674096746446506
                        },
                        {
                            "class_name": "HeaderConverter",
                            "similarity_score": 0.4987879039628297
                        },
                        {
                            "class_name": "SinkTaskMetricsGroup",
                            "similarity_score": 0.2726430074428989
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Converter",
                        "Converter",
                        "HeaderConverter"
                    ],
                    "llm_response_time": 3214,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "convertMessages": {
                    "target_classes": [
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.10775217826278552
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.10775217826278552
                        },
                        {
                            "class_name": "HeaderConverter",
                            "similarity_score": 0.07892441655340718
                        },
                        {
                            "class_name": "SinkTaskMetricsGroup",
                            "similarity_score": 0.37483221715257276
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Converter",
                        "Converter",
                        "SinkTaskMetricsGroup"
                    ],
                    "llm_response_time": 3175,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "e7fa0edd6351c0949b9082c626597046e9def854",
        "url": "https://github.com/apache/kafka/commit/e7fa0edd6351c0949b9082c626597046e9def854",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public getOrMaybeCreateGroup(groupType Group.GroupType, groupId String) : Group extracted from public testDeleteGroupAllOffsets(groupType Group.GroupType) : void in class org.apache.kafka.coordinator.group.OffsetMetadataManagerTest & moved to class org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2345,
                    "endLine": 2380,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testDeleteGroupAllOffsets(groupType Group.GroupType) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2350,
                    "endLine": 2350,
                    "startColumn": 13,
                    "endColumn": 26,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2356,
                    "endLine": 2356,
                    "startColumn": 13,
                    "endColumn": 27,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2362,
                    "endLine": 2362,
                    "startColumn": 13,
                    "endColumn": 21,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2363,
                    "endLine": 2363,
                    "startColumn": 17,
                    "endColumn": 88,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2351,
                    "endLine": 2354,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2357,
                    "endLine": 2360,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2349,
                    "endLine": 2364,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "SWITCH_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 184,
                    "endLine": 202,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public getOrMaybeCreateGroup(groupType Group.GroupType, groupId String) : Group"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 189,
                    "endLine": 189,
                    "startColumn": 17,
                    "endColumn": 30,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 194,
                    "endLine": 194,
                    "startColumn": 17,
                    "endColumn": 31,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 199,
                    "endLine": 199,
                    "startColumn": 17,
                    "endColumn": 25,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 200,
                    "endLine": 200,
                    "startColumn": 21,
                    "endColumn": 92,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 190,
                    "endLine": 193,
                    "startColumn": 21,
                    "endColumn": 23,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 195,
                    "endLine": 198,
                    "startColumn": 21,
                    "endColumn": 23,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 188,
                    "endLine": 201,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "SWITCH_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2391,
                    "endLine": 2412,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testDeleteGroupAllOffsets(groupType Group.GroupType) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2395,
                    "endLine": 2395,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "context.getOrMaybeCreateGroup(groupType,\"foo\")"
                }
            ],
            "isStatic": false
        },
        "ref_id": 568,
        "extraction_results": {
            "success": true,
            "newCommitHash": "fd74defe96b3f3fc1d3616cb408a0bfdb81ca73a",
            "newBranchName": "extract-getOrMaybeCreateGroup-testDeleteGroupAllOffsets-6ccc19c"
        },
        "telemetry": {
            "id": "f914a4cc-d785-40cb-832c-fd108a3a4baa",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2871,
                "lineStart": 92,
                "lineEnd": 2962,
                "bodyLineStart": 92,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                "sourceCode": "public class OffsetMetadataManagerTest {\n    static class OffsetMetadataManagerTestContext {\n        public static class Builder {\n            private final MockTime time = new MockTime();\n            private final MockCoordinatorTimer<Void, Record> timer = new MockCoordinatorTimer<>(time);\n            private final LogContext logContext = new LogContext();\n            private final SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n            private GroupMetadataManager groupMetadataManager = null;\n            private MetadataImage metadataImage = null;\n            private GroupCoordinatorConfig config = null;\n            private GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n\n            Builder withOffsetMetadataMaxSize(int offsetMetadataMaxSize) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(offsetMetadataMaxSize, 60000L, 24 * 60 * 1000);\n                return this;\n            }\n\n            Builder withOffsetsRetentionMs(long offsetsRetentionMs) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, offsetsRetentionMs);\n                return this;\n            }\n\n            Builder withGroupMetadataManager(GroupMetadataManager groupMetadataManager) {\n                this.groupMetadataManager = groupMetadataManager;\n                return this;\n            }\n\n            OffsetMetadataManagerTestContext build() {\n                if (metadataImage == null) metadataImage = MetadataImage.EMPTY;\n                if (config == null) {\n                    config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, 24 * 60 * 1000);\n                }\n\n                if (groupMetadataManager == null) {\n                    groupMetadataManager = new GroupMetadataManager.Builder()\n                        .withTime(time)\n                        .withTimer(timer)\n                        .withSnapshotRegistry(snapshotRegistry)\n                        .withLogContext(logContext)\n                        .withMetadataImage(metadataImage)\n                        .withConsumerGroupAssignors(Collections.singletonList(new RangeAssignor()))\n                        .withGroupCoordinatorMetricsShard(metrics)\n                        .build();\n                }\n\n                OffsetMetadataManager offsetMetadataManager = new OffsetMetadataManager.Builder()\n                    .withTime(time)\n                    .withLogContext(logContext)\n                    .withSnapshotRegistry(snapshotRegistry)\n                    .withMetadataImage(metadataImage)\n                    .withGroupMetadataManager(groupMetadataManager)\n                    .withGroupCoordinatorConfig(config)\n                    .withGroupCoordinatorMetricsShard(metrics)\n                    .build();\n\n                return new OffsetMetadataManagerTestContext(\n                    time,\n                    timer,\n                    snapshotRegistry,\n                    metrics,\n                    groupMetadataManager,\n                    offsetMetadataManager\n                );\n            }\n        }\n\n        final MockTime time;\n        final MockCoordinatorTimer<Void, Record> timer;\n        final SnapshotRegistry snapshotRegistry;\n        final GroupCoordinatorMetricsShard metrics;\n        final GroupMetadataManager groupMetadataManager;\n        final OffsetMetadataManager offsetMetadataManager;\n\n        long lastCommittedOffset = 0L;\n        long lastWrittenOffset = 0L;\n\n        OffsetMetadataManagerTestContext(\n            MockTime time,\n            MockCoordinatorTimer<Void, Record> timer,\n            SnapshotRegistry snapshotRegistry,\n            GroupCoordinatorMetricsShard metrics,\n            GroupMetadataManager groupMetadataManager,\n            OffsetMetadataManager offsetMetadataManager\n        ) {\n            this.time = time;\n            this.timer = timer;\n            this.snapshotRegistry = snapshotRegistry;\n            this.metrics = metrics;\n            this.groupMetadataManager = groupMetadataManager;\n            this.offsetMetadataManager = offsetMetadataManager;\n        }\n\n        public void commit() {\n            long lastCommittedOffset = this.lastCommittedOffset;\n            this.lastCommittedOffset = lastWrittenOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastCommittedOffset);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, Record> commitOffset(\n            OffsetCommitRequestData request\n        ) {\n            return commitOffset(ApiKeys.OFFSET_COMMIT.latestVersion(), request);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, Record> commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.OFFSET_COMMIT,\n                    version,\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<OffsetCommitResponseData, Record> result = offsetMetadataManager.commitOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public CoordinatorResult<TxnOffsetCommitResponseData, Record> commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.TXN_OFFSET_COMMIT,\n                    ApiKeys.TXN_OFFSET_COMMIT.latestVersion(),\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<TxnOffsetCommitResponseData, Record> result = offsetMetadataManager.commitTransactionalOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(record -> replay(\n                request.producerId(),\n                record\n            ));\n\n            return result;\n        }\n\n        public CoordinatorResult<OffsetDeleteResponseData, Record> deleteOffsets(\n            OffsetDeleteRequestData request\n        ) {\n            CoordinatorResult<OffsetDeleteResponseData, Record> result = offsetMetadataManager.deleteOffsets(request);\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public int deleteAllOffsets(\n            String groupId,\n            List<Record> records\n        ) {\n            List<Record> addedRecords = new ArrayList<>();\n            int numDeletedOffsets = offsetMetadataManager.deleteAllOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return numDeletedOffsets;\n        }\n\n        public boolean cleanupExpiredOffsets(String groupId, List<Record> records) {\n            List<Record> addedRecords = new ArrayList<>();\n            boolean isOffsetsEmptyForGroup = offsetMetadataManager.cleanupExpiredOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return isOffsetsEmptyForGroup;\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            return fetchOffsets(\n                groupId,\n                null,\n                -1,\n                topics,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch)\n                    .setTopics(topics),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        ) {\n            return fetchAllOffsets(\n                groupId,\n                null,\n                -1,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchAllOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<MockCoordinatorTimer.ExpiredTimeout<Void, Record>> sleep(long ms) {\n            time.sleep(ms);\n            List<MockCoordinatorTimer.ExpiredTimeout<Void, Record>> timeouts = timer.poll();\n            timeouts.forEach(timeout -> {\n                if (timeout.result.replayRecords()) {\n                    timeout.result.records().forEach(this::replay);\n                }\n            });\n            return timeouts;\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        ) {\n            commitOffset(\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                time.milliseconds()\n            );\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            commitOffset(\n                RecordBatch.NO_PRODUCER_ID,\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                commitTimestamp\n            );\n        }\n\n        public void commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            replay(producerId, RecordHelpers.newOffsetCommitRecord(\n                groupId,\n                topic,\n                partition,\n                new OffsetAndMetadata(\n                    offset,\n                    OptionalInt.of(leaderEpoch),\n                    \"metadata\",\n                    commitTimestamp,\n                    OptionalLong.empty()\n                ),\n                MetadataVersion.latestTesting()\n            ));\n        }\n\n        public void deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            replay(RecordHelpers.newOffsetCommitTombstoneRecord(\n                groupId,\n                topic,\n                partition\n            ));\n        }\n\n        private ApiMessage messageOrNull(ApiMessageAndVersion apiMessageAndVersion) {\n            if (apiMessageAndVersion == null) {\n                return null;\n            } else {\n                return apiMessageAndVersion.message();\n            }\n        }\n\n        private void replay(\n            Record record\n        ) {\n            replay(\n                RecordBatch.NO_PRODUCER_ID,\n                record\n            );\n        }\n\n        private void replay(\n            long producerId,\n            Record record\n        ) {\n            snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n\n            ApiMessageAndVersion key = record.key();\n            ApiMessageAndVersion value = record.value();\n\n            if (key == null) {\n                throw new IllegalStateException(\"Received a null key in \" + record);\n            }\n\n            switch (key.version()) {\n                case OffsetCommitKey.HIGHEST_SUPPORTED_VERSION:\n                    offsetMetadataManager.replay(\n                        lastWrittenOffset,\n                        producerId,\n                        (OffsetCommitKey) key.message(),\n                        (OffsetCommitValue) messageOrNull(value)\n                    );\n                    break;\n\n                default:\n                    throw new IllegalStateException(\"Received an unknown record type \" + key.version()\n                        + \" in \" + record);\n            }\n\n            lastWrittenOffset++;\n        }\n\n        private void replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        ) {\n            snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n            offsetMetadataManager.replayEndTransactionMarker(producerId, result);\n            lastWrittenOffset++;\n        }\n\n        public void testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        ) {\n            final OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection requestTopicCollection =\n                new OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection(Collections.singletonList(\n                    new OffsetDeleteRequestData.OffsetDeleteRequestTopic()\n                        .setName(topic)\n                        .setPartitions(Collections.singletonList(\n                            new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(partition)\n                        ))\n                ).iterator());\n\n            final OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection expectedResponsePartitionCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection();\n            if (hasOffset(groupId, topic, partition)) {\n                expectedResponsePartitionCollection.add(\n                    new OffsetDeleteResponseData.OffsetDeleteResponsePartition()\n                        .setPartitionIndex(partition)\n                        .setErrorCode(expectedError.code())\n                );\n            }\n\n            final OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection expectedResponseTopicCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection(Collections.singletonList(\n                    new OffsetDeleteResponseData.OffsetDeleteResponseTopic()\n                        .setName(topic)\n                        .setPartitions(expectedResponsePartitionCollection)\n                ).iterator());\n\n            List<Record> expectedRecords = Collections.emptyList();\n            if (hasOffset(groupId, topic, partition) && expectedError == Errors.NONE) {\n                expectedRecords = Collections.singletonList(\n                    RecordHelpers.newOffsetCommitTombstoneRecord(groupId, topic, partition)\n                );\n            }\n\n            final CoordinatorResult<OffsetDeleteResponseData, Record> coordinatorResult = deleteOffsets(\n                new OffsetDeleteRequestData()\n                    .setGroupId(groupId)\n                    .setTopics(requestTopicCollection)\n            );\n\n            assertEquals(new OffsetDeleteResponseData().setTopics(expectedResponseTopicCollection), coordinatorResult.response());\n            assertEquals(expectedRecords, coordinatorResult.records());\n        }\n\n        public boolean hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            return offsetMetadataManager.offset(groupId, topic, partition) != null;\n        }\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testOffsetCommitWithUnknownGroup(short version) {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        Class<? extends Throwable> expectedType;\n        if (version >= 9) {\n            expectedType = GroupIdNotFoundException.class;\n        } else {\n            expectedType = IllegalGenerationException.class;\n        }\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(expectedType, () -> context.commitOffset(\n            version,\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(CoordinatorNotAvailableException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithIllegalGeneration() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(IllegalGenerationException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithUnknownInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member without static id.\n        group.add(mkGenericMember(\"member\", Optional.empty()));\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGroupInstanceId(\"instanceid\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithFencedInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member with static id.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGroupInstanceId(\"old-instance-id\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWhileInCompletingRebalanceState() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(RebalanceInProgressException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithoutMemberIdAndGeneration() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithRetentionTime() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.of(context.time.milliseconds() + 1234L)\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitMaintainsSession() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        // Schedule session timeout. This would be normally done when\n        // the group transitions to stable.\n        context.groupMetadataManager.rescheduleClassicGroupMemberHeartbeat(group, member);\n\n        // Advance time by half of the session timeout. No timeouts are\n        // expired.\n        assertEquals(Collections.emptyList(), context.sleep(5000 / 2));\n\n        // Commit.\n        context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        // Advance time by half of the session timeout. No timeouts are\n        // expired.\n        assertEquals(Collections.emptyList(), context.sleep(5000 / 2));\n\n        // Advance time by half of the session timeout again. The timeout should\n        // expire and the member is removed from the group.\n        List<MockCoordinatorTimer.ExpiredTimeout<Void, Record>> timeouts =\n            context.sleep(5000 / 2);\n        assertEquals(1, timeouts.size());\n        assertFalse(group.hasMemberId(member.memberId()));\n    }\n\n    @Test\n    public void testSimpleGroupOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n\n        // A generic should have been created.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            false\n        );\n        assertNotNull(group);\n        assertEquals(\"foo\", group.groupId());\n    }\n\n    @Test\n    public void testSimpleGroupOffsetCommitWithInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                // Instance id should be ignored.\n                .setGroupInstanceId(\"instance-id\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        OffsetCommitRequestData request = new OffsetCommitRequestData()\n            .setGroupId(\"foo\")\n            .setMemberId(\"member\")\n            .setGenerationIdOrMemberEpoch(9)\n            .setTopics(Collections.singletonList(\n                new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                    .setName(\"bar\")\n                    .setPartitions(Collections.singletonList(\n                        new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                            .setPartitionIndex(0)\n                            .setCommittedOffset(100L)\n                    ))\n            ));\n\n        // Verify that a smaller epoch is rejected.\n        assertThrows(StaleMemberEpochException.class, () -> context.commitOffset(request));\n\n        // Verify that a larger epoch is rejected.\n        request.setGenerationIdOrMemberEpoch(11);\n        assertThrows(StaleMemberEpochException.class, () -> context.commitOffset(request));\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testConsumerGroupOffsetCommitWithVersionSmallerThanVersion9(short version) {\n        // All the newer versions are fine.\n        if (version >= 9) return;\n        // Version 0 does not support MemberId and GenerationIdOrMemberEpoch fields.\n        if (version == 0) return;\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnsupportedVersionException.class, () -> context.commitOffset(\n            version,\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(9)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitFromAdminClient() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                                .setCommitTimestamp(context.time.milliseconds())\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithOffsetMetadataTooLarge() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withOffsetMetadataMaxSize(5)\n            .build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"toolarge\")\n                                .setCommitTimestamp(context.time.milliseconds()),\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(1)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"small\")\n                                .setCommitTimestamp(context.time.milliseconds())\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.OFFSET_METADATA_TOO_LARGE.code()),\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(1)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                1,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"small\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<TxnOffsetCommitResponseData, Record> result = context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new TxnOffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitResponseData.TxnOffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitResponseData.TxnOffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithUnknownGroupId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        assertThrows(UnknownMemberIdException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(100)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<TxnOffsetCommitResponseData, Record> result = context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(1)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new TxnOffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitResponseData.TxnOffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitResponseData.TxnOffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithUnknownGroupId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        assertThrows(UnknownMemberIdException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithIllegalGenerationId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(100)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupFetchOffsetsWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"group\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        List<OffsetFetchResponseData.OffsetFetchResponseTopics> expectedResponse = Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Collections.singletonList(\n                    mkInvalidOffsetPartitionResponse(0)\n                ))\n        );\n\n        assertEquals(expectedResponse, context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsWithUnknownGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        List<OffsetFetchResponseData.OffsetFetchResponseTopics> expectedResponse = Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Collections.singletonList(\n                    mkInvalidOffsetPartitionResponse(0)\n                ))\n        );\n\n        assertEquals(expectedResponse, context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsAtDifferentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        assertEquals(0, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        assertEquals(1, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        assertEquals(2, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n        assertEquals(3, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 111L, 2);\n        assertEquals(4, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 1, 210L, 2);\n        assertEquals(5, context.lastWrittenOffset);\n\n        // Always use the same request.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Arrays.asList(0, 1))\n        );\n\n        // Fetching with 0 should return all invalid offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 0L));\n\n        // Fetching with 1 should return data up to offset 1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 1L));\n\n        // Fetching with 2 should return data up to offset 2.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 2L));\n\n        // Fetching with 3 should return data up to offset 3.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 3L));\n\n        // Fetching with 4 should return data up to offset 4.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 4L));\n\n        // Fetching with 5 should return data up to offset 5.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, 5L));\n\n        // Fetching with Long.MAX_VALUE should return all offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsWithPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n\n        context.commit();\n\n        assertEquals(3, context.lastWrittenOffset);\n        assertEquals(3, context.lastCommittedOffset);\n\n        context.commitOffset(10L, \"group\", \"foo\", 1, 111L, 1, context.time.milliseconds());\n        context.commitOffset(10L, \"group\", \"bar\", 0, 201L, 1, context.time.milliseconds());\n        // Note that bar-1 does not exist in the initial commits. UNSTABLE_OFFSET_COMMIT errors\n        // must be returned in this case too.\n        context.commitOffset(10L, \"group\", \"bar\", 1, 211L, 1, context.time.milliseconds());\n\n        // Always use the same request.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Arrays.asList(0, 1))\n        );\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should return the committed offset for\n        // foo-0 and the UNSTABLE_OFFSET_COMMIT error for foo-1, bar-0 and bar-1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, Errors.UNSTABLE_OFFSET_COMMIT),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n\n        // Fetching offsets without \"require stable\" (lastCommittedOffset) should return the committed\n        // offset for foo-0, foo-1 and bar-0 and the INVALID_OFFSET for bar-1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, context.lastCommittedOffset));\n\n        // Commit the ongoing transaction.\n        context.replayEndTransactionMarker(10L, TransactionResult.COMMIT);\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should not return any errors now.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 201L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 211L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testGenericGroupFetchAllOffsetsWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"group\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsWithUnknownGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsAtDifferentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        assertEquals(0, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        assertEquals(1, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        assertEquals(2, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n        assertEquals(3, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 111L, 2);\n        assertEquals(4, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 1, 210L, 2);\n        assertEquals(5, context.lastWrittenOffset);\n\n        // Fetching with 0 should no offsets.\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", 0L));\n\n        // Fetching with 1 should return data up to offset 1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 1L));\n\n        // Fetching with 2 should return data up to offset 2.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 2L));\n\n        // Fetching with 3 should return data up to offset 3.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 3L));\n\n        // Fetching with 4 should return data up to offset 4.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 4L));\n\n        // Fetching with Long.MAX_VALUE should return all offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsWithPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n\n        context.commit();\n\n        assertEquals(3, context.lastWrittenOffset);\n        assertEquals(3, context.lastCommittedOffset);\n\n        context.commitOffset(10L, \"group\", \"foo\", 1, 111L, 1, context.time.milliseconds());\n        context.commitOffset(10L, \"group\", \"bar\", 0, 201L, 1, context.time.milliseconds());\n        // Note that bar-1 does not exist in the initial commits. The API does not return it at all until\n        // the transaction is committed.\n        context.commitOffset(10L, \"group\", \"bar\", 1, 211L, 1, context.time.milliseconds());\n\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should return the committed offset for\n        // foo-0 and the UNSTABLE_OFFSET_COMMIT error for foo-1 and bar-0.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, Errors.UNSTABLE_OFFSET_COMMIT)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n\n        // Fetching offsets without \"require stable\" (lastCommittedOffset) should the committed\n        // offset for the foo-0, foo-1 and bar-0.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", context.lastCommittedOffset));\n\n        // Commit the ongoing transaction.\n        context.replayEndTransactionMarker(10L, TransactionResult.COMMIT);\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should not return any errors now.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 201L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 211L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithMemberIdAndEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        // Create consumer group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n        // Create member.\n        group.getOrMaybeCreateMember(\"member\", true);\n        // Commit offset.\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", \"member\", 0, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", \"member\", 0, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchFromAdminClient() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        // Create consumer group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n        // Create member.\n        group.getOrMaybeCreateMember(\"member\", true);\n        // Commit offset.\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        // Fetch offsets cases.\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchOffsets(\"group\", \"\", 0, topics, Long.MAX_VALUE));\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchOffsets(\"group\", \"member\", 0, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets cases.\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchAllOffsets(\"group\", \"\", 0, Long.MAX_VALUE));\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchAllOffsets(\"group\", \"member\", 0, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n        group.getOrMaybeCreateMember(\"member\", true);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        // Fetch offsets case.\n        assertThrows(StaleMemberEpochException.class,\n            () -> context.fetchOffsets(\"group\", \"member\", 10, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertThrows(StaleMemberEpochException.class,\n            () -> context.fetchAllOffsets(\"group\", \"member\", 10, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testGenericGroupOffsetDelete() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        group.setSubscribedTopics(Optional.of(Collections.emptySet()));\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.NONE);\n    }\n\n    @Test\n    public void testGenericGroupOffsetDeleteWithErrors() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        group.setSubscribedTopics(Optional.of(Collections.singleton(\"bar\")));\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n\n        // Delete the offset whose topic partition doesn't exist.\n        context.testOffsetDeleteWith(\"foo\", \"bar1\", 0, Errors.NONE);\n        // Delete the offset from the topic that the group is subscribed to.\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.GROUP_SUBSCRIBED_TO_TOPIC);\n    }\n\n    @Test\n    public void testConsumerGroupOffsetDelete() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        assertFalse(group.isSubscribedToTopic(\"bar\"));\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.NONE);\n    }\n\n    @Test\n    public void testConsumerGroupOffsetDeleteWithErrors() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n        MetadataImage image = new GroupMetadataManagerTest.MetadataImageBuilder()\n            .addTopic(Uuid.randomUuid(), \"foo\", 1)\n            .addRacks()\n            .build();\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        group.computeSubscriptionMetadata(\n            null,\n            member1,\n            image.topics(),\n            image.cluster()\n        );\n        group.updateMember(member1);\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        assertTrue(group.isSubscribedToTopic(\"bar\"));\n\n        // Delete the offset whose topic partition doesn't exist.\n        context.testOffsetDeleteWith(\"foo\", \"bar1\", 0, Errors.NONE);\n        // Delete the offset from the topic that the group is subscribed to.\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.GROUP_SUBSCRIBED_TO_TOPIC);\n    }\n\n    @ParameterizedTest\n    @EnumSource(Group.GroupType.class)\n    public void testDeleteGroupAllOffsets(Group.GroupType groupType) {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        getOrMaybeCreateGroup(groupType, context);\n        context.commitOffset(\"foo\", \"bar-0\", 0, 100L, 0);\n        context.commitOffset(\"foo\", \"bar-0\", 1, 100L, 0);\n        context.commitOffset(\"foo\", \"bar-1\", 0, 100L, 0);\n\n        List<Record> expectedRecords = Arrays.asList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-1\", 0),\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-0\", 0),\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-0\", 1)\n        );\n\n        List<Record> records = new ArrayList<>();\n        int numDeleteOffsets = context.deleteAllOffsets(\"foo\", records);\n\n        assertEquals(expectedRecords, records);\n        assertEquals(3, numDeleteOffsets);\n    }\n\n    private void getOrMaybeCreateGroup(Group.GroupType groupType, OffsetMetadataManagerTestContext context) {\n        switch (groupType) {\n            case CLASSIC:\n                context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n                    \"foo\",\n                    true\n                );\n                break;\n            case CONSUMER:\n                context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n                    \"foo\",\n                    true\n                );\n                break;\n            default:\n                throw new IllegalArgumentException(\"Invalid group type: \" + groupType);\n        }\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsGroupHasNoOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .build();\n\n        List<Record> records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"unknown-group-id\", records));\n        assertEquals(Collections.emptyList(), records);\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsGroupDoesNotExist() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .build();\n\n        when(groupMetadataManager.group(\"unknown-group-id\")).thenThrow(GroupIdNotFoundException.class);\n        context.commitOffset(\"unknown-group-id\", \"topic\", 0, 100L, 0);\n        assertThrows(GroupIdNotFoundException.class, () -> context.cleanupExpiredOffsets(\"unknown-group-id\", new ArrayList<>()));\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsEmptyOffsetExpirationCondition() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .build();\n\n        context.commitOffset(\"group-id\", \"topic\", 0, 100L, 0);\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.empty());\n\n        List<Record> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(Collections.emptyList(), records);\n    }\n\n    @Test\n    public void testCleanupExpiredOffsets() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .withOffsetsRetentionMs(1000)\n            .build();\n\n        long commitTimestamp = context.time.milliseconds();\n\n        context.commitOffset(\"group-id\", \"firstTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 1, 100L, 0, commitTimestamp + 500);\n\n        context.time.sleep(1000);\n\n        // firstTopic-0: group is still subscribed to firstTopic. Do not expire.\n        // secondTopic-0: should expire as offset retention has passed.\n        // secondTopic-1: has not passed offset retention. Do not expire.\n        List<Record> expectedRecords = Collections.singletonList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.of(\n            new OffsetExpirationConditionImpl(offsetAndMetadata -> offsetAndMetadata.commitTimestampMs)));\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(true);\n        when(group.isSubscribedToTopic(\"secondTopic\")).thenReturn(false);\n\n        List<Record> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Expire secondTopic-1.\n        context.time.sleep(500);\n        expectedRecords = Collections.singletonList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 1)\n        );\n\n        records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Add 2 more commits, then expire all.\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(false);\n        context.commitOffset(\"group-id\", \"firstTopic\", 1, 100L, 0, commitTimestamp + 500);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 101L, 0, commitTimestamp + 500);\n\n        expectedRecords = Arrays.asList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"firstTopic\", 0),\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"firstTopic\", 1),\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n    }\n\n    static private OffsetFetchResponseData.OffsetFetchResponsePartitions mkOffsetPartitionResponse(\n        int partition,\n        long offset,\n        int leaderEpoch,\n        String metadata\n    ) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setCommittedOffset(offset)\n            .setCommittedLeaderEpoch(leaderEpoch)\n            .setMetadata(metadata);\n    }\n\n    static private OffsetFetchResponseData.OffsetFetchResponsePartitions mkInvalidOffsetPartitionResponse(int partition) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setCommittedOffset(INVALID_OFFSET)\n            .setCommittedLeaderEpoch(-1)\n            .setMetadata(\"\");\n    }\n\n    static private OffsetFetchResponseData.OffsetFetchResponsePartitions mkOffsetPartitionResponse(int partition, Errors error) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setErrorCode(error.code())\n            .setCommittedOffset(INVALID_OFFSET)\n            .setCommittedLeaderEpoch(-1)\n            .setMetadata(\"\");\n    }\n\n    @Test\n    public void testReplay() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            200L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            2L,\n            200L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            3L,\n            300L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.of(12345L)\n        ));\n    }\n\n    @Test\n    public void testTransactionalReplay() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        verifyTransactionalReplay(context, 5, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"bar\", \"zar\", 0, new OffsetAndMetadata(\n            2L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"bar\", \"zar\", 1, new OffsetAndMetadata(\n            3L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 6, \"foo\", \"bar\", 2, new OffsetAndMetadata(\n            4L,\n            102L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 6, \"foo\", \"bar\", 3, new OffsetAndMetadata(\n            5L,\n            102L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n    }\n\n    @Test\n    public void testReplayWithTombstone() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Verify replay adds the offset the map.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Create a tombstone record and replay it to delete the record.\n        context.replay(RecordHelpers.newOffsetCommitTombstoneRecord(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // Verify that the offset is gone.\n        assertNull(context.offsetMetadataManager.offset(\"foo\", \"bar\", 0));\n    }\n\n    @Test\n    public void testReplayTransactionEndMarkerWithCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Add regular offset commit.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            99L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add pending transactional commit for producer id 5.\n        verifyTransactionalReplay(context, 5L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add pending transactional commit for producer id 6.\n        verifyTransactionalReplay(context, 6L, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            2L,\n            200L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Replaying an end marker with an unknown producer id should not fail.\n        context.replayEndTransactionMarker(1L, TransactionResult.COMMIT);\n\n        // Replaying an end marker to commit transaction of producer id 5.\n        context.replayEndTransactionMarker(5L, TransactionResult.COMMIT);\n\n        // The pending offset is removed...\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            5L,\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // ... and added to the main offset storage.\n        assertEquals(new OffsetAndMetadata(\n            1L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ), context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // Replaying an end marker to abort transaction of producer id 6.\n        context.replayEndTransactionMarker(6L, TransactionResult.ABORT);\n\n        // The pending offset is removed from the pending offsets and\n        // it is not added to the main offset storage.\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            6L,\n            \"foo\",\n            \"bar\",\n            1\n        ));\n        assertNull(context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            1\n        ));\n    }\n\n    @Test\n    public void testReplayTransactionEndMarkerKeepsTheMostRecentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Add pending transactional offset commit for producer id 5.\n        verifyTransactionalReplay(context, 5L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add regular offset commit.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Replaying an end marker to commit transaction of producer id 5.\n        context.replayEndTransactionMarker(5L, TransactionResult.COMMIT);\n\n        // The pending offset is removed...\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            5L,\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // ... but it is not added to the main storage because the regular\n        // committed offset is more recent.\n        assertEquals(new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ), context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n    }\n\n    @Test\n    public void testOffsetCommitsSensor() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L),\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(1)\n                                .setCommittedOffset(150L)\n                        ))\n                ))\n        );\n\n        verify(context.metrics).record(OFFSET_COMMITS_SENSOR_NAME, 2);\n    }\n\n    @Test\n    public void testOffsetsExpiredSensor() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .withOffsetsRetentionMs(1000)\n            .build();\n\n        long commitTimestamp = context.time.milliseconds();\n\n        context.commitOffset(\"group-id\", \"firstTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 1, 100L, 0, commitTimestamp + 500);\n\n        context.time.sleep(1000);\n\n        // firstTopic-0: group is still subscribed to firstTopic. Do not expire.\n        // secondTopic-0: should expire as offset retention has passed.\n        // secondTopic-1: has not passed offset retention. Do not expire.\n        List<Record> expectedRecords = Collections.singletonList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.of(\n            new OffsetExpirationConditionImpl(offsetAndMetadata -> offsetAndMetadata.commitTimestampMs)));\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(true);\n        when(group.isSubscribedToTopic(\"secondTopic\")).thenReturn(false);\n\n        List<Record> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Expire secondTopic-1.\n        context.time.sleep(500);\n\n        records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        verify(context.metrics, times(2)).record(OFFSET_EXPIRED_SENSOR_NAME, 1);\n\n        // Add 2 more commits, then expire all.\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(false);\n        context.commitOffset(\"group-id\", \"firstTopic\", 1, 100L, 0, commitTimestamp + 500);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 101L, 0, commitTimestamp + 500);\n\n        records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"group-id\", records));\n        verify(context.metrics).record(OFFSET_EXPIRED_SENSOR_NAME, 3);\n    }\n\n    @Test\n    public void testOffsetDeletionsSensor() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\"foo\", true);\n\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        context.commitOffset(\"foo\", \"bar\", 1, 150L, 0);\n        group.setSubscribedTopics(Optional.of(Collections.emptySet()));\n\n        OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection requestTopicCollection =\n            new OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection(Collections.singletonList(\n                new OffsetDeleteRequestData.OffsetDeleteRequestTopic()\n                    .setName(\"bar\")\n                    .setPartitions(Arrays.asList(\n                        new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(0),\n                        new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(1)\n                    ))\n            ).iterator());\n\n        context.deleteOffsets(\n            new OffsetDeleteRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(requestTopicCollection)\n        );\n\n        verify(context.metrics).record(OFFSET_DELETIONS_SENSOR_NAME, 2);\n    }\n\n    private void verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    ) {\n        context.replay(RecordHelpers.newOffsetCommitRecord(\n            groupId,\n            topic,\n            partition,\n            offsetAndMetadata,\n            MetadataImage.EMPTY.features().metadataVersion()\n        ));\n\n        assertEquals(offsetAndMetadata, context.offsetMetadataManager.offset(\n            groupId,\n            topic,\n            partition\n        ));\n    }\n\n    private void verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    ) {\n        context.replay(producerId, RecordHelpers.newOffsetCommitRecord(\n            groupId,\n            topic,\n            partition,\n            offsetAndMetadata,\n            MetadataImage.EMPTY.features().metadataVersion()\n        ));\n\n        assertEquals(offsetAndMetadata, context.offsetMetadataManager.pendingTransactionalOffset(\n            producerId,\n            groupId,\n            topic,\n            partition\n        ));\n    }\n\n    private ClassicGroupMember mkGenericMember(\n        String memberId,\n        Optional<String> groupInstanceId\n    ) {\n        return new ClassicGroupMember(\n            memberId,\n            groupInstanceId,\n            \"client-id\",\n            \"host\",\n            5000,\n            5000,\n            \"consumer\",\n            new JoinGroupRequestData.JoinGroupRequestProtocolCollection(\n                Collections.singletonList(new JoinGroupRequestData.JoinGroupRequestProtocol()\n                    .setName(\"range\")\n                    .setMetadata(new byte[0])\n                ).iterator()\n            )\n        );\n    }\n}",
                "methodCount": 89
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 10,
                "candidates": [
                    {
                        "lineStart": 2917,
                        "lineEnd": 2939,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method verifyTransactionalReplay to class OffsetMetadataManagerTestContext",
                        "description": "Move method verifyTransactionalReplay to org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext\nRationale: The method `verifyTransactionalReplay` relies heavily on the `OffsetMetadataManagerTestContext` class, specifically its `replay` and `offsetMetadataManager` methods. Moreover, this method validates the transactional replay within the context of the `OffsetMetadataManagerTestContext`, making it a focused and cohesive unit of functionality that is best kept within this context. Moving it elsewhere would disrupt this cohesion and lead to potential issues with accessibility and maintainability.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2895,
                        "lineEnd": 2915,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method verifyReplay to class OffsetMetadataManagerTestContext",
                        "description": "Move method verifyReplay to org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext\nRationale: The method 'verifyReplay' interacts directly with the 'OffsetMetadataManagerTestContext' class by calling its 'replay' method and verifying its state using 'context.offsetMetadataManager.offset'. It ensures the correctness of the replay functionality within the context class, making it highly cohesive to keep 'verifyReplay' as part of the 'OffsetMetadataManagerTestContext'. Moving it ensures the method is closer to the related functionality and test context it aims to verify, improving code maintenance and readability.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 358,
                        "lineEnd": 373,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class MockTime",
                        "description": "Move method commitOffset to org.apache.kafka.common.utils.MockTime\nRationale: The method 'commitOffset' utilizes the current time obtained from the 'time.milliseconds()'. The 'MockTime' class provides the current time through the 'milliseconds()' method, indicating that 'commitOffset' could rely on 'MockTime' to obtain this timing information. Therefore, moving 'commitOffset' to 'MockTime' can streamline the usage of time-dependent functionalities and ensure that time-related operations are centrally managed within the 'MockTime' class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 536,
                        "lineEnd": 542,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method hasOffset to class OffsetMetadataManager",
                        "description": "Move method hasOffset to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The method hasOffset() primarily deals with checking offset availability based on the offset manager's data structures. The method directly interacts with offsetMetadataManager to fetch offsets and determine their presence. Hence, it is a logical fit within the OffsetMetadataManager class as it aligns with the core responsibilities of managing and querying offsets.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 189,
                        "lineEnd": 193,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class OffsetMetadataManager",
                        "description": "Move method commitOffset to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The method commitOffset() is heavily related to managing offsets, which is the main responsibility of the OffsetMetadataManager class. The logic and operations it performs fit naturally within the existing functionalities of OffsetMetadataManager such as handling commit, validation, and storage of offsets. Consequently, moving the method here would maintain coherent separation of concerns and enhance modularity.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 430,
                        "lineEnd": 436,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method messageOrNull to class ApiMessageAndVersion",
                        "description": "Move method messageOrNull to org.apache.kafka.server.common.ApiMessageAndVersion\nRationale: The 'messageOrNull' method directly deals with the 'ApiMessageAndVersion' class, accessing its 'message' method. By placing this method within 'ApiMessageAndVersion', it improves cohesion and encapsulation, making the utility method directly accessible in the context of its primary use-case. This eliminates an unnecessary dependency on external classes for such a basic utility function.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2508,
                        "lineEnd": 2515,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method mkOffsetPartitionResponse to class GroupCoordinatorService",
                        "description": "move method mkOffsetPartitionResponse to PsiClass:GroupCoordinatorService\nRationale: mkOffsetPartitionResponse is responsible for creating a response specific to offset fetch requests, which is closely related to group coordination tasks. GroupCoordinatorService is directly involved in managing consumer group states, including handling offset fetch requests, making it a suitable place for this method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 299,
                        "lineEnd": 316,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method fetchOffsets to class GroupMetadataManager",
                        "description": "Move method fetchOffsets to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The `fetchOffsets` method deals with fetching offsets based on group-related information. Since `GroupMetadataManager` manages all classic and consumer groups' metadata (which includes operations and state management of groups), it aligns with the responsibilities of `fetchOffsets`. The method's interaction primarily involves group details (group ID, member ID, member epoch), and the class already handles methods related to group state and operations, making it a more suitable place for this method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 330,
                        "lineEnd": 345,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method fetchAllOffsets to class GroupMetadataManager",
                        "description": "Move method fetchAllOffsets to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The fetchAllOffsets method interacts primarily with group metadata, fetching offsets based on group-related parameters such as groupId, memberId, and memberEpoch. The method also involves validation steps that are more aligned with group management rather than offset metadata management. Since the GroupMetadataManager class specializes in managing group metadata and handles similar requests and responses, it is the appropriate home for this method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2366,
                        "lineEnd": 2383,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method getOrMaybeCreateGroup to class OffsetMetadataManagerTestContext",
                        "description": "Move method getOrMaybeCreateGroup to org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext\nRationale: The getOrMaybeCreateGroup method relies heavily on the context.groupMetadataManager object, indicating that its functionality is closely tied to OffsetMetadataManagerTestContext. Maintaining it within this class ensures that it has direct access to the context and its associated GroupMetadataManager. This setup would retain strong cohesion and make the method more maintainable, preserving encapsulation within the test context.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [],
                    "llm_response_time": 9016
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public void commitOffset(String groupId, String topic, int partition, long offset, int leaderEpoch)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "The commitOffset method interacts directly with offset information for topics and partitions which fits the responsibility of OffsetMetadataManager class."
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public void commitOffset(String groupId, String topic, int partition, long offset, int leaderEpoch, long commitTimestamp)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "The commitOffset method with a timestamp parameter directly manages offset commits, which should be handled by OffsetMetadataManager."
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public void commitOffset(long producerId, String groupId, String topic, int partition, long offset, int leaderEpoch, long commitTimestamp)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "This version of commitOffset dealing with producerId is deeply connected to offset management and must be in OffsetMetadataManager."
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public void deleteOffset(String groupId, String topic, int partition)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Deleting offsets is a core responsibility of OffsetMetadataManager, so this method should be moved to better achieve separation of concerns."
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public CoordinatorResult<OffsetDeleteResponseData, Record> deleteOffsets(OffsetDeleteRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "The deletion of offsets is closely related to offset metadata management and should be encapsulated in OffsetMetadataManager."
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(String groupId, List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Fetching offsets from the metadata map should naturally be within the OffsetMetadataManager class as it centralizes metadata operations."
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(String groupId, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Fetching all offsets should be managed by the OffsetMetadataManager class to keep all offset fetching logic centralized."
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public boolean cleanupExpiredOffsets(String groupId, List<Record> records)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "The cleanupExpiredOffsets method deals with expired offsets which aligns with the responsibilities of offset metadata management."
                        },
                        {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public int deleteAllOffsets(String groupId, List<Record> records)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Deleting all offsets within a group is a task of offset metadata management and should be moved to OffsetMetadataManager."
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public CoordinatorResult<TxnOffsetCommitResponseData, Record> commitTransactionalOffset(TxnOffsetCommitRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Handling transactional offsets should be managed by the OffsetMetadataManager class as it deals directly with offset data."
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public CoordinatorResult<TxnOffsetCommitResponseData, Record> commitTransactionalOffset(short version, TxnOffsetCommitRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Managing the offsets within a transactional context should be the responsibility of OffsetMetadataManager class."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private void replay(Record record)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "The replay method deals with recording transaction log entries relevant to offsets and should be moved to OffsetMetadataManager."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private void replay(long producerId, Record record)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "The replay method with a producerId parameter is closely related to transactional offset management and belongs in OffsetMetadataManager."
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private void replayEndTransactionMarker(long producerId, TransactionResult result)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "This method interacts with end transaction markers, impacting offset metadata, and therefore should be under OffsetMetadataManager."
                        }
                    ],
                    "llm_response_time": 13018
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": "Builder withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "GroupCoordinatorConfigTest",
                            "rationale": "The method configures the GroupCoordinatorConfig, which is more closely related to the configuration behavior and belongs to the GroupCoordinatorConfigTest class."
                        },
                        {
                            "method_name": "withOffsetsRetentionMs",
                            "method_signature": "Builder withOffsetsRetentionMs(long offsetsRetentionMs)",
                            "target_class": "GroupCoordinatorConfigTest",
                            "rationale": "Similar to withOffsetMetadataMaxSize, this method also configures the GroupCoordinatorConfig and should be part of GroupCoordinatorConfigTest which deals with such configurations."
                        }
                    ],
                    "llm_response_time": 4107
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public deleteAllOffsets(\n            String groupId,\n            List<Record> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<Record> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            Record record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testOffsetDeleteWith",
                            "method_signature": "public testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testConsumerGroupOffsetCommitWithVersionSmallerThanVersion9",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testConsumerGroupOffsetCommitWithVersionSmallerThanVersion9(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "private getOrMaybeCreateGroup(Group.GroupType groupType, OffsetMetadataManagerTestContext context)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkOffsetPartitionResponse",
                            "method_signature": "static private mkOffsetPartitionResponse(int partition, Errors error)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyReplay",
                            "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyTransactionalReplay",
                            "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "verifyTransactionalReplay",
                            "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyReplay",
                            "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkOffsetPartitionResponse",
                            "method_signature": "static private mkOffsetPartitionResponse(int partition, Errors error)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            Record record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "private getOrMaybeCreateGroup(Group.GroupType groupType, OffsetMetadataManagerTestContext context)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )": {
                        "first": {
                            "method_name": "verifyTransactionalReplay",
                            "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.17700207178686908
                    },
                    "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )": {
                        "first": {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.17992824182661685
                    },
                    "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )": {
                        "first": {
                            "method_name": "verifyReplay",
                            "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.1808396379628798
                    },
                    "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.19069091305064945
                    },
                    "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                        "first": {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.19491347759936603
                    },
                    "public commitOffset(\n            OffsetCommitRequestData request\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.20709937263144884
                    },
                    "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)": {
                        "first": {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.2366464419842501
                    },
                    "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )": {
                        "first": {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.24805272351352323
                    },
                    "static private mkOffsetPartitionResponse(int partition, Errors error)": {
                        "first": {
                            "method_name": "mkOffsetPartitionResponse",
                            "method_signature": "static private mkOffsetPartitionResponse(int partition, Errors error)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.26182519392566833
                    },
                    "public sleep(long ms)": {
                        "first": {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.2646551514781297
                    },
                    "private replay(\n            long producerId,\n            Record record\n        )": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            Record record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.31612007449802504
                    },
                    "public fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )": {
                        "first": {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3319200620195475
                    },
                    "public fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        )": {
                        "first": {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3423938063721999
                    },
                    "public commit()": {
                        "first": {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3509923820034361
                    },
                    "private getOrMaybeCreateGroup(Group.GroupType groupType, OffsetMetadataManagerTestContext context)": {
                        "first": {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "private getOrMaybeCreateGroup(Group.GroupType groupType, OffsetMetadataManagerTestContext context)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.35958235438693964
                    }
                },
                "voyage": {
                    "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)": {
                        "first": {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.1815607539832357
                    },
                    "public sleep(long ms)": {
                        "first": {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3748306717738077
                    },
                    "public commit()": {
                        "first": {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4846832946850268
                    },
                    "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                        "first": {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.540624935199125
                    },
                    "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )": {
                        "first": {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.542963306365468
                    },
                    "public commitOffset(\n            OffsetCommitRequestData request\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5470997602310349
                    },
                    "private replay(\n            long producerId,\n            Record record\n        )": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            Record record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5515961952600481
                    },
                    "static private mkOffsetPartitionResponse(int partition, Errors error)": {
                        "first": {
                            "method_name": "mkOffsetPartitionResponse",
                            "method_signature": "static private mkOffsetPartitionResponse(int partition, Errors error)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5641287220568607
                    },
                    "public cleanupExpiredOffsets(String groupId, List<Record> records)": {
                        "first": {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<Record> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5656372252761493
                    },
                    "public deleteAllOffsets(\n            String groupId,\n            List<Record> records\n        )": {
                        "first": {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public deleteAllOffsets(\n            String groupId,\n            List<Record> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.578307691829562
                    },
                    "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )": {
                        "first": {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5940313791806998
                    },
                    "private getOrMaybeCreateGroup(Group.GroupType groupType, OffsetMetadataManagerTestContext context)": {
                        "first": {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "private getOrMaybeCreateGroup(Group.GroupType groupType, OffsetMetadataManagerTestContext context)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6012713570919299
                    },
                    "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6254124375056876
                    },
                    "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )": {
                        "first": {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6289553993432698
                    },
                    "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6442517509847436
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                        "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                        "public fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                        "public fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        )",
                        "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                        "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                        "public commit()",
                        "public sleep(long ms)",
                        "private replay(\n            long producerId,\n            Record record\n        )",
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                        "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                        "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                        "static private mkOffsetPartitionResponse(int partition, Errors error)",
                        "private getOrMaybeCreateGroup(Group.GroupType groupType, OffsetMetadataManagerTestContext context)"
                    ],
                    "llm_response_time": 8315
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                        "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                        "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                        "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                        "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )"
                    ],
                    "llm_response_time": 4030
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                        "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                        "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )"
                    ],
                    "llm_response_time": 3608
                },
                "voyage": {
                    "priority_method_names": [
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "public commit()",
                        "private replay(\n            long producerId,\n            Record record\n        )",
                        "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                        "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                        "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                        "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                        "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                        "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                        "public deleteAllOffsets(\n            String groupId,\n            List<Record> records\n        )",
                        "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                        "public cleanupExpiredOffsets(String groupId, List<Record> records)",
                        "public sleep(long ms)",
                        "static private mkOffsetPartitionResponse(int partition, Errors error)",
                        "private getOrMaybeCreateGroup(Group.GroupType groupType, OffsetMetadataManagerTestContext context)"
                    ],
                    "llm_response_time": 6535
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                        "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                        "public sleep(long ms)",
                        "public commit()"
                    ],
                    "llm_response_time": 7216
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "public sleep(long ms)",
                        "public commit()"
                    ],
                    "llm_response_time": 4791
                }
            },
            "targetClassMap": {
                "verifyTransactionalReplay": {
                    "target_classes": [
                        {
                            "class_name": "OffsetMetadataManagerTestContext",
                            "similarity_score": 0.4216995944551269
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManagerTestContext"
                    ],
                    "llm_response_time": 2469,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "replayEndTransactionMarker": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3790,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "verifyReplay": {
                    "target_classes": [
                        {
                            "class_name": "OffsetMetadataManagerTestContext",
                            "similarity_score": 0.4127327425271779
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManagerTestContext"
                    ],
                    "llm_response_time": 1916,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "commitOffset": {
                    "target_classes": [
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.39232156618424696
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.29179330416401866
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.29249854208121273
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.22690024465620917
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.2929069974435816
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManager",
                        "GroupCoordinatorMetricsShard",
                        "MockTime"
                    ],
                    "llm_response_time": 3418,
                    "similarity_computation_time": 7,
                    "similarity_metric": "cosine"
                },
                "hasOffset": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.21711431383910185
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.2884348722688805
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManager",
                        "GroupMetadataManager"
                    ],
                    "llm_response_time": 5506,
                    "similarity_computation_time": 25,
                    "similarity_metric": "cosine"
                },
                "messageOrNull": {
                    "target_classes": [
                        {
                            "class_name": "ApiMessageAndVersion",
                            "similarity_score": 0.6106401198187058
                        },
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.5914470146353319
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.4220009252215372
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.4615361902486966
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.38725094003252886
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.49188657722411167
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ApiMessageAndVersion",
                        "OffsetMetadataManager",
                        "MockTime"
                    ],
                    "llm_response_time": 3322,
                    "similarity_computation_time": 8,
                    "similarity_metric": "cosine"
                },
                "deleteOffsets": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1774,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "mkOffsetPartitionResponse": {
                    "target_classes": [
                        {
                            "class_name": "TestUtil",
                            "similarity_score": 0.2993924754260479
                        },
                        {
                            "class_name": "MetricsTestUtils",
                            "similarity_score": 0.3227714851234354
                        },
                        {
                            "class_name": "AssignmentTestUtil",
                            "similarity_score": 0.32914565153412806
                        },
                        {
                            "class_name": "Utils",
                            "similarity_score": 0.17149858514250885
                        },
                        {
                            "class_name": "FutureUtils",
                            "similarity_score": 0.1593851798082518
                        },
                        {
                            "class_name": "AdminUtils",
                            "similarity_score": 0.12499543179142365
                        },
                        {
                            "class_name": "CommandLineUtils",
                            "similarity_score": 0.3176818630814597
                        },
                        {
                            "class_name": "PropertiesUtils",
                            "similarity_score": 0.2529090663493589
                        },
                        {
                            "class_name": "LogFileUtils",
                            "similarity_score": 0.10390062193650013
                        },
                        {
                            "class_name": "CommandLineUtilsTest",
                            "similarity_score": 0.15573600876197752
                        },
                        {
                            "class_name": "FutureUtilsTest",
                            "similarity_score": 0.2074959547222085
                        },
                        {
                            "class_name": "LogEntry",
                            "similarity_score": 0.3319135580186501
                        },
                        {
                            "class_name": "RecordHelpers",
                            "similarity_score": 0.2259881174228343
                        },
                        {
                            "class_name": "DirectoryId",
                            "similarity_score": 0.22572543048919588
                        },
                        {
                            "class_name": "Replicas",
                            "similarity_score": 0.21524687740598686
                        },
                        {
                            "class_name": "ActivationRecordsGenerator",
                            "similarity_score": 0.24510221106170008
                        },
                        {
                            "class_name": "ServerTopicConfigSynonyms",
                            "similarity_score": 0.3089682853364634
                        },
                        {
                            "class_name": "Json",
                            "similarity_score": 0.2627469271226941
                        },
                        {
                            "class_name": "AssignmentsHelper",
                            "similarity_score": 0.30304576336566325
                        },
                        {
                            "class_name": "ControllerExceptions",
                            "similarity_score": 0.26631449847855204
                        },
                        {
                            "class_name": "GroupCoordinatorConfigTest",
                            "similarity_score": 0.09887865147828519
                        },
                        {
                            "class_name": "GroupCoordinatorRuntimeMetricsTest",
                            "similarity_score": 0.2657151150784843
                        },
                        {
                            "class_name": "OffsetAndMetadata",
                            "similarity_score": 0.1797036105255444
                        },
                        {
                            "class_name": "RecordHelpersTest",
                            "similarity_score": 0.28862821755434387
                        },
                        {
                            "class_name": "StripedReplicaPlacer",
                            "similarity_score": 0.1816625371845011
                        },
                        {
                            "class_name": "LogValidator",
                            "similarity_score": 0.24316964618106027
                        },
                        {
                            "class_name": "LogConfig",
                            "similarity_score": 0.25278102858351337
                        },
                        {
                            "class_name": "CurrentAssignmentBuilderTest",
                            "similarity_score": 0.11778885723266773
                        },
                        {
                            "class_name": "KafkaYammerMetrics",
                            "similarity_score": 0.38490360577425614
                        },
                        {
                            "class_name": "TopicMetadata",
                            "similarity_score": 0.2668256079766831
                        },
                        {
                            "class_name": "Assignment",
                            "similarity_score": 0.2562786546317016
                        },
                        {
                            "class_name": "TargetAssignmentBuilder",
                            "similarity_score": 0.2071137920333522
                        },
                        {
                            "class_name": "TargetAssignmentBuilderTest",
                            "similarity_score": 0.173529387821165
                        },
                        {
                            "class_name": "ClientAssignor",
                            "similarity_score": 0.2507980605541878
                        },
                        {
                            "class_name": "ClientQuotasImageNode",
                            "similarity_score": 0.32006596209106475
                        },
                        {
                            "class_name": "GroupMetadataManagerTest",
                            "similarity_score": 0.2158343150046658
                        },
                        {
                            "class_name": "ConsumerGroupMember",
                            "similarity_score": 0.24870740981458936
                        },
                        {
                            "class_name": "RangeAssignorTest",
                            "similarity_score": 0.18241408302809684
                        },
                        {
                            "class_name": "GroupCoordinatorService",
                            "similarity_score": 0.35841279376871676
                        },
                        {
                            "class_name": "GroupCoordinatorMetrics",
                            "similarity_score": 0.29761551808159326
                        },
                        {
                            "class_name": "ConsumerGroup",
                            "similarity_score": 0.201505045730486
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.21369644125316795
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.2669715580497876
                        },
                        {
                            "class_name": "ClientQuotaControlManager",
                            "similarity_score": 0.37048481829921537
                        },
                        {
                            "class_name": "ClassicGroupMember",
                            "similarity_score": 0.19842087539693246
                        },
                        {
                            "class_name": "CoordinatorRuntimeTest",
                            "similarity_score": 0.16841694424234505
                        },
                        {
                            "class_name": "GroupCoordinatorServiceTest",
                            "similarity_score": 0.21282904548626502
                        },
                        {
                            "class_name": "LogControl",
                            "similarity_score": 0.18090105089569858
                        },
                        {
                            "class_name": "GeneralUniformAssignmentBuilder",
                            "similarity_score": 0.21167816549912805
                        },
                        {
                            "class_name": "GeneralUniformAssignmentBuilderTest",
                            "similarity_score": 0.17185188947846622
                        },
                        {
                            "class_name": "LogValue",
                            "similarity_score": 0.26115358224785395
                        },
                        {
                            "class_name": "TargetAssignmentBuilderTestContext",
                            "similarity_score": 0.3079778263947206
                        },
                        {
                            "class_name": "TargetAssignmentResult",
                            "similarity_score": 0.20797258270192576
                        },
                        {
                            "class_name": "ClassicGroup",
                            "similarity_score": 0.19614784963966886
                        },
                        {
                            "class_name": "ClassicGroupMemberTest",
                            "similarity_score": 0.2000455833968398
                        },
                        {
                            "class_name": "ClassicGroupTest",
                            "similarity_score": 0.21391879339302472
                        },
                        {
                            "class_name": "GroupAssignment",
                            "similarity_score": 0.2634319380891059
                        },
                        {
                            "class_name": "GroupCoordinatorConfig",
                            "similarity_score": 0.024801185302277005
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.2580353263426371
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShardTest",
                            "similarity_score": 0.1369210291676815
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsTest",
                            "similarity_score": 0.1853720993737557
                        },
                        {
                            "class_name": "GroupCoordinatorRuntimeMetrics",
                            "similarity_score": 0.19777599731657527
                        },
                        {
                            "class_name": "GroupCoordinatorShard",
                            "similarity_score": 0.15036890607857667
                        },
                        {
                            "class_name": "GroupCoordinatorShardTest",
                            "similarity_score": 0.1913090693327113
                        },
                        {
                            "class_name": "GroupMetadataManagerTestContext",
                            "similarity_score": 0.30475571061232687
                        },
                        {
                            "class_name": "ClientAssignorTest",
                            "similarity_score": 0.1406742497601799
                        },
                        {
                            "class_name": "MemberAssignment",
                            "similarity_score": 0.271539426475639
                        },
                        {
                            "class_name": "MetadataImageBuilder",
                            "similarity_score": 0.23606684260939012
                        },
                        {
                            "class_name": "InMemoryPartitionWriter",
                            "similarity_score": 0.34380712873358227
                        },
                        {
                            "class_name": "OffsetAndMetadataTest",
                            "similarity_score": 0.2323831505086128
                        },
                        {
                            "class_name": "OffsetConfig",
                            "similarity_score": 0.06047172409169102
                        },
                        {
                            "class_name": "OffsetExpirationConditionImpl",
                            "similarity_score": 0.141509827535445
                        },
                        {
                            "class_name": "OffsetExpirationConditionImplTest",
                            "similarity_score": 0.07155160530930325
                        },
                        {
                            "class_name": "OffsetMetadataManagerTestContext",
                            "similarity_score": 0.34674643584619685
                        },
                        {
                            "class_name": "ScheduledTimeout",
                            "similarity_score": 0.12574886370696423
                        },
                        {
                            "class_name": "TopicMetadataTest",
                            "similarity_score": 0.18713770287101772
                        },
                        {
                            "class_name": "ExpiredTimeout",
                            "similarity_score": 0.21605041176519532
                        },
                        {
                            "class_name": "OptimizedUniformAssignmentBuilder",
                            "similarity_score": 0.12802275027318932
                        },
                        {
                            "class_name": "OptimizedUniformAssignmentBuilderTest",
                            "similarity_score": 0.18021688519156756
                        },
                        {
                            "class_name": "EventAccumulator",
                            "similarity_score": 0.2011505495326703
                        },
                        {
                            "class_name": "EventAccumulatorTest",
                            "similarity_score": 0.21725207468587054
                        },
                        {
                            "class_name": "MockCoordinatorTimer",
                            "similarity_score": 0.244717398736803
                        },
                        {
                            "class_name": "MockPartitionAssignor",
                            "similarity_score": 0.3280305126305215
                        },
                        {
                            "class_name": "PartitionAssignorException",
                            "similarity_score": 0.18632102821336108
                        },
                        {
                            "class_name": "ListenerInfo",
                            "similarity_score": 0.2782242524793903
                        },
                        {
                            "class_name": "AssignmentMemberSpec",
                            "similarity_score": 0.22230295691052512
                        },
                        {
                            "class_name": "AssignmentSpec",
                            "similarity_score": 0.26257524659082143
                        },
                        {
                            "class_name": "AssignmentTest",
                            "similarity_score": 0.14788343350385408
                        },
                        {
                            "class_name": "ConsumerGroupBuilder",
                            "similarity_score": 0.33979622012713573
                        },
                        {
                            "class_name": "ConsumerGroupMemberTest",
                            "similarity_score": 0.15436078653581461
                        },
                        {
                            "class_name": "MultiThreadedEventProcessor",
                            "similarity_score": 0.2644356480987015
                        },
                        {
                            "class_name": "ConsumerGroupTest",
                            "similarity_score": 0.14132924983516584
                        },
                        {
                            "class_name": "MultiThreadedEventProcessorTest",
                            "similarity_score": 0.2826861539887831
                        },
                        {
                            "class_name": "LoadSummary",
                            "similarity_score": 0.24220361170472504
                        },
                        {
                            "class_name": "UniformAssignor",
                            "similarity_score": 0.15651984551804954
                        },
                        {
                            "class_name": "CoordinatorResult",
                            "similarity_score": 0.14658709179027857
                        },
                        {
                            "class_name": "CoordinatorResultTest",
                            "similarity_score": 0.2408686937711101
                        },
                        {
                            "class_name": "CoordinatorRuntime",
                            "similarity_score": 0.16707859877161424
                        },
                        {
                            "class_name": "UnknownRecordTypeException",
                            "similarity_score": 0.21829911781752726
                        },
                        {
                            "class_name": "DeadlineAndEpoch",
                            "similarity_score": 0.2089785019092862
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupCoordinatorService",
                        "ClientQuotaControlManager",
                        "KafkaYammerMetrics"
                    ],
                    "llm_response_time": 4496,
                    "similarity_computation_time": 78,
                    "similarity_metric": "cosine"
                },
                "sleep": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2435,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "replay": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3853,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "fetchOffsets": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.2333733415094981
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.27672534625348943
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager",
                        "OffsetMetadataManager"
                    ],
                    "llm_response_time": 7831,
                    "similarity_computation_time": 6,
                    "similarity_metric": "cosine"
                },
                "fetchAllOffsets": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.24375050551513192
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.28903019771601773
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager",
                        "OffsetMetadataManager"
                    ],
                    "llm_response_time": 6110,
                    "similarity_computation_time": 10,
                    "similarity_metric": "cosine"
                },
                "commit": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2686,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "getOrMaybeCreateGroup": {
                    "target_classes": [
                        {
                            "class_name": "GroupType",
                            "similarity_score": 0.27684651149997397
                        },
                        {
                            "class_name": "OffsetMetadataManagerTestContext",
                            "similarity_score": 0.35599971949938714
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManagerTestContext",
                        "GroupType"
                    ],
                    "llm_response_time": 2642,
                    "similarity_computation_time": 9,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4f0a40590833a141c78341ce95ffc782747c5ac8",
        "url": "https://github.com/apache/kafka/commit/4f0a40590833a141c78341ce95ffc782747c5ac8",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public tasksMax() : int extracted from public connectorTaskConfigs(connName String, connConfig ConnectorConfig) : List<Map<String,String>> in class org.apache.kafka.connect.runtime.Worker & moved to class org.apache.kafka.connect.runtime.ConnectorConfig",
            "leftSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                    "startLine": 377,
                    "endLine": 414,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public connectorTaskConfigs(connName String, connConfig ConnectorConfig) : List<Map<String,String>>"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                    "startLine": 392,
                    "endLine": 392,
                    "startColumn": 13,
                    "endColumn": 80,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java",
                    "startLine": 295,
                    "endLine": 297,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public tasksMax() : int"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java",
                    "startLine": 296,
                    "endLine": 296,
                    "startColumn": 9,
                    "endColumn": 41,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                    "startLine": 377,
                    "endLine": 422,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public connectorTaskConfigs(connName String, connConfig ConnectorConfig) : List<Map<String,String>>"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                    "startLine": 392,
                    "endLine": 392,
                    "startColumn": 28,
                    "endColumn": 49,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "connConfig.tasksMax()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 572,
        "extraction_results": {
            "success": true,
            "newCommitHash": "617c4dda13dda70600b65f8dc16964d46f996a68",
            "newBranchName": "extract-tasksMax-connectorTaskConfigs-8da6508"
        },
        "telemetry": {
            "id": "f119acd7-da4a-4c58-8581-845b27e8c009",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2193,
                "lineStart": 128,
                "lineEnd": 2320,
                "bodyLineStart": 128,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                "sourceCode": "/**\n * <p>\n * Worker runs a (dynamic) set of tasks in a set of threads, doing the work of actually moving\n * data to/from Kafka.\n * </p>\n * <p>\n * Since each task has a dedicated thread, this is mainly just a container for them.\n * </p>\n */\npublic class Worker {\n\n    public static final long CONNECTOR_GRACEFUL_SHUTDOWN_TIMEOUT_MS = TimeUnit.SECONDS.toMillis(5);\n    public static final long EXECUTOR_SHUTDOWN_TERMINATION_TIMEOUT_MS = TimeUnit.SECONDS.toMillis(1);\n\n    private static final Logger log = LoggerFactory.getLogger(Worker.class);\n\n    protected Herder herder;\n    private final ExecutorService executor;\n    private final Time time;\n    private final String workerId;\n    //kafka cluster id\n    private final String kafkaClusterId;\n    private final Plugins plugins;\n    private final ConnectMetrics metrics;\n    private final WorkerMetricsGroup workerMetricsGroup;\n    private ConnectorStatusMetricsGroup connectorStatusMetricsGroup;\n    private final WorkerConfig config;\n    private final Converter internalKeyConverter;\n    private final Converter internalValueConverter;\n    private final OffsetBackingStore globalOffsetBackingStore;\n\n    private final ConcurrentMap<String, WorkerConnector> connectors = new ConcurrentHashMap<>();\n    private final ConcurrentMap<ConnectorTaskId, WorkerTask<?, ?>> tasks = new ConcurrentHashMap<>();\n    private Optional<SourceTaskOffsetCommitter> sourceTaskOffsetCommitter = Optional.empty();\n    private final WorkerConfigTransformer workerConfigTransformer;\n    private final ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy;\n    private final Function<Map<String, Object>, Admin> adminFactory;\n\n    public Worker(\n        String workerId,\n        Time time,\n        Plugins plugins,\n        WorkerConfig config,\n        OffsetBackingStore globalOffsetBackingStore,\n        ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy) {\n        this(workerId, time, plugins, config, globalOffsetBackingStore, Executors.newCachedThreadPool(), connectorClientConfigOverridePolicy, Admin::create);\n    }\n\n    @SuppressWarnings(\"this-escape\")\n    Worker(\n            String workerId,\n            Time time,\n            Plugins plugins,\n            WorkerConfig config,\n            OffsetBackingStore globalOffsetBackingStore,\n            ExecutorService executorService,\n            ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n            Function<Map<String, Object>, Admin> adminFactory\n    ) {\n        this.kafkaClusterId = config.kafkaClusterId();\n        this.metrics = new ConnectMetrics(workerId, config, time, kafkaClusterId);\n        this.executor = executorService;\n        this.workerId = workerId;\n        this.time = time;\n        this.plugins = plugins;\n        this.config = config;\n        this.connectorClientConfigOverridePolicy = connectorClientConfigOverridePolicy;\n        this.workerMetricsGroup = new WorkerMetricsGroup(this.connectors, this.tasks, metrics);\n\n        Map<String, String> internalConverterConfig = Collections.singletonMap(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG, \"false\");\n        this.internalKeyConverter = plugins.newInternalConverter(true, JsonConverter.class.getName(), internalConverterConfig);\n        this.internalValueConverter = plugins.newInternalConverter(false, JsonConverter.class.getName(), internalConverterConfig);\n\n        this.globalOffsetBackingStore = globalOffsetBackingStore;\n\n        this.workerConfigTransformer = initConfigTransformer();\n        this.adminFactory = adminFactory;\n    }\n\n    private WorkerConfigTransformer initConfigTransformer() {\n        final List<String> providerNames = config.getList(WorkerConfig.CONFIG_PROVIDERS_CONFIG);\n        Map<String, ConfigProvider> providerMap = new HashMap<>();\n        for (String providerName : providerNames) {\n            ConfigProvider configProvider = plugins.newConfigProvider(\n                    config,\n                    WorkerConfig.CONFIG_PROVIDERS_CONFIG + \".\" + providerName,\n                    ClassLoaderUsage.PLUGINS\n            );\n            providerMap.put(providerName, configProvider);\n        }\n        return new WorkerConfigTransformer(this, providerMap);\n    }\n\n    public WorkerConfigTransformer configTransformer() {\n        return workerConfigTransformer;\n    }\n\n    protected Herder herder() {\n        return herder;\n    }\n\n    /**\n     * Start worker.\n     */\n    public void start() {\n        log.info(\"Worker starting\");\n\n        globalOffsetBackingStore.start();\n\n        sourceTaskOffsetCommitter = config.exactlyOnceSourceEnabled()\n                ? Optional.empty()\n                : Optional.of(new SourceTaskOffsetCommitter(config));\n\n        connectorStatusMetricsGroup = new ConnectorStatusMetricsGroup(metrics, tasks, herder);\n\n        log.info(\"Worker started\");\n    }\n\n    /**\n     * Stop worker.\n     */\n    public void stop() {\n        log.info(\"Worker stopping\");\n\n        long started = time.milliseconds();\n        long limit = started + config.getLong(WorkerConfig.TASK_SHUTDOWN_GRACEFUL_TIMEOUT_MS_CONFIG);\n\n        if (!connectors.isEmpty()) {\n            log.warn(\"Shutting down connectors {} uncleanly; herder should have shut down connectors before the Worker is stopped\", connectors.keySet());\n            stopAndAwaitConnectors();\n        }\n\n        if (!tasks.isEmpty()) {\n            log.warn(\"Shutting down tasks {} uncleanly; herder should have shut down tasks before the Worker is stopped\", tasks.keySet());\n            stopAndAwaitTasks();\n        }\n\n        long timeoutMs = limit - time.milliseconds();\n        sourceTaskOffsetCommitter.ifPresent(committer -> committer.close(timeoutMs));\n\n        globalOffsetBackingStore.stop();\n        metrics.stop();\n\n        log.info(\"Worker stopped\");\n\n        workerMetricsGroup.close();\n        if (connectorStatusMetricsGroup != null) {\n            connectorStatusMetricsGroup.close();\n        }\n\n        workerConfigTransformer.close();\n        ThreadUtils.shutdownExecutorServiceQuietly(executor, EXECUTOR_SHUTDOWN_TERMINATION_TIMEOUT_MS, TimeUnit.MILLISECONDS);\n    }\n\n    /**\n     * Start a connector managed by this worker.\n     *\n     * @param connName the connector name.\n     * @param connProps the properties of the connector.\n     * @param ctx the connector runtime context.\n     * @param statusListener a listener for the runtime status transitions of the connector.\n     * @param initialState the initial state of the connector.\n     * @param onConnectorStateChange invoked when the initial state change of the connector is completed\n     */\n    public void startConnector(\n            String connName,\n            Map<String, String> connProps,\n            CloseableConnectorContext ctx,\n            ConnectorStatus.Listener statusListener,\n            TargetState initialState,\n            Callback<TargetState> onConnectorStateChange\n    ) {\n        final ConnectorStatus.Listener connectorStatusListener = workerMetricsGroup.wrapStatusListener(statusListener);\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            if (connectors.containsKey(connName)) {\n                onConnectorStateChange.onCompletion(\n                        new ConnectException(\"Connector with name \" + connName + \" already exists\"),\n                        null);\n                return;\n            }\n\n            final WorkerConnector workerConnector;\n            final String connClass = connProps.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n            ClassLoader connectorLoader = plugins.connectorLoader(connClass);\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n                log.info(\"Creating connector {} of type {}\", connName, connClass);\n                final Connector connector = plugins.newConnector(connClass);\n                final ConnectorConfig connConfig;\n                final CloseableOffsetStorageReader offsetReader;\n                final ConnectorOffsetBackingStore offsetStore;\n                if (ConnectUtils.isSinkConnector(connector)) {\n                    connConfig = new SinkConnectorConfig(plugins, connProps);\n                    offsetReader = null;\n                    offsetStore = null;\n                } else {\n                    SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins, connProps, config.topicCreationEnable());\n                    connConfig = sourceConfig;\n\n                    // Set up the offset backing store for this connector instance\n                    offsetStore = config.exactlyOnceSourceEnabled()\n                            ? offsetStoreForExactlyOnceSourceConnector(sourceConfig, connName, connector, null)\n                            : offsetStoreForRegularSourceConnector(sourceConfig, connName, connector, null);\n                    offsetStore.configure(config);\n                    offsetReader = new OffsetStorageReaderImpl(offsetStore, connName, internalKeyConverter, internalValueConverter);\n                }\n                workerConnector = new WorkerConnector(\n                        connName, connector, connConfig, ctx, metrics, connectorStatusListener, offsetReader, offsetStore, connectorLoader);\n                log.info(\"Instantiated connector {} with version {} of type {}\", connName, connector.version(), connector.getClass());\n                workerConnector.transitionTo(initialState, onConnectorStateChange);\n            } catch (Throwable t) {\n                log.error(\"Failed to start connector {}\", connName, t);\n                connectorStatusListener.onFailure(connName, t);\n                onConnectorStateChange.onCompletion(t, null);\n                return;\n            }\n\n            WorkerConnector existing = connectors.putIfAbsent(connName, workerConnector);\n            if (existing != null) {\n                onConnectorStateChange.onCompletion(\n                        new ConnectException(\"Connector with name \" + connName + \" already exists\"),\n                        null);\n                // Don't need to do any cleanup of the WorkerConnector instance (such as calling\n                // shutdown() on it) here because it hasn't actually started running yet\n                return;\n            }\n\n            executor.submit(plugins.withClassLoader(connectorLoader, workerConnector));\n\n            log.info(\"Finished creating connector {}\", connName);\n        }\n    }\n\n    /**\n     * Return true if the connector associated with this worker is a sink connector.\n     *\n     * @param connName the connector name.\n     * @return true if the connector belongs to the worker and is a sink connector.\n     * @throws ConnectException if the worker does not manage a connector with the given name.\n     */\n    public boolean isSinkConnector(String connName) {\n        WorkerConnector workerConnector = connectors.get(connName);\n        if (workerConnector == null)\n            throw new ConnectException(\"Connector \" + connName + \" not found in this worker.\");\n\n        try (LoaderSwap loaderSwap = plugins.withClassLoader(workerConnector.loader())) {\n            return workerConnector.isSinkConnector();\n        }\n    }\n\n    /**\n     * Get a list of updated task properties for the tasks of this connector.\n     *\n     * @param connName the connector name.\n     * @return a list of updated tasks properties.\n     */\n    public List<Map<String, String>> connectorTaskConfigs(String connName, ConnectorConfig connConfig) {\n        List<Map<String, String>> result = new ArrayList<>();\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            log.trace(\"Reconfiguring connector tasks for {}\", connName);\n\n            WorkerConnector workerConnector = connectors.get(connName);\n            if (workerConnector == null)\n                throw new ConnectException(\"Connector \" + connName + \" not found in this worker.\");\n\n            int maxTasks = tasksMax(connConfig);\n            Map<String, String> connOriginals = connConfig.originalsStrings();\n\n            Connector connector = workerConnector.connector();\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(workerConnector.loader())) {\n                String taskClassName = connector.taskClass().getName();\n                for (Map<String, String> taskProps : connector.taskConfigs(maxTasks)) {\n                    // Ensure we don't modify the connector's copy of the config\n                    Map<String, String> taskConfig = new HashMap<>(taskProps);\n                    taskConfig.put(TaskConfig.TASK_CLASS_CONFIG, taskClassName);\n                    if (connOriginals.containsKey(SinkTask.TOPICS_CONFIG)) {\n                        taskConfig.put(SinkTask.TOPICS_CONFIG, connOriginals.get(SinkTask.TOPICS_CONFIG));\n                    }\n                    if (connOriginals.containsKey(SinkTask.TOPICS_REGEX_CONFIG)) {\n                        taskConfig.put(SinkTask.TOPICS_REGEX_CONFIG, connOriginals.get(SinkTask.TOPICS_REGEX_CONFIG));\n                    }\n                    result.add(taskConfig);\n                }\n            }\n        }\n\n        return result;\n    }\n\n    private int tasksMax(ConnectorConfig connConfig) {\n        int maxTasks = connConfig.getInt(ConnectorConfig.TASKS_MAX_CONFIG);\n        return maxTasks;\n    }\n\n    /**\n     * Stop a connector managed by this worker.\n     *\n     * @param connName the connector name.\n     */\n    private void stopConnector(String connName) {\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            WorkerConnector workerConnector = connectors.get(connName);\n            log.info(\"Stopping connector {}\", connName);\n\n            if (workerConnector == null) {\n                log.warn(\"Ignoring stop request for unowned connector {}\", connName);\n                return;\n            }\n\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(workerConnector.loader())) {\n                workerConnector.shutdown();\n            }\n        }\n    }\n\n    private void stopConnectors(Collection<String> ids) {\n        // Herder is responsible for stopping connectors. This is an internal method to sequentially\n        // stop connectors that have not explicitly been stopped.\n        for (String connector: ids)\n            stopConnector(connector);\n    }\n\n    private void awaitStopConnector(String connName, long timeout) {\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            WorkerConnector connector = connectors.remove(connName);\n            if (connector == null) {\n                log.warn(\"Ignoring await stop request for non-present connector {}\", connName);\n                return;\n            }\n\n            if (!connector.awaitShutdown(timeout)) {\n                log.error(\"Connector '{}' failed to properly shut down, has become unresponsive, and \"\n                        + \"may be consuming external resources. Correct the configuration for \"\n                        + \"this connector or remove the connector. After fixing the connector, it \"\n                        + \"may be necessary to restart this worker to release any consumed \"\n                        + \"resources.\", connName);\n                connector.cancel();\n            } else {\n                log.debug(\"Graceful stop of connector {} succeeded.\", connName);\n            }\n        }\n    }\n\n    private void awaitStopConnectors(Collection<String> ids) {\n        long now = time.milliseconds();\n        long deadline = now + CONNECTOR_GRACEFUL_SHUTDOWN_TIMEOUT_MS;\n        for (String id : ids) {\n            long remaining = Math.max(0, deadline - time.milliseconds());\n            awaitStopConnector(id, remaining);\n        }\n    }\n\n    /**\n     * Stop asynchronously all the worker's connectors and await their termination.\n     */\n    public void stopAndAwaitConnectors() {\n        stopAndAwaitConnectors(new ArrayList<>(connectors.keySet()));\n    }\n\n    /**\n     * Stop asynchronously a collection of connectors that belong to this worker and await their\n     * termination.\n     *\n     * @param ids the collection of connectors to be stopped.\n     */\n    public void stopAndAwaitConnectors(Collection<String> ids) {\n        stopConnectors(ids);\n        awaitStopConnectors(ids);\n    }\n\n    /**\n     * Stop a connector that belongs to this worker and await its termination.\n     *\n     * @param connName the name of the connector to be stopped.\n     */\n    public void stopAndAwaitConnector(String connName) {\n        stopConnector(connName);\n        awaitStopConnectors(Collections.singletonList(connName));\n    }\n\n    /**\n     * Get the IDs of the connectors currently running in this worker.\n     *\n     * @return the set of connector IDs.\n     */\n    public Set<String> connectorNames() {\n        return connectors.keySet();\n    }\n\n    /**\n     * Return true if a connector with the given name is managed by this worker and is currently running.\n     *\n     * @param connName the connector name.\n     * @return true if the connector is running, false if the connector is not running or is not manages by this worker.\n     */\n    public boolean isRunning(String connName) {\n        WorkerConnector workerConnector = connectors.get(connName);\n        return workerConnector != null && workerConnector.isRunning();\n    }\n\n    /**\n     * Start a sink task managed by this worker.\n     *\n     * @param id the task ID.\n     * @param configState the most recent {@link ClusterConfigState} known to the worker\n     * @param connProps the connector properties.\n     * @param taskProps the tasks properties.\n     * @param statusListener a listener for the runtime status transitions of the task.\n     * @param initialState the initial state of the connector.\n     * @return true if the task started successfully.\n     */\n    public boolean startSinkTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState\n    ) {\n        return startTask(id, connProps, taskProps, statusListener,\n                new SinkTaskBuilder(id, configState, statusListener, initialState));\n    }\n\n    /**\n     * Start a source task managed by this worker using older behavior that does not provide exactly-once support.\n     *\n     * @param id the task ID.\n     * @param configState the most recent {@link ClusterConfigState} known to the worker\n     * @param connProps the connector properties.\n     * @param taskProps the tasks properties.\n     * @param statusListener a listener for the runtime status transitions of the task.\n     * @param initialState the initial state of the connector.\n     * @return true if the task started successfully.\n     */\n    public boolean startSourceTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState\n    ) {\n        return startTask(id, connProps, taskProps, statusListener,\n                new SourceTaskBuilder(id, configState, statusListener, initialState));\n    }\n\n    /**\n     * Start a source task with exactly-once support managed by this worker.\n     *\n     * @param id the task ID.\n     * @param configState the most recent {@link ClusterConfigState} known to the worker\n     * @param connProps the connector properties.\n     * @param taskProps the tasks properties.\n     * @param statusListener a listener for the runtime status transitions of the task.\n     * @param initialState the initial state of the connector.\n     * @param preProducerCheck a preflight check that should be performed before the task initializes its transactional producer.\n     * @param postProducerCheck a preflight check that should be performed after the task initializes its transactional producer,\n     *                          but before producing any source records or offsets.\n     * @return true if the task started successfully.\n     */\n    public boolean startExactlyOnceSourceTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState,\n            Runnable preProducerCheck,\n            Runnable postProducerCheck\n    ) {\n        return startTask(id, connProps, taskProps, statusListener,\n                new ExactlyOnceSourceTaskBuilder(id, configState, statusListener, initialState, preProducerCheck, postProducerCheck));\n    }\n\n    /**\n     * Start a task managed by this worker.\n     *\n     * @param id the task ID.\n     * @param connProps the connector properties.\n     * @param taskProps the tasks properties.\n     * @param statusListener a listener for the runtime status transitions of the task.\n     * @param taskBuilder the {@link TaskBuilder} used to create the {@link WorkerTask} that manages the lifecycle of the task.\n     * @return true if the task started successfully.\n     */\n    private boolean startTask(\n            ConnectorTaskId id,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TaskBuilder<?, ?> taskBuilder\n    ) {\n        final WorkerTask<?, ?> workerTask;\n        final TaskStatus.Listener taskStatusListener = workerMetricsGroup.wrapStatusListener(statusListener);\n        try (LoggingContext loggingContext = LoggingContext.forTask(id)) {\n            log.info(\"Creating task {}\", id);\n\n            if (tasks.containsKey(id))\n                throw new ConnectException(\"Task already exists in this worker: \" + id);\n\n            connectorStatusMetricsGroup.recordTaskAdded(id);\n            String connType = connProps.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n            ClassLoader connectorLoader = plugins.connectorLoader(connType);\n\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n                final ConnectorConfig connConfig = new ConnectorConfig(plugins, connProps);\n                final TaskConfig taskConfig = new TaskConfig(taskProps);\n                final Class<? extends Task> taskClass = taskConfig.getClass(TaskConfig.TASK_CLASS_CONFIG).asSubclass(Task.class);\n                final Task task = plugins.newTask(taskClass);\n                log.info(\"Instantiated task {} with version {} of type {}\", id, task.version(), taskClass.getName());\n\n                // By maintaining connector's specific class loader for this thread here, we first\n                // search for converters within the connector dependencies.\n                // If any of these aren't found, that means the connector didn't configure specific converters,\n                // so we should instantiate based upon the worker configuration\n                Converter keyConverter = plugins.newConverter(connConfig, WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage\n                                                                                                                           .CURRENT_CLASSLOADER);\n                Converter valueConverter = plugins.newConverter(connConfig, WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.CURRENT_CLASSLOADER);\n                HeaderConverter headerConverter = plugins.newHeaderConverter(connConfig, WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,\n                                                                             ClassLoaderUsage.CURRENT_CLASSLOADER);\n                if (keyConverter == null) {\n                    keyConverter = plugins.newConverter(config, WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);\n                    log.info(\"Set up the key converter {} for task {} using the worker config\", keyConverter.getClass(), id);\n                } else {\n                    log.info(\"Set up the key converter {} for task {} using the connector config\", keyConverter.getClass(), id);\n                }\n                if (valueConverter == null) {\n                    valueConverter = plugins.newConverter(config, WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);\n                    log.info(\"Set up the value converter {} for task {} using the worker config\", valueConverter.getClass(), id);\n                } else {\n                    log.info(\"Set up the value converter {} for task {} using the connector config\", valueConverter.getClass(), id);\n                }\n                if (headerConverter == null) {\n                    headerConverter = plugins.newHeaderConverter(config, WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG, ClassLoaderUsage\n                                                                                                                             .PLUGINS);\n                    log.info(\"Set up the header converter {} for task {} using the worker config\", headerConverter.getClass(), id);\n                } else {\n                    log.info(\"Set up the header converter {} for task {} using the connector config\", headerConverter.getClass(), id);\n                }\n\n                workerTask = taskBuilder\n                        .withTask(task)\n                        .withConnectorConfig(connConfig)\n                        .withKeyConverter(keyConverter)\n                        .withValueConverter(valueConverter)\n                        .withHeaderConverter(headerConverter)\n                        .withClassloader(connectorLoader)\n                        .build();\n\n                workerTask.initialize(taskConfig);\n            } catch (Throwable t) {\n                log.error(\"Failed to start task {}\", id, t);\n                connectorStatusMetricsGroup.recordTaskRemoved(id);\n                taskStatusListener.onFailure(id, t);\n                return false;\n            }\n\n            WorkerTask<?, ?> existing = tasks.putIfAbsent(id, workerTask);\n            if (existing != null)\n                throw new ConnectException(\"Task already exists in this worker: \" + id);\n\n            executor.submit(plugins.withClassLoader(connectorLoader, workerTask));\n            if (workerTask instanceof WorkerSourceTask) {\n                sourceTaskOffsetCommitter.ifPresent(committer -> committer.schedule(id, (WorkerSourceTask) workerTask));\n            }\n            return true;\n        }\n    }\n\n    /**\n     * Using the admin principal for this connector, perform a round of zombie fencing that disables transactional producers\n     * for the specified number of source tasks from sending any more records.\n     * @param connName the name of the connector\n     * @param numTasks the number of tasks to fence out\n     * @param connProps the configuration of the connector; may not be null\n     * @return a {@link KafkaFuture} that will complete when the producers have all been fenced out, or the attempt has failed\n     */\n    public KafkaFuture<Void> fenceZombies(String connName, int numTasks, Map<String, String> connProps) {\n        log.debug(\"Fencing out {} task producers for source connector {}\", numTasks, connName);\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            String connType = connProps.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n            ClassLoader connectorLoader = plugins.connectorLoader(connType);\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n                final SourceConnectorConfig connConfig = new SourceConnectorConfig(plugins, connProps, config.topicCreationEnable());\n                final Class<? extends Connector> connClass = plugins.connectorClass(\n                        connConfig.getString(ConnectorConfig.CONNECTOR_CLASS_CONFIG));\n\n                Map<String, Object> adminConfig = adminConfigs(\n                        connName,\n                        \"connector-worker-adminclient-\" + connName,\n                        config,\n                        connConfig,\n                        connClass,\n                        connectorClientConfigOverridePolicy,\n                        kafkaClusterId,\n                        ConnectorType.SOURCE);\n                final Admin admin = adminFactory.apply(adminConfig);\n\n                try {\n                    Collection<String> transactionalIds = IntStream.range(0, numTasks)\n                            .mapToObj(i -> new ConnectorTaskId(connName, i))\n                            .map(this::taskTransactionalId)\n                            .collect(Collectors.toList());\n                    FenceProducersOptions fencingOptions = new FenceProducersOptions()\n                            .timeoutMs((int) RestServer.DEFAULT_REST_REQUEST_TIMEOUT_MS);\n                    return admin.fenceProducers(transactionalIds, fencingOptions).all().whenComplete((ignored, error) -> {\n                        if (error == null)\n                            log.debug(\"Finished fencing out {} task producers for source connector {}\", numTasks, connName);\n                        Utils.closeQuietly(admin, \"Zombie fencing admin for connector \" + connName);\n                    });\n                } catch (Exception e) {\n                    Utils.closeQuietly(admin, \"Zombie fencing admin for connector \" + connName);\n                    throw e;\n                }\n            }\n        }\n    }\n\n    static Map<String, Object> exactlyOnceSourceTaskProducerConfigs(ConnectorTaskId id,\n                                                              WorkerConfig config,\n                                                              ConnectorConfig connConfig,\n                                                              Class<? extends Connector>  connectorClass,\n                                                              ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                              String clusterId) {\n        Map<String, Object> result = baseProducerConfigs(id.connector(), \"connector-producer-\" + id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy, clusterId);\n        // The base producer properties forcibly disable idempotence; remove it from those properties\n        // if not explicitly requested by the user\n        boolean connectorProducerIdempotenceConfigured = connConfig.originals().containsKey(\n                ConnectorConfig.CONNECTOR_CLIENT_PRODUCER_OVERRIDES_PREFIX + ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG\n        );\n        if (!connectorProducerIdempotenceConfigured) {\n            boolean workerProducerIdempotenceConfigured = config.originals().containsKey(\n                    \"producer.\" + ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG\n            );\n            if (!workerProducerIdempotenceConfigured) {\n                result.remove(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG);\n            }\n        }\n        ConnectUtils.ensureProperty(\n                result, ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\",\n                \"for connectors when exactly-once source support is enabled\",\n                false\n        );\n        String transactionalId = taskTransactionalId(config.groupId(), id.connector(), id.task());\n        ConnectUtils.ensureProperty(\n                result, ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId,\n                \"for connectors when exactly-once source support is enabled\",\n                true\n        );\n        return result;\n    }\n\n    static Map<String, Object> baseProducerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector>  connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId) {\n        Map<String, Object> producerProps = new HashMap<>();\n        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, config.bootstrapServers());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n        // These settings will execute infinite retries on retriable exceptions. They *may* be overridden via configs passed to the worker,\n        // but this may compromise the delivery guarantees of Kafka Connect.\n        producerProps.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, Long.toString(Long.MAX_VALUE));\n        // By default, Connect disables idempotent behavior for all producers, even though idempotence became\n        // default for Kafka producers. This is to ensure Connect continues to work with many Kafka broker versions, including older brokers that do not support\n        // idempotent producers or require explicit steps to enable them (e.g. adding the IDEMPOTENT_WRITE ACL to brokers older than 2.8).\n        // These settings might change when https://cwiki.apache.org/confluence/display/KAFKA/KIP-318%3A+Make+Kafka+Connect+Source+idempotent\n        // gets approved and scheduled for release.\n        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"false\");\n        producerProps.put(ProducerConfig.ACKS_CONFIG, \"all\");\n        producerProps.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, \"1\");\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.toString(Integer.MAX_VALUE));\n        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, defaultClientId);\n        // User-specified overrides\n        producerProps.putAll(config.originalsWithPrefix(\"producer.\"));\n        //add client metrics.context properties\n        ConnectUtils.addMetricsContextProperties(producerProps, config, clusterId);\n\n        // Connector-specified overrides\n        Map<String, Object> producerOverrides =\n            connectorClientConfigOverrides(connName, connConfig, connectorClass, ConnectorConfig.CONNECTOR_CLIENT_PRODUCER_OVERRIDES_PREFIX,\n                                           ConnectorType.SOURCE, ConnectorClientConfigRequest.ClientType.PRODUCER,\n                                           connectorClientConfigOverridePolicy);\n        producerProps.putAll(producerOverrides);\n\n        return producerProps;\n    }\n\n    static Map<String, Object> exactlyOnceSourceOffsetsConsumerConfigs(String connName,\n                                                                       String defaultClientId,\n                                                                       WorkerConfig config,\n                                                                       ConnectorConfig connConfig,\n                                                                       Class<? extends Connector> connectorClass,\n                                                                       ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                       String clusterId) {\n        Map<String, Object> result = baseConsumerConfigs(\n                connName, defaultClientId, config, connConfig, connectorClass,\n                connectorClientConfigOverridePolicy, clusterId, ConnectorType.SOURCE);\n        ConnectUtils.ensureProperty(\n                result, ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.name().toLowerCase(Locale.ROOT),\n                \"for source connectors' offset consumers when exactly-once source support is enabled\",\n                false\n        );\n        return result;\n    }\n\n    static Map<String, Object> regularSourceOffsetsConsumerConfigs(String connName,\n                                                                   String defaultClientId,\n                                                                   WorkerConfig config,\n                                                                   ConnectorConfig connConfig,\n                                                                   Class<? extends Connector> connectorClass,\n                                                                   ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                   String clusterId) {\n        Map<String, Object> result = baseConsumerConfigs(\n                connName, defaultClientId, config, connConfig, connectorClass,\n                connectorClientConfigOverridePolicy, clusterId, ConnectorType.SOURCE);\n        // Users can disable this if they want to since the task isn't exactly-once anyways\n        result.putIfAbsent(\n                ConsumerConfig.ISOLATION_LEVEL_CONFIG,\n                IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT));\n        return result;\n    }\n\n    static Map<String, Object> baseConsumerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector> connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId,\n                                               ConnectorType connectorType) {\n        // Include any unknown worker configs so consumer configs can be set globally on the worker\n        // and through to the task\n        Map<String, Object> consumerProps = new HashMap<>();\n\n        consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, SinkUtils.consumerGroupId(connName));\n        consumerProps.put(ConsumerConfig.CLIENT_ID_CONFIG, defaultClientId);\n        consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, config.bootstrapServers());\n        consumerProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false\");\n        consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");\n\n        consumerProps.putAll(config.originalsWithPrefix(\"consumer.\"));\n        //add client metrics.context properties\n        ConnectUtils.addMetricsContextProperties(consumerProps, config, clusterId);\n        // Connector-specified overrides\n        Map<String, Object> consumerOverrides =\n            connectorClientConfigOverrides(connName, connConfig, connectorClass, ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX,\n                                           connectorType, ConnectorClientConfigRequest.ClientType.CONSUMER,\n                                           connectorClientConfigOverridePolicy);\n        consumerProps.putAll(consumerOverrides);\n\n        return consumerProps;\n    }\n\n    static Map<String, Object> adminConfigs(String connName,\n                                            String defaultClientId,\n                                            WorkerConfig config,\n                                            ConnectorConfig connConfig,\n                                            Class<? extends Connector> connectorClass,\n                                            ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                            String clusterId,\n                                            ConnectorType connectorType) {\n        Map<String, Object> adminProps = new HashMap<>();\n        // Use the top-level worker configs to retain backwards compatibility with older releases which\n        // did not require a prefix for connector admin client configs in the worker configuration file\n        // Ignore configs that begin with \"admin.\" since those will be added next (with the prefix stripped)\n        // and those that begin with \"producer.\" and \"consumer.\", since we know they aren't intended for\n        // the admin client\n        Map<String, Object> nonPrefixedWorkerConfigs = config.originals().entrySet().stream()\n                .filter(e -> !e.getKey().startsWith(\"admin.\")\n                        && !e.getKey().startsWith(\"producer.\")\n                        && !e.getKey().startsWith(\"consumer.\"))\n                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n        adminProps.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, config.bootstrapServers());\n        adminProps.put(AdminClientConfig.CLIENT_ID_CONFIG, defaultClientId);\n        adminProps.putAll(nonPrefixedWorkerConfigs);\n\n        // Admin client-specific overrides in the worker config\n        adminProps.putAll(config.originalsWithPrefix(\"admin.\"));\n\n        // Connector-specified overrides\n        Map<String, Object> adminOverrides =\n                connectorClientConfigOverrides(connName, connConfig, connectorClass, ConnectorConfig.CONNECTOR_CLIENT_ADMIN_OVERRIDES_PREFIX,\n                        connectorType, ConnectorClientConfigRequest.ClientType.ADMIN,\n                        connectorClientConfigOverridePolicy);\n        adminProps.putAll(adminOverrides);\n\n        //add client metrics.context properties\n        ConnectUtils.addMetricsContextProperties(adminProps, config, clusterId);\n\n        return adminProps;\n    }\n\n    private static Map<String, Object> connectorClientConfigOverrides(String connName,\n                                                                      ConnectorConfig connConfig,\n                                                                      Class<? extends Connector> connectorClass,\n                                                                      String clientConfigPrefix,\n                                                                      ConnectorType connectorType,\n                                                                      ConnectorClientConfigRequest.ClientType clientType,\n                                                                      ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy) {\n        Map<String, Object> clientOverrides = connConfig.originalsWithPrefix(clientConfigPrefix);\n        ConnectorClientConfigRequest connectorClientConfigRequest = new ConnectorClientConfigRequest(\n            connName,\n            connectorType,\n            connectorClass,\n            clientOverrides,\n            clientType\n        );\n        List<ConfigValue> configValues = connectorClientConfigOverridePolicy.validate(connectorClientConfigRequest);\n        List<ConfigValue> errorConfigs = configValues.stream().\n            filter(configValue -> configValue.errorMessages().size() > 0).collect(Collectors.toList());\n        // These should be caught when the herder validates the connector configuration, but just in case\n        if (errorConfigs.size() > 0) {\n            throw new ConnectException(\"Client Config Overrides not allowed \" + errorConfigs);\n        }\n        return clientOverrides;\n    }\n\n    private String taskTransactionalId(ConnectorTaskId id) {\n        return taskTransactionalId(config.groupId(), id.connector(), id.task());\n    }\n\n    /**\n     * @return the {@link ProducerConfig#TRANSACTIONAL_ID_CONFIG transactional ID} to use for a task that writes\n     * records and/or offsets in a transaction. Not to be confused with {@link DistributedConfig#transactionalProducerId()},\n     * which is not used by tasks at all, but instead, by the worker itself.\n     */\n    public static String taskTransactionalId(String groupId, String connector, int taskId) {\n        return String.format(\"%s-%s-%d\", groupId, connector, taskId);\n    }\n\n    ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {\n        return new ErrorHandlingMetrics(id, metrics);\n    }\n\n    private List<ErrorReporter<ConsumerRecord<byte[], byte[]>>> sinkTaskReporters(ConnectorTaskId id, SinkConnectorConfig connConfig,\n                                                     ErrorHandlingMetrics errorHandlingMetrics,\n                                                     Class<? extends Connector> connectorClass) {\n        ArrayList<ErrorReporter<ConsumerRecord<byte[], byte[]>>> reporters = new ArrayList<>();\n        LogReporter<ConsumerRecord<byte[], byte[]>> logReporter = new LogReporter.Sink(id, connConfig, errorHandlingMetrics);\n        reporters.add(logReporter);\n\n        // check if topic for dead letter queue exists\n        String topic = connConfig.dlqTopicName();\n        if (topic != null && !topic.isEmpty()) {\n            Map<String, Object> producerProps = baseProducerConfigs(id.connector(), \"connector-dlq-producer-\" + id, config, connConfig, connectorClass,\n                                                                connectorClientConfigOverridePolicy, kafkaClusterId);\n            Map<String, Object> adminProps = adminConfigs(id.connector(), \"connector-dlq-adminclient-\", config, connConfig, connectorClass, connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SINK);\n            DeadLetterQueueReporter reporter = DeadLetterQueueReporter.createAndSetup(adminProps, id, connConfig, producerProps, errorHandlingMetrics);\n\n            reporters.add(reporter);\n        }\n\n        return reporters;\n    }\n\n    private List<ErrorReporter<SourceRecord>> sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,\n                                                       ErrorHandlingMetrics errorHandlingMetrics) {\n        List<ErrorReporter<SourceRecord>> reporters = new ArrayList<>();\n        LogReporter<SourceRecord> logReporter = new LogReporter.Source(id, connConfig, errorHandlingMetrics);\n        reporters.add(logReporter);\n\n        return reporters;\n    }\n\n    private WorkerErrantRecordReporter createWorkerErrantRecordReporter(\n        SinkConnectorConfig connConfig,\n        RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n        Converter keyConverter,\n        Converter valueConverter,\n        HeaderConverter headerConverter\n    ) {\n        // check if errant record reporter topic is configured\n        if (connConfig.enableErrantRecordReporter()) {\n            return new WorkerErrantRecordReporter(retryWithToleranceOperator, keyConverter, valueConverter, headerConverter);\n        }\n        return null;\n    }\n\n    private void stopTask(ConnectorTaskId taskId) {\n        try (LoggingContext loggingContext = LoggingContext.forTask(taskId)) {\n            WorkerTask<?, ?> task = tasks.get(taskId);\n            if (task == null) {\n                log.warn(\"Ignoring stop request for unowned task {}\", taskId);\n                return;\n            }\n\n            log.info(\"Stopping task {}\", task.id());\n            if (task instanceof WorkerSourceTask)\n                sourceTaskOffsetCommitter.ifPresent(committer -> committer.remove(task.id()));\n\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(task.loader())) {\n                task.stop();\n            }\n        }\n    }\n\n    private void stopTasks(Collection<ConnectorTaskId> ids) {\n        // Herder is responsible for stopping tasks. This is an internal method to sequentially\n        // stop the tasks that have not explicitly been stopped.\n        for (ConnectorTaskId taskId : ids) {\n            stopTask(taskId);\n        }\n    }\n\n    private void awaitStopTask(ConnectorTaskId taskId, long timeout) {\n        try (LoggingContext loggingContext = LoggingContext.forTask(taskId)) {\n            WorkerTask<?, ?> task = tasks.remove(taskId);\n            if (task == null) {\n                log.warn(\"Ignoring await stop request for non-present task {}\", taskId);\n                return;\n            }\n\n            if (!task.awaitStop(timeout)) {\n                log.error(\"Graceful stop of task {} failed.\", task.id());\n                task.cancel();\n            } else {\n                log.debug(\"Graceful stop of task {} succeeded.\", task.id());\n            }\n\n            try {\n                task.removeMetrics();\n            } finally {\n                connectorStatusMetricsGroup.recordTaskRemoved(taskId);\n            }\n        }\n    }\n\n    private void awaitStopTasks(Collection<ConnectorTaskId> ids) {\n        long now = time.milliseconds();\n        long deadline = now + config.getLong(WorkerConfig.TASK_SHUTDOWN_GRACEFUL_TIMEOUT_MS_CONFIG);\n        for (ConnectorTaskId id : ids) {\n            long remaining = Math.max(0, deadline - time.milliseconds());\n            awaitStopTask(id, remaining);\n        }\n    }\n\n    /**\n     * Stop asynchronously all the worker's tasks and await their termination.\n     */\n    public void stopAndAwaitTasks() {\n        stopAndAwaitTasks(new ArrayList<>(tasks.keySet()));\n    }\n\n    /**\n     * Stop asynchronously a collection of tasks that belong to this worker and await their termination.\n     *\n     * @param ids the collection of tasks to be stopped.\n     */\n    public void stopAndAwaitTasks(Collection<ConnectorTaskId> ids) {\n        stopTasks(ids);\n        awaitStopTasks(ids);\n    }\n\n    /**\n     * Stop a task that belongs to this worker and await its termination.\n     *\n     * @param taskId the ID of the task to be stopped.\n     */\n    public void stopAndAwaitTask(ConnectorTaskId taskId) {\n        stopTask(taskId);\n        awaitStopTasks(Collections.singletonList(taskId));\n    }\n\n    /**\n     * Get the IDs of the tasks currently running in this worker.\n     */\n    public Set<ConnectorTaskId> taskIds() {\n        return tasks.keySet();\n    }\n\n    public Converter getInternalKeyConverter() {\n        return internalKeyConverter;\n    }\n\n    public Converter getInternalValueConverter() {\n        return internalValueConverter;\n    }\n\n    public Plugins getPlugins() {\n        return plugins;\n    }\n\n    public String workerId() {\n        return workerId;\n    }\n\n    /**\n     * Returns whether this worker is configured to allow source connectors to create the topics\n     * that they use with custom configurations, if these topics don't already exist.\n     *\n     * @return true if topic creation by source connectors is allowed; false otherwise\n     */\n    public boolean isTopicCreationEnabled() {\n        return config.topicCreationEnable();\n    }\n\n    /**\n     * Get the {@link ConnectMetrics} that uses Kafka Metrics and manages the JMX reporter.\n     * @return the Connect-specific metrics; never null\n     */\n    public ConnectMetrics metrics() {\n        return metrics;\n    }\n\n    public void setTargetState(String connName, TargetState state, Callback<TargetState> stateChangeCallback) {\n        log.info(\"Setting connector {} state to {}\", connName, state);\n\n        WorkerConnector workerConnector = connectors.get(connName);\n        if (workerConnector != null) {\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(workerConnector.loader())) {\n                workerConnector.transitionTo(state, stateChangeCallback);\n            }\n        }\n\n        for (Map.Entry<ConnectorTaskId, WorkerTask<?, ?>> taskEntry : tasks.entrySet()) {\n            if (taskEntry.getKey().connector().equals(connName)) {\n                WorkerTask<?, ?> workerTask = taskEntry.getValue();\n                try (LoaderSwap loaderSwap = plugins.withClassLoader(workerTask.loader())) {\n                    workerTask.transitionTo(state);\n                }\n            }\n        }\n    }\n\n    /**\n     * Get the current offsets for a connector. This method is asynchronous and the passed callback is completed when the\n     * request finishes processing.\n     *\n     * @param connName the name of the connector whose offsets are to be retrieved\n     * @param connectorConfig the connector's configurations\n     * @param cb callback to invoke upon completion of the request\n     */\n    public void connectorOffsets(String connName, Map<String, String> connectorConfig, Callback<ConnectorOffsets> cb) {\n        String connectorClassOrAlias = connectorConfig.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n        ClassLoader connectorLoader = plugins.connectorLoader(connectorClassOrAlias);\n\n        try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n            Connector connector = plugins.newConnector(connectorClassOrAlias);\n            if (ConnectUtils.isSinkConnector(connector)) {\n                log.debug(\"Fetching offsets for sink connector: {}\", connName);\n                sinkConnectorOffsets(connName, connector, connectorConfig, cb);\n            } else {\n                log.debug(\"Fetching offsets for source connector: {}\", connName);\n                sourceConnectorOffsets(connName, connector, connectorConfig, cb);\n            }\n        }\n    }\n\n    /**\n     * Get the current consumer group offsets for a sink connector.\n     * <p>\n     * Visible for testing.\n     *\n     * @param connName the name of the sink connector whose offsets are to be retrieved\n     * @param connector the sink connector\n     * @param connectorConfig the sink connector's configurations\n     * @param cb callback to invoke upon completion of the request\n     */\n    void sinkConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                              Callback<ConnectorOffsets> cb) {\n        Map<String, Object> adminConfig = adminConfigs(\n                connName,\n                \"connector-worker-adminclient-\" + connName,\n                config,\n                new SinkConnectorConfig(plugins, connectorConfig),\n                connector.getClass(),\n                connectorClientConfigOverridePolicy,\n                kafkaClusterId,\n                ConnectorType.SINK);\n        String groupId = (String) baseConsumerConfigs(\n                connName, \"connector-consumer-\", config, new SinkConnectorConfig(plugins, connectorConfig),\n                connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SINK).get(ConsumerConfig.GROUP_ID_CONFIG);\n        Admin admin = adminFactory.apply(adminConfig);\n        try {\n            ListConsumerGroupOffsetsOptions listOffsetsOptions = new ListConsumerGroupOffsetsOptions()\n                    .timeoutMs((int) RestServer.DEFAULT_REST_REQUEST_TIMEOUT_MS);\n            ListConsumerGroupOffsetsResult listConsumerGroupOffsetsResult = admin.listConsumerGroupOffsets(groupId, listOffsetsOptions);\n            listConsumerGroupOffsetsResult.partitionsToOffsetAndMetadata().whenComplete((result, error) -> {\n                if (error != null) {\n                    log.error(\"Failed to retrieve consumer group offsets for sink connector {}\", connName, error);\n                    cb.onCompletion(new ConnectException(\"Failed to retrieve consumer group offsets for sink connector \" + connName, error), null);\n                } else {\n                    ConnectorOffsets offsets = SinkUtils.consumerGroupOffsetsToConnectorOffsets(result);\n                    cb.onCompletion(null, offsets);\n                }\n                Utils.closeQuietly(admin, \"Offset fetch admin for sink connector \" + connName);\n            });\n        } catch (Throwable t) {\n            Utils.closeQuietly(admin, \"Offset fetch admin for sink connector \" + connName);\n            log.error(\"Failed to retrieve consumer group offsets for sink connector {}\", connName, t);\n            cb.onCompletion(new ConnectException(\"Failed to retrieve consumer group offsets for sink connector \" + connName, t), null);\n        }\n    }\n\n    /**\n     * Get the current offsets for a source connector.\n     *\n     * @param connName the name of the source connector whose offsets are to be retrieved\n     * @param connector the source connector\n     * @param connectorConfig the source connector's configurations\n     * @param cb callback to invoke upon completion of the request\n     */\n    private void sourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                        Callback<ConnectorOffsets> cb) {\n        SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins, connectorConfig, config.topicCreationEnable());\n        ConnectorOffsetBackingStore offsetStore = config.exactlyOnceSourceEnabled()\n                ? offsetStoreForExactlyOnceSourceConnector(sourceConfig, connName, connector, null)\n                : offsetStoreForRegularSourceConnector(sourceConfig, connName, connector, null);\n        CloseableOffsetStorageReader offsetReader = new OffsetStorageReaderImpl(offsetStore, connName, internalKeyConverter, internalValueConverter);\n        sourceConnectorOffsets(connName, offsetStore, offsetReader, cb);\n    }\n\n    // Visible for testing\n    void sourceConnectorOffsets(String connName, ConnectorOffsetBackingStore offsetStore,\n                                CloseableOffsetStorageReader offsetReader, Callback<ConnectorOffsets> cb) {\n        executor.submit(() -> {\n            try {\n                offsetStore.configure(config);\n                offsetStore.start();\n                Set<Map<String, Object>> connectorPartitions = offsetStore.connectorPartitions(connName);\n                List<ConnectorOffset> connectorOffsets = offsetReader.offsets(connectorPartitions).entrySet().stream()\n                        .map(entry -> new ConnectorOffset(entry.getKey(), entry.getValue()))\n                        .collect(Collectors.toList());\n                cb.onCompletion(null, new ConnectorOffsets(connectorOffsets));\n            } catch (Throwable t) {\n                log.error(\"Failed to retrieve offsets for source connector {}\", connName, t);\n                cb.onCompletion(ConnectUtils.maybeWrap(t, \"Failed to retrieve offsets for source connector \" + connName), null);\n            } finally {\n                Utils.closeQuietly(offsetReader, \"Offset reader for connector \" + connName);\n                Utils.closeQuietly(offsetStore::stop, \"Offset store for connector \" + connName);\n            }\n        });\n    }\n\n    /**\n     * Modify (alter / reset) a connector's offsets.\n     *\n     * @param connName the name of the connector whose offsets are to be modified\n     * @param connectorConfig the connector's configurations\n     * @param offsets a mapping from partitions (either source partitions for source connectors, or Kafka topic\n     *                partitions for sink connectors) to offsets that need to be written; this should be {@code null}\n     *                for offsets reset requests\n     * @param cb callback to invoke upon completion\n     */\n    public void modifyConnectorOffsets(String connName, Map<String, String> connectorConfig,\n                                      Map<Map<String, ?>, Map<String, ?>> offsets, Callback<Message> cb) {\n        String connectorClassOrAlias = connectorConfig.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n        ClassLoader connectorLoader = plugins.connectorLoader(connectorClassOrAlias);\n        Connector connector;\n\n        try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n            connector = plugins.newConnector(connectorClassOrAlias);\n            if (ConnectUtils.isSinkConnector(connector)) {\n                log.debug(\"Modifying offsets for sink connector: {}\", connName);\n                modifySinkConnectorOffsets(connName, connector, connectorConfig, offsets, connectorLoader, cb);\n            } else {\n                log.debug(\"Modifying offsets for source connector: {}\", connName);\n                modifySourceConnectorOffsets(connName, connector, connectorConfig, offsets, connectorLoader, cb);\n            }\n        }\n    }\n\n    /**\n     * Modify (alter / reset) a sink connector's consumer group offsets.\n     * <p>\n     * Visible for testing.\n     *\n     * @param connName the name of the sink connector whose offsets are to be modified\n     * @param connector an instance of the sink connector\n     * @param connectorConfig the sink connector's configuration\n     * @param offsets a mapping from topic partitions to offsets that need to be written; this should be {@code null}\n     *                for offsets reset requests\n     * @param connectorLoader the connector plugin's classloader to be used as the thread context classloader\n     * @param cb callback to invoke upon completion\n     */\n    void modifySinkConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                    Map<Map<String, ?>, Map<String, ?>> offsets, ClassLoader connectorLoader, Callback<Message> cb) {\n        executor.submit(plugins.withClassLoader(connectorLoader, () -> {\n            try {\n                Timer timer = time.timer(Duration.ofMillis(RestServer.DEFAULT_REST_REQUEST_TIMEOUT_MS));\n                boolean isReset = offsets == null;\n                SinkConnectorConfig sinkConnectorConfig = new SinkConnectorConfig(plugins, connectorConfig);\n                Class<? extends Connector> sinkConnectorClass = connector.getClass();\n                Map<String, Object> adminConfig = adminConfigs(\n                        connName,\n                        \"connector-worker-adminclient-\" + connName,\n                        config,\n                        sinkConnectorConfig,\n                        sinkConnectorClass,\n                        connectorClientConfigOverridePolicy,\n                        kafkaClusterId,\n                        ConnectorType.SINK);\n\n                String groupId = (String) baseConsumerConfigs(\n                        connName, \"connector-consumer-\", config, sinkConnectorConfig,\n                        sinkConnectorClass, connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SINK).get(ConsumerConfig.GROUP_ID_CONFIG);\n\n                Admin admin = adminFactory.apply(adminConfig);\n\n                try {\n                    Map<TopicPartition, Long> offsetsToWrite;\n                    if (isReset) {\n                        offsetsToWrite = new HashMap<>();\n                        ListConsumerGroupOffsetsOptions listConsumerGroupOffsetsOptions = new ListConsumerGroupOffsetsOptions()\n                                .timeoutMs((int) timer.remainingMs());\n                        try {\n                            admin.listConsumerGroupOffsets(groupId, listConsumerGroupOffsetsOptions)\n                                    .partitionsToOffsetAndMetadata()\n                                    .get(timer.remainingMs(), TimeUnit.MILLISECONDS)\n                                    .forEach((topicPartition, offsetAndMetadata) -> offsetsToWrite.put(topicPartition, null));\n\n                            timer.update();\n                            log.debug(\"Found the following topic partitions (to reset offsets) for sink connector {} and consumer group ID {}: {}\",\n                                    connName, groupId, offsetsToWrite.keySet());\n                        } catch (Exception e) {\n                            Utils.closeQuietly(admin, \"Offset reset admin for sink connector \" + connName);\n                            log.error(\"Failed to list offsets prior to resetting offsets for sink connector {}\", connName, e);\n                            cb.onCompletion(new ConnectException(\"Failed to list offsets prior to resetting offsets for sink connector \" + connName, e), null);\n                            return;\n                        }\n                    } else {\n                        offsetsToWrite = SinkUtils.parseSinkConnectorOffsets(offsets);\n                    }\n\n                    boolean alterOffsetsResult;\n                    try {\n                        alterOffsetsResult = ((SinkConnector) connector).alterOffsets(connectorConfig, offsetsToWrite);\n                    } catch (UnsupportedOperationException e) {\n                        log.error(\"Failed to modify offsets for connector {} because it doesn't support external modification of offsets\",\n                                connName, e);\n                        throw new ConnectException(\"Failed to modify offsets for connector \" + connName + \" because it doesn't support external \" +\n                                \"modification of offsets\", e);\n                    }\n                    updateTimerAndCheckExpiry(timer, \"Timed out while calling the 'alterOffsets' method for sink connector \" + connName);\n\n                    if (isReset) {\n                        resetSinkConnectorOffsets(connName, groupId, admin, cb, alterOffsetsResult, timer);\n                    } else {\n                        alterSinkConnectorOffsets(connName, groupId, admin, offsetsToWrite, cb, alterOffsetsResult, timer);\n                    }\n                } catch (Throwable t) {\n                    Utils.closeQuietly(admin, \"Offset modification admin for sink connector \" + connName);\n                    throw t;\n                }\n            } catch (Throwable t) {\n                log.error(\"Failed to modify offsets for sink connector {}\", connName, t);\n                cb.onCompletion(ConnectUtils.maybeWrap(t, \"Failed to modify offsets for sink connector \" + connName), null);\n            }\n        }));\n    }\n\n    /**\n     * Alter a sink connector's consumer group offsets. This is done via calls to {@link Admin#alterConsumerGroupOffsets}\n     * and / or {@link Admin#deleteConsumerGroupOffsets}.\n     *\n     * @param connName the name of the sink connector whose offsets are to be altered\n     * @param groupId the sink connector's consumer group ID\n     * @param admin the {@link Admin admin client} to be used for altering the consumer group offsets; will be closed after use\n     * @param offsetsToWrite a mapping from topic partitions to offsets that need to be written; may not be null or empty\n     * @param cb callback to invoke upon completion\n     * @param alterOffsetsResult the result of the call to {@link SinkConnector#alterOffsets} for the connector\n     * @param timer {@link Timer} to bound the total runtime of admin client requests\n     */\n    private void alterSinkConnectorOffsets(String connName, String groupId, Admin admin, Map<TopicPartition, Long> offsetsToWrite,\n                                           Callback<Message> cb, boolean alterOffsetsResult, Timer timer) {\n        List<KafkaFuture<Void>> adminFutures = new ArrayList<>();\n\n        Map<TopicPartition, OffsetAndMetadata> offsetsToAlter = offsetsToWrite.entrySet()\n                .stream()\n                .filter(entry -> entry.getValue() != null)\n                .collect(Collectors.toMap(Map.Entry::getKey, e -> new OffsetAndMetadata(e.getValue())));\n\n        if (!offsetsToAlter.isEmpty()) {\n            log.debug(\"Committing the following consumer group offsets using an admin client for sink connector {}: {}.\",\n                    connName, offsetsToAlter);\n            AlterConsumerGroupOffsetsOptions alterConsumerGroupOffsetsOptions = new AlterConsumerGroupOffsetsOptions()\n                    .timeoutMs((int) timer.remainingMs());\n            AlterConsumerGroupOffsetsResult alterConsumerGroupOffsetsResult = admin.alterConsumerGroupOffsets(groupId, offsetsToAlter,\n                    alterConsumerGroupOffsetsOptions);\n\n            adminFutures.add(alterConsumerGroupOffsetsResult.all());\n        }\n\n        Set<TopicPartition> partitionsToReset = offsetsToWrite.entrySet()\n                .stream()\n                .filter(entry -> entry.getValue() == null)\n                .map(Map.Entry::getKey)\n                .collect(Collectors.toSet());\n\n        if (!partitionsToReset.isEmpty()) {\n            log.debug(\"Deleting the consumer group offsets for the following topic partitions using an admin client for sink connector {}: {}.\",\n                    connName, partitionsToReset);\n            DeleteConsumerGroupOffsetsOptions deleteConsumerGroupOffsetsOptions = new DeleteConsumerGroupOffsetsOptions()\n                    .timeoutMs((int) timer.remainingMs());\n            DeleteConsumerGroupOffsetsResult deleteConsumerGroupOffsetsResult = admin.deleteConsumerGroupOffsets(groupId, partitionsToReset,\n                    deleteConsumerGroupOffsetsOptions);\n\n            adminFutures.add(deleteConsumerGroupOffsetsResult.all());\n        }\n\n        @SuppressWarnings(\"rawtypes\")\n        KafkaFuture<Void> compositeAdminFuture = KafkaFuture.allOf(adminFutures.toArray(new KafkaFuture[0]));\n\n        compositeAdminFuture.whenComplete((ignored, error) -> {\n            if (error != null) {\n                // When a consumer group is non-empty, only group members can commit offsets. An attempt to alter offsets via the admin client\n                // will result in an UnknownMemberIdException if the consumer group is non-empty (i.e. if the sink tasks haven't stopped\n                // completely or if the connector is resumed while the alter offsets request is being processed). Similarly, an attempt to\n                // delete consumer group offsets for a non-empty consumer group will result in a GroupSubscribedToTopicException\n                if (error instanceof UnknownMemberIdException || error instanceof GroupSubscribedToTopicException) {\n                    String errorMsg = \"Failed to alter consumer group offsets for connector \" + connName + \" either because its tasks \" +\n                            \"haven't stopped completely yet or the connector was resumed before the request to alter its offsets could be successfully \" +\n                            \"completed. If the connector is in a stopped state, this operation can be safely retried. If it doesn't eventually succeed, the \" +\n                            \"Connect cluster may need to be restarted to get rid of the zombie sink tasks.\";\n                    log.error(errorMsg, error);\n                    cb.onCompletion(new ConnectException(errorMsg, error), null);\n                } else {\n                    log.error(\"Failed to alter consumer group offsets for connector {}\", connName, error);\n                    cb.onCompletion(new ConnectException(\"Failed to alter consumer group offsets for connector \" + connName, error), null);\n                }\n            } else {\n                completeModifyOffsetsCallback(alterOffsetsResult, false, cb);\n            }\n        }).whenComplete((ignored, ignoredError) -> {\n            // errors originating from the original future are handled in the prior whenComplete invocation which isn't expected to throw\n            // an exception itself, and we can thus ignore the error here\n            Utils.closeQuietly(admin, \"Offset alter admin for sink connector \" + connName);\n        });\n    }\n\n    /**\n     * Reset a sink connector's consumer group offsets. This is done by deleting the consumer group via a call to\n     * {@link Admin#deleteConsumerGroups}\n     *\n     * @param connName the name of the sink connector whose offsets are to be reset\n     * @param groupId the sink connector's consumer group ID\n     * @param admin the {@link Admin admin client} to be used for resetting the consumer group offsets; will be closed after use\n     * @param cb callback to invoke upon completion\n     * @param alterOffsetsResult the result of the call to {@link SinkConnector#alterOffsets} for the connector\n     * @param timer {@link Timer} to bound the total runtime of admin client requests\n     */\n    private void resetSinkConnectorOffsets(String connName, String groupId, Admin admin, Callback<Message> cb, boolean alterOffsetsResult, Timer timer) {\n        DeleteConsumerGroupsOptions deleteConsumerGroupsOptions = new DeleteConsumerGroupsOptions().timeoutMs((int) timer.remainingMs());\n\n        admin.deleteConsumerGroups(Collections.singleton(groupId), deleteConsumerGroupsOptions)\n                .all()\n                .whenComplete((ignored, error) -> {\n                    // We treat GroupIdNotFoundException as a non-error here because resetting a connector's offsets is expected to be an idempotent operation\n                    // and the consumer group could have already been deleted in a prior offsets reset request\n                    if (error != null && !(error instanceof GroupIdNotFoundException)) {\n                        // When a consumer group is non-empty, attempts to delete it via the admin client result in a GroupNotEmptyException. This can occur\n                        // if the sink tasks haven't stopped completely or if the connector is resumed while the reset offsets request is being processed\n                        if (error instanceof GroupNotEmptyException) {\n                            String errorMsg = \"Failed to reset consumer group offsets for connector \" + connName + \" either because its tasks \" +\n                                    \"haven't stopped completely yet or the connector was resumed before the request to reset its offsets could be successfully \" +\n                                    \"completed. If the connector is in a stopped state, this operation can be safely retried. If it doesn't eventually succeed, the \" +\n                                    \"Connect cluster may need to be restarted to get rid of the zombie sink tasks.\";\n                            log.error(errorMsg, error);\n                            cb.onCompletion(new ConnectException(errorMsg, error), null);\n                        } else {\n                            log.error(\"Failed to reset consumer group offsets for sink connector {}\", connName, error);\n                            cb.onCompletion(new ConnectException(\"Failed to reset consumer group offsets for sink connector \" + connName, error), null);\n                        }\n                    } else {\n                        completeModifyOffsetsCallback(alterOffsetsResult, true, cb);\n                    }\n                }).whenComplete((ignored, ignoredError) -> {\n                    // errors originating from the original future are handled in the prior whenComplete invocation which isn't expected to throw\n                    // an exception itself, and we can thus ignore the error here\n                    Utils.closeQuietly(admin, \"Offset reset admin for sink connector \" + connName);\n                });\n    }\n\n    /**\n     * Modify (alter / reset) a source connector's offsets.\n     *\n     * @param connName the name of the source connector whose offsets are to be modified\n     * @param connector an instance of the source connector\n     * @param connectorConfig the source connector's configuration\n     * @param offsets a mapping from partitions to offsets that need to be written; this should be {@code null} for\n     *                offsets reset requests\n     * @param connectorLoader the connector plugin's classloader to be used as the thread context classloader\n     * @param cb callback to invoke upon completion\n     */\n    private void modifySourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                              Map<Map<String, ?>, Map<String, ?>> offsets, ClassLoader connectorLoader, Callback<Message> cb) {\n        SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins, connectorConfig, config.topicCreationEnable());\n        Map<String, Object> producerProps = config.exactlyOnceSourceEnabled()\n                ? exactlyOnceSourceTaskProducerConfigs(new ConnectorTaskId(connName, 0), config, sourceConfig,\n                connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId)\n                : baseProducerConfigs(connName, \"connector-offset-producer-\" + connName, config, sourceConfig,\n                connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId);\n        KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(producerProps);\n\n        ConnectorOffsetBackingStore offsetStore = config.exactlyOnceSourceEnabled()\n                ? offsetStoreForExactlyOnceSourceConnector(sourceConfig, connName, connector, producer)\n                : offsetStoreForRegularSourceConnector(sourceConfig, connName, connector, producer);\n        offsetStore.configure(config);\n\n        OffsetStorageWriter offsetWriter = new OffsetStorageWriter(offsetStore, connName, internalKeyConverter, internalValueConverter);\n        modifySourceConnectorOffsets(connName, connector, connectorConfig, offsets, offsetStore, producer, offsetWriter, connectorLoader, cb);\n    }\n\n    // Visible for testing\n    void modifySourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                      Map<Map<String, ?>, Map<String, ?>> offsets, ConnectorOffsetBackingStore offsetStore,\n                                      KafkaProducer<byte[], byte[]> producer, OffsetStorageWriter offsetWriter,\n                                      ClassLoader connectorLoader, Callback<Message> cb) {\n        executor.submit(plugins.withClassLoader(connectorLoader, () -> {\n            try {\n                Timer timer = time.timer(Duration.ofMillis(RestServer.DEFAULT_REST_REQUEST_TIMEOUT_MS));\n                // This reads to the end of the offsets topic and can be a potentially time-consuming operation\n                offsetStore.start();\n                updateTimerAndCheckExpiry(timer, \"Timed out while trying to read to the end of the offsets topic prior to modifying \" +\n                        \"offsets for source connector \" + connName);\n                Map<Map<String, ?>, Map<String, ?>> offsetsToWrite;\n\n                // If the offsets argument is null, it indicates an offsets reset operation - i.e. a null offset should\n                // be written for every source partition of the connector\n                boolean isReset;\n                if (offsets == null) {\n                    isReset = true;\n                    offsetsToWrite = new HashMap<>();\n                    offsetStore.connectorPartitions(connName).forEach(partition -> offsetsToWrite.put(partition, null));\n                    log.debug(\"Found the following partitions (to reset offsets) for source connector {}: {}\", connName, offsetsToWrite.keySet());\n                } else {\n                    isReset = false;\n                    offsetsToWrite = offsets;\n                }\n\n                Map<Map<String, ?>, Map<String, ?>> normalizedOffsets = normalizeSourceConnectorOffsets(offsetsToWrite);\n\n                boolean alterOffsetsResult;\n                try {\n                    alterOffsetsResult = ((SourceConnector) connector).alterOffsets(connectorConfig, normalizedOffsets);\n                } catch (UnsupportedOperationException e) {\n                    log.error(\"Failed to modify offsets for connector {} because it doesn't support external modification of offsets\",\n                            connName, e);\n                    throw new ConnectException(\"Failed to modify offsets for connector \" + connName + \" because it doesn't support external \" +\n                            \"modification of offsets\", e);\n                }\n                updateTimerAndCheckExpiry(timer, \"Timed out while calling the 'alterOffsets' method for source connector \" + connName);\n\n                // This should only occur for an offsets reset request when there are no source partitions found for the source connector in the\n                // offset store - either because there was a prior attempt to reset offsets or if there are no offsets committed by this source\n                // connector so far\n                if (normalizedOffsets.isEmpty()) {\n                    log.info(\"No offsets found for source connector {} - this can occur due to a prior attempt to reset offsets or if the \" +\n                            \"source connector hasn't committed any offsets yet\", connName);\n                    completeModifyOffsetsCallback(alterOffsetsResult, isReset, cb);\n                    return;\n                }\n\n                // The modifySourceConnectorOffsets method should only be called after all the connector's tasks have been stopped, and it's\n                // safe to write offsets via an offset writer\n                normalizedOffsets.forEach(offsetWriter::offset);\n\n                // We can call begin flush without a timeout because this newly created single-purpose offset writer can't do concurrent\n                // offset writes. We can also ignore the return value since it returns false if and only if there is no data to be flushed,\n                // and we've just put some data in the previous statement\n                offsetWriter.beginFlush();\n\n                if (config.exactlyOnceSourceEnabled()) {\n                    producer.initTransactions();\n                    producer.beginTransaction();\n                }\n                log.debug(\"Committing the following offsets for source connector {}: {}\", connName, normalizedOffsets);\n                FutureCallback<Void> offsetWriterCallback = new FutureCallback<>();\n                offsetWriter.doFlush(offsetWriterCallback);\n                if (config.exactlyOnceSourceEnabled()) {\n                    producer.commitTransaction();\n                }\n\n                try {\n                    offsetWriterCallback.get(timer.remainingMs(), TimeUnit.MILLISECONDS);\n                } catch (ExecutionException e) {\n                    throw new ConnectException(\"Failed to modify offsets for source connector \" + connName, e.getCause());\n                } catch (TimeoutException e) {\n                    throw new ConnectException(\"Timed out while attempting to modify offsets for source connector \" + connName, e);\n                } catch (InterruptedException e) {\n                    throw new ConnectException(\"Unexpectedly interrupted while attempting to modify offsets for source connector \" + connName, e);\n                }\n\n                completeModifyOffsetsCallback(alterOffsetsResult, isReset, cb);\n            } catch (Throwable t) {\n                log.error(\"Failed to modify offsets for source connector {}\", connName, t);\n                cb.onCompletion(ConnectUtils.maybeWrap(t, \"Failed to modify offsets for source connector \" + connName), null);\n            } finally {\n                Utils.closeQuietly(offsetStore::stop, \"Offset store for offset modification request for connector \" + connName);\n            }\n        }));\n    }\n\n    /**\n     * \"Normalize\" source connector offsets by serializing and deserializing them using the internal {@link JsonConverter}.\n     * This is done in order to prevent type mismatches between the offsets passed to {@link SourceConnector#alterOffsets(Map, Map)}\n     * and the offsets that connectors and tasks retrieve via an instance of {@link OffsetStorageReader}.\n     * <p>\n     * Visible for testing.\n     *\n     * @param originalOffsets the offsets that are to be normalized\n     * @return the normalized offsets\n     */\n    @SuppressWarnings(\"unchecked\")\n    Map<Map<String, ?>, Map<String, ?>> normalizeSourceConnectorOffsets(Map<Map<String, ?>, Map<String, ?>> originalOffsets) {\n        Map<Map<String, ?>, Map<String, ?>> normalizedOffsets = new HashMap<>();\n        for (Map.Entry<Map<String, ?>, Map<String, ?>> entry : originalOffsets.entrySet()) {\n            OffsetUtils.validateFormat(entry.getKey());\n            OffsetUtils.validateFormat(entry.getValue());\n            byte[] serializedKey = internalKeyConverter.fromConnectData(\"\", null, entry.getKey());\n            byte[] serializedValue = internalValueConverter.fromConnectData(\"\", null, entry.getValue());\n            Object deserializedKey = internalKeyConverter.toConnectData(\"\", serializedKey).value();\n            Object deserializedValue = internalValueConverter.toConnectData(\"\", serializedValue).value();\n            normalizedOffsets.put((Map<String, ?>) deserializedKey, (Map<String, ?>) deserializedValue);\n        }\n\n        return normalizedOffsets;\n    }\n\n    /**\n     * Update the provided timer, check if it's expired and throw a {@link ConnectException} with the provided error\n     * message if it is.\n     *\n     * @param timer {@link Timer} to check\n     * @param errorMessageIfExpired error message indicating the cause for the timer expiry\n     * @throws ConnectException if the timer has expired\n     */\n    private void updateTimerAndCheckExpiry(Timer timer, String errorMessageIfExpired) {\n        timer.update();\n        if (timer.isExpired()) {\n            log.error(errorMessageIfExpired);\n            throw new ConnectException(errorMessageIfExpired);\n        }\n    }\n\n    /**\n     * Complete the alter / reset offsets callback with a potential-success or a definite-success message.\n     *\n     * @param alterOffsetsResult the result of the call to {@link SinkConnector#alterOffsets} / {@link SourceConnector#alterOffsets}\n     * @param isReset whether this callback if for an offsets reset operation\n     * @param cb the callback to complete\n     *\n     * @see <a href=\"https://cwiki.apache.org/confluence/display/KAFKA/KIP-875%3A+First-class+offsets+support+in+Kafka+Connect\">KIP-875</a>\n     */\n    private void completeModifyOffsetsCallback(boolean alterOffsetsResult, boolean isReset, Callback<Message> cb) {\n        String modificationType = isReset ? \"reset\" : \"altered\";\n        if (alterOffsetsResult) {\n            cb.onCompletion(null, new Message(\"The offsets for this connector have been \" + modificationType + \" successfully\"));\n        } else {\n            cb.onCompletion(null, new Message(\"The Connect framework-managed offsets for this connector have been \" +\n                    modificationType + \" successfully. However, if this connector manages offsets externally, they will need to be \" +\n                    \"manually \" + modificationType + \" in the system that the connector uses.\"));\n        }\n    }\n\n    ConnectorStatusMetricsGroup connectorStatusMetricsGroup() {\n        return connectorStatusMetricsGroup;\n    }\n\n    WorkerMetricsGroup workerMetricsGroup() {\n        return workerMetricsGroup;\n    }\n\n    abstract class TaskBuilder<T, R extends ConnectRecord<R>> {\n\n        private final ConnectorTaskId id;\n        private final ClusterConfigState configState;\n        private final TaskStatus.Listener statusListener;\n        private final TargetState initialState;\n\n        private Task task = null;\n        private ConnectorConfig connectorConfig = null;\n        private Converter keyConverter = null;\n        private Converter valueConverter = null;\n        private HeaderConverter headerConverter = null;\n        private ClassLoader classLoader = null;\n\n        public TaskBuilder(ConnectorTaskId id,\n                           ClusterConfigState configState,\n                           TaskStatus.Listener statusListener,\n                           TargetState initialState) {\n            this.id = id;\n            this.configState = configState;\n            this.statusListener = statusListener;\n            this.initialState = initialState;\n        }\n\n        public TaskBuilder<T, R> withTask(Task task) {\n            this.task = task;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withConnectorConfig(ConnectorConfig connectorConfig) {\n            this.connectorConfig = connectorConfig;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withKeyConverter(Converter keyConverter) {\n            this.keyConverter = keyConverter;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withValueConverter(Converter valueConverter) {\n            this.valueConverter = valueConverter;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withHeaderConverter(HeaderConverter headerConverter) {\n            this.headerConverter = headerConverter;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withClassloader(ClassLoader classLoader) {\n            this.classLoader = classLoader;\n            return this;\n        }\n\n        public WorkerTask<T, R> build() {\n            Objects.requireNonNull(task, \"Task cannot be null\");\n            Objects.requireNonNull(connectorConfig, \"Connector config used by task cannot be null\");\n            Objects.requireNonNull(keyConverter, \"Key converter used by task cannot be null\");\n            Objects.requireNonNull(valueConverter, \"Value converter used by task cannot be null\");\n            Objects.requireNonNull(headerConverter, \"Header converter used by task cannot be null\");\n            Objects.requireNonNull(classLoader, \"Classloader used by task cannot be null\");\n\n            ErrorHandlingMetrics errorHandlingMetrics = errorHandlingMetrics(id);\n            final Class<? extends Connector> connectorClass = plugins.connectorClass(\n                    connectorConfig.getString(ConnectorConfig.CONNECTOR_CLASS_CONFIG));\n\n            RetryWithToleranceOperator<T> retryWithToleranceOperator = new RetryWithToleranceOperator<>(connectorConfig.errorRetryTimeout(),\n                    connectorConfig.errorMaxDelayInMillis(), connectorConfig.errorToleranceType(), Time.SYSTEM, errorHandlingMetrics);\n\n            TransformationChain<T, R> transformationChain = new TransformationChain<>(connectorConfig.<R>transformationStages(), retryWithToleranceOperator);\n            log.info(\"Initializing: {}\", transformationChain);\n\n            return doBuild(task, id, configState, statusListener, initialState,\n                    connectorConfig, keyConverter, valueConverter, headerConverter, classLoader,\n                    retryWithToleranceOperator, transformationChain,\n                    errorHandlingMetrics, connectorClass);\n        }\n\n        abstract WorkerTask<T, R> doBuild(\n                Task task,\n                ConnectorTaskId id,\n                ClusterConfigState configState,\n                TaskStatus.Listener statusListener,\n                TargetState initialState,\n                ConnectorConfig connectorConfig,\n                Converter keyConverter,\n                Converter valueConverter,\n                HeaderConverter headerConverter,\n                ClassLoader classLoader,\n                RetryWithToleranceOperator<T> retryWithToleranceOperator,\n                TransformationChain<T, R> transformationChain,\n                ErrorHandlingMetrics errorHandlingMetrics,\n                Class<? extends Connector> connectorClass\n        );\n\n    }\n\n    class SinkTaskBuilder extends TaskBuilder<ConsumerRecord<byte[], byte[]>, SinkRecord> {\n        public SinkTaskBuilder(ConnectorTaskId id,\n                               ClusterConfigState configState,\n                               TaskStatus.Listener statusListener,\n                               TargetState initialState) {\n            super(id, configState, statusListener, initialState);\n        }\n\n        @Override\n        public WorkerTask<ConsumerRecord<byte[], byte[]>, SinkRecord> doBuild(\n                Task task,\n                ConnectorTaskId id,\n                ClusterConfigState configState,\n                TaskStatus.Listener statusListener,\n                TargetState initialState,\n                ConnectorConfig connectorConfig,\n                Converter keyConverter,\n                Converter valueConverter,\n                HeaderConverter headerConverter,\n                ClassLoader classLoader,\n                RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n                TransformationChain<ConsumerRecord<byte[], byte[]>, SinkRecord> transformationChain,\n                ErrorHandlingMetrics errorHandlingMetrics,\n                Class<? extends Connector> connectorClass\n        ) {\n            SinkConnectorConfig sinkConfig = new SinkConnectorConfig(plugins, connectorConfig.originalsStrings());\n            WorkerErrantRecordReporter workerErrantRecordReporter = createWorkerErrantRecordReporter(sinkConfig, retryWithToleranceOperator,\n                    keyConverter, valueConverter, headerConverter);\n\n            Map<String, Object> consumerProps = baseConsumerConfigs(\n                    id.connector(),  \"connector-consumer-\" + id, config, connectorConfig, connectorClass,\n                    connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SINK);\n            KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n            return new WorkerSinkTask(id, (SinkTask) task, statusListener, initialState, config, configState, metrics, keyConverter,\n                    valueConverter, errorHandlingMetrics, headerConverter, transformationChain, consumer, classLoader, time,\n                    retryWithToleranceOperator, workerErrantRecordReporter, herder.statusBackingStore(),\n                    () -> sinkTaskReporters(id, sinkConfig, errorHandlingMetrics, connectorClass));\n        }\n    }\n\n    class SourceTaskBuilder extends TaskBuilder<SourceRecord, SourceRecord> {\n        public SourceTaskBuilder(ConnectorTaskId id,\n                               ClusterConfigState configState,\n                               TaskStatus.Listener statusListener,\n                               TargetState initialState) {\n            super(id, configState, statusListener, initialState);\n        }\n\n        @Override\n        public WorkerTask<SourceRecord, SourceRecord> doBuild(\n                Task task,\n                ConnectorTaskId id,\n                ClusterConfigState configState,\n                TaskStatus.Listener statusListener,\n                TargetState initialState,\n                ConnectorConfig connectorConfig,\n                Converter keyConverter,\n                Converter valueConverter,\n                HeaderConverter headerConverter,\n                ClassLoader classLoader,\n                RetryWithToleranceOperator<SourceRecord> retryWithToleranceOperator,\n                TransformationChain<SourceRecord, SourceRecord> transformationChain,\n                ErrorHandlingMetrics errorHandlingMetrics,\n                Class<? extends Connector> connectorClass\n        ) {\n            SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins,\n                    connectorConfig.originalsStrings(), config.topicCreationEnable());\n\n            Map<String, Object> producerProps = baseProducerConfigs(id.connector(), \"connector-producer-\" + id, config, sourceConfig, connectorClass,\n                    connectorClientConfigOverridePolicy, kafkaClusterId);\n            KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(producerProps);\n\n            TopicAdmin topicAdmin = null;\n            final boolean topicCreationEnabled = sourceConnectorTopicCreationEnabled(sourceConfig);\n            if (topicCreationEnabled || regularSourceTaskUsesConnectorSpecificOffsetsStore(sourceConfig)) {\n                Map<String, Object> adminOverrides = adminConfigs(id.connector(), \"connector-adminclient-\" + id, config,\n                        sourceConfig, connectorClass, connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SOURCE);\n                topicAdmin = new TopicAdmin(adminOverrides);\n            }\n\n            Map<String, TopicCreationGroup> topicCreationGroups = topicCreationEnabled\n                    ? TopicCreationGroup.configuredGroups(sourceConfig)\n                    : null;\n\n            // Set up the offset backing store for this task instance\n            ConnectorOffsetBackingStore offsetStore = offsetStoreForRegularSourceTask(\n                    id, sourceConfig, connectorClass, producer, producerProps, topicAdmin);\n            offsetStore.configure(config);\n\n            CloseableOffsetStorageReader offsetReader = new OffsetStorageReaderImpl(offsetStore, id.connector(), internalKeyConverter, internalValueConverter);\n            OffsetStorageWriter offsetWriter = new OffsetStorageWriter(offsetStore, id.connector(), internalKeyConverter, internalValueConverter);\n\n            // Note we pass the configState as it performs dynamic transformations under the covers\n            return new WorkerSourceTask(id, (SourceTask) task, statusListener, initialState, keyConverter, valueConverter, errorHandlingMetrics,\n                    headerConverter, transformationChain, producer, topicAdmin, topicCreationGroups,\n                    offsetReader, offsetWriter, offsetStore, config, configState, metrics, classLoader, time,\n                    retryWithToleranceOperator, herder.statusBackingStore(), executor, () -> sourceTaskReporters(id, sourceConfig, errorHandlingMetrics));\n        }\n    }\n\n    class ExactlyOnceSourceTaskBuilder extends TaskBuilder<SourceRecord, SourceRecord> {\n        private final Runnable preProducerCheck;\n        private final Runnable postProducerCheck;\n\n        public ExactlyOnceSourceTaskBuilder(ConnectorTaskId id,\n                                            ClusterConfigState configState,\n                                            TaskStatus.Listener statusListener,\n                                            TargetState initialState,\n                                            Runnable preProducerCheck,\n                                            Runnable postProducerCheck) {\n            super(id, configState, statusListener, initialState);\n            this.preProducerCheck = preProducerCheck;\n            this.postProducerCheck = postProducerCheck;\n        }\n\n        @Override\n        public WorkerTask<SourceRecord, SourceRecord> doBuild(\n                Task task,\n                ConnectorTaskId id,\n                ClusterConfigState configState,\n                TaskStatus.Listener statusListener,\n                TargetState initialState,\n                ConnectorConfig connectorConfig,\n                Converter keyConverter,\n                Converter valueConverter,\n                HeaderConverter headerConverter,\n                ClassLoader classLoader,\n                RetryWithToleranceOperator<SourceRecord> retryWithToleranceOperator,\n                TransformationChain<SourceRecord, SourceRecord> transformationChain,\n                ErrorHandlingMetrics errorHandlingMetrics,\n                Class<? extends Connector> connectorClass\n        ) {\n            SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins,\n                    connectorConfig.originalsStrings(), config.topicCreationEnable());\n            Map<String, Object> producerProps = exactlyOnceSourceTaskProducerConfigs(\n                    id, config, sourceConfig, connectorClass,\n                    connectorClientConfigOverridePolicy, kafkaClusterId);\n            KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(producerProps);\n\n            // Create a topic admin that the task will use for its offsets topic and, potentially, automatic topic creation\n            Map<String, Object> adminOverrides = adminConfigs(id.connector(), \"connector-adminclient-\" + id, config,\n                    sourceConfig, connectorClass, connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SOURCE);\n            TopicAdmin topicAdmin = new TopicAdmin(adminOverrides);\n\n            Map<String, TopicCreationGroup> topicCreationGroups = sourceConnectorTopicCreationEnabled(sourceConfig)\n                    ? TopicCreationGroup.configuredGroups(sourceConfig)\n                    : null;\n\n            // Set up the offset backing store for this task instance\n            ConnectorOffsetBackingStore offsetStore = offsetStoreForExactlyOnceSourceTask(\n                    id, sourceConfig, connectorClass, producer, producerProps, topicAdmin);\n            offsetStore.configure(config);\n\n            CloseableOffsetStorageReader offsetReader = new OffsetStorageReaderImpl(offsetStore, id.connector(), internalKeyConverter, internalValueConverter);\n            OffsetStorageWriter offsetWriter = new OffsetStorageWriter(offsetStore, id.connector(), internalKeyConverter, internalValueConverter);\n\n            // Note we pass the configState as it performs dynamic transformations under the covers\n            return new ExactlyOnceWorkerSourceTask(id, (SourceTask) task, statusListener, initialState, keyConverter, valueConverter,\n                    headerConverter, transformationChain, producer, topicAdmin, topicCreationGroups,\n                    offsetReader, offsetWriter, offsetStore, config, configState, metrics, errorHandlingMetrics, classLoader, time, retryWithToleranceOperator,\n                    herder.statusBackingStore(), sourceConfig, executor, preProducerCheck, postProducerCheck,\n                    () -> sourceTaskReporters(id, sourceConfig, errorHandlingMetrics));\n        }\n    }\n\n    /**\n     * Builds and returns an offset backing store for a regular source connector (i.e. when exactly-once support for source connectors is disabled).\n     * The offset backing store will either be just the worker's global offset backing store (if the connector doesn't define a connector-specific\n     * offset topic via its configs), just a connector-specific offset backing store (if the connector defines a connector-specific offsets\n     * topic which appears to be the same as the worker's global offset topic) or a combination of both the worker's global offset backing store\n     * and a connector-specific offset backing store.\n     * <p>\n     * Visible for testing.\n     * @param sourceConfig the source connector's config\n     * @param connName the source connector's name\n     * @param connector the source connector\n     * @param producer the Kafka producer for the offset backing store; may be {@code null} if a read-only offset backing store is required\n     * @return An offset backing store for a regular source connector\n     */\n    ConnectorOffsetBackingStore offsetStoreForRegularSourceConnector(\n            SourceConnectorConfig sourceConfig,\n            String connName,\n            Connector connector,\n            Producer<byte[], byte[]> producer\n    ) {\n        String connectorSpecificOffsetsTopic = sourceConfig.offsetsTopic();\n\n        Map<String, Object> producerProps = baseProducerConfigs(connName, \"connector-producer-\" + connName, config, sourceConfig, connector.getClass(),\n                connectorClientConfigOverridePolicy, kafkaClusterId);\n\n        // We use a connector-specific store (i.e., a dedicated KafkaOffsetBackingStore for this connector)\n        // if the worker supports per-connector offsets topics (which may be the case in distributed but not standalone mode, for example)\n        // and if the connector is explicitly configured with an offsets topic\n        final boolean usesConnectorSpecificStore = connectorSpecificOffsetsTopic != null\n                && config.connectorOffsetsTopicsPermitted();\n\n        if (usesConnectorSpecificStore) {\n            Map<String, Object> consumerProps = regularSourceOffsetsConsumerConfigs(\n                        connName, \"connector-consumer-\" + connName, config, sourceConfig, connector.getClass(),\n                        connectorClientConfigOverridePolicy, kafkaClusterId);\n            KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n            Map<String, Object> adminOverrides = adminConfigs(connName, \"connector-adminclient-\" + connName, config,\n                    sourceConfig, connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SOURCE);\n\n            TopicAdmin admin = new TopicAdmin(adminOverrides);\n\n            KafkaOffsetBackingStore connectorStore = producer == null\n                    ? KafkaOffsetBackingStore.readOnlyStore(connectorSpecificOffsetsTopic, consumer, admin, internalKeyConverter)\n                    : KafkaOffsetBackingStore.readWriteStore(connectorSpecificOffsetsTopic, producer, consumer, admin, internalKeyConverter);\n\n            // If the connector's offsets topic is the same as the worker-global offsets topic, there's no need to construct\n            // an offset store that has a primary and a secondary store which both read from that same topic.\n            // So, if the user has explicitly configured the connector with a connector-specific offsets topic\n            // but we know that that topic is the same as the worker-global offsets topic, we ignore the worker-global\n            // offset store and build a store backed exclusively by a connector-specific offsets store.\n            // It may seem reasonable to instead build a store backed exclusively by the worker-global offset store, but that\n            // would prevent users from being able to customize the config properties used for the Kafka clients that\n            // access the offsets topic, and we would not be able to establish reasonable defaults like setting\n            // isolation.level=read_committed for the offsets topic consumer for this connector\n            if (sameOffsetTopicAsWorker(connectorSpecificOffsetsTopic, producerProps)) {\n                return ConnectorOffsetBackingStore.withOnlyConnectorStore(\n                        () -> LoggingContext.forConnector(connName),\n                        connectorStore,\n                        connectorSpecificOffsetsTopic,\n                        admin\n                );\n            } else {\n                return ConnectorOffsetBackingStore.withConnectorAndWorkerStores(\n                        () -> LoggingContext.forConnector(connName),\n                        globalOffsetBackingStore,\n                        connectorStore,\n                        connectorSpecificOffsetsTopic,\n                        admin\n                );\n            }\n        } else {\n            Utils.closeQuietly(producer, \"Unused producer for offset store\");\n            return ConnectorOffsetBackingStore.withOnlyWorkerStore(\n                    () -> LoggingContext.forConnector(connName),\n                    globalOffsetBackingStore,\n                    config.offsetsTopic()\n            );\n        }\n    }\n\n    /**\n     * Builds and returns an offset backing store for an exactly-once source connector. The offset backing store will either be just\n     * a connector-specific offset backing store (if the connector's offsets topic is the same as the worker's global offset topic)\n     * or a combination of both the worker's global offset backing store and a connector-specific offset backing store.\n     * <p>\n     * Visible for testing.\n     * @param sourceConfig the source connector's config\n     * @param connName the source connector's name\n     * @param connector the source connector\n     * @param producer the Kafka producer for the offset backing store; may be {@code null} if a read-only offset backing store is required\n     * @return An offset backing store for an exactly-once source connector\n     */\n    ConnectorOffsetBackingStore offsetStoreForExactlyOnceSourceConnector(\n            SourceConnectorConfig sourceConfig,\n            String connName,\n            Connector connector,\n            Producer<byte[], byte[]> producer\n    ) {\n        String connectorSpecificOffsetsTopic = Optional.ofNullable(sourceConfig.offsetsTopic()).orElse(config.offsetsTopic());\n\n        Map<String, Object> producerProps = baseProducerConfigs(connName, \"connector-producer-\" + connName, config, sourceConfig, connector.getClass(),\n                connectorClientConfigOverridePolicy, kafkaClusterId);\n\n        Map<String, Object> consumerProps = exactlyOnceSourceOffsetsConsumerConfigs(\n                    connName, \"connector-consumer-\" + connName, config, sourceConfig, connector.getClass(),\n                    connectorClientConfigOverridePolicy, kafkaClusterId);\n        KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n        Map<String, Object> adminOverrides = adminConfigs(connName, \"connector-adminclient-\" + connName, config,\n                sourceConfig, connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SOURCE);\n\n        TopicAdmin admin = new TopicAdmin(adminOverrides);\n\n        KafkaOffsetBackingStore connectorStore = producer == null\n                ? KafkaOffsetBackingStore.readOnlyStore(connectorSpecificOffsetsTopic, consumer, admin, internalKeyConverter)\n                : KafkaOffsetBackingStore.readWriteStore(connectorSpecificOffsetsTopic, producer, consumer, admin, internalKeyConverter);\n\n        // If the connector's offsets topic is the same as the worker-global offsets topic, there's no need to construct\n        // an offset store that has a primary and a secondary store which both read from that same topic.\n        // So, even if the user has explicitly configured the connector with a connector-specific offsets topic,\n        // if we know that that topic is the same as the worker-global offsets topic, we ignore the worker-global\n        // offset store and build a store backed exclusively by a connector-specific offsets store.\n        // It may seem reasonable to instead build a store backed exclusively by the worker-global offset store, but that\n        // would prevent users from being able to customize the config properties used for the Kafka clients that\n        // access the offsets topic, and may lead to confusion for them when tasks are created for the connector\n        // since they will all have their own dedicated offsets stores anyways\n        if (sameOffsetTopicAsWorker(connectorSpecificOffsetsTopic, producerProps)) {\n            return ConnectorOffsetBackingStore.withOnlyConnectorStore(\n                    () -> LoggingContext.forConnector(connName),\n                    connectorStore,\n                    connectorSpecificOffsetsTopic,\n                    admin\n            );\n        } else {\n            return ConnectorOffsetBackingStore.withConnectorAndWorkerStores(\n                    () -> LoggingContext.forConnector(connName),\n                    globalOffsetBackingStore,\n                    connectorStore,\n                    connectorSpecificOffsetsTopic,\n                    admin\n            );\n        }\n    }\n\n    // Visible for testing\n    ConnectorOffsetBackingStore offsetStoreForRegularSourceTask(\n            ConnectorTaskId id,\n            SourceConnectorConfig sourceConfig,\n            Class<? extends Connector> connectorClass,\n            Producer<byte[], byte[]> producer,\n            Map<String, Object> producerProps,\n            TopicAdmin topicAdmin\n    ) {\n        String connectorSpecificOffsetsTopic = sourceConfig.offsetsTopic();\n\n        if (regularSourceTaskUsesConnectorSpecificOffsetsStore(sourceConfig)) {\n            Objects.requireNonNull(topicAdmin, \"Source tasks require a non-null topic admin when configured to use their own offsets topic\");\n\n            Map<String, Object> consumerProps = regularSourceOffsetsConsumerConfigs(\n                    id.connector(), \"connector-consumer-\" + id, config, sourceConfig, connectorClass,\n                    connectorClientConfigOverridePolicy, kafkaClusterId);\n            KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n            KafkaOffsetBackingStore connectorStore =\n                    KafkaOffsetBackingStore.readWriteStore(sourceConfig.offsetsTopic(), producer, consumer, topicAdmin, internalKeyConverter);\n\n            // If the connector's offsets topic is the same as the worker-global offsets topic, there's no need to construct\n            // an offset store that has a primary and a secondary store which both read from that same topic.\n            // So, if the user has (implicitly or explicitly) configured the connector with a connector-specific offsets topic\n            // but we know that that topic is the same as the worker-global offsets topic, we ignore the worker-global\n            // offset store and build a store backed exclusively by a connector-specific offsets store.\n            // It may seem reasonable to instead build a store backed exclusively by the worker-global offset store, but that\n            // would prevent users from being able to customize the config properties used for the Kafka clients that\n            // access the offsets topic, and we would not be able to establish reasonable defaults like setting\n            // isolation.level=read_committed for the offsets topic consumer for this task\n            if (sameOffsetTopicAsWorker(sourceConfig.offsetsTopic(), producerProps)) {\n                return ConnectorOffsetBackingStore.withOnlyConnectorStore(\n                        () -> LoggingContext.forTask(id),\n                        connectorStore,\n                        connectorSpecificOffsetsTopic,\n                        topicAdmin\n                );\n            } else {\n                return ConnectorOffsetBackingStore.withConnectorAndWorkerStores(\n                        () -> LoggingContext.forTask(id),\n                        globalOffsetBackingStore,\n                        connectorStore,\n                        connectorSpecificOffsetsTopic,\n                        topicAdmin\n                );\n            }\n        } else {\n            return ConnectorOffsetBackingStore.withOnlyWorkerStore(\n                    () -> LoggingContext.forTask(id),\n                    globalOffsetBackingStore,\n                    config.offsetsTopic()\n            );\n        }\n    }\n\n    // Visible for testing\n    ConnectorOffsetBackingStore offsetStoreForExactlyOnceSourceTask(\n            ConnectorTaskId id,\n            SourceConnectorConfig sourceConfig,\n            Class<? extends Connector> connectorClass,\n            Producer<byte[], byte[]> producer,\n            Map<String, Object> producerProps,\n            TopicAdmin topicAdmin\n    ) {\n        Objects.requireNonNull(topicAdmin, \"Source tasks require a non-null topic admin when exactly-once support is enabled\");\n\n        Map<String, Object> consumerProps = exactlyOnceSourceOffsetsConsumerConfigs(\n                id.connector(), \"connector-consumer-\" + id, config, sourceConfig, connectorClass,\n                connectorClientConfigOverridePolicy, kafkaClusterId);\n        KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n        String connectorOffsetsTopic = Optional.ofNullable(sourceConfig.offsetsTopic()).orElse(config.offsetsTopic());\n\n        KafkaOffsetBackingStore connectorStore =\n                KafkaOffsetBackingStore.readWriteStore(connectorOffsetsTopic, producer, consumer, topicAdmin, internalKeyConverter);\n\n        // If the connector's offsets topic is the same as the worker-global offsets topic, there's no need to construct\n        // an offset store that has a primary and a secondary store which both read from that same topic.\n        // So, if the user has (implicitly or explicitly) configured the connector with a connector-specific offsets topic\n        // but we know that that topic is the same as the worker-global offsets topic, we ignore the worker-global\n        // offset store and build a store backed exclusively by a connector-specific offsets store.\n        // We cannot under any circumstances build an offset store backed exclusively by the worker-global offset store\n        // as that would prevent us from being able to write source records and source offset information for the task\n        // with the same producer, and therefore, in the same transaction.\n        if (sameOffsetTopicAsWorker(connectorOffsetsTopic, producerProps)) {\n            return ConnectorOffsetBackingStore.withOnlyConnectorStore(\n                    () -> LoggingContext.forTask(id),\n                    connectorStore,\n                    connectorOffsetsTopic,\n                    topicAdmin\n            );\n        } else {\n            return ConnectorOffsetBackingStore.withConnectorAndWorkerStores(\n                    () -> LoggingContext.forTask(id),\n                    globalOffsetBackingStore,\n                    connectorStore,\n                    connectorOffsetsTopic,\n                    topicAdmin\n            );\n        }\n    }\n\n    /**\n     * Gives a best-effort guess for whether the given offsets topic is the same topic as the worker-global offsets topic.\n     * Even if the name of the topic is the same as the name of the worker's offsets topic, the two may still be different topics\n     * if the connector is configured to produce to a different Kafka cluster than the one that hosts the worker's offsets topic.\n     * @param offsetsTopic the name of the offsets topic for the connector\n     * @param producerProps the producer configuration for the connector\n     * @return whether it appears that the connector's offsets topic is the same topic as the worker-global offsets topic.\n     * If {@code true}, it is guaranteed that the two are the same;\n     * if {@code false}, it is likely but not guaranteed that the two are not the same\n     */\n    private boolean sameOffsetTopicAsWorker(String offsetsTopic, Map<String, Object> producerProps) {\n        // We can check the offset topic name and the Kafka cluster's bootstrap servers,\n        // although this isn't exact and can lead to some false negatives if the user\n        // provides an overridden bootstrap servers value for their producer that is different than\n        // the worker's but still resolves to the same Kafka cluster used by the worker.\n        // At the moment this is probably adequate, especially since we don't want to put\n        // a network ping to a remote Kafka cluster inside the herder's tick thread (which is where this\n        // logic takes place right now) in case that takes a while.\n        Set<String> workerBootstrapServers = new HashSet<>(config.getList(BOOTSTRAP_SERVERS_CONFIG));\n        Set<String> producerBootstrapServers = new HashSet<>();\n        try {\n            String rawBootstrapServers = producerProps.getOrDefault(BOOTSTRAP_SERVERS_CONFIG, \"\").toString();\n            @SuppressWarnings(\"unchecked\")\n            List<String> parsedBootstrapServers = (List<String>) ConfigDef.parseType(BOOTSTRAP_SERVERS_CONFIG, rawBootstrapServers, ConfigDef.Type.LIST);\n            producerBootstrapServers.addAll(parsedBootstrapServers);\n        } catch (Exception e) {\n            // Should never happen by this point, but if it does, make sure to present a readable error message to the user\n            throw new ConnectException(\"Failed to parse bootstrap servers property in producer config\", e);\n        }\n        return offsetsTopic.equals(config.offsetsTopic())\n                && workerBootstrapServers.equals(producerBootstrapServers);\n    }\n\n    private boolean regularSourceTaskUsesConnectorSpecificOffsetsStore(SourceConnectorConfig sourceConfig) {\n        // We use a connector-specific store (i.e., a dedicated KafkaOffsetBackingStore for this task)\n        // if the worker supports per-connector offsets topics (which may be the case in distributed mode but not standalone, for example)\n        // and the user has explicitly specified an offsets topic for the connector\n        return sourceConfig.offsetsTopic() != null && config.connectorOffsetsTopicsPermitted();\n    }\n\n    private boolean sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig) {\n        return config.topicCreationEnable() && sourceConfig.usesTopicCreation();\n    }\n\n    static class ConnectorStatusMetricsGroup {\n        private final ConnectMetrics connectMetrics;\n        private final ConnectMetricsRegistry registry;\n        private final ConcurrentMap<String, MetricGroup> connectorStatusMetrics = new ConcurrentHashMap<>();\n        private final Herder herder;\n        private final ConcurrentMap<ConnectorTaskId, WorkerTask<?, ?>> tasks;\n\n\n        protected ConnectorStatusMetricsGroup(\n                ConnectMetrics connectMetrics, ConcurrentMap<ConnectorTaskId, WorkerTask<?, ?>> tasks, Herder herder) {\n            this.connectMetrics = connectMetrics;\n            this.registry = connectMetrics.registry();\n            this.tasks = tasks;\n            this.herder = herder;\n        }\n\n        protected ConnectMetrics.LiteralSupplier<Long> taskCounter(String connName) {\n            return now -> tasks.keySet()\n                .stream()\n                .filter(taskId -> taskId.connector().equals(connName))\n                .count();\n        }\n\n        protected ConnectMetrics.LiteralSupplier<Long> taskStatusCounter(String connName, TaskStatus.State state) {\n            return now -> tasks.values()\n                .stream()\n                .filter(task ->\n                    task.id().connector().equals(connName) &&\n                    herder.taskStatus(task.id()).state().equalsIgnoreCase(state.toString()))\n                .count();\n        }\n\n        protected synchronized void recordTaskAdded(ConnectorTaskId connectorTaskId) {\n            if (connectorStatusMetrics.containsKey(connectorTaskId.connector())) {\n                return;\n            }\n\n            String connName = connectorTaskId.connector();\n\n            MetricGroup metricGroup = connectMetrics.group(registry.workerGroupName(),\n                registry.connectorTagName(), connName);\n\n            metricGroup.addValueMetric(registry.connectorTotalTaskCount, taskCounter(connName));\n            for (Map.Entry<MetricNameTemplate, TaskStatus.State> statusMetric : registry.connectorStatusMetrics\n                .entrySet()) {\n                metricGroup.addValueMetric(statusMetric.getKey(), taskStatusCounter(connName,\n                    statusMetric.getValue()));\n            }\n            connectorStatusMetrics.put(connectorTaskId.connector(), metricGroup);\n        }\n\n        protected synchronized void recordTaskRemoved(ConnectorTaskId connectorTaskId) {\n            // Unregister connector task count metric if we remove the last task of the connector\n            if (tasks.keySet().stream().noneMatch(id -> id.connector().equals(connectorTaskId.connector()))) {\n                connectorStatusMetrics.get(connectorTaskId.connector()).close();\n                connectorStatusMetrics.remove(connectorTaskId.connector());\n            }\n        }\n\n        protected synchronized void close() {\n            for (MetricGroup metricGroup: connectorStatusMetrics.values()) {\n                metricGroup.close();\n            }\n        }\n\n        protected MetricGroup metricGroup(String connectorId) {\n            return connectorStatusMetrics.get(connectorId);\n        }\n    }\n\n}",
                "methodCount": 97
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 14,
                "candidates": [
                    {
                        "lineStart": 2265,
                        "lineEnd": 2270,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method taskCounter to class Herder",
                        "description": "Move method taskCounter to org.apache.kafka.connect.runtime.Herder\nRationale: The method taskCounter() is concerned with counting tasks related to a specific connector, a responsibility that falls within the domain of the Herder class. The Herder class is designed to manage workers and connectors, making it responsible for actions like obtaining the state of connectors and tasks. Therefore, placing taskCounter() in Herder allows this method to leverage existing functionalities more naturally and aligns it with the Herder's responsibilities to manage tasks and connectors coherently.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2272,
                        "lineEnd": 2279,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method taskStatusCounter to class Herder",
                        "description": "Move method taskStatusCounter to org.apache.kafka.connect.runtime.Herder\nRationale: The method taskStatusCounter() heavily interacts with task IDs and their statuses, which are managed by the Herder class. The use of methods such as herder.taskStatus() suggests that the logic of this method is deeply connected to the responsibilities of the Herder. Moving the method to the Herder class centralizes task status operations and can enhance cohesion by keeping related functionalities together.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2245,
                        "lineEnd": 2247,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method sourceConnectorTopicCreationEnabled to class WorkerConfig",
                        "description": "Move method sourceConnectorTopicCreationEnabled to org.apache.kafka.connect.runtime.WorkerConfig\nRationale: The method `sourceConnectorTopicCreationEnabled` checks if topic creation is enabled and whether a source config uses topic creation. This functionality fits better within the `WorkerConfig` class, which is responsible for managing configurations related to the worker, including whether topic creation is enabled. The method directly operates on configuration settings that are already handled by the `WorkerConfig` class, like `topicCreationEnable()`. Therefore, it makes sense for this method to be moved to `WorkerConfig`.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 948,
                        "lineEnd": 950,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method taskTransactionalId to class WorkerConfig",
                        "description": "Move method taskTransactionalId to org.apache.kafka.connect.runtime.WorkerConfig\nRationale: The method `taskTransactionalId(ConnectorTaskId id)` heavily relies on the configuration values, specifically `config.groupId()`. Since this configuration aspect is managed and better suited within the `WorkerConfig` class, it makes the most sense for this method to be relocated here. This ensures that all configuration-related methods remain centralized, maintaining a cohesive structure in the codebase.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 816,
                        "lineEnd": 832,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method exactlyOnceSourceOffsetsConsumerConfigs to class SourceTaskBuilder",
                        "description": "move method exactlyOnceSourceOffsetsConsumerConfigs to PsiClass:SourceTaskBuilder\nRationale: The method exactlyOnceSourceOffsetsConsumerConfigs() configures specific settings for source connectors when exactly-once semantics are required. The only suitable class that directly deals with source tasks and their configurations, including consumer settings, is SourceTaskBuilder. The method is closely related to the logic within SourceTaskBuilder, especially its responsibility in constructing and managing consumer configurations for source tasks. Moving the method here makes the class more cohesive, ensuring that all the configurations related to source tasks are kept together.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 415,
                        "lineEnd": 418,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method tasksMax to class WorkerMetricsGroup",
                        "description": "Move method tasksMax to org.apache.kafka.connect.runtime.WorkerMetricsGroup\nRationale: The method 'tasksMax' retrieves the maximum number of tasks from the ConnectorConfig, which is a component that directly affects metrics such as task counts and startup attempts/successes/failures measured within WorkerMetricsGroup. By moving the method into WorkerMetricsGroup, it can directly utilize the maximum task value for initializing or updating relevant metrics, enhancing cohesion and encapsulating all task-related metrics logic within one class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 995,
                        "lineEnd": 1007,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method createWorkerErrantRecordReporter to class ConnectorStatusMetricsGroup",
                        "description": "Move method createWorkerErrantRecordReporter to org.apache.kafka.connect.runtime.Worker.ConnectorStatusMetricsGroup\nRationale: The method 'createWorkerErrantRecordReporter' is closely related to metrics and error reporting for workers, which fits well within the responsibilities of the ConnectorStatusMetricsGroup class. The existing WorkerErrantRecordReporter creation and its enabling configuration are aligned better with tasks and their statuses, which ConnectorStatusMetricsGroup manages.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 228,
                        "lineEnd": 243,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method start to class Herder",
                        "description": "Move method start to org.apache.kafka.connect.runtime.Herder\nRationale: The start() method appears to be responsible for initializing various components and configuring tasks related to a worker. Herder class is designed to manage workers, connectors, and should be responsible for initiating various processes. This suggests that start() truly belongs to the Herder class, as it aligns closely with its purpose of orchestrating and controlling the lifecycle and state of connectors and tasks.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2308,
                        "lineEnd": 2312,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method close to class Herder",
                        "description": "Move method close to org.apache.kafka.connect.runtime.Herder\nRationale: The close() method involves closing metric groups, which seems to be associated with the management and cleanup of resources related to connectors. The Herder class is responsible for managing connectors and their states in the cluster, making it well-suited to handle resource management tasks such as this one. Additionally, moving close() to Herder aligns with its responsibilities for handling the lifecycle actions of connectors, further encapsulating the logic related to connector management.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 469,
                        "lineEnd": 476,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method awaitStopConnectors to class ConnectorStatusMetricsGroup",
                        "description": "Move method awaitStopConnectors to org.apache.kafka.connect.runtime.Worker.ConnectorStatusMetricsGroup\nRationale: The `awaitStopConnectors` method deals with stopping connectors, and the `ConnectorStatusMetricsGroup` class handles metrics for connectors and tasks. By moving the method to this class, we keep all connector-related operations together, thereby enhancing cohesion. Additionally, the `ConnectorStatusMetricsGroup` class already manages tasks and connector statuses, which means it should have sufficient context to handle stopping connectors effectively.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1058,
                        "lineEnd": 1065,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method awaitStopTasks to class ConnectorStatusMetricsGroup",
                        "description": "Move method awaitStopTasks to org.apache.kafka.connect.runtime.Worker.ConnectorStatusMetricsGroup\nRationale: null",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 834,
                        "lineEnd": 849,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method regularSourceOffsetsConsumerConfigs to class SourceTaskBuilder",
                        "description": "move method regularSourceOffsetsConsumerConfigs to PsiClass:SourceTaskBuilder\nRationale: The method regularSourceOffsetsConsumerConfigs concerns configuring consumer offsets for source connectors, which is directly related to the responsibilities of the SourceTaskBuilder class. This class already deals with building source tasks, setting offset stores, and configuring various source-related settings. Therefore, placing the method in SourceTaskBuilder ensures that all source task configurations are centralized, improving cohesion and maintainability.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 851,
                        "lineEnd": 882,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method baseConsumerConfigs to class SinkTaskBuilder",
                        "description": "move method baseConsumerConfigs to PsiClass:SinkTaskBuilder\nRationale: The method baseConsumerConfigs() is specifically configuring consumer properties which are heavily associated with Sink connectors. SinkTaskBuilder is responsible for building and configuring Sink tasks, which entails initializing KafkaConsumer instances. As seen in the doBuild() method of SinkTaskBuilder, it already makes use of the baseConsumerConfigs() method to create consumer properties. Moving this method to SinkTaskBuilder aligns the constructed consumer configurations with the appropriate task builder handling Sink tasks.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2281,
                        "lineEnd": 2298,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method recordTaskAdded to class Herder",
                        "description": "Move method recordTaskAdded to org.apache.kafka.connect.runtime.Herder\nRationale: The method `recordTaskAdded` is closely related to managing the state and metrics of connector tasks, which falls under the responsibility of the Herder interface. The Herder interface is responsible for tracking and managing workers and connectors, including tasks management. Since `recordTaskAdded` deals with recording metrics and status of connector tasks, it logically fits into the scope of Herder's responsibilities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "adminConfigs",
                            "method_signature": "private static Map<String, Object> adminConfigs(String connName, String defaultClientId, WorkerConfig config, ConnectorConfig connConfig, Class<? extends Connector> connectorClass, ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy, String clusterId, ConnectorType connectorType)",
                            "target_class": "AdminConfigBuilder",
                            "rationale": "The adminConfigs method is heavily focused on building a configuration map for an Admin client. This functionality represents a separable concern that is distinct from the primary responsibilities of the Worker class, which revolves around task and connector management. Moving this method into a dedicated builder class encapsulates the configuration logic and improves the separation of concerns."
                        },
                        {
                            "method_name": "baseProducerConfigs",
                            "method_signature": "private static Map<String, Object> baseProducerConfigs(String connName, String defaultClientId, WorkerConfig config, ConnectorConfig connConfig, Class<? extends Connector> connectorClass, ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy, String clusterId)",
                            "target_class": "ProducerConfigBuilder",
                            "rationale": "The baseProducerConfigs method is specialized in setting up producer configurations, which is a concern distinct from the worker's core responsibilities. Moving this method to a ProducerConfigBuilder class will help keep the Worker class focused and easier to maintain."
                        },
                        {
                            "method_name": "baseConsumerConfigs",
                            "method_signature": "private static Map<String, Object> baseConsumerConfigs(String connName, String defaultClientId, WorkerConfig config, ConnectorConfig connConfig, Class<? extends Connector> connectorClass, ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy, String clusterId, ConnectorType connectorType)",
                            "target_class": "ConsumerConfigBuilder",
                            "rationale": "This method is focused on building consumer configuration maps, a separate concern from the worker's primary focus on managing tasks and connectors. By moving this method to a new ConsumerConfigBuilder class, the responsibilities of the Worker class become more clear and the code becomes more maintainable."
                        },
                        {
                            "method_name": "exactlyOnceSourceOffsetsConsumerConfigs",
                            "method_signature": "private static Map<String, Object> exactlyOnceSourceOffsetsConsumerConfigs(String connName, String defaultClientId, WorkerConfig config, ConnectorConfig connConfig, Class<? extends Connector> connectorClass, ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy, String clusterId)",
                            "target_class": "ConsumerConfigBuilder",
                            "rationale": "This method is focused on building consumer configurations with exactly-once semantics for source connectors, which is another specialized concern. Moving this method to ConsumerConfigBuilder will help consolidate configuration-related methods, keeping the Worker class focused."
                        },
                        {
                            "method_name": "regularSourceOffsetsConsumerConfigs",
                            "method_signature": "private static Map<String, Object> regularSourceOffsetsConsumerConfigs(String connName, String defaultClientId, WorkerConfig config, ConnectorConfig connConfig, Class<? extends Connector> connectorClass, ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy, String clusterId)",
                            "target_class": "ConsumerConfigBuilder",
                            "rationale": "Similar to baseConsumerConfigs and exactlyOnceSourceOffsetsConsumerConfigs, this method focuses on consumer configuration but for regular (non-exactly-once) source connectors. It should also be moved to ConsumerConfigBuilder to consolidate consumer configuration logic."
                        },
                        {
                            "method_name": "stopAndAwaitConnectors",
                            "method_signature": "public void stopAndAwaitConnectors()",
                            "target_class": "WorkerManager",
                            "rationale": "The method stopAndAwaitConnectors focuses on stopping and awaiting the termination of connectors, which is a higher-level management operation. Moving this method to a WorkerManager class will help the Worker class focus more on actual worker tasks while delegating the managerial operations."
                        },
                        {
                            "method_name": "stopAndAwaitTasks",
                            "method_signature": "public void stopAndAwaitTasks()",
                            "target_class": "WorkerManager",
                            "rationale": "Like stopAndAwaitConnectors, this method focuses on stopping and awaiting the termination of tasks, which is a management-level operation. It makes sense to move this to the WorkerManager class to keep the Worker class focused on its core task-handling responsibilities."
                        }
                    ],
                    "llm_response_time": 12515
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "initConfigTransformer",
                            "method_signature": "private WorkerConfigTransformer initConfigTransformer()",
                            "target_class": "WorkerConfig",
                            "rationale": "The initConfigTransformer() method is related to configuration setup which makes it more appropriate to be part of the WorkerConfig class instead of Worker."
                        },
                        {
                            "method_name": "stopAndAwaitTasks",
                            "method_signature": "public void stopAndAwaitTasks()",
                            "target_class": "Herder",
                            "rationale": "The Herder is responsible for managing connectors and tasks. Thus, stopping and awaiting tasks is more appropriate within Herder than Worker."
                        },
                        {
                            "method_name": "stopAndAwaitConnectors",
                            "method_signature": "public void stopAndAwaitConnectors()",
                            "target_class": "Herder",
                            "rationale": "Connector lifecycle management is more logically part of the responsibilities of Herder, which coordinates the lifecycle of connectors and tasks."
                        },
                        {
                            "method_name": "sourceConnectorOffsets",
                            "method_signature": "private void sourceConnectorOffsets(String connName, ConnectorOffsetBackingStore offsetStore, CloseableOffsetStorageReader offsetReader, Callback<ConnectorOffsets> cb)",
                            "target_class": "OffsetBackingStore",
                            "rationale": "The method sourceConnectorOffsets() deals with handling offsets which should be managed by OffsetBackingStore class, meaning this functionality aligns more closely with responsibilities of OffsetBackingStore."
                        },
                        {
                            "method_name": "baseProducerConfigs",
                            "method_signature": "static Map<String, Object> baseProducerConfigs(String connName, String defaultClientId, WorkerConfig config, ConnectorConfig connConfig, Class<? extends Connector> connectorClass, ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy, String clusterId)",
                            "target_class": "WorkerConfig",
                            "rationale": "The method baseProducerConfigs() constructs producer configurations, which would be more appropriate within WorkerConfig class as it manages worker-specific configurations."
                        },
                        {
                            "method_name": "baseConsumerConfigs",
                            "method_signature": "static Map<String, Object> baseConsumerConfigs(String connName, String defaultClientId, WorkerConfig config, ConnectorConfig connConfig, Class<? extends Connector> connectorClass, ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy, String clusterId, ConnectorType connectorType)",
                            "target_class": "WorkerConfig",
                            "rationale": "This method configures consumer properties specific to the worker, and thus, it is consistent to have it in WorkerConfig class."
                        }
                    ],
                    "llm_response_time": 8853
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "fenceZombies",
                            "method_signature": "public KafkaFuture<Void> fenceZombies(String connName, int numTasks, Map<String, String> connProps)",
                            "target_class": "Connector",
                            "rationale": "The method 'fenceZombies' focuses on managing tasks related to a specific connector which might be more appropriate to move this method to the 'Connector' class."
                        },
                        {
                            "method_name": "normalizeSourceConnectorOffsets",
                            "method_signature": "Map<Map<String, ?>, Map<String, ?>> normalizeSourceConnectorOffsets(Map<Map<String, ?>, Map<String, ?>> originalOffsets)",
                            "target_class": "SourceConnector",
                            "rationale": "The method 'normalizeSourceConnectorOffsets' pertains to handling offsets related to source connectors, which suggests it would be more suited within the 'SourceConnector' class."
                        },
                        {
                            "method_name": "sourceConnectorTopicCreationEnabled",
                            "method_signature": "boolean sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                            "target_class": "SourceConnectorConfig",
                            "rationale": "The 'sourceConnectorTopicCreationEnabled' method deals with configurations related to source connectors, thus it should be moved to the 'SourceConnectorConfig' class."
                        },
                        {
                            "method_name": "offsetStoreForRegularSourceConnector",
                            "method_signature": "ConnectorOffsetBackingStore offsetStoreForRegularSourceConnector(SourceConnectorConfig sourceConfig, String connName, Connector connector, Producer<byte[], byte[]> producer)",
                            "target_class": "SourceConnector",
                            "rationale": "The method 'offsetStoreForRegularSourceConnector' focuses on creating an offset backing store for source connectors and would be better suited in the 'SourceConnector' class."
                        },
                        {
                            "method_name": "offsetStoreForExactlyOnceSourceConnector",
                            "method_signature": "ConnectorOffsetBackingStore offsetStoreForExactlyOnceSourceConnector(SourceConnectorConfig sourceConfig, String connName, Connector connector, Producer<byte[], byte[]> producer)",
                            "target_class": "SourceConnector",
                            "rationale": "The method 'offsetStoreForExactlyOnceSourceConnector' is specific to handling exactly once semantics for source connectors. This functionality would be more cohesively placed within the 'SourceConnector' class."
                        }
                    ],
                    "llm_response_time": 7662
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "initConfigTransformer",
                            "method_signature": "private initConfigTransformer()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "start",
                            "method_signature": "public start()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stop",
                            "method_signature": "public stop()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "startConnector",
                            "method_signature": "public startConnector(\n            String connName,\n            Map<String, String> connProps,\n            CloseableConnectorContext ctx,\n            ConnectorStatus.Listener statusListener,\n            TargetState initialState,\n            Callback<TargetState> onConnectorStateChange\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isSinkConnector",
                            "method_signature": "public isSinkConnector(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "connectorTaskConfigs",
                            "method_signature": "public connectorTaskConfigs(String connName, ConnectorConfig connConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tasksMax",
                            "method_signature": "private tasksMax(ConnectorConfig connConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopConnector",
                            "method_signature": "private stopConnector(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitStopConnectors",
                            "method_signature": "private awaitStopConnectors(Collection<String> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "startTask",
                            "method_signature": "private startTask(\n            ConnectorTaskId id,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TaskBuilder<?, ?> taskBuilder\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fenceZombies",
                            "method_signature": "public fenceZombies(String connName, int numTasks, Map<String, String> connProps)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "exactlyOnceSourceTaskProducerConfigs",
                            "method_signature": "static exactlyOnceSourceTaskProducerConfigs(ConnectorTaskId id,\n                                                              WorkerConfig config,\n                                                              ConnectorConfig connConfig,\n                                                              Class<? extends Connector>  connectorClass,\n                                                              ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                              String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "baseProducerConfigs",
                            "method_signature": "static baseProducerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector>  connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "exactlyOnceSourceOffsetsConsumerConfigs",
                            "method_signature": "static exactlyOnceSourceOffsetsConsumerConfigs(String connName,\n                                                                       String defaultClientId,\n                                                                       WorkerConfig config,\n                                                                       ConnectorConfig connConfig,\n                                                                       Class<? extends Connector> connectorClass,\n                                                                       ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                       String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "regularSourceOffsetsConsumerConfigs",
                            "method_signature": "static regularSourceOffsetsConsumerConfigs(String connName,\n                                                                   String defaultClientId,\n                                                                   WorkerConfig config,\n                                                                   ConnectorConfig connConfig,\n                                                                   Class<? extends Connector> connectorClass,\n                                                                   ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                   String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "baseConsumerConfigs",
                            "method_signature": "static baseConsumerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector> connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId,\n                                               ConnectorType connectorType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "adminConfigs",
                            "method_signature": "static adminConfigs(String connName,\n                                            String defaultClientId,\n                                            WorkerConfig config,\n                                            ConnectorConfig connConfig,\n                                            Class<? extends Connector> connectorClass,\n                                            ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                            String clusterId,\n                                            ConnectorType connectorType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "connectorClientConfigOverrides",
                            "method_signature": "private static connectorClientConfigOverrides(String connName,\n                                                                      ConnectorConfig connConfig,\n                                                                      Class<? extends Connector> connectorClass,\n                                                                      String clientConfigPrefix,\n                                                                      ConnectorType connectorType,\n                                                                      ConnectorClientConfigRequest.ClientType clientType,\n                                                                      ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskTransactionalId",
                            "method_signature": "private taskTransactionalId(ConnectorTaskId id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sinkTaskReporters",
                            "method_signature": "private sinkTaskReporters(ConnectorTaskId id, SinkConnectorConfig connConfig,\n                                                     ErrorHandlingMetrics errorHandlingMetrics,\n                                                     Class<? extends Connector> connectorClass)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createWorkerErrantRecordReporter",
                            "method_signature": "private createWorkerErrantRecordReporter(\n        SinkConnectorConfig connConfig,\n        RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n        Converter keyConverter,\n        Converter valueConverter,\n        HeaderConverter headerConverter\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopTask",
                            "method_signature": "private stopTask(ConnectorTaskId taskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitStopTask",
                            "method_signature": "private awaitStopTask(ConnectorTaskId taskId, long timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitStopTasks",
                            "method_signature": "private awaitStopTasks(Collection<ConnectorTaskId> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isTopicCreationEnabled",
                            "method_signature": "public isTopicCreationEnabled()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "setTargetState",
                            "method_signature": "public setTargetState(String connName, TargetState state, Callback<TargetState> stateChangeCallback)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "connectorOffsets",
                            "method_signature": "public connectorOffsets(String connName, Map<String, String> connectorConfig, Callback<ConnectorOffsets> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sinkConnectorOffsets",
                            "method_signature": " sinkConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                              Callback<ConnectorOffsets> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceConnectorOffsets",
                            "method_signature": "private sourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                        Callback<ConnectorOffsets> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceConnectorOffsets",
                            "method_signature": " sourceConnectorOffsets(String connName, ConnectorOffsetBackingStore offsetStore,\n                                CloseableOffsetStorageReader offsetReader, Callback<ConnectorOffsets> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "modifyConnectorOffsets",
                            "method_signature": "public modifyConnectorOffsets(String connName, Map<String, String> connectorConfig,\n                                      Map<Map<String, ?>, Map<String, ?>> offsets, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "modifySinkConnectorOffsets",
                            "method_signature": " modifySinkConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                    Map<Map<String, ?>, Map<String, ?>> offsets, ClassLoader connectorLoader, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "alterSinkConnectorOffsets",
                            "method_signature": "private alterSinkConnectorOffsets(String connName, String groupId, Admin admin, Map<TopicPartition, Long> offsetsToWrite,\n                                           Callback<Message> cb, boolean alterOffsetsResult, Timer timer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetSinkConnectorOffsets",
                            "method_signature": "private resetSinkConnectorOffsets(String connName, String groupId, Admin admin, Callback<Message> cb, boolean alterOffsetsResult, Timer timer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "modifySourceConnectorOffsets",
                            "method_signature": "private modifySourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                              Map<Map<String, ?>, Map<String, ?>> offsets, ClassLoader connectorLoader, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "modifySourceConnectorOffsets",
                            "method_signature": " modifySourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                      Map<Map<String, ?>, Map<String, ?>> offsets, ConnectorOffsetBackingStore offsetStore,\n                                      KafkaProducer<byte[], byte[]> producer, OffsetStorageWriter offsetWriter,\n                                      ClassLoader connectorLoader, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "normalizeSourceConnectorOffsets",
                            "method_signature": "@SuppressWarnings(\"unchecked\") normalizeSourceConnectorOffsets(Map<Map<String, ?>, Map<String, ?>> originalOffsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateTimerAndCheckExpiry",
                            "method_signature": "private updateTimerAndCheckExpiry(Timer timer, String errorMessageIfExpired)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeModifyOffsetsCallback",
                            "method_signature": "private completeModifyOffsetsCallback(boolean alterOffsetsResult, boolean isReset, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "offsetStoreForRegularSourceConnector",
                            "method_signature": " offsetStoreForRegularSourceConnector(\n            SourceConnectorConfig sourceConfig,\n            String connName,\n            Connector connector,\n            Producer<byte[], byte[]> producer\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "offsetStoreForExactlyOnceSourceConnector",
                            "method_signature": " offsetStoreForExactlyOnceSourceConnector(\n            SourceConnectorConfig sourceConfig,\n            String connName,\n            Connector connector,\n            Producer<byte[], byte[]> producer\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "offsetStoreForRegularSourceTask",
                            "method_signature": " offsetStoreForRegularSourceTask(\n            ConnectorTaskId id,\n            SourceConnectorConfig sourceConfig,\n            Class<? extends Connector> connectorClass,\n            Producer<byte[], byte[]> producer,\n            Map<String, Object> producerProps,\n            TopicAdmin topicAdmin\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "offsetStoreForExactlyOnceSourceTask",
                            "method_signature": " offsetStoreForExactlyOnceSourceTask(\n            ConnectorTaskId id,\n            SourceConnectorConfig sourceConfig,\n            Class<? extends Connector> connectorClass,\n            Producer<byte[], byte[]> producer,\n            Map<String, Object> producerProps,\n            TopicAdmin topicAdmin\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sameOffsetTopicAsWorker",
                            "method_signature": "private sameOffsetTopicAsWorker(String offsetsTopic, Map<String, Object> producerProps)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "regularSourceTaskUsesConnectorSpecificOffsetsStore",
                            "method_signature": "private regularSourceTaskUsesConnectorSpecificOffsetsStore(SourceConnectorConfig sourceConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceConnectorTopicCreationEnabled",
                            "method_signature": "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskCounter",
                            "method_signature": "protected taskCounter(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskStatusCounter",
                            "method_signature": "protected taskStatusCounter(String connName, TaskStatus.State state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordTaskAdded",
                            "method_signature": "protected synchronized recordTaskAdded(ConnectorTaskId connectorTaskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordTaskRemoved",
                            "method_signature": "protected synchronized recordTaskRemoved(ConnectorTaskId connectorTaskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "protected synchronized close()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "taskCounter",
                            "method_signature": "protected taskCounter(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskStatusCounter",
                            "method_signature": "protected taskStatusCounter(String connName, TaskStatus.State state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceConnectorTopicCreationEnabled",
                            "method_signature": "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskTransactionalId",
                            "method_signature": "private taskTransactionalId(ConnectorTaskId id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "exactlyOnceSourceOffsetsConsumerConfigs",
                            "method_signature": "static exactlyOnceSourceOffsetsConsumerConfigs(String connName,\n                                                                       String defaultClientId,\n                                                                       WorkerConfig config,\n                                                                       ConnectorConfig connConfig,\n                                                                       Class<? extends Connector> connectorClass,\n                                                                       ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                       String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tasksMax",
                            "method_signature": "private tasksMax(ConnectorConfig connConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createWorkerErrantRecordReporter",
                            "method_signature": "private createWorkerErrantRecordReporter(\n        SinkConnectorConfig connConfig,\n        RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n        Converter keyConverter,\n        Converter valueConverter,\n        HeaderConverter headerConverter\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "start",
                            "method_signature": "public start()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "protected synchronized close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitStopConnectors",
                            "method_signature": "private awaitStopConnectors(Collection<String> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitStopTasks",
                            "method_signature": "private awaitStopTasks(Collection<ConnectorTaskId> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "regularSourceOffsetsConsumerConfigs",
                            "method_signature": "static regularSourceOffsetsConsumerConfigs(String connName,\n                                                                   String defaultClientId,\n                                                                   WorkerConfig config,\n                                                                   ConnectorConfig connConfig,\n                                                                   Class<? extends Connector> connectorClass,\n                                                                   ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                   String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "baseConsumerConfigs",
                            "method_signature": "static baseConsumerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector> connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId,\n                                               ConnectorType connectorType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordTaskAdded",
                            "method_signature": "protected synchronized recordTaskAdded(ConnectorTaskId connectorTaskId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "protected taskCounter(String connName)": {
                        "first": {
                            "method_name": "taskCounter",
                            "method_signature": "protected taskCounter(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.19003285668326841
                    },
                    "protected taskStatusCounter(String connName, TaskStatus.State state)": {
                        "first": {
                            "method_name": "taskStatusCounter",
                            "method_signature": "protected taskStatusCounter(String connName, TaskStatus.State state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.19381993703809114
                    },
                    "public build()": {
                        "first": {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.2283753052143277
                    },
                    "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)": {
                        "first": {
                            "method_name": "sourceConnectorTopicCreationEnabled",
                            "method_signature": "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.24543519642527495
                    },
                    "private taskTransactionalId(ConnectorTaskId id)": {
                        "first": {
                            "method_name": "taskTransactionalId",
                            "method_signature": "private taskTransactionalId(ConnectorTaskId id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.254616044423966
                    },
                    "static exactlyOnceSourceOffsetsConsumerConfigs(String connName,\n                                                                       String defaultClientId,\n                                                                       WorkerConfig config,\n                                                                       ConnectorConfig connConfig,\n                                                                       Class<? extends Connector> connectorClass,\n                                                                       ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                       String clusterId)": {
                        "first": {
                            "method_name": "exactlyOnceSourceOffsetsConsumerConfigs",
                            "method_signature": "static exactlyOnceSourceOffsetsConsumerConfigs(String connName,\n                                                                       String defaultClientId,\n                                                                       WorkerConfig config,\n                                                                       ConnectorConfig connConfig,\n                                                                       Class<? extends Connector> connectorClass,\n                                                                       ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                       String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.27724909049258634
                    },
                    "private tasksMax(ConnectorConfig connConfig)": {
                        "first": {
                            "method_name": "tasksMax",
                            "method_signature": "private tasksMax(ConnectorConfig connConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.2884071803738167
                    },
                    "private createWorkerErrantRecordReporter(\n        SinkConnectorConfig connConfig,\n        RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n        Converter keyConverter,\n        Converter valueConverter,\n        HeaderConverter headerConverter\n    )": {
                        "first": {
                            "method_name": "createWorkerErrantRecordReporter",
                            "method_signature": "private createWorkerErrantRecordReporter(\n        SinkConnectorConfig connConfig,\n        RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n        Converter keyConverter,\n        Converter valueConverter,\n        HeaderConverter headerConverter\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3115864223983106
                    },
                    "public start()": {
                        "first": {
                            "method_name": "start",
                            "method_signature": "public start()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.32848055601388637
                    },
                    "protected synchronized close()": {
                        "first": {
                            "method_name": "close",
                            "method_signature": "protected synchronized close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.34094744696456153
                    },
                    "private awaitStopConnectors(Collection<String> ids)": {
                        "first": {
                            "method_name": "awaitStopConnectors",
                            "method_signature": "private awaitStopConnectors(Collection<String> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3654280220005326
                    },
                    "private awaitStopTasks(Collection<ConnectorTaskId> ids)": {
                        "first": {
                            "method_name": "awaitStopTasks",
                            "method_signature": "private awaitStopTasks(Collection<ConnectorTaskId> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3654280220005326
                    },
                    "static regularSourceOffsetsConsumerConfigs(String connName,\n                                                                   String defaultClientId,\n                                                                   WorkerConfig config,\n                                                                   ConnectorConfig connConfig,\n                                                                   Class<? extends Connector> connectorClass,\n                                                                   ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                   String clusterId)": {
                        "first": {
                            "method_name": "regularSourceOffsetsConsumerConfigs",
                            "method_signature": "static regularSourceOffsetsConsumerConfigs(String connName,\n                                                                   String defaultClientId,\n                                                                   WorkerConfig config,\n                                                                   ConnectorConfig connConfig,\n                                                                   Class<? extends Connector> connectorClass,\n                                                                   ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                   String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.37175912274956097
                    },
                    "static baseConsumerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector> connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId,\n                                               ConnectorType connectorType)": {
                        "first": {
                            "method_name": "baseConsumerConfigs",
                            "method_signature": "static baseConsumerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector> connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId,\n                                               ConnectorType connectorType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.39264628849275457
                    },
                    "protected synchronized recordTaskAdded(ConnectorTaskId connectorTaskId)": {
                        "first": {
                            "method_name": "recordTaskAdded",
                            "method_signature": "protected synchronized recordTaskAdded(ConnectorTaskId connectorTaskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3997794337546244
                    }
                },
                "voyage": {
                    "private updateTimerAndCheckExpiry(Timer timer, String errorMessageIfExpired)": {
                        "first": {
                            "method_name": "updateTimerAndCheckExpiry",
                            "method_signature": "private updateTimerAndCheckExpiry(Timer timer, String errorMessageIfExpired)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3462610809913813
                    },
                    "protected synchronized close()": {
                        "first": {
                            "method_name": "close",
                            "method_signature": "protected synchronized close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3651933899257402
                    },
                    "private initConfigTransformer()": {
                        "first": {
                            "method_name": "initConfigTransformer",
                            "method_signature": "private initConfigTransformer()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3751603123720797
                    },
                    "private awaitStopConnectors(Collection<String> ids)": {
                        "first": {
                            "method_name": "awaitStopConnectors",
                            "method_signature": "private awaitStopConnectors(Collection<String> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4151234979514168
                    },
                    "private taskTransactionalId(ConnectorTaskId id)": {
                        "first": {
                            "method_name": "taskTransactionalId",
                            "method_signature": "private taskTransactionalId(ConnectorTaskId id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.44226117796369707
                    },
                    "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)": {
                        "first": {
                            "method_name": "sourceConnectorTopicCreationEnabled",
                            "method_signature": "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4463049258915329
                    },
                    "private static connectorClientConfigOverrides(String connName,\n                                                                      ConnectorConfig connConfig,\n                                                                      Class<? extends Connector> connectorClass,\n                                                                      String clientConfigPrefix,\n                                                                      ConnectorType connectorType,\n                                                                      ConnectorClientConfigRequest.ClientType clientType,\n                                                                      ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy)": {
                        "first": {
                            "method_name": "connectorClientConfigOverrides",
                            "method_signature": "private static connectorClientConfigOverrides(String connName,\n                                                                      ConnectorConfig connConfig,\n                                                                      Class<? extends Connector> connectorClass,\n                                                                      String clientConfigPrefix,\n                                                                      ConnectorType connectorType,\n                                                                      ConnectorClientConfigRequest.ClientType clientType,\n                                                                      ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.44679774916743514
                    },
                    "private createWorkerErrantRecordReporter(\n        SinkConnectorConfig connConfig,\n        RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n        Converter keyConverter,\n        Converter valueConverter,\n        HeaderConverter headerConverter\n    )": {
                        "first": {
                            "method_name": "createWorkerErrantRecordReporter",
                            "method_signature": "private createWorkerErrantRecordReporter(\n        SinkConnectorConfig connConfig,\n        RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n        Converter keyConverter,\n        Converter valueConverter,\n        HeaderConverter headerConverter\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.45160005957317184
                    },
                    "protected synchronized recordTaskRemoved(ConnectorTaskId connectorTaskId)": {
                        "first": {
                            "method_name": "recordTaskRemoved",
                            "method_signature": "protected synchronized recordTaskRemoved(ConnectorTaskId connectorTaskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4628336197095373
                    },
                    "private awaitStopTasks(Collection<ConnectorTaskId> ids)": {
                        "first": {
                            "method_name": "awaitStopTasks",
                            "method_signature": "private awaitStopTasks(Collection<ConnectorTaskId> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4684582975632226
                    },
                    "public setTargetState(String connName, TargetState state, Callback<TargetState> stateChangeCallback)": {
                        "first": {
                            "method_name": "setTargetState",
                            "method_signature": "public setTargetState(String connName, TargetState state, Callback<TargetState> stateChangeCallback)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.47092169276230667
                    },
                    "private awaitStopTask(ConnectorTaskId taskId, long timeout)": {
                        "first": {
                            "method_name": "awaitStopTask",
                            "method_signature": "private awaitStopTask(ConnectorTaskId taskId, long timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.47580284111678683
                    },
                    "private tasksMax(ConnectorConfig connConfig)": {
                        "first": {
                            "method_name": "tasksMax",
                            "method_signature": "private tasksMax(ConnectorConfig connConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4780855049293642
                    },
                    "protected taskCounter(String connName)": {
                        "first": {
                            "method_name": "taskCounter",
                            "method_signature": "protected taskCounter(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.48168201914991227
                    },
                    "protected synchronized recordTaskAdded(ConnectorTaskId connectorTaskId)": {
                        "first": {
                            "method_name": "recordTaskAdded",
                            "method_signature": "protected synchronized recordTaskAdded(ConnectorTaskId connectorTaskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.48531391854114847
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                        "private taskTransactionalId(ConnectorTaskId id)",
                        "private tasksMax(ConnectorConfig connConfig)",
                        "protected taskCounter(String connName)",
                        "protected taskStatusCounter(String connName, TaskStatus.State state)",
                        "public start()",
                        "public build()",
                        "protected synchronized close()",
                        "protected synchronized recordTaskAdded(ConnectorTaskId connectorTaskId)",
                        "private awaitStopConnectors(Collection<String> ids)",
                        "private awaitStopTasks(Collection<ConnectorTaskId> ids)"
                    ],
                    "llm_response_time": 10003
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "protected taskCounter(String connName)",
                        "protected taskStatusCounter(String connName, TaskStatus.State state)",
                        "private taskTransactionalId(ConnectorTaskId id)",
                        "public build()",
                        "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)"
                    ],
                    "llm_response_time": 6447
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "public build()",
                        "protected taskStatusCounter(String connName, TaskStatus.State state)",
                        "protected taskCounter(String connName)"
                    ],
                    "llm_response_time": 5673
                },
                "voyage": {
                    "priority_method_names": [
                        "private updateTimerAndCheckExpiry(Timer timer, String errorMessageIfExpired)",
                        "private initConfigTransformer()",
                        "private awaitStopConnectors(Collection<String> ids)",
                        "private awaitStopTasks(Collection<ConnectorTaskId> ids)",
                        "private awaitStopTask(ConnectorTaskId taskId, long timeout)",
                        "private createWorkerErrantRecordReporter(\n        SinkConnectorConfig connConfig,\n        RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n        Converter keyConverter,\n        Converter valueConverter,\n        HeaderConverter headerConverter\n    )",
                        "private static connectorClientConfigOverrides(String connName,\n                                                                      ConnectorConfig connConfig,\n                                                                      Class<? extends Connector> connectorClass,\n                                                                      String clientConfigPrefix,\n                                                                      ConnectorType connectorType,\n                                                                      ConnectorClientConfigRequest.ClientType clientType,\n                                                                      ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy)",
                        "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                        "private taskTransactionalId(ConnectorTaskId id)",
                        "private tasksMax(ConnectorConfig connConfig)",
                        "public setTargetState(String connName, TargetState state, Callback<TargetState> stateChangeCallback)",
                        "protected synchronized recordTaskAdded(ConnectorTaskId connectorTaskId)",
                        "protected synchronized recordTaskRemoved(ConnectorTaskId connectorTaskId)",
                        "protected taskCounter(String connName)",
                        "protected synchronized close()"
                    ],
                    "llm_response_time": 6336
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private initConfigTransformer()",
                        "private taskTransactionalId(ConnectorTaskId id)",
                        "private updateTimerAndCheckExpiry(Timer timer, String errorMessageIfExpired)",
                        "private awaitStopConnectors(Collection<String> ids)",
                        "protected synchronized close()"
                    ],
                    "llm_response_time": 6636
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private initConfigTransformer()",
                        "protected synchronized close()",
                        "private updateTimerAndCheckExpiry(Timer timer, String errorMessageIfExpired)"
                    ],
                    "llm_response_time": 5762
                }
            },
            "targetClassMap": {
                "taskCounter": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.020772590192419116
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Herder"
                    ],
                    "llm_response_time": 2371,
                    "similarity_computation_time": 9,
                    "similarity_metric": "cosine"
                },
                "taskStatusCounter": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.030651329352501994
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Herder"
                    ],
                    "llm_response_time": 1812,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "build": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3114,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "sourceConnectorTopicCreationEnabled": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.03791780158451814
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.12016297604277221
                        },
                        {
                            "class_name": "ConnectorStatusMetricsGroup",
                            "similarity_score": 0.40931462414438785
                        },
                        {
                            "class_name": "WorkerConfig",
                            "similarity_score": 0.14579712273792464
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.04941997354933901
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.04941997354933901
                        },
                        {
                            "class_name": "OffsetBackingStore",
                            "similarity_score": 0.016574838603294898
                        },
                        {
                            "class_name": "ConnectorClientConfigOverridePolicy",
                            "similarity_score": 0.022689215826037595
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "WorkerConfig",
                        "ConnectorStatusMetricsGroup",
                        "Time"
                    ],
                    "llm_response_time": 4610,
                    "similarity_computation_time": 6,
                    "similarity_metric": "cosine"
                },
                "taskTransactionalId": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.0335843385462875
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.12016297604277221
                        },
                        {
                            "class_name": "ConnectorStatusMetricsGroup",
                            "similarity_score": 0.40931462414438785
                        },
                        {
                            "class_name": "WorkerConfig",
                            "similarity_score": 0.21792833082931892
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.045301642420227425
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.045301642420227425
                        },
                        {
                            "class_name": "OffsetBackingStore",
                            "similarity_score": 0.016574838603294898
                        },
                        {
                            "class_name": "ConnectorClientConfigOverridePolicy",
                            "similarity_score": 0.022689215826037595
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "WorkerConfig",
                        "ConnectorStatusMetricsGroup",
                        "Time"
                    ],
                    "llm_response_time": 3494,
                    "similarity_computation_time": 6,
                    "similarity_metric": "cosine"
                },
                "exactlyOnceSourceOffsetsConsumerConfigs": {
                    "target_classes": [
                        {
                            "class_name": "TopicCreationConfig",
                            "similarity_score": 0.14172038399970197
                        },
                        {
                            "class_name": "SinkConnectorConfig",
                            "similarity_score": 0.25335972757370595
                        },
                        {
                            "class_name": "WorkerConfig",
                            "similarity_score": 0.20335129604352015
                        },
                        {
                            "class_name": "SourceConnectorConfig",
                            "similarity_score": 0.21868600978672972
                        },
                        {
                            "class_name": "ConnectorConfig",
                            "similarity_score": 0.25602348094553473
                        },
                        {
                            "class_name": "WorkerInfo",
                            "similarity_score": 0.1871305500273533
                        },
                        {
                            "class_name": "SourceTaskOffsetCommitter",
                            "similarity_score": 0.1600461314389397
                        },
                        {
                            "class_name": "ConnectMetrics",
                            "similarity_score": 0.12058417011094277
                        },
                        {
                            "class_name": "ExactlyOnceWorkerSourceTask",
                            "similarity_score": 0.17551422738173697
                        },
                        {
                            "class_name": "Created",
                            "similarity_score": 0.18362925952195544
                        },
                        {
                            "class_name": "RestartPlan",
                            "similarity_score": 0.08573778039501867
                        },
                        {
                            "class_name": "RestartRequest",
                            "similarity_score": 0.12990414468369396
                        },
                        {
                            "class_name": "ClassRecommender",
                            "similarity_score": 0.2440476622816901
                        },
                        {
                            "class_name": "HerderConnectorContext",
                            "similarity_score": 0.12414088329035519
                        },
                        {
                            "class_name": "ExactlyOnceSourceTaskBuilder",
                            "similarity_score": 0.2482817665807104
                        },
                        {
                            "class_name": "CommittableOffsets",
                            "similarity_score": 0.13023522507370866
                        },
                        {
                            "class_name": "InternalSinkRecord",
                            "similarity_score": 0.12409125671100242
                        },
                        {
                            "class_name": "Connect",
                            "similarity_score": 0.16110648406231642
                        },
                        {
                            "class_name": "ConnectMetricsRegistry",
                            "similarity_score": 0.09498092723288509
                        },
                        {
                            "class_name": "ConnectorMetricsGroup",
                            "similarity_score": 0.17179232415197013
                        },
                        {
                            "class_name": "ConnectorStatus",
                            "similarity_score": 0.05797516958761953
                        },
                        {
                            "class_name": "ConnectorStatusListener",
                            "similarity_score": 0.1871305500273533
                        },
                        {
                            "class_name": "ConnectorStatusMetricsGroup",
                            "similarity_score": 0.17903852803390657
                        },
                        {
                            "class_name": "SessionKey",
                            "similarity_score": 0.09217710503460784
                        },
                        {
                            "class_name": "TopicStatus",
                            "similarity_score": 0.15791794233054138
                        },
                        {
                            "class_name": "TaskConfig",
                            "similarity_score": 0.18006330337866241
                        },
                        {
                            "class_name": "TransactionMetricsGroup",
                            "similarity_score": 0.16496103676474583
                        },
                        {
                            "class_name": "TaskMetricsGroup",
                            "similarity_score": 0.15568749326108358
                        },
                        {
                            "class_name": "TransformationChain",
                            "similarity_score": 0.1698087670672462
                        },
                        {
                            "class_name": "WorkerConfigTransformer",
                            "similarity_score": 0.25011327847620274
                        },
                        {
                            "class_name": "WorkerConnector",
                            "similarity_score": 0.19024746337951673
                        },
                        {
                            "class_name": "TransformationStage",
                            "similarity_score": 0.217180026942987
                        },
                        {
                            "class_name": "WorkerMetricsGroup",
                            "similarity_score": 0.18476681373468243
                        },
                        {
                            "class_name": "TaskStatus",
                            "similarity_score": 0.044954199990417804
                        },
                        {
                            "class_name": "TaskStatusListener",
                            "similarity_score": 0.12286997991157288
                        },
                        {
                            "class_name": "WorkerSinkTask",
                            "similarity_score": 0.18261935983325064
                        },
                        {
                            "class_name": "WorkerSinkTaskContext",
                            "similarity_score": 0.1713355468738194
                        },
                        {
                            "class_name": "WorkerSourceTask",
                            "similarity_score": 0.18156904038393343
                        },
                        {
                            "class_name": "WorkerSourceTaskContext",
                            "similarity_score": 0.15456351944507035
                        },
                        {
                            "class_name": "WorkerTransactionContext",
                            "similarity_score": 0.1487733852568057
                        },
                        {
                            "class_name": "MetricGroup",
                            "similarity_score": 0.07435619734063506
                        },
                        {
                            "class_name": "MetricGroupId",
                            "similarity_score": 0.20731536028072048
                        },
                        {
                            "class_name": "StateTracker",
                            "similarity_score": 0.08772810018741284
                        },
                        {
                            "class_name": "Loggers",
                            "similarity_score": 0.16429535829994593
                        },
                        {
                            "class_name": "SinkTaskBuilder",
                            "similarity_score": 0.21423957942279692
                        },
                        {
                            "class_name": "SinkTaskMetricsGroup",
                            "similarity_score": 0.14750648300875357
                        },
                        {
                            "class_name": "SourceRecordWriteCounter",
                            "similarity_score": 0.14390879073085522
                        },
                        {
                            "class_name": "SourceTaskBuilder",
                            "similarity_score": 0.272027473353741
                        },
                        {
                            "class_name": "SourceTaskMetricsGroup",
                            "similarity_score": 0.12094358529595363
                        },
                        {
                            "class_name": "SubmittedRecord",
                            "similarity_score": 0.22330492437170055
                        },
                        {
                            "class_name": "SubmittedRecords",
                            "similarity_score": 0.19133771852161546
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SourceTaskBuilder",
                        "ConnectorConfig",
                        "SinkConnectorConfig"
                    ],
                    "llm_response_time": 4335,
                    "similarity_computation_time": 15,
                    "similarity_metric": "cosine"
                },
                "tasksMax": {
                    "target_classes": [
                        {
                            "class_name": "ConnectorConfig",
                            "similarity_score": 0.33106591951688563
                        },
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.029299557795755153
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.12463721647779649
                        },
                        {
                            "class_name": "Plugins",
                            "similarity_score": 0.3434051313734477
                        },
                        {
                            "class_name": "ConnectMetrics",
                            "similarity_score": 0.2128322068442647
                        },
                        {
                            "class_name": "WorkerMetricsGroup",
                            "similarity_score": 0.40962283318350806
                        },
                        {
                            "class_name": "ConnectorStatusMetricsGroup",
                            "similarity_score": 0.40883108632154813
                        },
                        {
                            "class_name": "WorkerConfig",
                            "similarity_score": 0.18807424735078157
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.03828687583764507
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.03828687583764507
                        },
                        {
                            "class_name": "OffsetBackingStore",
                            "similarity_score": 0.014008295367013146
                        },
                        {
                            "class_name": "WorkerConfigTransformer",
                            "similarity_score": 0.4161082220485403
                        },
                        {
                            "class_name": "ConnectorClientConfigOverridePolicy",
                            "similarity_score": 0.019175887291829265
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "WorkerMetricsGroup",
                        "ConnectorStatusMetricsGroup",
                        "WorkerConfigTransformer"
                    ],
                    "llm_response_time": 5184,
                    "similarity_computation_time": 6,
                    "similarity_metric": "cosine"
                },
                "createWorkerErrantRecordReporter": {
                    "target_classes": [
                        {
                            "class_name": "SinkConnectorConfig",
                            "similarity_score": 0.21878943359864744
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.07443439676034484
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.07443439676034484
                        },
                        {
                            "class_name": "HeaderConverter",
                            "similarity_score": 0.05926955847701997
                        },
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.057343386443115006
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.14102703183162957
                        },
                        {
                            "class_name": "Plugins",
                            "similarity_score": 0.3793637712025199
                        },
                        {
                            "class_name": "ConnectMetrics",
                            "similarity_score": 0.18829088617802703
                        },
                        {
                            "class_name": "WorkerMetricsGroup",
                            "similarity_score": 0.3232830934600918
                        },
                        {
                            "class_name": "ConnectorStatusMetricsGroup",
                            "similarity_score": 0.3576195434980279
                        },
                        {
                            "class_name": "WorkerConfig",
                            "similarity_score": 0.14991818832934453
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.07443439676034484
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.07443439676034484
                        },
                        {
                            "class_name": "OffsetBackingStore",
                            "similarity_score": 0.024964362063136587
                        },
                        {
                            "class_name": "WorkerConfigTransformer",
                            "similarity_score": 0.37804600361571783
                        },
                        {
                            "class_name": "ConnectorClientConfigOverridePolicy",
                            "similarity_score": 0.05370136140890239
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ConnectorStatusMetricsGroup",
                        "WorkerConfigTransformer",
                        "Plugins"
                    ],
                    "llm_response_time": 3116,
                    "similarity_computation_time": 8,
                    "similarity_metric": "cosine"
                },
                "start": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.18487569907599527
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.25857675222327653
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.15189346708911683
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.15189346708911683
                        },
                        {
                            "class_name": "OffsetBackingStore",
                            "similarity_score": 0.22354163475930489
                        },
                        {
                            "class_name": "ConnectorClientConfigOverridePolicy",
                            "similarity_score": 0.143635025780694
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Herder",
                        "OffsetBackingStore",
                        "Time"
                    ],
                    "llm_response_time": 4217,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "close": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.07228861386961853
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Herder"
                    ],
                    "llm_response_time": 2020,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "awaitStopConnectors": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.048951671353878665
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1903864929726999
                        },
                        {
                            "class_name": "ConnectorStatusMetricsGroup",
                            "similarity_score": 0.4483588306542439
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.049622931173563235
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.049622931173563235
                        },
                        {
                            "class_name": "OffsetBackingStore",
                            "similarity_score": 0.046362386688682235
                        },
                        {
                            "class_name": "ConnectorClientConfigOverridePolicy",
                            "similarity_score": 0.02929165167758312
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ConnectorStatusMetricsGroup",
                        "Time",
                        "Converter"
                    ],
                    "llm_response_time": 3535,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "awaitStopTasks": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.048951671353878665
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1903864929726999
                        },
                        {
                            "class_name": "ConnectorStatusMetricsGroup",
                            "similarity_score": 0.4483588306542439
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.049622931173563235
                        },
                        {
                            "class_name": "Converter",
                            "similarity_score": 0.049622931173563235
                        },
                        {
                            "class_name": "OffsetBackingStore",
                            "similarity_score": 0.046362386688682235
                        },
                        {
                            "class_name": "ConnectorClientConfigOverridePolicy",
                            "similarity_score": 0.02929165167758312
                        }
                    ],
                    "llm_response_time": 4703,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine",
                    "target_class_priority_explanation": "[\n    {\n        \"target_class\": \"ConnectorStatusMetricsGroup\",\n        \"rationale\": \"The awaitStopTasks() method is highly related to task management, including stopping tasks within a specified timeout. The ConnectorStatusMetricsGroup class already deals with tasks via its metrics (\"tasks\" field) and operations such as recordTaskAdded and recordTaskRemoved. Placing the awaitStopTasks() method here aligns with existing responsibilities and allows for better organization and encapsulation of the task management logic.\"\n    },\n    {\n        \"target_class\": \"Time\",\n        \"rationale\": \"While the awaitStopTasks() method uses time-based operations extensively, the core responsibility of the method is related to task management rather than purely time operations. Hence, it would be slightly less appropriate to place it in the Time class. However, if task management needs to be decoupled from Connect metrics and ConnectorStatusMetricsGroup is not suitable, Time could be a decent fallback given its integral use for calculating deadlines and waiting durations.\"\n    },\n    {\n        \"target_class\": \"Converter\",\n        \"rationale\": \"The awaitStopTasks() method is not related to data conversion at all. The primary responsibilities of the Converter interface are centered around schema transformations and serialization. Placing the method here would violate the Single Responsibility Principle and misalign functional concerns.\"\n    }\n]"
                },
                "regularSourceOffsetsConsumerConfigs": {
                    "target_classes": [
                        {
                            "class_name": "TopicCreationConfig",
                            "similarity_score": 0.20100916030427213
                        },
                        {
                            "class_name": "SinkConnectorConfig",
                            "similarity_score": 0.3009343885031181
                        },
                        {
                            "class_name": "WorkerConfig",
                            "similarity_score": 0.27739641144289773
                        },
                        {
                            "class_name": "SourceConnectorConfig",
                            "similarity_score": 0.2780017751182665
                        },
                        {
                            "class_name": "ConnectorConfig",
                            "similarity_score": 0.300761172695726
                        },
                        {
                            "class_name": "WorkerInfo",
                            "similarity_score": 0.2077512877754607
                        },
                        {
                            "class_name": "SourceTaskOffsetCommitter",
                            "similarity_score": 0.24331053404182262
                        },
                        {
                            "class_name": "ConnectMetrics",
                            "similarity_score": 0.19943163614614
                        },
                        {
                            "class_name": "ExactlyOnceWorkerSourceTask",
                            "similarity_score": 0.2881299528146496
                        },
                        {
                            "class_name": "Loggers",
                            "similarity_score": 0.24814854594830488
                        },
                        {
                            "class_name": "TaskConfig",
                            "similarity_score": 0.1994403741808211
                        },
                        {
                            "class_name": "Created",
                            "similarity_score": 0.20030840419244383
                        },
                        {
                            "class_name": "TaskMetricsGroup",
                            "similarity_score": 0.16257103666804412
                        },
                        {
                            "class_name": "TaskStatus",
                            "similarity_score": 0.15177190645025407
                        },
                        {
                            "class_name": "TaskStatusListener",
                            "similarity_score": 0.12909944487358058
                        },
                        {
                            "class_name": "RestartPlan",
                            "similarity_score": 0.16081288645059114
                        },
                        {
                            "class_name": "RestartRequest",
                            "similarity_score": 0.20670126780227888
                        },
                        {
                            "class_name": "ClassRecommender",
                            "similarity_score": 0.24573659359149527
                        },
                        {
                            "class_name": "HerderConnectorContext",
                            "similarity_score": 0.21354166666666666
                        },
                        {
                            "class_name": "CommittableOffsets",
                            "similarity_score": 0.19408204614013716
                        },
                        {
                            "class_name": "MetricGroup",
                            "similarity_score": 0.15959297680257384
                        },
                        {
                            "class_name": "MetricGroupId",
                            "similarity_score": 0.25377461407984414
                        },
                        {
                            "class_name": "TopicStatus",
                            "similarity_score": 0.21901489351915618
                        },
                        {
                            "class_name": "TransactionMetricsGroup",
                            "similarity_score": 0.16610264925668733
                        },
                        {
                            "class_name": "TransformationChain",
                            "similarity_score": 0.2249788529816829
                        },
                        {
                            "class_name": "TransformationStage",
                            "similarity_score": 0.23902562743146344
                        },
                        {
                            "class_name": "ExactlyOnceSourceTaskBuilder",
                            "similarity_score": 0.2990196078431373
                        },
                        {
                            "class_name": "Connect",
                            "similarity_score": 0.18249909877210785
                        },
                        {
                            "class_name": "ConnectMetricsRegistry",
                            "similarity_score": 0.1670237249939796
                        },
                        {
                            "class_name": "ConnectorMetricsGroup",
                            "similarity_score": 0.19574189841551182
                        },
                        {
                            "class_name": "ConnectorStatus",
                            "similarity_score": 0.09390983934711412
                        },
                        {
                            "class_name": "ConnectorStatusListener",
                            "similarity_score": 0.19325701188414948
                        },
                        {
                            "class_name": "ConnectorStatusMetricsGroup",
                            "similarity_score": 0.21725757685488142
                        },
                        {
                            "class_name": "SessionKey",
                            "similarity_score": 0.1635312195250112
                        },
                        {
                            "class_name": "SinkTaskBuilder",
                            "similarity_score": 0.2231609213387259
                        },
                        {
                            "class_name": "SinkTaskMetricsGroup",
                            "similarity_score": 0.15995247832289886
                        },
                        {
                            "class_name": "StateTracker",
                            "similarity_score": 0.15244949657022358
                        },
                        {
                            "class_name": "SourceRecordWriteCounter",
                            "similarity_score": 0.16905549640051765
                        },
                        {
                            "class_name": "SourceTaskBuilder",
                            "similarity_score": 0.31525495041423823
                        },
                        {
                            "class_name": "SourceTaskMetricsGroup",
                            "similarity_score": 0.13917780012784423
                        },
                        {
                            "class_name": "SubmittedRecord",
                            "similarity_score": 0.2937560444252401
                        },
                        {
                            "class_name": "SubmittedRecords",
                            "similarity_score": 0.2807143664122746
                        },
                        {
                            "class_name": "InternalSinkRecord",
                            "similarity_score": 0.15493803717521737
                        },
                        {
                            "class_name": "WorkerTransactionContext",
                            "similarity_score": 0.2384618724302923
                        },
                        {
                            "class_name": "WorkerMetricsGroup",
                            "similarity_score": 0.18965802588902025
                        },
                        {
                            "class_name": "WorkerSourceTask",
                            "similarity_score": 0.26720662938762707
                        },
                        {
                            "class_name": "WorkerSourceTaskContext",
                            "similarity_score": 0.15563317594128032
                        },
                        {
                            "class_name": "WorkerSinkTask",
                            "similarity_score": 0.2636498515401611
                        },
                        {
                            "class_name": "WorkerSinkTaskContext",
                            "similarity_score": 0.20821533086941516
                        },
                        {
                            "class_name": "WorkerConfigTransformer",
                            "similarity_score": 0.2806263796546246
                        },
                        {
                            "class_name": "WorkerConnector",
                            "similarity_score": 0.2576001321118367
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SourceTaskBuilder",
                        "ConnectorConfig",
                        "SinkConnectorConfig"
                    ],
                    "llm_response_time": 5330,
                    "similarity_computation_time": 15,
                    "similarity_metric": "cosine"
                },
                "baseConsumerConfigs": {
                    "target_classes": [
                        {
                            "class_name": "SinkTaskBuilder",
                            "similarity_score": 0.21930194007811735
                        },
                        {
                            "class_name": "SourceTaskBuilder",
                            "similarity_score": 0.3359785155059217
                        },
                        {
                            "class_name": "ExactlyOnceSourceTaskBuilder",
                            "similarity_score": 0.33349806441990637
                        },
                        {
                            "class_name": "ConnectorStatusMetricsGroup",
                            "similarity_score": 0.19218555339901833
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SinkTaskBuilder",
                        "SourceTaskBuilder",
                        "ExactlyOnceSourceTaskBuilder"
                    ],
                    "llm_response_time": 3708,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "recordTaskAdded": {
                    "target_classes": [
                        {
                            "class_name": "Herder",
                            "similarity_score": 0.07459302301543416
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Herder"
                    ],
                    "llm_response_time": 2478,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "7dbdc15c668dfb5a4a91c79f339c22fb7178c368",
        "url": "https://github.com/apache/kafka/commit/7dbdc15c668dfb5a4a91c79f339c22fb7178c368",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public maybeCheckpoint() : void extracted from package pollAndUpdate() : void in class org.apache.kafka.streams.processor.internals.GlobalStreamThread.StateConsumer & moved to class org.apache.kafka.streams.processor.internals.GlobalStateMaintainer",
            "leftSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 265,
                    "endLine": 275,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package pollAndUpdate() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 270,
                    "endLine": 270,
                    "startColumn": 13,
                    "endColumn": 50,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 273,
                    "endLine": 273,
                    "startColumn": 17,
                    "endColumn": 33,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 272,
                    "endLine": 272,
                    "startColumn": 17,
                    "endColumn": 46,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 271,
                    "endLine": 274,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 271,
                    "endLine": 271,
                    "startColumn": 17,
                    "endColumn": 49,
                    "codeElementType": "INFIX_EXPRESSION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 271,
                    "endLine": 274,
                    "startColumn": 51,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateMaintainer.java",
                    "startLine": 163,
                    "endLine": 170,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public maybeCheckpoint() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateMaintainer.java",
                    "startLine": 165,
                    "endLine": 165,
                    "startColumn": 9,
                    "endColumn": 46,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateMaintainer.java",
                    "startLine": 168,
                    "endLine": 168,
                    "startColumn": 13,
                    "endColumn": 29,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateMaintainer.java",
                    "startLine": 167,
                    "endLine": 167,
                    "startColumn": 13,
                    "endColumn": 26,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateMaintainer.java",
                    "startLine": 166,
                    "endLine": 169,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateMaintainer.java",
                    "startLine": 166,
                    "endLine": 166,
                    "startColumn": 13,
                    "endColumn": 45,
                    "codeElementType": "INFIX_EXPRESSION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateMaintainer.java",
                    "startLine": 166,
                    "endLine": 169,
                    "startColumn": 129,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 256,
                    "endLine": 262,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package pollAndUpdate() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 261,
                    "endLine": 261,
                    "startColumn": 13,
                    "endColumn": 46,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "stateMaintainer.maybeCheckpoint()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 574,
        "extraction_results": {
            "success": true,
            "newCommitHash": "1d75be12be5762c23b7a31c0b37559f1ba8a719b",
            "newBranchName": "extract-maybeCheckpoint-pollAndUpdate-2c0cab3"
        },
        "telemetry": {
            "id": "31f90b08-d053-4322-964b-bbfe16819f36",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 65,
                "lineStart": 228,
                "lineEnd": 292,
                "bodyLineStart": 228,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                "sourceCode": "static class StateConsumer {\n        private final Consumer<byte[], byte[]> globalConsumer;\n        private final GlobalStateMaintainer stateMaintainer;\n        private final Time time;\n        private final Duration pollTime;\n        private final long flushInterval;\n        private final Logger log;\n\n        private long lastFlush;\n\n        StateConsumer(final LogContext logContext,\n                      final Consumer<byte[], byte[]> globalConsumer,\n                      final GlobalStateMaintainer stateMaintainer,\n                      final Time time,\n                      final Duration pollTime,\n                      final long flushInterval) {\n            this.log = logContext.logger(getClass());\n            this.globalConsumer = globalConsumer;\n            this.stateMaintainer = stateMaintainer;\n            this.time = time;\n            this.pollTime = pollTime;\n            this.flushInterval = flushInterval;\n        }\n\n        /**\n         * @throws IllegalStateException If store gets registered after initialized is already finished\n         * @throws StreamsException      if the store's change log does not contain the partition\n         */\n        void initialize() {\n            final Map<TopicPartition, Long> partitionOffsets = stateMaintainer.initialize();\n            globalConsumer.assign(partitionOffsets.keySet());\n            for (final Map.Entry<TopicPartition, Long> entry : partitionOffsets.entrySet()) {\n                globalConsumer.seek(entry.getKey(), entry.getValue());\n            }\n            lastFlush = time.milliseconds();\n        }\n\n        void pollAndUpdate() {\n            final ConsumerRecords<byte[], byte[]> received = globalConsumer.poll(pollTime);\n            for (final ConsumerRecord<byte[], byte[]> record : received) {\n                stateMaintainer.update(record);\n            }\n            final long now = time.milliseconds();\n            maybeCheckpoint(now);\n        }\n\n        private void maybeCheckpoint(long now) {\n            if (now - flushInterval >= lastFlush) {\n                stateMaintainer.flushState();\n                lastFlush = now;\n            }\n        }\n\n        public void close(final boolean wipeStateStore) throws IOException {\n            try {\n                globalConsumer.close();\n            } catch (final RuntimeException e) {\n                // just log an error if the consumer throws an exception during close\n                // so we can always attempt to close the state stores.\n                log.error(\"Failed to close global consumer due to the following error:\", e);\n            }\n\n            stateMaintainer.close(wipeStateStore);\n        }\n    }",
                "methodCount": 5
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 4,
                "candidates": [
                    {
                        "lineStart": 280,
                        "lineEnd": 290,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method close to class GlobalStateMaintainer",
                        "description": "Move method close to org.apache.kafka.streams.processor.internals.GlobalStateMaintainer\nRationale: The close method directly references globalConsumer and stateMaintainer. Since stateMaintainer seems to be an instance of GlobalStateMaintainer, it makes sense to place this method within GlobalStateMaintainer. This class appears responsible for maintaining state stores, including their lifecycle management, which includes closing them. Moreover, the presence of a close method signature in GlobalStateMaintainer reinforces that it is responsible for such operations. The method does not relate to time-based functionality, making the Time class an inappropriate choice.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 251,
                        "lineEnd": 262,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method initialize to class GlobalStateMaintainer",
                        "description": "Move method initialize to org.apache.kafka.streams.processor.internals.GlobalStateMaintainer\nRationale: The method `initialize` directly interacts with the `stateMaintainer` to initialize state, assigns partitions to a consumer, and seeks to specific offsets. All these operations are closely related to managing global state, a responsibility of `GlobalStateMaintainer`. Moving the `initialize` method there would encapsulate the initialization logic within the class designed to maintain global state, improving cohesion and making the code more maintainable. Additionally, `GlobalStateMaintainer` already has an `initialize` method, so consolidating these related operations would make the class more robust and focused.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 273,
                        "lineEnd": 278,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method maybeCheckpoint to class GlobalStateMaintainer",
                        "description": "Move method maybeCheckpoint to org.apache.kafka.streams.processor.internals.GlobalStateMaintainer\nRationale: The 'maybeCheckpoint' method seems to be managing state and ensuring that the state is flushed periodically based on the current time interval. This functionality is closely related to the responsibilities of the 'GlobalStateMaintainer' interface, which already has methods for state management like 'flushState'. Since it deals with state maintenance, moving this method to 'GlobalStateMaintainer' would ensure that state-related logic is encapsulated within the same interface, promoting cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 264,
                        "lineEnd": 271,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method pollAndUpdate to class GlobalStateMaintainer",
                        "description": "Move method pollAndUpdate to org.apache.kafka.streams.processor.internals.GlobalStateMaintainer\nRationale: The method `pollAndUpdate()` uses `globalConsumer.poll()` to fetch records and then calls the `update(record)` method on `stateMaintainer`, which is an instance of `GlobalStateMaintainer`. This indicates that the method is closely tied to the functionality of maintaining global state. Moving this method to the `GlobalStateMaintainer` class consolidates the state management logic in one place, which aligns with the single responsibility principle. Additionally, `maybeCheckpoint(now)` seems to be logically consistent with state maintenance responsibilities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "initialize",
                            "method_signature": "void initialize()",
                            "target_class": "GlobalStateMaintainer",
                            "rationale": "The method heavily relies on the GlobalStateMaintainer and only uses the globalConsumer and time from StateConsumer. Moving it to GlobalStateMaintainer would encapsulate the state initialization logic better."
                        },
                        {
                            "method_name": "pollAndUpdate",
                            "method_signature": "void pollAndUpdate()",
                            "target_class": "GlobalStateMaintainer",
                            "rationale": "This method extensively interacts with the stateMaintainer and contains logic specific to maintaining and updating the global state. It would be more suitable and coherent to place it within the GlobalStateMaintainer class."
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public void close(final boolean wipeStateStore) throws IOException",
                            "target_class": "GlobalStateMaintainer",
                            "rationale": "The close method involves closing the globalConsumer but is more responsible for closing the state management. It should be moved to GlobalStateMaintainer for a more cohesive and relevant class design."
                        }
                    ],
                    "llm_response_time": 3555
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "maybeCheckpoint",
                            "method_signature": "private void maybeCheckpoint(long now)",
                            "target_class": "GlobalStateMaintainer",
                            "rationale": "The maybeCheckpoint() method heavily relies on functionality provided by the GlobalStateMaintainer class. It deals with state flushing, which is a core responsibility of the GlobalStateMaintainer."
                        }
                    ],
                    "llm_response_time": 1778
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "initialize",
                            "method_signature": "void initialize()",
                            "target_class": "GlobalStateMaintainer",
                            "rationale": "The method initialize() is interacting heavily with the GlobalStateMaintainer. It initializes partition offsets and involves state maintenance, which indicates that it should belong to the GlobalStateMaintainer class."
                        },
                        {
                            "method_name": "maybeCheckpoint",
                            "method_signature": "private void maybeCheckpoint(long now)",
                            "target_class": "GlobalStateMaintainer",
                            "rationale": "The method maybeCheckpoint() is concerned with state management functions like flushing state and maintaining the last flush time, which are core functionalities of the GlobalStateMaintainer class."
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public void close(final boolean wipeStateStore) throws IOException",
                            "target_class": "GlobalStateMaintainer",
                            "rationale": "Since the close() method is primarily concerned with closing the state store and global consumer, it makes more sense for it to reside in a class dedicated to state management, such as GlobalStateMaintainer."
                        }
                    ],
                    "llm_response_time": 2519
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "initialize",
                            "method_signature": " initialize()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollAndUpdate",
                            "method_signature": " pollAndUpdate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCheckpoint",
                            "method_signature": "private maybeCheckpoint(long now)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public close(final boolean wipeStateStore)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "close",
                            "method_signature": "public close(final boolean wipeStateStore)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": " initialize()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCheckpoint",
                            "method_signature": "private maybeCheckpoint(long now)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollAndUpdate",
                            "method_signature": " pollAndUpdate()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "public close(final boolean wipeStateStore)": {
                        "first": {
                            "method_name": "close",
                            "method_signature": "public close(final boolean wipeStateStore)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.24691646752593047
                    },
                    " initialize()": {
                        "first": {
                            "method_name": "initialize",
                            "method_signature": " initialize()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.40548095615472896
                    },
                    "private maybeCheckpoint(long now)": {
                        "first": {
                            "method_name": "maybeCheckpoint",
                            "method_signature": "private maybeCheckpoint(long now)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.45891198567940084
                    },
                    " pollAndUpdate()": {
                        "first": {
                            "method_name": "pollAndUpdate",
                            "method_signature": " pollAndUpdate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5871043647858919
                    }
                },
                "voyage": {
                    "public close(final boolean wipeStateStore)": {
                        "first": {
                            "method_name": "close",
                            "method_signature": "public close(final boolean wipeStateStore)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5061086326701493
                    },
                    "private maybeCheckpoint(long now)": {
                        "first": {
                            "method_name": "maybeCheckpoint",
                            "method_signature": "private maybeCheckpoint(long now)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5505768178561731
                    },
                    " initialize()": {
                        "first": {
                            "method_name": "initialize",
                            "method_signature": " initialize()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6231106328011317
                    },
                    " pollAndUpdate()": {
                        "first": {
                            "method_name": "pollAndUpdate",
                            "method_signature": " pollAndUpdate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6346169038834558
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private maybeCheckpoint(long now)",
                        "public close(final boolean wipeStateStore)"
                    ],
                    "llm_response_time": 3826
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private maybeCheckpoint(long now)",
                        "public close(final boolean wipeStateStore)"
                    ],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "public close(final boolean wipeStateStore)",
                        "private maybeCheckpoint(long now)"
                    ],
                    "llm_response_time": 3369
                },
                "voyage": {
                    "priority_method_names": [
                        "public close(final boolean wipeStateStore)",
                        " initialize()",
                        " pollAndUpdate()",
                        "private maybeCheckpoint(long now)"
                    ],
                    "llm_response_time": 3897
                },
                "voyage-5": {
                    "priority_method_names": [
                        "public close(final boolean wipeStateStore)",
                        " initialize()",
                        " pollAndUpdate()",
                        "private maybeCheckpoint(long now)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private maybeCheckpoint(long now)",
                        "public close(final boolean wipeStateStore)"
                    ],
                    "llm_response_time": 2942
                }
            },
            "targetClassMap": {
                "close": {
                    "target_classes": [
                        {
                            "class_name": "GlobalStateMaintainer",
                            "similarity_score": 0.2822767727858068
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.34029856810846043
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GlobalStateMaintainer",
                        "Time"
                    ],
                    "llm_response_time": 2579,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "initialize": {
                    "target_classes": [
                        {
                            "class_name": "GlobalStateMaintainer",
                            "similarity_score": 0.2912758640177315
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.43148008839966806
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GlobalStateMaintainer",
                        "Time"
                    ],
                    "llm_response_time": 2938,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "maybeCheckpoint": {
                    "target_classes": [
                        {
                            "class_name": "GlobalStateMaintainer",
                            "similarity_score": 0.2389760596996216
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.16939262038071365
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GlobalStateMaintainer",
                        "Time"
                    ],
                    "llm_response_time": 3233,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "pollAndUpdate": {
                    "target_classes": [
                        {
                            "class_name": "GlobalStateMaintainer",
                            "similarity_score": 0.26688025634181184
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1698434317306439
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GlobalStateMaintainer",
                        "Time"
                    ],
                    "llm_response_time": 2966,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "ebeef4eb64a36d46565629d2a370eeb1635a7293",
        "url": "https://github.com/apache/kafka/commit/ebeef4eb64a36d46565629d2a370eeb1635a7293",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method private shouldInitialize() : boolean extracted from public resetInitializingPositions() : void in class org.apache.kafka.clients.consumer.internals.SubscriptionState & moved to class org.apache.kafka.clients.consumer.internals.SubscriptionState.TopicPartitionState",
            "leftSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 763,
                    "endLine": 776,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public resetInitializingPositions() : void"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 766,
                    "endLine": 766,
                    "startColumn": 17,
                    "endColumn": 75,
                    "codeElementType": "IF_STATEMENT_CONDITION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 1102,
                    "endLine": 1110,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "private shouldInitialize() : boolean"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 1109,
                    "endLine": 1109,
                    "startColumn": 13,
                    "endColumn": 94,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 764,
                    "endLine": 789,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public resetInitializingPositions() : void"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 779,
                    "endLine": 779,
                    "startColumn": 17,
                    "endColumn": 50,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "partitionState.shouldInitialize()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 580,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a8cc7a63b60777f7bafd567555e2ddc504e07391",
            "newBranchName": "extract-shouldInitialize-resetInitializingPositions-a1ca788"
        },
        "telemetry": {
            "id": "699ab01e-6913-46f5-9525-a8d7ff368d38",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1250,
                "lineStart": 54,
                "lineEnd": 1303,
                "bodyLineStart": 54,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                "sourceCode": "/**\n * A class for tracking the topics, partitions, and offsets for the consumer. A partition\n * is \"assigned\" either directly with {@link #assignFromUser(Set)} (manual assignment)\n * or with {@link #assignFromSubscribed(Collection)} (automatic assignment from subscription).\n * <p>\n * Once assigned, the partition is not considered \"fetchable\" until its initial position has\n * been set with {@link #seekValidated(TopicPartition, FetchPosition)}. Fetchable partitions\n * track a position which is the last offset that has been returned to the user. You can\n * suspend fetching from a partition through {@link #pause(TopicPartition)} without affecting the consumed\n * position. The partition will remain unfetchable until the {@link #resume(TopicPartition)} is\n * used. You can also query the pause state independently with {@link #isPaused(TopicPartition)}.\n * <p>\n * Note that pause state as well as the consumed positions are not preserved when partition\n * assignment is changed whether directly by the user or through a group rebalance.\n * <p>\n * Thread Safety: this class is thread-safe.\n */\npublic class SubscriptionState {\n    private static final String SUBSCRIPTION_EXCEPTION_MESSAGE =\n            \"Subscription to topics, partitions and pattern are mutually exclusive\";\n\n    private final Logger log;\n\n    private enum SubscriptionType {\n        NONE, AUTO_TOPICS, AUTO_PATTERN, USER_ASSIGNED\n    }\n\n    /* the type of subscription */\n    private SubscriptionType subscriptionType;\n\n    /* the pattern user has requested */\n    private Pattern subscribedPattern;\n\n    /* the list of topics the user has requested */\n    private Set<String> subscription;\n\n    /* The list of topics the group has subscribed to. This may include some topics which are not part\n     * of `subscription` for the leader of a group since it is responsible for detecting metadata changes\n     * which require a group rebalance. */\n    private Set<String> groupSubscription;\n\n    /* the partitions that are currently assigned, note that the order of partition matters (see FetchBuilder for more details) */\n    private final PartitionStates<TopicPartitionState> assignment;\n\n    /* Default offset reset strategy */\n    private final OffsetResetStrategy defaultResetStrategy;\n\n    /* User-provided listener to be invoked when assignment changes */\n    private Optional<ConsumerRebalanceListener> rebalanceListener;\n\n    private int assignmentId = 0;\n\n    @Override\n    public synchronized String toString() {\n        return \"SubscriptionState{\" +\n            \"type=\" + subscriptionType +\n            \", subscribedPattern=\" + subscribedPattern +\n            \", subscription=\" + String.join(\",\", subscription) +\n            \", groupSubscription=\" + String.join(\",\", groupSubscription) +\n            \", defaultResetStrategy=\" + defaultResetStrategy +\n            \", assignment=\" + assignment.partitionStateValues() + \" (id=\" + assignmentId + \")}\";\n    }\n\n    public synchronized String prettyString() {\n        switch (subscriptionType) {\n            case NONE:\n                return \"None\";\n            case AUTO_TOPICS:\n                return \"Subscribe(\" + String.join(\",\", subscription) + \")\";\n            case AUTO_PATTERN:\n                return \"Subscribe(\" + subscribedPattern + \")\";\n            case USER_ASSIGNED:\n                return \"Assign(\" + assignedPartitions() + \" , id=\" + assignmentId + \")\";\n            default:\n                throw new IllegalStateException(\"Unrecognized subscription type: \" + subscriptionType);\n        }\n    }\n\n    public SubscriptionState(LogContext logContext, OffsetResetStrategy defaultResetStrategy) {\n        this.log = logContext.logger(this.getClass());\n        this.defaultResetStrategy = defaultResetStrategy;\n        this.subscription = new TreeSet<>(); // use a sorted set for better logging\n        this.assignment = new PartitionStates<>();\n        this.groupSubscription = new HashSet<>();\n        this.subscribedPattern = null;\n        this.subscriptionType = SubscriptionType.NONE;\n    }\n\n    /**\n     * Monotonically increasing id which is incremented after every assignment change. This can\n     * be used to check when an assignment has changed.\n     *\n     * @return The current assignment Id\n     */\n    synchronized int assignmentId() {\n        return assignmentId;\n    }\n\n    /**\n     * This method sets the subscription type if it is not already set (i.e. when it is NONE),\n     * or verifies that the subscription type is equal to the give type when it is set (i.e.\n     * when it is not NONE)\n     * @param type The given subscription type\n     */\n    private void setSubscriptionType(SubscriptionType type) {\n        if (this.subscriptionType == SubscriptionType.NONE)\n            this.subscriptionType = type;\n        else if (this.subscriptionType != type)\n            throw new IllegalStateException(SUBSCRIPTION_EXCEPTION_MESSAGE);\n    }\n\n    public synchronized boolean subscribe(Set<String> topics, Optional<ConsumerRebalanceListener> listener) {\n        registerRebalanceListener(listener);\n        setSubscriptionType(SubscriptionType.AUTO_TOPICS);\n        return changeSubscription(topics);\n    }\n\n    public synchronized void subscribe(Pattern pattern, Optional<ConsumerRebalanceListener> listener) {\n        registerRebalanceListener(listener);\n        setSubscriptionType(SubscriptionType.AUTO_PATTERN);\n        this.subscribedPattern = pattern;\n    }\n\n    public synchronized boolean subscribeFromPattern(Set<String> topics) {\n        if (subscriptionType != SubscriptionType.AUTO_PATTERN)\n            throw new IllegalArgumentException(\"Attempt to subscribe from pattern while subscription type set to \" +\n                    subscriptionType);\n\n        return changeSubscription(topics);\n    }\n\n    private boolean changeSubscription(Set<String> topicsToSubscribe) {\n        if (subscription.equals(topicsToSubscribe))\n            return false;\n\n        subscription = topicsToSubscribe;\n        return true;\n    }\n\n    /**\n     * Set the current group subscription. This is used by the group leader to ensure\n     * that it receives metadata updates for all topics that the group is interested in.\n     *\n     * @param topics All topics from the group subscription\n     * @return true if the group subscription contains topics which are not part of the local subscription\n     */\n    synchronized boolean groupSubscribe(Collection<String> topics) {\n        if (!hasAutoAssignedPartitions())\n            throw new IllegalStateException(SUBSCRIPTION_EXCEPTION_MESSAGE);\n        groupSubscription = new HashSet<>(topics);\n        return !subscription.containsAll(groupSubscription);\n    }\n\n    /**\n     * Reset the group's subscription to only contain topics subscribed by this consumer.\n     */\n    synchronized void resetGroupSubscription() {\n        groupSubscription = Collections.emptySet();\n    }\n\n    /**\n     * Change the assignment to the specified partitions provided by the user,\n     * note this is different from {@link #assignFromSubscribed(Collection)}\n     * whose input partitions are provided from the subscribed topics.\n     */\n    public synchronized boolean assignFromUser(Set<TopicPartition> partitions) {\n        setSubscriptionType(SubscriptionType.USER_ASSIGNED);\n\n        if (this.assignment.partitionSet().equals(partitions))\n            return false;\n\n        assignmentId++;\n\n        // update the subscribed topics\n        Set<String> manualSubscribedTopics = new HashSet<>();\n        Map<TopicPartition, TopicPartitionState> partitionToState = new HashMap<>();\n        for (TopicPartition partition : partitions) {\n            TopicPartitionState state = assignment.stateValue(partition);\n            if (state == null)\n                state = new TopicPartitionState();\n            partitionToState.put(partition, state);\n\n            manualSubscribedTopics.add(partition.topic());\n        }\n\n        this.assignment.set(partitionToState);\n        return changeSubscription(manualSubscribedTopics);\n    }\n\n    /**\n     * @return true if assignments matches subscription, otherwise false\n     */\n    public synchronized boolean checkAssignmentMatchedSubscription(Collection<TopicPartition> assignments) {\n        for (TopicPartition topicPartition : assignments) {\n            if (this.subscribedPattern != null) {\n                if (!this.subscribedPattern.matcher(topicPartition.topic()).matches()) {\n                    log.info(\"Assigned partition {} for non-subscribed topic regex pattern; subscription pattern is {}\",\n                        topicPartition,\n                        this.subscribedPattern);\n\n                    return false;\n                }\n            } else {\n                if (!this.subscription.contains(topicPartition.topic())) {\n                    log.info(\"Assigned partition {} for non-subscribed topic; subscription is {}\", topicPartition, this.subscription);\n\n                    return false;\n                }\n            }\n        }\n\n        return true;\n    }\n\n    /**\n     * Change the assignment to the specified partitions returned from the coordinator, note this is\n     * different from {@link #assignFromUser(Set)} which directly set the assignment from user inputs.\n     */\n    public synchronized void assignFromSubscribed(Collection<TopicPartition> assignments) {\n        if (!this.hasAutoAssignedPartitions())\n            throw new IllegalArgumentException(\"Attempt to dynamically assign partitions while manual assignment in use\");\n\n        Map<TopicPartition, TopicPartitionState> assignedPartitionStates = new HashMap<>(assignments.size());\n        for (TopicPartition tp : assignments) {\n            TopicPartitionState state = this.assignment.stateValue(tp);\n            if (state == null)\n                state = new TopicPartitionState();\n            assignedPartitionStates.put(tp, state);\n        }\n\n        assignmentId++;\n        this.assignment.set(assignedPartitionStates);\n    }\n\n    private void registerRebalanceListener(Optional<ConsumerRebalanceListener> listener) {\n        this.rebalanceListener = Objects.requireNonNull(listener, \"RebalanceListener cannot be null\");\n    }\n\n    /**\n     * Check whether pattern subscription is in use.\n     *\n     */\n    synchronized boolean hasPatternSubscription() {\n        return this.subscriptionType == SubscriptionType.AUTO_PATTERN;\n    }\n\n    public synchronized boolean hasNoSubscriptionOrUserAssignment() {\n        return this.subscriptionType == SubscriptionType.NONE;\n    }\n\n    public synchronized void unsubscribe() {\n        this.subscription = Collections.emptySet();\n        this.groupSubscription = Collections.emptySet();\n        this.assignment.clear();\n        this.subscribedPattern = null;\n        this.subscriptionType = SubscriptionType.NONE;\n        this.assignmentId++;\n    }\n\n    /**\n     * Check whether a topic matches a subscribed pattern.\n     *\n     * @return true if pattern subscription is in use and the topic matches the subscribed pattern, false otherwise\n     */\n    synchronized boolean matchesSubscribedPattern(String topic) {\n        Pattern pattern = this.subscribedPattern;\n        if (hasPatternSubscription() && pattern != null)\n            return pattern.matcher(topic).matches();\n        return false;\n    }\n\n    public synchronized Set<String> subscription() {\n        if (hasAutoAssignedPartitions())\n            return this.subscription;\n        return Collections.emptySet();\n    }\n\n    public synchronized Set<TopicPartition> pausedPartitions() {\n        return collectPartitions(TopicPartitionState::isPaused);\n    }\n\n    /**\n     * Get the subscription topics for which metadata is required. For the leader, this will include\n     * the union of the subscriptions of all group members. For followers, it is just that member's\n     * subscription. This is used when querying topic metadata to detect the metadata changes which would\n     * require rebalancing. The leader fetches metadata for all topics in the group so that it\n     * can do the partition assignment (which requires at least partition counts for all topics\n     * to be assigned).\n     *\n     * @return The union of all subscribed topics in the group if this member is the leader\n     *   of the current generation; otherwise it returns the same set as {@link #subscription()}\n     */\n    synchronized Set<String> metadataTopics() {\n        if (groupSubscription.isEmpty())\n            return subscription;\n        else if (groupSubscription.containsAll(subscription))\n            return groupSubscription;\n        else {\n            // When subscription changes `groupSubscription` may be outdated, ensure that\n            // new subscription topics are returned.\n            Set<String> topics = new HashSet<>(groupSubscription);\n            topics.addAll(subscription);\n            return topics;\n        }\n    }\n\n    synchronized boolean needsMetadata(String topic) {\n        return subscription.contains(topic) || groupSubscription.contains(topic);\n    }\n\n    private TopicPartitionState assignedState(TopicPartition tp) {\n        TopicPartitionState state = this.assignment.stateValue(tp);\n        if (state == null)\n            throw new IllegalStateException(\"No current assignment for partition \" + tp);\n        return state;\n    }\n\n    private TopicPartitionState assignedStateOrNull(TopicPartition tp) {\n        return this.assignment.stateValue(tp);\n    }\n\n    public synchronized void seekValidated(TopicPartition tp, FetchPosition position) {\n        assignedState(tp).seekValidated(position);\n    }\n\n    public void seek(TopicPartition tp, long offset) {\n        seekValidated(tp, new FetchPosition(offset));\n    }\n\n    public void seekUnvalidated(TopicPartition tp, FetchPosition position) {\n        assignedState(tp).seekUnvalidated(position);\n    }\n\n    synchronized void maybeSeekUnvalidated(TopicPartition tp, FetchPosition position, OffsetResetStrategy requestedResetStrategy) {\n        TopicPartitionState state = assignedStateOrNull(tp);\n        if (state == null) {\n            log.debug(\"Skipping reset of partition {} since it is no longer assigned\", tp);\n        } else if (!state.awaitingReset()) {\n            log.debug(\"Skipping reset of partition {} since reset is no longer needed\", tp);\n        } else if (requestedResetStrategy != state.resetStrategy) {\n            log.debug(\"Skipping reset of partition {} since an alternative reset has been requested\", tp);\n        } else {\n            log.info(\"Resetting offset for partition {} to position {}.\", tp, position);\n            state.seekUnvalidated(position);\n        }\n    }\n\n    /**\n     * @return a modifiable copy of the currently assigned partitions\n     */\n    public synchronized Set<TopicPartition> assignedPartitions() {\n        return new HashSet<>(this.assignment.partitionSet());\n    }\n\n    /**\n     * @return a modifiable copy of the currently assigned partitions as a list\n     */\n    public synchronized List<TopicPartition> assignedPartitionsList() {\n        return new ArrayList<>(this.assignment.partitionSet());\n    }\n\n    /**\n     * Provides the number of assigned partitions in a thread safe manner.\n     * @return the number of assigned partitions.\n     */\n    synchronized int numAssignedPartitions() {\n        return this.assignment.size();\n    }\n\n    // Visible for testing\n    public synchronized List<TopicPartition> fetchablePartitions(Predicate<TopicPartition> isAvailable) {\n        // Since this is in the hot-path for fetching, we do this instead of using java.util.stream API\n        List<TopicPartition> result = new ArrayList<>();\n        assignment.forEach((topicPartition, topicPartitionState) -> {\n            // Cheap check is first to avoid evaluating the predicate if possible\n            if (topicPartitionState.isFetchable() && isAvailable.test(topicPartition)) {\n                result.add(topicPartition);\n            }\n        });\n        return result;\n    }\n\n    public synchronized boolean hasAutoAssignedPartitions() {\n        return this.subscriptionType == SubscriptionType.AUTO_TOPICS || this.subscriptionType == SubscriptionType.AUTO_PATTERN;\n    }\n\n    public synchronized void position(TopicPartition tp, FetchPosition position) {\n        assignedState(tp).position(position);\n    }\n\n    /**\n     * Enter the offset validation state if the leader for this partition is known to support a usable version of the\n     * OffsetsForLeaderEpoch API. If the leader node does not support the API, simply complete the offset validation.\n     *\n     * @param apiVersions supported API versions\n     * @param tp topic partition to validate\n     * @param leaderAndEpoch leader epoch of the topic partition\n     * @return true if we enter the offset validation state\n     */\n    public synchronized boolean maybeValidatePositionForCurrentLeader(ApiVersions apiVersions,\n                                                                      TopicPartition tp,\n                                                                      Metadata.LeaderAndEpoch leaderAndEpoch) {\n        TopicPartitionState state = assignedStateOrNull(tp);\n        if (state == null) {\n            log.debug(\"Skipping validating position for partition {} which is not currently assigned.\", tp);\n            return false;\n        }\n        if (leaderAndEpoch.leader.isPresent()) {\n            NodeApiVersions nodeApiVersions = apiVersions.get(leaderAndEpoch.leader.get().idString());\n            if (nodeApiVersions == null || hasUsableOffsetForLeaderEpochVersion(nodeApiVersions)) {\n                return state.maybeValidatePosition(leaderAndEpoch);\n            } else {\n                // If the broker does not support a newer version of OffsetsForLeaderEpoch, we skip validation\n                state.updatePositionLeaderNoValidation(leaderAndEpoch);\n                return false;\n            }\n        } else {\n            return state.maybeValidatePosition(leaderAndEpoch);\n        }\n    }\n\n    /**\n     * Attempt to complete validation with the end offset returned from the OffsetForLeaderEpoch request.\n     * @return Log truncation details if detected and no reset policy is defined.\n     */\n    public synchronized Optional<LogTruncation> maybeCompleteValidation(TopicPartition tp,\n                                                                        FetchPosition requestPosition,\n                                                                        EpochEndOffset epochEndOffset) {\n        TopicPartitionState state = assignedStateOrNull(tp);\n        if (state == null) {\n            log.debug(\"Skipping completed validation for partition {} which is not currently assigned.\", tp);\n        } else if (!state.awaitingValidation()) {\n            log.debug(\"Skipping completed validation for partition {} which is no longer expecting validation.\", tp);\n        } else {\n            SubscriptionState.FetchPosition currentPosition = state.position;\n            if (!currentPosition.equals(requestPosition)) {\n                log.debug(\"Skipping completed validation for partition {} since the current position {} \" +\n                          \"no longer matches the position {} when the request was sent\",\n                          tp, currentPosition, requestPosition);\n            } else if (epochEndOffset.endOffset() == UNDEFINED_EPOCH_OFFSET ||\n                        epochEndOffset.leaderEpoch() == UNDEFINED_EPOCH) {\n                if (hasDefaultOffsetResetPolicy()) {\n                    log.info(\"Truncation detected for partition {} at offset {}, resetting offset\",\n                             tp, currentPosition);\n                    requestOffsetReset(tp);\n                } else {\n                    log.warn(\"Truncation detected for partition {} at offset {}, but no reset policy is set\",\n                             tp, currentPosition);\n                    return Optional.of(new LogTruncation(tp, requestPosition, Optional.empty()));\n                }\n            } else if (epochEndOffset.endOffset() < currentPosition.offset) {\n                if (hasDefaultOffsetResetPolicy()) {\n                    SubscriptionState.FetchPosition newPosition = new SubscriptionState.FetchPosition(\n                            epochEndOffset.endOffset(), Optional.of(epochEndOffset.leaderEpoch()),\n                            currentPosition.currentLeader);\n                    log.info(\"Truncation detected for partition {} at offset {}, resetting offset to \" +\n                             \"the first offset known to diverge {}\", tp, currentPosition, newPosition);\n                    state.seekValidated(newPosition);\n                } else {\n                    OffsetAndMetadata divergentOffset = new OffsetAndMetadata(epochEndOffset.endOffset(),\n                        Optional.of(epochEndOffset.leaderEpoch()), null);\n                    log.warn(\"Truncation detected for partition {} at offset {} (the end offset from the \" +\n                             \"broker is {}), but no reset policy is set\", tp, currentPosition, divergentOffset);\n                    return Optional.of(new LogTruncation(tp, requestPosition, Optional.of(divergentOffset)));\n                }\n            } else {\n                state.completeValidation();\n            }\n        }\n\n        return Optional.empty();\n    }\n\n    public synchronized boolean awaitingValidation(TopicPartition tp) {\n        return assignedState(tp).awaitingValidation();\n    }\n\n    public synchronized void completeValidation(TopicPartition tp) {\n        assignedState(tp).completeValidation();\n    }\n\n    public synchronized FetchPosition validPosition(TopicPartition tp) {\n        return assignedState(tp).validPosition();\n    }\n\n    public synchronized FetchPosition position(TopicPartition tp) {\n        return assignedState(tp).position;\n    }\n\n    public synchronized FetchPosition positionOrNull(TopicPartition tp) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state == null) {\n            return null;\n        }\n        return assignedState(tp).position;\n    }\n\n    public synchronized Long partitionLag(TopicPartition tp, IsolationLevel isolationLevel) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        if (topicPartitionState.position == null) {\n            return null;\n        } else if (isolationLevel == IsolationLevel.READ_COMMITTED) {\n            return topicPartitionState.lastStableOffset == null ? null : topicPartitionState.lastStableOffset - topicPartitionState.position.offset;\n        } else {\n            return topicPartitionState.highWatermark == null ? null : topicPartitionState.highWatermark - topicPartitionState.position.offset;\n        }\n    }\n\n    public synchronized Long partitionEndOffset(TopicPartition tp, IsolationLevel isolationLevel) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        if (isolationLevel == IsolationLevel.READ_COMMITTED) {\n            return topicPartitionState.lastStableOffset;\n        } else {\n            return topicPartitionState.highWatermark;\n        }\n    }\n\n    public synchronized void requestPartitionEndOffset(TopicPartition tp) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        topicPartitionState.requestEndOffset();\n    }\n\n    public synchronized boolean partitionEndOffsetRequested(TopicPartition tp) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        return topicPartitionState.endOffsetRequested();\n    }\n\n    synchronized Long partitionLead(TopicPartition tp) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        return topicPartitionState.logStartOffset == null ? null : topicPartitionState.position.offset - topicPartitionState.logStartOffset;\n    }\n\n    synchronized void updateHighWatermark(TopicPartition tp, long highWatermark) {\n        assignedState(tp).highWatermark(highWatermark);\n    }\n\n    synchronized boolean tryUpdatingHighWatermark(TopicPartition tp, long highWatermark) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state != null) {\n            assignedState(tp).highWatermark(highWatermark);\n            return true;\n        }\n        return false;\n    }\n\n    synchronized boolean tryUpdatingLogStartOffset(TopicPartition tp, long highWatermark) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state != null) {\n            assignedState(tp).logStartOffset(highWatermark);\n            return true;\n        }\n        return false;\n    }\n\n    synchronized void updateLastStableOffset(TopicPartition tp, long lastStableOffset) {\n        assignedState(tp).lastStableOffset(lastStableOffset);\n    }\n\n    synchronized boolean tryUpdatingLastStableOffset(TopicPartition tp, long lastStableOffset) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state != null) {\n            assignedState(tp).lastStableOffset(lastStableOffset);\n            return true;\n        }\n        return false;\n    }\n\n    /**\n     * Set the preferred read replica with a lease timeout. After this time, the replica will no longer be valid and\n     * {@link #preferredReadReplica(TopicPartition, long)} will return an empty result.\n     *\n     * @param tp The topic partition\n     * @param preferredReadReplicaId The preferred read replica\n     * @param timeMs The time at which this preferred replica is no longer valid\n     */\n    public synchronized void updatePreferredReadReplica(TopicPartition tp, int preferredReadReplicaId, LongSupplier timeMs) {\n        assignedState(tp).updatePreferredReadReplica(preferredReadReplicaId, timeMs);\n    }\n\n    /**\n     * Tries to set the preferred read replica with a lease timeout. After this time, the replica will no longer be valid and\n     * {@link #preferredReadReplica(TopicPartition, long)} will return an empty result. If the preferred replica of\n     * the partition could not be updated (e.g. because the partition is not assigned) this method will return\n     * {@code false}, otherwise it will return {@code true}.\n     *\n     * @param tp The topic partition\n     * @param preferredReadReplicaId The preferred read replica\n     * @param timeMs The time at which this preferred replica is no longer valid\n     * @return {@code true} if the preferred read replica was updated, {@code false} otherwise.\n     */\n    public synchronized boolean tryUpdatingPreferredReadReplica(TopicPartition tp,\n                                                             int preferredReadReplicaId,\n                                                             LongSupplier timeMs) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state != null) {\n            assignedState(tp).updatePreferredReadReplica(preferredReadReplicaId, timeMs);\n            return true;\n        }\n        return false;\n    }\n\n    /**\n     * Get the preferred read replica\n     *\n     * @param tp The topic partition\n     * @param timeMs The current time\n     * @return Returns the current preferred read replica, if it has been set and if it has not expired.\n     */\n    public synchronized Optional<Integer> preferredReadReplica(TopicPartition tp, long timeMs) {\n        final TopicPartitionState topicPartitionState = assignedStateOrNull(tp);\n        if (topicPartitionState == null) {\n            return Optional.empty();\n        } else {\n            return topicPartitionState.preferredReadReplica(timeMs);\n        }\n    }\n\n    /**\n     * Unset the preferred read replica. This causes the fetcher to go back to the leader for fetches.\n     *\n     * @param tp The topic partition\n     * @return the removed preferred read replica if set, Empty otherwise.\n     */\n    public synchronized Optional<Integer> clearPreferredReadReplica(TopicPartition tp) {\n        final TopicPartitionState topicPartitionState = assignedStateOrNull(tp);\n        if (topicPartitionState == null) {\n            return Optional.empty();\n        } else {\n            return topicPartitionState.clearPreferredReadReplica();\n        }\n    }\n\n    public synchronized Map<TopicPartition, OffsetAndMetadata> allConsumed() {\n        Map<TopicPartition, OffsetAndMetadata> allConsumed = new HashMap<>();\n        assignment.forEach((topicPartition, partitionState) -> {\n            if (partitionState.hasValidPosition())\n                allConsumed.put(topicPartition, new OffsetAndMetadata(partitionState.position.offset,\n                        partitionState.position.offsetEpoch, \"\"));\n        });\n        return allConsumed;\n    }\n\n    public synchronized void requestOffsetReset(TopicPartition partition, OffsetResetStrategy offsetResetStrategy) {\n        assignedState(partition).reset(offsetResetStrategy);\n    }\n\n    public synchronized void requestOffsetReset(Collection<TopicPartition> partitions, OffsetResetStrategy offsetResetStrategy) {\n        partitions.forEach(tp -> {\n            log.info(\"Seeking to {} offset of partition {}\", offsetResetStrategy, tp);\n            assignedState(tp).reset(offsetResetStrategy);\n        });\n    }\n\n    public void requestOffsetReset(TopicPartition partition) {\n        requestOffsetReset(partition, defaultResetStrategy);\n    }\n\n    public synchronized void requestOffsetResetIfPartitionAssigned(TopicPartition partition) {\n        final TopicPartitionState state = assignedStateOrNull(partition);\n        if (state != null) {\n            state.reset(defaultResetStrategy);\n        }\n    }\n\n\n    synchronized void setNextAllowedRetry(Set<TopicPartition> partitions, long nextAllowResetTimeMs) {\n        for (TopicPartition partition : partitions) {\n            assignedState(partition).setNextAllowedRetry(nextAllowResetTimeMs);\n        }\n    }\n\n    boolean hasDefaultOffsetResetPolicy() {\n        return defaultResetStrategy != OffsetResetStrategy.NONE;\n    }\n\n    public synchronized boolean isOffsetResetNeeded(TopicPartition partition) {\n        return assignedState(partition).awaitingReset();\n    }\n\n    public synchronized OffsetResetStrategy resetStrategy(TopicPartition partition) {\n        return assignedState(partition).resetStrategy();\n    }\n\n    public synchronized boolean hasAllFetchPositions() {\n        // Since this is in the hot-path for fetching, we do this instead of using java.util.stream API\n        Iterator<TopicPartitionState> it = assignment.stateIterator();\n        while (it.hasNext()) {\n            if (!it.next().hasValidPosition()) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    public synchronized Set<TopicPartition> initializingPartitions() {\n        return collectPartitions(state -> shouldInitialize(state) && !state.pendingOnAssignedCallback);\n    }\n\n    private Set<TopicPartition> collectPartitions(Predicate<TopicPartitionState> filter) {\n        Set<TopicPartition> result = new HashSet<>();\n        assignment.forEach((topicPartition, topicPartitionState) -> {\n            if (filter.test(topicPartitionState)) {\n                result.add(topicPartition);\n            }\n        });\n        return result;\n    }\n\n\n    public synchronized void resetInitializingPositions() {\n        final Set<TopicPartition> partitionsWithNoOffsets = new HashSet<>();\n        assignment.forEach((tp, partitionState) -> {\n            if (shouldInitialize(partitionState)) {\n                if (defaultResetStrategy == OffsetResetStrategy.NONE)\n                    partitionsWithNoOffsets.add(tp);\n                else\n                    requestOffsetReset(tp);\n            }\n        });\n\n        if (!partitionsWithNoOffsets.isEmpty())\n            throw new NoOffsetForPartitionException(partitionsWithNoOffsets);\n    }\n\n    private boolean shouldInitialize(TopicPartitionState partitionState) {\n        return partitionState.fetchState.equals(FetchStates.INITIALIZING) && !partitionState.pendingOnAssignedCallback;\n    }\n\n    public synchronized Set<TopicPartition> partitionsNeedingReset(long nowMs) {\n        return collectPartitions(state -> state.awaitingReset() && !state.awaitingRetryBackoff(nowMs));\n    }\n\n    public synchronized Set<TopicPartition> partitionsNeedingValidation(long nowMs) {\n        return collectPartitions(state -> state.awaitingValidation() && !state.awaitingRetryBackoff(nowMs));\n    }\n\n    public synchronized boolean isAssigned(TopicPartition tp) {\n        return assignment.contains(tp);\n    }\n\n    public synchronized boolean isPaused(TopicPartition tp) {\n        TopicPartitionState assignedOrNull = assignedStateOrNull(tp);\n        return assignedOrNull != null && assignedOrNull.isPaused();\n    }\n\n    synchronized boolean isFetchable(TopicPartition tp) {\n        TopicPartitionState assignedOrNull = assignedStateOrNull(tp);\n        return assignedOrNull != null && assignedOrNull.isFetchable();\n    }\n\n    public synchronized boolean hasValidPosition(TopicPartition tp) {\n        TopicPartitionState assignedOrNull = assignedStateOrNull(tp);\n        return assignedOrNull != null && assignedOrNull.hasValidPosition();\n    }\n\n    public synchronized void pause(TopicPartition tp) {\n        assignedState(tp).pause();\n    }\n\n    public synchronized void markPendingRevocation(Set<TopicPartition> tps) {\n        tps.forEach(tp -> assignedState(tp).markPendingRevocation());\n    }\n\n    // Visible for testing\n    synchronized void markPendingOnAssignedCallback(Collection<TopicPartition> tps,\n                                                    boolean pendingOnAssignedCallback) {\n        tps.forEach(tp -> assignedState(tp).markPendingOnAssignedCallback(pendingOnAssignedCallback));\n    }\n\n    /**\n     * Change the assignment to the specified partitions returned from the coordinator and mark\n     * them as awaiting onPartitionsAssigned callback. This will ensure that the partitions are\n     * included in the assignment, but are not fetchable or initialize positions while the\n     * callback runs. This is expected to be used by the async consumer.\n     *\n     * @param fullAssignment  Full collection of partitions assigned. Includes previously owned\n     *                        and newly added partitions.\n     * @param addedPartitions Subset of the fullAssignment containing the added partitions. These\n     *                        are not fetchable until the onPartitionsAssigned callback completes.\n     */\n    public synchronized void assignFromSubscribedAwaitingCallback(Collection<TopicPartition> fullAssignment,\n                                                                  Collection<TopicPartition> addedPartitions) {\n        assignFromSubscribed(fullAssignment);\n        markPendingOnAssignedCallback(addedPartitions, true);\n    }\n\n    /**\n     * Enable fetching and updating positions for the given partitions that were added to the\n     * assignment, but waiting for the onPartitionsAssigned callback to complete. This is\n     * expected to be used by the async consumer.\n     */\n    public synchronized void enablePartitionsAwaitingCallback(Collection<TopicPartition> partitions) {\n        markPendingOnAssignedCallback(partitions, false);\n    }\n\n    public synchronized void resume(TopicPartition tp) {\n        assignedState(tp).resume();\n    }\n\n    synchronized void requestFailed(Set<TopicPartition> partitions, long nextRetryTimeMs) {\n        for (TopicPartition partition : partitions) {\n            // by the time the request failed, the assignment may no longer\n            // contain this partition any more, in which case we would just ignore.\n            final TopicPartitionState state = assignedStateOrNull(partition);\n            if (state != null)\n                state.requestFailed(nextRetryTimeMs);\n        }\n    }\n\n    synchronized void movePartitionToEnd(TopicPartition tp) {\n        assignment.moveToEnd(tp);\n    }\n\n    public synchronized Optional<ConsumerRebalanceListener> rebalanceListener() {\n        return rebalanceListener;\n    }\n\n    private static class TopicPartitionState {\n\n        private FetchState fetchState;\n        private FetchPosition position; // last consumed position\n\n        private Long highWatermark; // the high watermark from last fetch\n        private Long logStartOffset; // the log start offset\n        private Long lastStableOffset;\n        private boolean paused;  // whether this partition has been paused by the user\n        private boolean pendingRevocation;\n        private boolean pendingOnAssignedCallback;\n        private OffsetResetStrategy resetStrategy;  // the strategy to use if the offset needs resetting\n        private Long nextRetryTimeMs;\n        private Integer preferredReadReplica;\n        private Long preferredReadReplicaExpireTimeMs;\n        private boolean endOffsetRequested;\n        \n        TopicPartitionState() {\n            this.paused = false;\n            this.pendingRevocation = false;\n            this.pendingOnAssignedCallback = false;\n            this.endOffsetRequested = false;\n            this.fetchState = FetchStates.INITIALIZING;\n            this.position = null;\n            this.highWatermark = null;\n            this.logStartOffset = null;\n            this.lastStableOffset = null;\n            this.resetStrategy = null;\n            this.nextRetryTimeMs = null;\n            this.preferredReadReplica = null;\n        }\n\n        public boolean endOffsetRequested() {\n            return endOffsetRequested;\n        }\n\n        public void requestEndOffset() {\n            endOffsetRequested = true;\n        }\n\n        private void transitionState(FetchState newState, Runnable runIfTransitioned) {\n            FetchState nextState = this.fetchState.transitionTo(newState);\n            if (nextState.equals(newState)) {\n                this.fetchState = nextState;\n                runIfTransitioned.run();\n                if (this.position == null && nextState.requiresPosition()) {\n                    throw new IllegalStateException(\"Transitioned subscription state to \" + nextState + \", but position is null\");\n                } else if (!nextState.requiresPosition()) {\n                    this.position = null;\n                }\n            }\n        }\n\n        private Optional<Integer> preferredReadReplica(long timeMs) {\n            if (preferredReadReplicaExpireTimeMs != null && timeMs > preferredReadReplicaExpireTimeMs) {\n                preferredReadReplica = null;\n                return Optional.empty();\n            } else {\n                return Optional.ofNullable(preferredReadReplica);\n            }\n        }\n\n        private void updatePreferredReadReplica(int preferredReadReplica, LongSupplier timeMs) {\n            if (this.preferredReadReplica == null || preferredReadReplica != this.preferredReadReplica) {\n                this.preferredReadReplica = preferredReadReplica;\n                this.preferredReadReplicaExpireTimeMs = timeMs.getAsLong();\n            }\n        }\n\n        private Optional<Integer> clearPreferredReadReplica() {\n            if (preferredReadReplica != null) {\n                int removedReplicaId = this.preferredReadReplica;\n                this.preferredReadReplica = null;\n                this.preferredReadReplicaExpireTimeMs = null;\n                return Optional.of(removedReplicaId);\n            } else {\n                return Optional.empty();\n            }\n        }\n\n        private void reset(OffsetResetStrategy strategy) {\n            transitionState(FetchStates.AWAIT_RESET, () -> {\n                this.resetStrategy = strategy;\n                this.nextRetryTimeMs = null;\n            });\n        }\n\n        /**\n         * Check if the position exists and needs to be validated. If so, enter the AWAIT_VALIDATION state. This method\n         * also will update the position with the current leader and epoch.\n         *\n         * @param currentLeaderAndEpoch leader and epoch to compare the offset with\n         * @return true if the position is now awaiting validation\n         */\n        private boolean maybeValidatePosition(Metadata.LeaderAndEpoch currentLeaderAndEpoch) {\n            if (this.fetchState.equals(FetchStates.AWAIT_RESET)) {\n                return false;\n            }\n\n            if (!currentLeaderAndEpoch.leader.isPresent()) {\n                return false;\n            }\n\n            if (position != null && !position.currentLeader.equals(currentLeaderAndEpoch)) {\n                FetchPosition newPosition = new FetchPosition(position.offset, position.offsetEpoch, currentLeaderAndEpoch);\n                validatePosition(newPosition);\n                preferredReadReplica = null;\n            }\n            return this.fetchState.equals(FetchStates.AWAIT_VALIDATION);\n        }\n\n        /**\n         * For older versions of the API, we cannot perform offset validation so we simply transition directly to FETCHING\n         */\n        private void updatePositionLeaderNoValidation(Metadata.LeaderAndEpoch currentLeaderAndEpoch) {\n            if (position != null) {\n                transitionState(FetchStates.FETCHING, () -> {\n                    this.position = new FetchPosition(position.offset, position.offsetEpoch, currentLeaderAndEpoch);\n                    this.nextRetryTimeMs = null;\n                });\n            }\n        }\n\n        private void validatePosition(FetchPosition position) {\n            if (position.offsetEpoch.isPresent() && position.currentLeader.epoch.isPresent()) {\n                transitionState(FetchStates.AWAIT_VALIDATION, () -> {\n                    this.position = position;\n                    this.nextRetryTimeMs = null;\n                });\n            } else {\n                // If we have no epoch information for the current position, then we can skip validation\n                transitionState(FetchStates.FETCHING, () -> {\n                    this.position = position;\n                    this.nextRetryTimeMs = null;\n                });\n            }\n        }\n\n        /**\n         * Clear the awaiting validation state and enter fetching.\n         */\n        private void completeValidation() {\n            if (hasPosition()) {\n                transitionState(FetchStates.FETCHING, () -> this.nextRetryTimeMs = null);\n            }\n        }\n\n        private boolean awaitingValidation() {\n            return fetchState.equals(FetchStates.AWAIT_VALIDATION);\n        }\n\n        private boolean awaitingRetryBackoff(long nowMs) {\n            return nextRetryTimeMs != null && nowMs < nextRetryTimeMs;\n        }\n\n        private boolean awaitingReset() {\n            return fetchState.equals(FetchStates.AWAIT_RESET);\n        }\n\n        private void setNextAllowedRetry(long nextAllowedRetryTimeMs) {\n            this.nextRetryTimeMs = nextAllowedRetryTimeMs;\n        }\n\n        private void requestFailed(long nextAllowedRetryTimeMs) {\n            this.nextRetryTimeMs = nextAllowedRetryTimeMs;\n        }\n\n        private boolean hasValidPosition() {\n            return fetchState.hasValidPosition();\n        }\n\n        private boolean hasPosition() {\n            return position != null;\n        }\n\n        private boolean isPaused() {\n            return paused;\n        }\n\n        private void seekValidated(FetchPosition position) {\n            transitionState(FetchStates.FETCHING, () -> {\n                this.position = position;\n                this.resetStrategy = null;\n                this.nextRetryTimeMs = null;\n            });\n        }\n\n        private void seekUnvalidated(FetchPosition fetchPosition) {\n            seekValidated(fetchPosition);\n            validatePosition(fetchPosition);\n        }\n\n        private void position(FetchPosition position) {\n            if (!hasValidPosition())\n                throw new IllegalStateException(\"Cannot set a new position without a valid current position\");\n            this.position = position;\n        }\n\n        private FetchPosition validPosition() {\n            if (hasValidPosition()) {\n                return position;\n            } else {\n                return null;\n            }\n        }\n\n        private void pause() {\n            this.paused = true;\n        }\n\n        private void markPendingRevocation() {\n            this.pendingRevocation = true;\n        }\n\n        private void markPendingOnAssignedCallback(boolean pendingOnAssignedCallback) {\n            this.pendingOnAssignedCallback = pendingOnAssignedCallback;\n        }\n\n        private void resume() {\n            this.paused = false;\n        }\n\n        private boolean isFetchable() {\n            return !paused && !pendingRevocation && !pendingOnAssignedCallback && hasValidPosition();\n        }\n\n        private void highWatermark(Long highWatermark) {\n            this.highWatermark = highWatermark;\n            this.endOffsetRequested = false;\n        }\n\n        private void logStartOffset(Long logStartOffset) {\n            this.logStartOffset = logStartOffset;\n        }\n\n        private void lastStableOffset(Long lastStableOffset) {\n            this.lastStableOffset = lastStableOffset;\n            this.endOffsetRequested = false;\n        }\n\n        private OffsetResetStrategy resetStrategy() {\n            return resetStrategy;\n        }\n    }\n\n    /**\n     * The fetch state of a partition. This class is used to determine valid state transitions and expose the some of\n     * the behavior of the current fetch state. Actual state variables are stored in the {@link TopicPartitionState}.\n     */\n    interface FetchState {\n        default FetchState transitionTo(FetchState newState) {\n            if (validTransitions().contains(newState)) {\n                return newState;\n            } else {\n                return this;\n            }\n        }\n\n        /**\n         * Return the valid states which this state can transition to\n         */\n        Collection<FetchState> validTransitions();\n\n        /**\n         * Test if this state requires a position to be set\n         */\n        boolean requiresPosition();\n\n        /**\n         * Test if this state is considered to have a valid position which can be used for fetching\n         */\n        boolean hasValidPosition();\n    }\n\n    /**\n     * An enumeration of all the possible fetch states. The state transitions are encoded in the values returned by\n     * {@link FetchState#validTransitions}.\n     */\n    enum FetchStates implements FetchState {\n        INITIALIZING() {\n            @Override\n            public Collection<FetchState> validTransitions() {\n                return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET, FetchStates.AWAIT_VALIDATION);\n            }\n\n            @Override\n            public boolean requiresPosition() {\n                return false;\n            }\n\n            @Override\n            public boolean hasValidPosition() {\n                return false;\n            }\n        },\n\n        FETCHING() {\n            @Override\n            public Collection<FetchState> validTransitions() {\n                return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET, FetchStates.AWAIT_VALIDATION);\n            }\n\n            @Override\n            public boolean requiresPosition() {\n                return true;\n            }\n\n            @Override\n            public boolean hasValidPosition() {\n                return true;\n            }\n        },\n\n        AWAIT_RESET() {\n            @Override\n            public Collection<FetchState> validTransitions() {\n                return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET);\n            }\n\n            @Override\n            public boolean requiresPosition() {\n                return false;\n            }\n\n            @Override\n            public boolean hasValidPosition() {\n                return false;\n            }\n        },\n\n        AWAIT_VALIDATION() {\n            @Override\n            public Collection<FetchState> validTransitions() {\n                return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET, FetchStates.AWAIT_VALIDATION);\n            }\n\n            @Override\n            public boolean requiresPosition() {\n                return true;\n            }\n\n            @Override\n            public boolean hasValidPosition() {\n                return false;\n            }\n        }\n    }\n\n    /**\n     * Represents the position of a partition subscription.\n     *\n     * This includes the offset and epoch from the last record in\n     * the batch from a FetchResponse. It also includes the leader epoch at the time the batch was consumed.\n     */\n    public static class FetchPosition {\n        public final long offset;\n        final Optional<Integer> offsetEpoch;\n        final Metadata.LeaderAndEpoch currentLeader;\n\n        FetchPosition(long offset) {\n            this(offset, Optional.empty(), Metadata.LeaderAndEpoch.noLeaderOrEpoch());\n        }\n\n        public FetchPosition(long offset, Optional<Integer> offsetEpoch, Metadata.LeaderAndEpoch currentLeader) {\n            this.offset = offset;\n            this.offsetEpoch = Objects.requireNonNull(offsetEpoch);\n            this.currentLeader = Objects.requireNonNull(currentLeader);\n        }\n\n        @Override\n        public boolean equals(Object o) {\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n            FetchPosition that = (FetchPosition) o;\n            return offset == that.offset &&\n                    offsetEpoch.equals(that.offsetEpoch) &&\n                    currentLeader.equals(that.currentLeader);\n        }\n\n        @Override\n        public int hashCode() {\n            return Objects.hash(offset, offsetEpoch, currentLeader);\n        }\n\n        @Override\n        public String toString() {\n            return \"FetchPosition{\" +\n                    \"offset=\" + offset +\n                    \", offsetEpoch=\" + offsetEpoch +\n                    \", currentLeader=\" + currentLeader +\n                    '}';\n        }\n    }\n\n    public static class LogTruncation {\n        public final TopicPartition topicPartition;\n        public final FetchPosition fetchPosition;\n        public final Optional<OffsetAndMetadata> divergentOffsetOpt;\n\n        public LogTruncation(TopicPartition topicPartition,\n                             FetchPosition fetchPosition,\n                             Optional<OffsetAndMetadata> divergentOffsetOpt) {\n            this.topicPartition = topicPartition;\n            this.fetchPosition = fetchPosition;\n            this.divergentOffsetOpt = divergentOffsetOpt;\n        }\n\n        @Override\n        public String toString() {\n            StringBuilder bldr = new StringBuilder()\n                .append(\"(partition=\")\n                .append(topicPartition)\n                .append(\", fetchOffset=\")\n                .append(fetchPosition.offset)\n                .append(\", fetchEpoch=\")\n                .append(fetchPosition.offsetEpoch);\n\n            if (divergentOffsetOpt.isPresent()) {\n                OffsetAndMetadata divergentOffset = divergentOffsetOpt.get();\n                bldr.append(\", divergentOffset=\")\n                    .append(divergentOffset.offset())\n                    .append(\", divergentEpoch=\")\n                    .append(divergentOffset.leaderEpoch());\n            } else {\n                bldr.append(\", divergentOffset=unknown\")\n                    .append(\", divergentEpoch=unknown\");\n            }\n\n            return bldr.append(\")\").toString();\n\n        }\n    }\n}",
                "methodCount": 141
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 6,
                "candidates": [
                    {
                        "lineStart": 777,
                        "lineEnd": 779,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method shouldInitialize to class TopicPartitionState",
                        "description": "Move method shouldInitialize to org.apache.kafka.clients.consumer.internals.SubscriptionState.TopicPartitionState\nRationale: null",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 363,
                        "lineEnd": 368,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method assignedState to class SubscriptionType",
                        "description": "Move method assignedState to org.apache.kafka.clients.consumer.internals.SubscriptionState.SubscriptionType\nRationale: The method assignedState(TopicPartition tp) checks and retrieves the state of a TopicPartition that belongs to an assigned subscription. Given that SubscriptionType encompasses different types of subscription states (NONE, AUTO_TOPICS, AUTO_PATTERN, USER_ASSIGNED), it is logical to associate this functionality here. Furthermore, SubscriptionType is more relevant to topics and their states than the existing class, ensuring a cohesive distribution of responsibilities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 370,
                        "lineEnd": 372,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method assignedStateOrNull to class SubscriptionType",
                        "description": "Move method assignedStateOrNull to org.apache.kafka.clients.consumer.internals.SubscriptionState.SubscriptionType\nRationale: The assignedStateOrNull() method references assignment and state management closely associated with a subscription's state. Given that SubscriptionType represents different subscription states and types (NONE, AUTO_TOPICS, AUTO_PATTERN, USER_ASSIGNED), the method fits naturally within SubscriptionType. This alignment suggests that SubscriptionType is the logical class to handle the state evaluation of topic partitions, making state management more cohesive and intuitively located. Moving the method will strengthen the cohesion of SubscriptionType by centralizing the logic related to state management and reducing redundancy.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1037,
                        "lineEnd": 1039,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method hasValidPosition to class FetchPosition",
                        "description": "Move method hasValidPosition to org.apache.kafka.clients.consumer.internals.SubscriptionState.FetchPosition\nRationale: The method `hasValidPosition()` directly relies on the `FetchPosition` class. Since the method checks an attribute (i.e., `fetchState`'s validity) that seems to belong to the fetching position logic, it should be encapsulated within the FetchPosition class itself. This enhances cohesion by grouping related functionalities together, making the code easier to maintain and understand.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1017,
                        "lineEnd": 1019,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method awaitingValidation to class FetchState",
                        "description": "Move method awaitingValidation to org.apache.kafka.clients.consumer.internals.SubscriptionState.FetchState\nRationale: The method awaitingValidation() checks if the current fetch state is equal to FetchStates.AWAIT_VALIDATION. This is directly related to the FetchState interface, which defines the states a partition can be in. Moving this method to FetchState makes logical sense as it deals with fetch state validation, making the state transitions and validations more cohesive within the FetchState class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1025,
                        "lineEnd": 1027,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method awaitingReset to class FetchState",
                        "description": "Move method awaitingReset to org.apache.kafka.clients.consumer.internals.SubscriptionState.FetchState\nRationale: The method awaitingReset() relies on the fetchState variable and its comparison to a specific FetchStates enum. FetchState is directly concerned with the state transitions and behaviors related to fetching, making it the appropriate place for this method. This makes the behavior encapsulated within one conceptually cohesive class, improving maintainability and readability.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "fetchablePartitions",
                            "method_signature": "public synchronized List<TopicPartition> fetchablePartitions(Predicate<TopicPartition> isAvailable)",
                            "target_class": "PartitionStates",
                            "rationale": "The fetchablePartitions method is dealing with the state of partitions, which is within the responsibility of PartitionStates class. Moving it there helps keep SubscriptionState focused on subscription logic and makes PartitionStates the focal point for partition state logic."
                        },
                        {
                            "method_name": "movePartitionToEnd",
                            "method_signature": "synchronized void movePartitionToEnd(TopicPartition tp)",
                            "target_class": "PartitionStates",
                            "rationale": "The movePartitionToEnd method modifies the state of a partition. It is better suited for the PartitionStates class, keeping the SubscriptionState class focused on managing subscriptions."
                        },
                        {
                            "method_name": "pausedPartitions",
                            "method_signature": "public synchronized Set<TopicPartition> pausedPartitions()",
                            "target_class": "PartitionStates",
                            "rationale": "The pausedPartitions method involves tracking the state of partitions\u2014whether they are paused or not\u2014making it more appropriate to move to the PartitionStates class."
                        },
                        {
                            "method_name": "collectPartitions",
                            "method_signature": "private Set<TopicPartition> collectPartitions(Predicate<TopicPartitionState> filter)",
                            "target_class": "PartitionStates",
                            "rationale": "The collectPartitions method is a utility for working with partition states and would logically belong to the PartitionStates class, which maintains partition state data."
                        },
                        {
                            "method_name": "metadataTopics",
                            "method_signature": "synchronized Set<String> metadataTopics()",
                            "target_class": "GroupSubscription",
                            "rationale": "The metadataTopics method handles group subscription logic, which is better suited for a class responsible for group subscription, such as GroupSubscription."
                        }
                    ],
                    "llm_response_time": 6978
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "seek",
                            "method_signature": "public void seek(TopicPartition tp, long offset)",
                            "target_class": "TopicPartitionState",
                            "rationale": "The seek method primarily interacts with the state of a specific TopicPartition and should be encapsulated within the TopicPartitionState class to maintain single responsibility principle and improve cohesion."
                        },
                        {
                            "method_name": "seekUnvalidated",
                            "method_signature": "public void seekUnvalidated(TopicPartition tp, FetchPosition position)",
                            "target_class": "TopicPartitionState",
                            "rationale": "The seekUnvalidated method also deals specifically with the state of a TopicPartition and should be moved to the TopicPartitionState class to follow the single responsibility principle."
                        },
                        {
                            "method_name": "tryUpdatingHighWatermark",
                            "method_signature": "synchronized boolean tryUpdatingHighWatermark(TopicPartition tp, long highWatermark)",
                            "target_class": "TopicPartitionState",
                            "rationale": "The tryUpdatingHighWatermark method updates the high watermark of a TopicPartition, thus it fits better in the TopicPartitionState class which encapsulates the state of each topic partition."
                        },
                        {
                            "method_name": "tryUpdatingLastStableOffset",
                            "method_signature": "synchronized boolean tryUpdatingLastStableOffset(TopicPartition tp, long lastStableOffset)",
                            "target_class": "TopicPartitionState",
                            "rationale": "Similar to tryUpdatingHighWatermark, the tryUpdatingLastStableOffset method updates the last stable offset of a TopicPartition and should be placed in the TopicPartitionState class that holds such state information."
                        },
                        {
                            "method_name": "pause",
                            "method_signature": "public synchronized void pause(TopicPartition tp)",
                            "target_class": "TopicPartitionState",
                            "rationale": "The pause method controls the paused state of a TopicPartition and should be a part of the TopicPartitionState class to enhance encapsulation and maintain the single responsibility principle."
                        },
                        {
                            "method_name": "resume",
                            "method_signature": "public synchronized void resume(TopicPartition tp)",
                            "target_class": "TopicPartitionState",
                            "rationale": "The resume method also manages the paused state of a TopicPartition. It should be moved to TopicPartitionState for consistency and to ensure encapsulation of partition-specific state operations."
                        }
                    ],
                    "llm_response_time": 8082
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "partitionLag",
                            "method_signature": "public synchronized Long partitionLag(TopicPartition tp, IsolationLevel isolationLevel)",
                            "target_class": "TopicPartitionState",
                            "rationale": "The partitionLag method closely interacts with the state of a specific TopicPartition, which is managed by the TopicPartitionState class. By moving it to TopicPartitionState, the method will have direct access to the partition-specific data it requires."
                        },
                        {
                            "method_name": "partitionEndOffset",
                            "method_signature": "public synchronized Long partitionEndOffset(TopicPartition tp, IsolationLevel isolationLevel)",
                            "target_class": "TopicPartitionState",
                            "rationale": "The partitionEndOffset method needs to access the current state of a specific TopicPartition to determine the end offset, which aligns with the responsibilities of TopicPartitionState class."
                        },
                        {
                            "method_name": "preferredReadReplica",
                            "method_signature": "public synchronized Optional<Integer> preferredReadReplica(TopicPartition tp, long timeMs)",
                            "target_class": "TopicPartitionState",
                            "rationale": "The preferredReadReplica method deals directly with the read replica state of a TopicPartition. Thus, it should be part of TopicPartitionState to encapsulate this functionality within the partition state."
                        },
                        {
                            "method_name": "clearPreferredReadReplica",
                            "method_signature": "public synchronized Optional<Integer> clearPreferredReadReplica(TopicPartition tp)",
                            "target_class": "TopicPartitionState",
                            "rationale": "Managing and clearing the preferred read replica is a function that directly pertains to the state of a specific TopicPartition, suggesting that it should reside in TopicPartitionState."
                        },
                        {
                            "method_name": "assignedStateOrNull",
                            "method_signature": "private TopicPartitionState assignedStateOrNull(TopicPartition tp)",
                            "target_class": "PartitionStates",
                            "rationale": "The assignedStateOrNull method is primarily a utility function to fetch the state of a partition. It would logically fit better within the PartitionStates class, which manages all partition states."
                        }
                    ],
                    "llm_response_time": 6023
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "assignFromUser",
                            "method_signature": "public synchronized assignFromUser(Set<TopicPartition> partitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "checkAssignmentMatchedSubscription",
                            "method_signature": "public synchronized checkAssignmentMatchedSubscription(Collection<TopicPartition> assignments)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignFromSubscribed",
                            "method_signature": "public synchronized assignFromSubscribed(Collection<TopicPartition> assignments)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unsubscribe",
                            "method_signature": "public synchronized unsubscribe()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedState",
                            "method_signature": "private assignedState(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedStateOrNull",
                            "method_signature": "private assignedStateOrNull(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedPartitions",
                            "method_signature": "public synchronized assignedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedPartitionsList",
                            "method_signature": "public synchronized assignedPartitionsList()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "numAssignedPartitions",
                            "method_signature": "synchronized numAssignedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchablePartitions",
                            "method_signature": "public synchronized fetchablePartitions(Predicate<TopicPartition> isAvailable)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeValidatePositionForCurrentLeader",
                            "method_signature": "public synchronized maybeValidatePositionForCurrentLeader(ApiVersions apiVersions,\n                                                                      TopicPartition tp,\n                                                                      Metadata.LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionLag",
                            "method_signature": "public synchronized partitionLag(TopicPartition tp, IsolationLevel isolationLevel)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionLead",
                            "method_signature": "synchronized partitionLead(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allConsumed",
                            "method_signature": "public synchronized allConsumed()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasAllFetchPositions",
                            "method_signature": "public synchronized hasAllFetchPositions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initializingPartitions",
                            "method_signature": "public synchronized initializingPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "collectPartitions",
                            "method_signature": "private collectPartitions(Predicate<TopicPartitionState> filter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetInitializingPositions",
                            "method_signature": "public synchronized resetInitializingPositions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shouldInitialize",
                            "method_signature": "private shouldInitialize(TopicPartitionState partitionState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionsNeedingReset",
                            "method_signature": "public synchronized partitionsNeedingReset(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionsNeedingValidation",
                            "method_signature": "public synchronized partitionsNeedingValidation(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isAssigned",
                            "method_signature": "public synchronized isAssigned(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "movePartitionToEnd",
                            "method_signature": "synchronized movePartitionToEnd(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionState",
                            "method_signature": "private transitionState(FetchState newState, Runnable runIfTransitioned)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeValidatePosition",
                            "method_signature": "private maybeValidatePosition(Metadata.LeaderAndEpoch currentLeaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updatePositionLeaderNoValidation",
                            "method_signature": "private updatePositionLeaderNoValidation(Metadata.LeaderAndEpoch currentLeaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validatePosition",
                            "method_signature": "private validatePosition(FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingValidation",
                            "method_signature": "private awaitingValidation()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingReset",
                            "method_signature": "private awaitingReset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasValidPosition",
                            "method_signature": "private hasValidPosition()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "partitionLead",
                            "method_signature": "synchronized partitionLead(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unsubscribe",
                            "method_signature": "public synchronized unsubscribe()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allConsumed",
                            "method_signature": "public synchronized allConsumed()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionsNeedingReset",
                            "method_signature": "public synchronized partitionsNeedingReset(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionsNeedingValidation",
                            "method_signature": "public synchronized partitionsNeedingValidation(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "movePartitionToEnd",
                            "method_signature": "synchronized movePartitionToEnd(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initializingPartitions",
                            "method_signature": "public synchronized initializingPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shouldInitialize",
                            "method_signature": "private shouldInitialize(TopicPartitionState partitionState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "numAssignedPartitions",
                            "method_signature": "synchronized numAssignedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedState",
                            "method_signature": "private assignedState(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedPartitionsList",
                            "method_signature": "public synchronized assignedPartitionsList()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedStateOrNull",
                            "method_signature": "private assignedStateOrNull(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasValidPosition",
                            "method_signature": "private hasValidPosition()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingValidation",
                            "method_signature": "private awaitingValidation()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingReset",
                            "method_signature": "private awaitingReset()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "synchronized partitionLead(TopicPartition tp)": {
                        "first": {
                            "method_name": "partitionLead",
                            "method_signature": "synchronized partitionLead(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3941991721519669
                    },
                    "public synchronized unsubscribe()": {
                        "first": {
                            "method_name": "unsubscribe",
                            "method_signature": "public synchronized unsubscribe()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.40692954073999543
                    },
                    "public synchronized allConsumed()": {
                        "first": {
                            "method_name": "allConsumed",
                            "method_signature": "public synchronized allConsumed()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4437879047608692
                    },
                    "public synchronized partitionsNeedingReset(long nowMs)": {
                        "first": {
                            "method_name": "partitionsNeedingReset",
                            "method_signature": "public synchronized partitionsNeedingReset(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4465165375170002
                    },
                    "public synchronized partitionsNeedingValidation(long nowMs)": {
                        "first": {
                            "method_name": "partitionsNeedingValidation",
                            "method_signature": "public synchronized partitionsNeedingValidation(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4465165375170002
                    },
                    "synchronized movePartitionToEnd(TopicPartition tp)": {
                        "first": {
                            "method_name": "movePartitionToEnd",
                            "method_signature": "synchronized movePartitionToEnd(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.46113044592360225
                    },
                    "public synchronized initializingPartitions()": {
                        "first": {
                            "method_name": "initializingPartitions",
                            "method_signature": "public synchronized initializingPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4629313531937039
                    },
                    "private shouldInitialize(TopicPartitionState partitionState)": {
                        "first": {
                            "method_name": "shouldInitialize",
                            "method_signature": "private shouldInitialize(TopicPartitionState partitionState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.46334311175198317
                    },
                    "synchronized numAssignedPartitions()": {
                        "first": {
                            "method_name": "numAssignedPartitions",
                            "method_signature": "synchronized numAssignedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.465922037982423
                    },
                    "private assignedState(TopicPartition tp)": {
                        "first": {
                            "method_name": "assignedState",
                            "method_signature": "private assignedState(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.47267712209142126
                    },
                    "public synchronized assignedPartitionsList()": {
                        "first": {
                            "method_name": "assignedPartitionsList",
                            "method_signature": "public synchronized assignedPartitionsList()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.49712608949399983
                    },
                    "private assignedStateOrNull(TopicPartition tp)": {
                        "first": {
                            "method_name": "assignedStateOrNull",
                            "method_signature": "private assignedStateOrNull(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5039484687908252
                    },
                    "private hasValidPosition()": {
                        "first": {
                            "method_name": "hasValidPosition",
                            "method_signature": "private hasValidPosition()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5320148796823104
                    },
                    "private awaitingValidation()": {
                        "first": {
                            "method_name": "awaitingValidation",
                            "method_signature": "private awaitingValidation()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5366512798112313
                    },
                    "private awaitingReset()": {
                        "first": {
                            "method_name": "awaitingReset",
                            "method_signature": "private awaitingReset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5379501031996715
                    }
                },
                "voyage": {
                    "private awaitingValidation()": {
                        "first": {
                            "method_name": "awaitingValidation",
                            "method_signature": "private awaitingValidation()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3360063711060235
                    },
                    "public synchronized unsubscribe()": {
                        "first": {
                            "method_name": "unsubscribe",
                            "method_signature": "public synchronized unsubscribe()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.41642040566105654
                    },
                    "private awaitingReset()": {
                        "first": {
                            "method_name": "awaitingReset",
                            "method_signature": "private awaitingReset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4313988535490519
                    },
                    "private hasValidPosition()": {
                        "first": {
                            "method_name": "hasValidPosition",
                            "method_signature": "private hasValidPosition()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4455076333757614
                    },
                    "public synchronized isAssigned(TopicPartition tp)": {
                        "first": {
                            "method_name": "isAssigned",
                            "method_signature": "public synchronized isAssigned(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4680481312729264
                    },
                    "synchronized numAssignedPartitions()": {
                        "first": {
                            "method_name": "numAssignedPartitions",
                            "method_signature": "synchronized numAssignedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.46810631180961515
                    },
                    "private transitionState(FetchState newState, Runnable runIfTransitioned)": {
                        "first": {
                            "method_name": "transitionState",
                            "method_signature": "private transitionState(FetchState newState, Runnable runIfTransitioned)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4829617547399626
                    },
                    "public synchronized partitionLag(TopicPartition tp, IsolationLevel isolationLevel)": {
                        "first": {
                            "method_name": "partitionLag",
                            "method_signature": "public synchronized partitionLag(TopicPartition tp, IsolationLevel isolationLevel)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4905300860116981
                    },
                    "private validatePosition(FetchPosition position)": {
                        "first": {
                            "method_name": "validatePosition",
                            "method_signature": "private validatePosition(FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5052944064154559
                    },
                    "public synchronized checkAssignmentMatchedSubscription(Collection<TopicPartition> assignments)": {
                        "first": {
                            "method_name": "checkAssignmentMatchedSubscription",
                            "method_signature": "public synchronized checkAssignmentMatchedSubscription(Collection<TopicPartition> assignments)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5280501500514388
                    },
                    "synchronized movePartitionToEnd(TopicPartition tp)": {
                        "first": {
                            "method_name": "movePartitionToEnd",
                            "method_signature": "synchronized movePartitionToEnd(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5333473220527569
                    },
                    "private maybeValidatePosition(Metadata.LeaderAndEpoch currentLeaderAndEpoch)": {
                        "first": {
                            "method_name": "maybeValidatePosition",
                            "method_signature": "private maybeValidatePosition(Metadata.LeaderAndEpoch currentLeaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5389528230005733
                    },
                    "private assignedStateOrNull(TopicPartition tp)": {
                        "first": {
                            "method_name": "assignedStateOrNull",
                            "method_signature": "private assignedStateOrNull(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5428832486871221
                    },
                    "private shouldInitialize(TopicPartitionState partitionState)": {
                        "first": {
                            "method_name": "shouldInitialize",
                            "method_signature": "private shouldInitialize(TopicPartitionState partitionState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5498457780151175
                    },
                    "private updatePositionLeaderNoValidation(Metadata.LeaderAndEpoch currentLeaderAndEpoch)": {
                        "first": {
                            "method_name": "updatePositionLeaderNoValidation",
                            "method_signature": "private updatePositionLeaderNoValidation(Metadata.LeaderAndEpoch currentLeaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5539170182934444
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private assignedState(TopicPartition tp)",
                        "private assignedStateOrNull(TopicPartition tp)",
                        "private shouldInitialize(TopicPartitionState partitionState)",
                        "private hasValidPosition()",
                        "private awaitingValidation()",
                        "private awaitingReset()",
                        "public synchronized unsubscribe()",
                        "public synchronized allConsumed()",
                        "public synchronized partitionsNeedingReset(long nowMs)",
                        "public synchronized partitionsNeedingValidation(long nowMs)",
                        "public synchronized initializingPartitions()",
                        "public synchronized assignedPartitionsList()",
                        "synchronized numAssignedPartitions()",
                        "synchronized movePartitionToEnd(TopicPartition tp)",
                        "synchronized partitionLead(TopicPartition tp)"
                    ],
                    "llm_response_time": 4817
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "public synchronized allConsumed()",
                        "public synchronized unsubscribe()",
                        "public synchronized partitionsNeedingReset(long nowMs)",
                        "public synchronized partitionsNeedingValidation(long nowMs)"
                    ],
                    "llm_response_time": 5220
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "public synchronized allConsumed()",
                        "public synchronized unsubscribe()",
                        "synchronized partitionLead(TopicPartition tp)"
                    ],
                    "llm_response_time": 1574
                },
                "voyage": {
                    "priority_method_names": [
                        "private assignedStateOrNull(TopicPartition tp)",
                        "private awaitingValidation()",
                        "private awaitingReset()",
                        "private hasValidPosition()",
                        "private maybeValidatePosition(Metadata.LeaderAndEpoch currentLeaderAndEpoch)",
                        "private validatePosition(FetchPosition position)",
                        "private updatePositionLeaderNoValidation(Metadata.LeaderAndEpoch currentLeaderAndEpoch)",
                        "private shouldInitialize(TopicPartitionState partitionState)",
                        "private transitionState(FetchState newState, Runnable runIfTransitioned)",
                        "public synchronized unsubscribe()",
                        "public synchronized isAssigned(TopicPartition tp)",
                        "public synchronized partitionLag(TopicPartition tp, IsolationLevel isolationLevel)",
                        "public synchronized checkAssignmentMatchedSubscription(Collection<TopicPartition> assignments)",
                        "synchronized numAssignedPartitions()",
                        "synchronized movePartitionToEnd(TopicPartition tp)"
                    ],
                    "llm_response_time": 7532
                },
                "voyage-5": {
                    "priority_method_names": [],
                    "llm_response_time": 4144
                },
                "voyage-3": {
                    "priority_method_names": [
                        "public synchronized unsubscribe()",
                        "private awaitingValidation()",
                        "private awaitingReset()"
                    ],
                    "llm_response_time": 3507
                }
            },
            "targetClassMap": {
                "partitionLead": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1890,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "unsubscribe": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3621,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "allConsumed": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2719,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "partitionsNeedingReset": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3579,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "partitionsNeedingValidation": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 5090,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "movePartitionToEnd": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2859,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "initializingPartitions": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3466,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "shouldInitialize": {
                    "target_classes": [
                        {
                            "class_name": "TopicPartitionState",
                            "similarity_score": 0.5262572393896099
                        },
                        {
                            "class_name": "SubscriptionType",
                            "similarity_score": 0.31622776601683794
                        }
                    ],
                    "llm_response_time": 3354,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine",
                    "target_classes_sorted_by_llm": ["TopicPartitionState", "SubscriptionType"],
                    "target_class_priority_explanation": "[\n    {\n        \"target_class\": \"TopicPartitionState\",\n        \"rationale\": \"The method `shouldInitialize` explicitly checks the state of `partitionState.fetchState` and `partitionState.pendingOnAssignedCallback`, both of which are fields in the `TopicPartitionState` class. Since the method solely operates on data from `TopicPartitionState` and does not involve any logic tied to `SubscriptionType`, it is most appropriate to place it within the `TopicPartitionState` class. Furthermore, clustering functionally related methods and data in the same class adheres to the principles of encapsulation and improves code maintainability and readability.\",\n    },\n    {\n        \"target_class\": \"SubscriptionType\",\n        \"rationale\": \"The `SubscriptionType` class is an enumeration that represents various subscription states but does not involve operational logic or state transitions akin to `shouldInitialize`. This method does not fit within the context of `SubscriptionType` as it does not deal with high-level subscription categorizations but rather specific state checks within a partition state. Including such a method in `SubscriptionType` would violate the Single Responsibility Principle by giving the enum additional responsibilities it was not designed for.\"\n    }\n]"
                },
                "numAssignedPartitions": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 4019,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "assignedState": {
                    "target_classes": [
                        {
                            "class_name": "SubscriptionType",
                            "similarity_score": 0.1889822365046136
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SubscriptionType"
                    ],
                    "llm_response_time": 1658,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "assignedPartitionsList": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2329,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "assignedStateOrNull": {
                    "target_classes": [
                        {
                            "class_name": "SubscriptionType",
                            "similarity_score": 0.35355339059327373
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SubscriptionType"
                    ],
                    "llm_response_time": 1897,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "hasValidPosition": {
                    "target_classes": [
                        {
                            "class_name": "FetchPosition",
                            "similarity_score": 0.33001145893015604
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "FetchPosition"
                    ],
                    "llm_response_time": 1680,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "awaitingValidation": {
                    "target_classes": [
                        {
                            "class_name": "FetchState",
                            "similarity_score": 0.2768465114999739
                        },
                        {
                            "class_name": "FetchPosition",
                            "similarity_score": 0.33001145893015604
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "FetchState",
                        "FetchPosition"
                    ],
                    "llm_response_time": 2283,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "awaitingReset": {
                    "target_classes": [
                        {
                            "class_name": "FetchState",
                            "similarity_score": 0.2768465114999739
                        },
                        {
                            "class_name": "FetchPosition",
                            "similarity_score": 0.33001145893015604
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "FetchState",
                        "FetchPosition"
                    ],
                    "llm_response_time": 2717,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "459da4795a511f6933e940fcf105a824bd9e589c",
        "url": "https://github.com/apache/kafka/commit/459da4795a511f6933e940fcf105a824bd9e589c",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public hasAnyInflightRequest(currentTimeMs long) : boolean extracted from private pollFollowerAsObserver(state FollowerState, currentTimeMs long) : long in class org.apache.kafka.raft.KafkaRaftClient & moved to class org.apache.kafka.raft.RequestManager",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 2157,
                    "endLine": 2178,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private pollFollowerAsObserver(state FollowerState, currentTimeMs long) : long"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 2166,
                    "endLine": 2166,
                    "startColumn": 13,
                    "endColumn": 87,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 63,
                    "endLine": 94,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public hasAnyInflightRequest(currentTimeMs long) : boolean"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 78,
                    "endLine": 78,
                    "startColumn": 13,
                    "endColumn": 58,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 2220,
                    "endLine": 2243,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private pollFollowerAsObserver(state FollowerState, currentTimeMs long) : long"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 2235,
                    "endLine": 2235,
                    "startColumn": 25,
                    "endColumn": 76,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "requestManager.hasAnyInflightRequest(currentTimeMs)"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 74,
                    "endLine": 74,
                    "startColumn": 9,
                    "endColumn": 32,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 76,
                    "endLine": 76,
                    "startColumn": 9,
                    "endColumn": 78,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 81,
                    "endLine": 81,
                    "startColumn": 17,
                    "endColumn": 35,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 84,
                    "endLine": 84,
                    "startColumn": 17,
                    "endColumn": 35,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 88,
                    "endLine": 88,
                    "startColumn": 17,
                    "endColumn": 31,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 89,
                    "endLine": 89,
                    "startColumn": 17,
                    "endColumn": 23,
                    "codeElementType": "BREAK_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 93,
                    "endLine": 93,
                    "startColumn": 9,
                    "endColumn": 23,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 79,
                    "endLine": 82,
                    "startColumn": 63,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 82,
                    "endLine": 85,
                    "startColumn": 69,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 85,
                    "endLine": 90,
                    "startColumn": 70,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 85,
                    "endLine": 90,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 82,
                    "endLine": 90,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 77,
                    "endLine": 91,
                    "startColumn": 36,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 77,
                    "endLine": 91,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "WHILE_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 79,
                    "endLine": 90,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 581,
        "extraction_results": {
            "success": true,
            "newCommitHash": "f62b73b6006f87b0004fd8ae26fec3980db04ca8",
            "newBranchName": "extract-hasAnyInflightRequest-pollFollowerAsObserver-8a882a7"
        },
        "telemetry": {
            "id": "21488ffe-e1dc-4087-8afc-fa0956c5cb1a",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2693,
                "lineStart": 109,
                "lineEnd": 2801,
                "bodyLineStart": 109,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                "sourceCode": "/**\n * This class implements a Kafkaesque version of the Raft protocol. Leader election\n * is more or less pure Raft, but replication is driven by replica fetching and we use Kafka's\n * log reconciliation protocol to truncate the log to a common point following each leader\n * election.\n *\n * Like Zookeeper, this protocol distinguishes between voters and observers. Voters are\n * the only ones who are eligible to handle protocol requests and they are the only ones\n * who take part in elections. The protocol does not yet support dynamic quorum changes.\n *\n * These are the APIs in this protocol:\n *\n * 1) {@link VoteRequestData}: Sent by valid voters when their election timeout expires and they\n *    become a candidate. This request includes the last offset in the log which electors use\n *    to tell whether or not to grant the vote.\n *\n * 2) {@link BeginQuorumEpochRequestData}: Sent by the leader of an epoch only to valid voters to\n *    assert its leadership of the new epoch. This request will be retried indefinitely for\n *    each voter until it acknowledges the request or a new election occurs.\n *\n *    This is not needed in usual Raft because the leader can use an empty data push\n *    to achieve the same purpose. The Kafka Raft implementation, however, is driven by\n *    fetch requests from followers, so there must be a way to find the new leader after\n *    an election has completed.\n *\n * 3) {@link EndQuorumEpochRequestData}: Sent by the leader of an epoch to valid voters in order to\n *    gracefully resign from the current epoch. This causes remaining voters to immediately\n *    begin a new election.\n *\n * 4) {@link FetchRequestData}: This is the same as the usual Fetch API in Kafka, but we add snapshot\n *    check before responding, and we also piggyback some additional metadata on responses (i.e. current\n *    leader and epoch). Unlike partition replication, we also piggyback truncation detection on this API\n *    rather than through a separate truncation state.\n *\n * 5) {@link FetchSnapshotRequestData}: Sent by the follower to the epoch leader in order to fetch a snapshot.\n *    This happens when a FetchResponse includes a snapshot ID due to the follower's log end offset being less\n *    than the leader's log start offset. This API is similar to the Fetch API since the snapshot is stored\n *    as FileRecords, but we use {@link UnalignedRecords} in FetchSnapshotResponse because the records\n *    are not necessarily offset-aligned.\n */\nfinal public class KafkaRaftClient<T> implements RaftClient<T> {\n    private static final int RETRY_BACKOFF_BASE_MS = 100;\n    public static final int MAX_FETCH_WAIT_MS = 500;\n    public static final int MAX_BATCH_SIZE_BYTES = 8 * 1024 * 1024;\n    public static final int MAX_FETCH_SIZE_BYTES = MAX_BATCH_SIZE_BYTES;\n\n    private final OptionalInt nodeId;\n    private final Uuid nodeDirectoryId;\n    private final AtomicReference<GracefulShutdown> shutdown = new AtomicReference<>();\n    private final LogContext logContext;\n    private final Logger logger;\n    private final Time time;\n    private final int fetchMaxWaitMs;\n    private final String clusterId;\n    private final NetworkChannel channel;\n    private final ReplicatedLog log;\n    private final Random random;\n    private final FuturePurgatory<Long> appendPurgatory;\n    private final FuturePurgatory<Long> fetchPurgatory;\n    private final RecordSerde<T> serde;\n    private final MemoryPool memoryPool;\n    private final RaftMessageQueue messageQueue;\n    private final QuorumConfig quorumConfig;\n    private final RaftMetadataLogCleanerManager snapshotCleaner;\n\n    private final Map<Listener<T>, ListenerContext> listenerContexts = new IdentityHashMap<>();\n    private final ConcurrentLinkedQueue<Registration<T>> pendingRegistrations = new ConcurrentLinkedQueue<>();\n\n    // These components need to be initialized by the method initialize() because they depend on\n    // the voter set\n    /*\n     * The key invariant for the kraft control record state machine is that it has always read to\n     * the LEO. This is achieved by:\n     *\n     * 1. reading the entire partition (snapshot and log) at start up,\n     * 2. updating the state when a snapshot is replaced, because of FETCH_SNAPSHOT, on the\n     *    followers\n     * 3. updating the state when the leader (call to append()) or follower (FETCH) appends to the\n     *    log\n     * 4. truncate new entries when a follower truncates their log\n     * 5. truncate old entries when a snapshot gets generated\n     */\n    private volatile KRaftControlRecordStateMachine partitionState;\n    private volatile KafkaRaftMetrics kafkaRaftMetrics;\n    private volatile QuorumState quorum;\n    private volatile RequestManager requestManager;\n\n    /**\n     * Create a new instance.\n     *\n     * Note that if the node ID is empty, then the client will behave as a\n     * non-participating observer.\n     */\n    public KafkaRaftClient(\n        OptionalInt nodeId,\n        Uuid nodeDirectoryId,\n        RecordSerde<T> serde,\n        NetworkChannel channel,\n        ReplicatedLog log,\n        Time time,\n        ExpirationService expirationService,\n        LogContext logContext,\n        String clusterId,\n        QuorumConfig quorumConfig\n    ) {\n        this(\n            nodeId,\n            nodeDirectoryId,\n            serde,\n            channel,\n            new BlockingMessageQueue(),\n            log,\n            new BatchMemoryPool(5, MAX_BATCH_SIZE_BYTES),\n            time,\n            expirationService,\n            MAX_FETCH_WAIT_MS,\n            clusterId,\n            logContext,\n            new Random(),\n            quorumConfig\n        );\n    }\n\n    KafkaRaftClient(\n        OptionalInt nodeId,\n        Uuid nodeDirectoryId,\n        RecordSerde<T> serde,\n        NetworkChannel channel,\n        RaftMessageQueue messageQueue,\n        ReplicatedLog log,\n        MemoryPool memoryPool,\n        Time time,\n        ExpirationService expirationService,\n        int fetchMaxWaitMs,\n        String clusterId,\n        LogContext logContext,\n        Random random,\n        QuorumConfig quorumConfig\n    ) {\n        this.nodeId = nodeId;\n        this.nodeDirectoryId = nodeDirectoryId;\n        this.logContext = logContext;\n        this.serde = serde;\n        this.channel = channel;\n        this.messageQueue = messageQueue;\n        this.log = log;\n        this.memoryPool = memoryPool;\n        this.fetchPurgatory = new ThresholdPurgatory<>(expirationService);\n        this.appendPurgatory = new ThresholdPurgatory<>(expirationService);\n        this.time = time;\n        this.clusterId = clusterId;\n        this.fetchMaxWaitMs = fetchMaxWaitMs;\n        this.logger = logContext.logger(KafkaRaftClient.class);\n        this.random = random;\n        this.quorumConfig = quorumConfig;\n        this.snapshotCleaner = new RaftMetadataLogCleanerManager(logger, time, 60000, log::maybeClean);\n    }\n\n    private void updateFollowerHighWatermark(\n        FollowerState state,\n        OptionalLong highWatermarkOpt\n    ) {\n        highWatermarkOpt.ifPresent(highWatermark -> {\n            long newHighWatermark = Math.min(endOffset().offset(), highWatermark);\n            if (state.updateHighWatermark(OptionalLong.of(newHighWatermark))) {\n                logger.debug(\"Follower high watermark updated to {}\", newHighWatermark);\n                log.updateHighWatermark(new LogOffsetMetadata(newHighWatermark));\n                updateListenersProgress(newHighWatermark);\n            }\n        });\n    }\n\n    private void updateLeaderEndOffsetAndTimestamp(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n\n        if (state.updateLocalState(endOffsetMetadata)) {\n            onUpdateLeaderHighWatermark(state, currentTimeMs);\n        }\n\n        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n    }\n\n    private void onUpdateLeaderHighWatermark(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        state.highWatermark().ifPresent(highWatermark -> {\n            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n            log.updateHighWatermark(highWatermark);\n\n            // After updating the high watermark, we first clear the append\n            // purgatory so that we have an opportunity to route the pending\n            // records still held in memory directly to the listener\n            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n\n            // It is also possible that the high watermark is being updated\n            // for the first time following the leader election, so we need\n            // to give lagging listeners an opportunity to catch up as well\n            updateListenersProgress(highWatermark.offset);\n        });\n    }\n\n    private void updateListenersProgress(long highWatermark) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.nextExpectedOffset().ifPresent(nextExpectedOffset -> {\n                // Send snapshot to the listener, if the listener is at the beginning of the log and there is a snapshot,\n                // or the listener is trying to read an offset for which there isn't a segment in the log.\n                if (nextExpectedOffset < highWatermark &&\n                    ((nextExpectedOffset == 0 && latestSnapshot().isPresent()) ||\n                     nextExpectedOffset < log.startOffset())\n                ) {\n                    SnapshotReader<T> snapshot = latestSnapshot().orElseThrow(() -> new IllegalStateException(\n                        String.format(\n                            \"Snapshot expected since next offset of %s is %d, log start offset is %d and high-watermark is %d\",\n                            listenerContext.listenerName(),\n                            nextExpectedOffset,\n                            log.startOffset(),\n                            highWatermark\n                        )\n                    ));\n                    listenerContext.fireHandleSnapshot(snapshot);\n                }\n            });\n\n            // Re-read the expected offset in case the snapshot had to be reloaded\n            listenerContext.nextExpectedOffset().ifPresent(nextExpectedOffset -> {\n                if (nextExpectedOffset < highWatermark) {\n                    LogFetchInfo readInfo = log.read(nextExpectedOffset, Isolation.COMMITTED);\n                    listenerContext.fireHandleCommit(nextExpectedOffset, readInfo.records);\n                }\n            });\n        }\n    }\n\n    private Optional<SnapshotReader<T>> latestSnapshot() {\n        return log.latestSnapshot().map(reader ->\n            RecordsSnapshotReader.of(reader,\n                serde,\n                BufferSupplier.create(),\n                MAX_BATCH_SIZE_BYTES,\n                true /* Validate batch CRC*/\n            )\n        );\n    }\n\n    private void maybeFireHandleCommit(long baseOffset, int epoch, long appendTimestamp, int sizeInBytes, List<T> records) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.nextExpectedOffset().ifPresent(nextOffset -> {\n                if (nextOffset == baseOffset) {\n                    listenerContext.fireHandleCommit(baseOffset, epoch, appendTimestamp, sizeInBytes, records);\n                }\n            });\n        }\n    }\n\n    private void maybeFireLeaderChange(LeaderState<T> state) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.maybeFireLeaderChange(quorum.leaderAndEpoch(), state.epochStartOffset());\n        }\n    }\n\n    private void maybeFireLeaderChange() {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.maybeFireLeaderChange(quorum.leaderAndEpoch());\n        }\n    }\n\n    public void initialize(\n        Map<Integer, InetSocketAddress> voterAddresses,\n        String listenerName,\n        QuorumStateStore quorumStateStore,\n        Metrics metrics\n    ) {\n        partitionState = new KRaftControlRecordStateMachine(\n            Optional.of(VoterSet.fromInetSocketAddresses(listenerName, voterAddresses)),\n            log,\n            serde,\n            BufferSupplier.create(),\n            MAX_BATCH_SIZE_BYTES,\n            logContext\n        );\n        // Read the entire log\n        logger.info(\"Reading KRaft snapshot and log as part of the initialization\");\n        partitionState.updateState();\n\n        VoterSet lastVoterSet = partitionState.lastVoterSet();\n        requestManager = new RequestManager(\n            lastVoterSet.voterIds(),\n            quorumConfig.retryBackoffMs(),\n            quorumConfig.requestTimeoutMs(),\n            random\n        );\n\n        quorum = new QuorumState(\n            nodeId,\n            nodeDirectoryId,\n            partitionState::lastVoterSet,\n            partitionState::lastKraftVersion,\n            quorumConfig.electionTimeoutMs(),\n            quorumConfig.fetchTimeoutMs(),\n            quorumStateStore,\n            time,\n            logContext,\n            random\n        );\n\n        kafkaRaftMetrics = new KafkaRaftMetrics(metrics, \"raft\", quorum);\n        // All Raft voters are statically configured and known at startup\n        // so there are no unknown voter connections. Report this metric as 0.\n        kafkaRaftMetrics.updateNumUnknownVoterConnections(0);\n\n        for (Integer voterId : lastVoterSet.voterIds()) {\n            channel.updateEndpoint(voterId, lastVoterSet.voterAddress(voterId, listenerName).get());\n        }\n\n        quorum.initialize(new OffsetAndEpoch(log.endOffset().offset, log.lastFetchedEpoch()));\n\n        long currentTimeMs = time.milliseconds();\n        if (quorum.isLeader()) {\n            throw new IllegalStateException(\"Voter cannot initialize as a Leader\");\n        } else if (quorum.isCandidate()) {\n            onBecomeCandidate(currentTimeMs);\n        } else if (quorum.isFollower()) {\n            onBecomeFollower(currentTimeMs);\n        }\n\n        // When there is only a single voter, become candidate immediately\n        if (quorum.isOnlyVoter() && !quorum.isCandidate()) {\n            transitionToCandidate(currentTimeMs);\n        }\n    }\n\n    @Override\n    public void register(Listener<T> listener) {\n        pendingRegistrations.add(Registration.register(listener));\n        wakeup();\n    }\n\n    @Override\n    public void unregister(Listener<T> listener) {\n        pendingRegistrations.add(Registration.unregister(listener));\n        // No need to wake up the polling thread. It is a removal so the updates can be\n        // delayed until the polling thread wakes up for other reasons.\n    }\n\n    @Override\n    public LeaderAndEpoch leaderAndEpoch() {\n        if (isInitialized()) {\n            return quorum.leaderAndEpoch();\n        } else {\n            return LeaderAndEpoch.UNKNOWN;\n        }\n    }\n\n    @Override\n    public OptionalInt nodeId() {\n        return nodeId;\n    }\n\n    private OffsetAndEpoch endOffset() {\n        return new OffsetAndEpoch(log.endOffset().offset, log.lastFetchedEpoch());\n    }\n\n    private void resetConnections() {\n        requestManager.resetAll();\n    }\n\n    private void onBecomeLeader(long currentTimeMs) {\n        long endOffset = log.endOffset().offset;\n\n        BatchAccumulator<T> accumulator = new BatchAccumulator<>(\n            quorum.epoch(),\n            endOffset,\n            quorumConfig.appendLingerMs(),\n            MAX_BATCH_SIZE_BYTES,\n            memoryPool,\n            time,\n            Compression.NONE,\n            serde\n        );\n\n        LeaderState<T> state = quorum.transitionToLeader(endOffset, accumulator);\n\n        log.initializeLeaderEpoch(quorum.epoch());\n\n        // The high watermark can only be advanced once we have written a record\n        // from the new leader's epoch. Hence we write a control message immediately\n        // to ensure there is no delay committing pending data.\n        state.appendLeaderChangeMessage(currentTimeMs);\n\n        resetConnections();\n        kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n    }\n\n    private void flushLeaderLog(LeaderState<T> state, long currentTimeMs) {\n        // We update the end offset before flushing so that parked fetches can return sooner.\n        updateLeaderEndOffsetAndTimestamp(state, currentTimeMs);\n        log.flush(false);\n    }\n\n    private boolean maybeTransitionToLeader(CandidateState state, long currentTimeMs) {\n        if (state.isVoteGranted()) {\n            onBecomeLeader(currentTimeMs);\n            return true;\n        } else {\n            return false;\n        }\n    }\n\n    private void onBecomeCandidate(long currentTimeMs) {\n        CandidateState state = quorum.candidateStateOrThrow();\n        if (!maybeTransitionToLeader(state, currentTimeMs)) {\n            resetConnections();\n            kafkaRaftMetrics.updateElectionStartMs(currentTimeMs);\n        }\n    }\n\n    private void transitionToCandidate(long currentTimeMs) {\n        quorum.transitionToCandidate();\n        maybeFireLeaderChange();\n        onBecomeCandidate(currentTimeMs);\n    }\n\n    private void transitionToUnattached(int epoch) {\n        quorum.transitionToUnattached(epoch);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void transitionToResigned(List<Integer> preferredSuccessors) {\n        fetchPurgatory.completeAllExceptionally(\n            Errors.NOT_LEADER_OR_FOLLOWER.exception(\"Not handling request since this node is resigning\"));\n        quorum.transitionToResigned(preferredSuccessors);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void transitionToVoted(ReplicaKey candidateKey, int epoch) {\n        quorum.transitionToVoted(epoch, candidateKey);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void onBecomeFollower(long currentTimeMs) {\n        kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n\n        resetConnections();\n\n        // After becoming a follower, we need to complete all pending fetches so that\n        // they can be re-sent to the leader without waiting for their expirations\n        fetchPurgatory.completeAllExceptionally(new NotLeaderOrFollowerException(\n            \"Cannot process the fetch request because the node is no longer the leader.\"));\n\n        // Clearing the append purgatory should complete all futures exceptionally since this node is no longer the leader\n        appendPurgatory.completeAllExceptionally(new NotLeaderOrFollowerException(\n            \"Failed to receive sufficient acknowledgments for this append before leader change.\"));\n    }\n\n    private void transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    ) {\n        quorum.transitionToFollower(epoch, leaderId);\n        maybeFireLeaderChange();\n        onBecomeFollower(currentTimeMs);\n    }\n\n    private VoteResponseData buildVoteResponse(Errors partitionLevelError, boolean voteGranted) {\n        return VoteResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel(),\n            voteGranted);\n    }\n\n    /**\n     * Handle a Vote request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#INVALID_REQUEST} if the last epoch or offset are invalid\n     */\n    private VoteResponseData handleVoteRequest(\n        RaftRequest.Inbound requestMetadata\n    ) {\n        VoteRequestData request = (VoteRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new VoteResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat individual topic partition mismatches as invalid requests\n            return new VoteResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        VoteRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int candidateId = partitionRequest.candidateId();\n        int candidateEpoch = partitionRequest.candidateEpoch();\n\n        int lastEpoch = partitionRequest.lastOffsetEpoch();\n        long lastEpochEndOffset = partitionRequest.lastOffset();\n        if (lastEpochEndOffset < 0 || lastEpoch < 0 || lastEpoch >= candidateEpoch) {\n            return buildVoteResponse(Errors.INVALID_REQUEST, false);\n        }\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(candidateId, candidateEpoch);\n        if (errorOpt.isPresent()) {\n            return buildVoteResponse(errorOpt.get(), false);\n        }\n\n        if (candidateEpoch > quorum.epoch()) {\n            transitionToUnattached(candidateEpoch);\n        }\n\n        OffsetAndEpoch lastEpochEndOffsetAndEpoch = new OffsetAndEpoch(lastEpochEndOffset, lastEpoch);\n        ReplicaKey candidateKey = ReplicaKey.of(candidateId, Optional.empty());\n        boolean voteGranted = quorum.canGrantVote(\n            candidateKey,\n            lastEpochEndOffsetAndEpoch.compareTo(endOffset()) >= 0\n        );\n\n        if (voteGranted && quorum.isUnattached()) {\n            transitionToVoted(candidateKey, candidateEpoch);\n        }\n\n        logger.info(\"Vote request {} with epoch {} is {}\", request, candidateEpoch, voteGranted ? \"granted\" : \"rejected\");\n        return buildVoteResponse(Errors.NONE, voteGranted);\n    }\n\n    private boolean handleVoteResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        int remoteNodeId = responseMetadata.sourceId();\n        VoteResponseData response = (VoteResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        VoteResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors error = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (error == Errors.NONE) {\n            if (quorum.isLeader()) {\n                logger.debug(\"Ignoring vote response {} since we already became leader for epoch {}\",\n                    partitionResponse, quorum.epoch());\n            } else if (quorum.isCandidate()) {\n                CandidateState state = quorum.candidateStateOrThrow();\n                if (partitionResponse.voteGranted()) {\n                    state.recordGrantedVote(remoteNodeId);\n                    maybeTransitionToLeader(state, currentTimeMs);\n                } else {\n                    state.recordRejectedVote(remoteNodeId);\n\n                    // If our vote is rejected, we go immediately to the random backoff. This\n                    // ensures that we are not stuck waiting for the election timeout when the\n                    // vote has become gridlocked.\n                    if (state.isVoteRejected() && !state.isBackingOff()) {\n                        logger.info(\"Insufficient remaining votes to become leader (rejected by {}). \" +\n                            \"We will backoff before retrying election again\", state.rejectingVoters());\n\n                        state.startBackingOff(\n                            currentTimeMs,\n                            binaryExponentialElectionBackoffMs(state.retries())\n                        );\n                    }\n                }\n            } else {\n                logger.debug(\"Ignoring vote response {} since we are no longer a candidate in epoch {}\",\n                    partitionResponse, quorum.epoch());\n            }\n            return true;\n        } else {\n            return handleUnexpectedError(error, responseMetadata);\n        }\n    }\n\n    private int binaryExponentialElectionBackoffMs(int retries) {\n        if (retries <= 0) {\n            throw new IllegalArgumentException(\"Retries \" + retries + \" should be larger than zero\");\n        }\n        // upper limit exponential co-efficients at 20 to avoid overflow\n        return Math.min(RETRY_BACKOFF_BASE_MS * random.nextInt(2 << Math.min(20, retries - 1)),\n                quorumConfig.electionBackoffMaxMs());\n    }\n\n    private int strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors) {\n        if (positionInSuccessors <= 0 || positionInSuccessors >= totalNumSuccessors) {\n            throw new IllegalArgumentException(\"Position \" + positionInSuccessors + \" should be larger than zero\" +\n                    \" and smaller than total number of successors \" + totalNumSuccessors);\n        }\n\n        int retryBackOffBaseMs = quorumConfig.electionBackoffMaxMs() >> (totalNumSuccessors - 1);\n        return Math.min(quorumConfig.electionBackoffMaxMs(), retryBackOffBaseMs << (positionInSuccessors - 1));\n    }\n\n    private BeginQuorumEpochResponseData buildBeginQuorumEpochResponse(Errors partitionLevelError) {\n        return BeginQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel());\n    }\n\n    /**\n     * Handle a BeginEpoch request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     */\n    private BeginQuorumEpochResponseData handleBeginQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        BeginQuorumEpochRequestData request = (BeginQuorumEpochRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new BeginQuorumEpochResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return new BeginQuorumEpochResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        BeginQuorumEpochRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int requestLeaderId = partitionRequest.leaderId();\n        int requestEpoch = partitionRequest.leaderEpoch();\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(requestLeaderId, requestEpoch);\n        if (errorOpt.isPresent()) {\n            return buildBeginQuorumEpochResponse(errorOpt.get());\n        }\n\n        maybeTransition(OptionalInt.of(requestLeaderId), requestEpoch, currentTimeMs);\n        return buildBeginQuorumEpochResponse(Errors.NONE);\n    }\n\n    private boolean handleBeginQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        int remoteNodeId = responseMetadata.sourceId();\n        BeginQuorumEpochResponseData response = (BeginQuorumEpochResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        BeginQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors partitionError = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            partitionError, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (partitionError == Errors.NONE) {\n            if (quorum.isLeader()) {\n                LeaderState<T> state = quorum.leaderStateOrThrow();\n                state.addAcknowledgementFrom(remoteNodeId);\n            } else {\n                logger.debug(\"Ignoring BeginQuorumEpoch response {} since \" +\n                    \"this node is not the leader anymore\", response);\n            }\n            return true;\n        } else {\n            return handleUnexpectedError(partitionError, responseMetadata);\n        }\n    }\n\n    private EndQuorumEpochResponseData buildEndQuorumEpochResponse(Errors partitionLevelError) {\n        return EndQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel());\n    }\n\n    /**\n     * Handle an EndEpoch request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     */\n    private EndQuorumEpochResponseData handleEndQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        EndQuorumEpochRequestData request = (EndQuorumEpochRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new EndQuorumEpochResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return new EndQuorumEpochResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        EndQuorumEpochRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int requestEpoch = partitionRequest.leaderEpoch();\n        int requestLeaderId = partitionRequest.leaderId();\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(requestLeaderId, requestEpoch);\n        if (errorOpt.isPresent()) {\n            return buildEndQuorumEpochResponse(errorOpt.get());\n        }\n        maybeTransition(OptionalInt.of(requestLeaderId), requestEpoch, currentTimeMs);\n\n        if (quorum.isFollower()) {\n            FollowerState state = quorum.followerStateOrThrow();\n            if (state.leaderId() == requestLeaderId) {\n                List<Integer> preferredSuccessors = partitionRequest.preferredSuccessors();\n                long electionBackoffMs = endEpochElectionBackoff(preferredSuccessors);\n                logger.debug(\"Overriding follower fetch timeout to {} after receiving \" +\n                    \"EndQuorumEpoch request from leader {} in epoch {}\", electionBackoffMs,\n                    requestLeaderId, requestEpoch);\n                state.overrideFetchTimeout(currentTimeMs, electionBackoffMs);\n            }\n        }\n        return buildEndQuorumEpochResponse(Errors.NONE);\n    }\n\n    private long endEpochElectionBackoff(List<Integer> preferredSuccessors) {\n        // Based on the priority inside the preferred successors, choose the corresponding delayed\n        // election backoff time based on strict exponential mechanism so that the most up-to-date\n        // voter has a higher chance to be elected. If the node's priority is highest, become\n        // candidate immediately instead of waiting for next poll.\n        int position = preferredSuccessors.indexOf(quorum.localIdOrThrow());\n        if (position <= 0) {\n            return 0;\n        } else {\n            return strictExponentialElectionBackoffMs(position, preferredSuccessors.size());\n        }\n    }\n\n    private boolean handleEndQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        EndQuorumEpochResponseData response = (EndQuorumEpochResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        EndQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors partitionError = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            partitionError, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (partitionError == Errors.NONE) {\n            ResignedState resignedState = quorum.resignedStateOrThrow();\n            resignedState.acknowledgeResignation(responseMetadata.sourceId());\n            return true;\n        } else {\n            return handleUnexpectedError(partitionError, responseMetadata);\n        }\n    }\n\n    private FetchResponseData buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    ) {\n        return RaftUtil.singletonFetchResponse(log.topicPartition(), log.topicId(), Errors.NONE, partitionData -> {\n            partitionData\n                .setRecords(records)\n                .setErrorCode(error.code())\n                .setLogStartOffset(log.startOffset())\n                .setHighWatermark(\n                    highWatermark.map(offsetMetadata -> offsetMetadata.offset).orElse(-1L)\n                );\n\n            partitionData.currentLeader()\n                .setLeaderEpoch(quorum.epoch())\n                .setLeaderId(quorum.leaderIdOrSentinel());\n\n            switch (validOffsetAndEpoch.kind()) {\n                case DIVERGING:\n                    partitionData.divergingEpoch()\n                        .setEpoch(validOffsetAndEpoch.offsetAndEpoch().epoch())\n                        .setEndOffset(validOffsetAndEpoch.offsetAndEpoch().offset());\n                    break;\n                case SNAPSHOT:\n                    partitionData.snapshotId()\n                        .setEpoch(validOffsetAndEpoch.offsetAndEpoch().epoch())\n                        .setEndOffset(validOffsetAndEpoch.offsetAndEpoch().offset());\n                    break;\n                default:\n            }\n        });\n    }\n\n    private FetchResponseData buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    ) {\n        return buildFetchResponse(\n            error,\n            MemoryRecords.EMPTY,\n            ValidOffsetAndEpoch.valid(),\n            highWatermark\n        );\n    }\n\n    private boolean hasValidClusterId(String requestClusterId) {\n        // We don't enforce the cluster id if it is not provided.\n        if (requestClusterId == null) {\n            return true;\n        }\n        return clusterId.equals(requestClusterId);\n    }\n\n    /**\n     * Handle a Fetch request. The fetch offset and last fetched epoch are always\n     * validated against the current log. In the case that they do not match, the response will\n     * indicate the diverging offset/epoch. A follower is expected to truncate its log in this\n     * case and resend the fetch.\n     *\n     * This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *     but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INVALID_REQUEST} if the request epoch is larger than the leader's current epoch\n     *     or if either the fetch offset or the last fetched epoch is invalid\n     */\n    private CompletableFuture<FetchResponseData> handleFetchRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        FetchRequestData request = (FetchRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return completedFuture(new FetchResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code()));\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition(), log.topicId())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return completedFuture(new FetchResponseData().setErrorCode(Errors.INVALID_REQUEST.code()));\n        }\n        // If the ID is valid, we can set the topic name.\n        request.topics().get(0).setTopic(log.topicPartition().topic());\n\n        FetchRequestData.FetchPartition fetchPartition = request.topics().get(0).partitions().get(0);\n        if (request.maxWaitMs() < 0\n            || fetchPartition.fetchOffset() < 0\n            || fetchPartition.lastFetchedEpoch() < 0\n            || fetchPartition.lastFetchedEpoch() > fetchPartition.currentLeaderEpoch()) {\n            return completedFuture(\n                buildEmptyFetchResponse(Errors.INVALID_REQUEST, Optional.empty())\n            );\n        }\n\n        int replicaId = FetchRequest.replicaId(request);\n        FetchResponseData response = tryCompleteFetchRequest(replicaId, fetchPartition, currentTimeMs);\n        FetchResponseData.PartitionData partitionResponse =\n            response.responses().get(0).partitions().get(0);\n\n        if (partitionResponse.errorCode() != Errors.NONE.code()\n            || FetchResponse.recordsSize(partitionResponse) > 0\n            || request.maxWaitMs() == 0\n            || isPartitionDiverged(partitionResponse)\n            || isPartitionSnapshotted(partitionResponse)) {\n            // Reply immediately if any of the following is true\n            // 1. The response contains an error\n            // 2. There are records in the response\n            // 3. The fetching replica doesn't want to wait for the partition to contain new data\n            // 4. The fetching replica needs to truncate because the log diverged\n            // 5. The fetching replica needs to fetch a snapshot\n            return completedFuture(response);\n        }\n\n        CompletableFuture<Long> future = fetchPurgatory.await(\n            fetchPartition.fetchOffset(),\n            request.maxWaitMs());\n\n        return future.handle((completionTimeMs, exception) -> {\n            if (exception != null) {\n                Throwable cause = exception instanceof ExecutionException ?\n                    exception.getCause() : exception;\n\n                Errors error = Errors.forException(cause);\n                if (error == Errors.REQUEST_TIMED_OUT) {\n                    // Note that for this case the calling thread is the expiration service thread and not the\n                    // polling thread.\n                    //\n                    // If the fetch request timed out in purgatory, it means no new data is available,\n                    // just return the original fetch response.\n                    return response;\n                } else {\n                    // If there was any error other than REQUEST_TIMED_OUT, return it.\n                    logger.info(\"Failed to handle fetch from {} at {} due to {}\",\n                        replicaId, fetchPartition.fetchOffset(), error);\n                    return buildEmptyFetchResponse(error, Optional.empty());\n                }\n            }\n\n            // FIXME: `completionTimeMs`, which can be null\n            logger.trace(\"Completing delayed fetch from {} starting at offset {} at {}\",\n                replicaId, fetchPartition.fetchOffset(), completionTimeMs);\n\n            // It is safe to call tryCompleteFetchRequest because only the polling thread completes this\n            // future successfully. This is true because only the polling thread appends record batches to\n            // the log from maybeAppendBatches.\n            return tryCompleteFetchRequest(replicaId, fetchPartition, time.milliseconds());\n        });\n    }\n\n    private FetchResponseData tryCompleteFetchRequest(\n        int replicaId,\n        FetchRequestData.FetchPartition request,\n        long currentTimeMs\n    ) {\n        try {\n            Optional<Errors> errorOpt = validateLeaderOnlyRequest(request.currentLeaderEpoch());\n            if (errorOpt.isPresent()) {\n                return buildEmptyFetchResponse(errorOpt.get(), Optional.empty());\n            }\n\n            long fetchOffset = request.fetchOffset();\n            int lastFetchedEpoch = request.lastFetchedEpoch();\n            LeaderState<T> state = quorum.leaderStateOrThrow();\n\n            Optional<OffsetAndEpoch> latestSnapshotId = log.latestSnapshotId();\n            final ValidOffsetAndEpoch validOffsetAndEpoch;\n            if (fetchOffset == 0 && latestSnapshotId.isPresent()) {\n                // If the follower has an empty log and a snapshot exist, it is always more efficient\n                // to reply with a snapshot id (FETCH_SNAPSHOT) instead of fetching from the log segments.\n                validOffsetAndEpoch = ValidOffsetAndEpoch.snapshot(latestSnapshotId.get());\n            } else {\n                validOffsetAndEpoch = log.validateOffsetAndEpoch(fetchOffset, lastFetchedEpoch);\n            }\n\n            final Records records;\n            if (validOffsetAndEpoch.kind() == ValidOffsetAndEpoch.Kind.VALID) {\n                LogFetchInfo info = log.read(fetchOffset, Isolation.UNCOMMITTED);\n\n                if (state.updateReplicaState(replicaId, currentTimeMs, info.startOffsetMetadata)) {\n                    onUpdateLeaderHighWatermark(state, currentTimeMs);\n                }\n\n                records = info.records;\n            } else {\n                records = MemoryRecords.EMPTY;\n            }\n\n            return buildFetchResponse(Errors.NONE, records, validOffsetAndEpoch, state.highWatermark());\n        } catch (Exception e) {\n            logger.error(\"Caught unexpected error in fetch completion of request {}\", request, e);\n            return buildEmptyFetchResponse(Errors.UNKNOWN_SERVER_ERROR, Optional.empty());\n        }\n    }\n\n    private static boolean isPartitionDiverged(FetchResponseData.PartitionData partitionResponseData) {\n        FetchResponseData.EpochEndOffset divergingEpoch = partitionResponseData.divergingEpoch();\n\n        return divergingEpoch.epoch() != -1 || divergingEpoch.endOffset() != -1;\n    }\n\n    private static boolean isPartitionSnapshotted(FetchResponseData.PartitionData partitionResponseData) {\n        FetchResponseData.SnapshotId snapshotId = partitionResponseData.snapshotId();\n\n        return snapshotId.epoch() != -1 || snapshotId.endOffset() != -1;\n    }\n\n    private static OptionalInt optionalLeaderId(int leaderIdOrNil) {\n        if (leaderIdOrNil < 0)\n            return OptionalInt.empty();\n        return OptionalInt.of(leaderIdOrNil);\n    }\n\n    private static String listenerName(Listener<?> listener) {\n        return String.format(\"%s@%d\", listener.getClass().getTypeName(), System.identityHashCode(listener));\n    }\n\n    private boolean handleFetchResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        FetchResponseData response = (FetchResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!RaftUtil.hasValidTopicPartition(response, log.topicPartition(), log.topicId())) {\n            return false;\n        }\n        // If the ID is valid, we can set the topic name.\n        response.responses().get(0).setTopic(log.topicPartition().topic());\n\n        FetchResponseData.PartitionData partitionResponse =\n            response.responses().get(0).partitions().get(0);\n\n        FetchResponseData.LeaderIdAndEpoch currentLeaderIdAndEpoch = partitionResponse.currentLeader();\n        OptionalInt responseLeaderId = optionalLeaderId(currentLeaderIdAndEpoch.leaderId());\n        int responseEpoch = currentLeaderIdAndEpoch.leaderEpoch();\n        Errors error = Errors.forCode(partitionResponse.errorCode());\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        }\n\n        FollowerState state = quorum.followerStateOrThrow();\n        if (error == Errors.NONE) {\n            FetchResponseData.EpochEndOffset divergingEpoch = partitionResponse.divergingEpoch();\n            if (divergingEpoch.epoch() >= 0) {\n                // The leader is asking us to truncate before continuing\n                final OffsetAndEpoch divergingOffsetAndEpoch = new OffsetAndEpoch(\n                    divergingEpoch.endOffset(), divergingEpoch.epoch());\n\n                state.highWatermark().ifPresent(highWatermark -> {\n                    if (divergingOffsetAndEpoch.offset() < highWatermark.offset) {\n                        throw new KafkaException(\"The leader requested truncation to offset \" +\n                            divergingOffsetAndEpoch.offset() + \", which is below the current high watermark\" +\n                            \" \" + highWatermark);\n                    }\n                });\n\n                long truncationOffset = log.truncateToEndOffset(divergingOffsetAndEpoch);\n                logger.info(\n                    \"Truncated to offset {} from Fetch response from leader {}\",\n                    truncationOffset,\n                    quorum.leaderIdOrSentinel()\n                );\n\n                // Update the internal listener to the new end offset\n                partitionState.truncateNewEntries(truncationOffset);\n            } else if (partitionResponse.snapshotId().epoch() >= 0 ||\n                       partitionResponse.snapshotId().endOffset() >= 0) {\n                // The leader is asking us to fetch a snapshot\n\n                if (partitionResponse.snapshotId().epoch() < 0) {\n                    logger.error(\n                        \"The leader sent a snapshot id with a valid end offset {} but with an invalid epoch {}\",\n                        partitionResponse.snapshotId().endOffset(),\n                        partitionResponse.snapshotId().epoch()\n                    );\n                    return false;\n                } else if (partitionResponse.snapshotId().endOffset() < 0) {\n                    logger.error(\n                        \"The leader sent a snapshot id with a valid epoch {} but with an invalid end offset {}\",\n                        partitionResponse.snapshotId().epoch(),\n                        partitionResponse.snapshotId().endOffset()\n                    );\n                    return false;\n                } else {\n                    final OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n                        partitionResponse.snapshotId().endOffset(),\n                        partitionResponse.snapshotId().epoch()\n                    );\n\n                    // Do not validate the snapshot id against the local replicated log since this\n                    // snapshot is expected to reference offsets and epochs greater than the log\n                    // end offset and high-watermark.\n                    state.setFetchingSnapshot(log.createNewSnapshotUnchecked(snapshotId));\n                    logger.info(\n                        \"Fetching snapshot {} from Fetch response from leader {}\",\n                        snapshotId,\n                        quorum.leaderIdOrSentinel()\n                    );\n                }\n            } else {\n                Records records = FetchResponse.recordsOrFail(partitionResponse);\n                if (records.sizeInBytes() > 0) {\n                    appendAsFollower(records);\n                }\n\n                OptionalLong highWatermark = partitionResponse.highWatermark() < 0 ?\n                    OptionalLong.empty() : OptionalLong.of(partitionResponse.highWatermark());\n                updateFollowerHighWatermark(state, highWatermark);\n            }\n\n            state.resetFetchTimeout(currentTimeMs);\n            return true;\n        } else {\n            return handleUnexpectedError(error, responseMetadata);\n        }\n    }\n\n    private void appendAsFollower(\n        Records records\n    ) {\n        LogAppendInfo info = log.appendAsFollower(records);\n        if (quorum.isVoter()) {\n            // the leader only requires that voters have flushed their log before sending\n            // a Fetch request\n            log.flush(false);\n        }\n\n        partitionState.updateState();\n\n        OffsetAndEpoch endOffset = endOffset();\n        kafkaRaftMetrics.updateFetchedRecords(info.lastOffset - info.firstOffset + 1);\n        kafkaRaftMetrics.updateLogEnd(endOffset);\n        logger.trace(\"Follower end offset updated to {} after append\", endOffset);\n    }\n\n    private LogAppendInfo appendAsLeader(\n        Records records\n    ) {\n        LogAppendInfo info = log.appendAsLeader(records, quorum.epoch());\n\n        partitionState.updateState();\n\n        OffsetAndEpoch endOffset = endOffset();\n        kafkaRaftMetrics.updateAppendRecords(info.lastOffset - info.firstOffset + 1);\n        kafkaRaftMetrics.updateLogEnd(endOffset);\n        logger.trace(\"Leader appended records at base offset {}, new end offset is {}\", info.firstOffset, endOffset);\n        return info;\n    }\n\n    private DescribeQuorumResponseData handleDescribeQuorumRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        DescribeQuorumRequestData describeQuorumRequestData = (DescribeQuorumRequestData) requestMetadata.data;\n        if (!hasValidTopicPartition(describeQuorumRequestData, log.topicPartition())) {\n            return DescribeQuorumRequest.getPartitionLevelErrorResponse(\n                describeQuorumRequestData, Errors.UNKNOWN_TOPIC_OR_PARTITION);\n        }\n\n        if (!quorum.isLeader()) {\n            return DescribeQuorumResponse.singletonErrorResponse(\n                log.topicPartition(),\n                Errors.NOT_LEADER_OR_FOLLOWER\n            );\n        }\n\n        LeaderState<T> leaderState = quorum.leaderStateOrThrow();\n        return DescribeQuorumResponse.singletonResponse(\n            log.topicPartition(),\n            leaderState.describeQuorum(currentTimeMs)\n        );\n    }\n\n    /**\n     * Handle a FetchSnapshot request, similar to the Fetch request but we use {@link UnalignedRecords}\n     * in response because the records are not necessarily offset-aligned.\n     *\n     * This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *     but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INVALID_REQUEST} if the request epoch is larger than the leader's current epoch\n     *     or if either the fetch offset or the last fetched epoch is invalid\n     * - {@link Errors#SNAPSHOT_NOT_FOUND} if the request snapshot id does not exists\n     * - {@link Errors#POSITION_OUT_OF_RANGE} if the request snapshot offset out of range\n     */\n    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(data.clusterId())) {\n            return new FetchSnapshotResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n            return FetchSnapshotResponse.withTopLevelError(Errors.INVALID_REQUEST);\n        }\n\n        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n            .forTopicPartition(data, log.topicPartition());\n        if (!partitionSnapshotOpt.isPresent()) {\n            // The Raft client assumes that there is only one topic partition.\n            TopicPartition unknownTopicPartition = new TopicPartition(\n                data.topics().get(0).name(),\n                data.topics().get(0).partitions().get(0).partition()\n            );\n\n            return FetchSnapshotResponse.singleton(\n                unknownTopicPartition,\n                responsePartitionSnapshot -> responsePartitionSnapshot\n                    .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code())\n            );\n        }\n\n        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n                partitionSnapshot.currentLeaderEpoch()\n        );\n        if (leaderValidation.isPresent()) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(leaderValidation.get().code())\n            );\n        }\n\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n            partitionSnapshot.snapshotId().endOffset(),\n            partitionSnapshot.snapshotId().epoch()\n        );\n        Optional<RawSnapshotReader> snapshotOpt = log.readSnapshot(snapshotId);\n        if (!snapshotOpt.isPresent()) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code())\n            );\n        }\n\n        RawSnapshotReader snapshot = snapshotOpt.get();\n        long snapshotSize = snapshot.sizeInBytes();\n        if (partitionSnapshot.position() < 0 || partitionSnapshot.position() >= snapshotSize) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(Errors.POSITION_OUT_OF_RANGE.code())\n            );\n        }\n\n        if (partitionSnapshot.position() > Integer.MAX_VALUE) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Trying to fetch a snapshot with size (%d) and a position (%d) larger than %d\",\n                    snapshotSize,\n                    partitionSnapshot.position(),\n                    Integer.MAX_VALUE\n                )\n            );\n        }\n\n        int maxSnapshotSize;\n        try {\n            maxSnapshotSize = Math.toIntExact(snapshotSize);\n        } catch (ArithmeticException e) {\n            maxSnapshotSize = Integer.MAX_VALUE;\n        }\n\n        UnalignedRecords records = snapshot.slice(partitionSnapshot.position(), Math.min(data.maxBytes(), maxSnapshotSize));\n\n        LeaderState<T> state = quorum.leaderStateOrThrow();\n        state.updateCheckQuorumForFollowingVoter(data.replicaId(), currentTimeMs);\n\n        return FetchSnapshotResponse.singleton(\n            log.topicPartition(),\n            responsePartitionSnapshot -> {\n                addQuorumLeader(responsePartitionSnapshot)\n                    .snapshotId()\n                    .setEndOffset(snapshotId.offset())\n                    .setEpoch(snapshotId.epoch());\n\n                return responsePartitionSnapshot\n                    .setSize(snapshotSize)\n                    .setPosition(partitionSnapshot.position())\n                    .setUnalignedRecords(records);\n            }\n        );\n    }\n\n    private boolean handleFetchSnapshotResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        FetchSnapshotResponseData data = (FetchSnapshotResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(data.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n            return false;\n        }\n\n        Optional<FetchSnapshotResponseData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotResponse\n            .forTopicPartition(data, log.topicPartition());\n        if (!partitionSnapshotOpt.isPresent()) {\n            return false;\n        }\n\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n\n        FetchSnapshotResponseData.LeaderIdAndEpoch currentLeaderIdAndEpoch = partitionSnapshot.currentLeader();\n        OptionalInt responseLeaderId = optionalLeaderId(currentLeaderIdAndEpoch.leaderId());\n        int responseEpoch = currentLeaderIdAndEpoch.leaderEpoch();\n        Errors error = Errors.forCode(partitionSnapshot.errorCode());\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        }\n\n        FollowerState state = quorum.followerStateOrThrow();\n\n        if (Errors.forCode(partitionSnapshot.errorCode()) == Errors.SNAPSHOT_NOT_FOUND ||\n            partitionSnapshot.snapshotId().endOffset() < 0 ||\n            partitionSnapshot.snapshotId().epoch() < 0) {\n\n            /* The leader deleted the snapshot before the follower could download it. Start over by\n             * resetting the fetching snapshot state and sending another fetch request.\n             */\n            logger.info(\n                \"Leader doesn't know about snapshot id {}, returned error {} and snapshot id {}\",\n                state.fetchingSnapshot(),\n                partitionSnapshot.errorCode(),\n                partitionSnapshot.snapshotId()\n            );\n            state.setFetchingSnapshot(Optional.empty());\n            state.resetFetchTimeout(currentTimeMs);\n            return true;\n        }\n\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n            partitionSnapshot.snapshotId().endOffset(),\n            partitionSnapshot.snapshotId().epoch()\n        );\n\n        RawSnapshotWriter snapshot;\n        if (state.fetchingSnapshot().isPresent()) {\n            snapshot = state.fetchingSnapshot().get();\n        } else {\n            throw new IllegalStateException(\n                String.format(\"Received unexpected fetch snapshot response: %s\", partitionSnapshot)\n            );\n        }\n\n        if (!snapshot.snapshotId().equals(snapshotId)) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Received fetch snapshot response with an invalid id. Expected %s; Received %s\",\n                    snapshot.snapshotId(),\n                    snapshotId\n                )\n            );\n        }\n        if (snapshot.sizeInBytes() != partitionSnapshot.position()) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Received fetch snapshot response with an invalid position. Expected %d; Received %d\",\n                    snapshot.sizeInBytes(),\n                    partitionSnapshot.position()\n                )\n            );\n        }\n\n        final UnalignedMemoryRecords records;\n        if (partitionSnapshot.unalignedRecords() instanceof MemoryRecords) {\n            records = new UnalignedMemoryRecords(((MemoryRecords) partitionSnapshot.unalignedRecords()).buffer());\n        } else if (partitionSnapshot.unalignedRecords() instanceof UnalignedMemoryRecords) {\n            records = (UnalignedMemoryRecords) partitionSnapshot.unalignedRecords();\n        } else {\n            throw new IllegalStateException(String.format(\"Received unexpected fetch snapshot response: %s\", partitionSnapshot));\n        }\n        snapshot.append(records);\n\n        if (snapshot.sizeInBytes() == partitionSnapshot.size()) {\n            // Finished fetching the snapshot.\n            snapshot.freeze();\n            state.setFetchingSnapshot(Optional.empty());\n\n            if (log.truncateToLatestSnapshot()) {\n                logger.info(\n                    \"Fully truncated the log at ({}, {}) after downloading snapshot {} from leader {}\",\n                    log.endOffset(),\n                    log.lastFetchedEpoch(),\n                    snapshot.snapshotId(),\n                    quorum.leaderIdOrSentinel()\n                );\n\n                // This will always reload the snapshot because the internal next offset\n                // is always less than the snapshot id just downloaded.\n                partitionState.updateState();\n\n                updateFollowerHighWatermark(state, OptionalLong.of(log.highWatermark().offset));\n            } else {\n                throw new IllegalStateException(\n                    String.format(\n                        \"Full log truncation expected but didn't happen. Snapshot of %s, log end offset %s, last fetched %d\",\n                        snapshot.snapshotId(),\n                        log.endOffset(),\n                        log.lastFetchedEpoch()\n                    )\n                );\n            }\n        }\n\n        state.resetFetchTimeout(currentTimeMs);\n        return true;\n    }\n\n    private boolean hasConsistentLeader(int epoch, OptionalInt leaderId) {\n        // Only elected leaders are sent in the request/response header, so if we have an elected\n        // leaderId, it should be consistent with what is in the message.\n        if (leaderId.isPresent() && leaderId.getAsInt() == quorum.localIdOrSentinel()) {\n            // The response indicates that we should be the leader, so we verify that is the case\n            return quorum.isLeader();\n        } else {\n            return epoch != quorum.epoch()\n                || !leaderId.isPresent()\n                || !quorum.leaderId().isPresent()\n                || leaderId.equals(quorum.leaderId());\n        }\n    }\n\n    /**\n     * Handle response errors that are common across request types.\n     *\n     * @param error Error from the received response\n     * @param leaderId Optional leaderId from the response\n     * @param epoch Epoch received from the response\n     * @param currentTimeMs Current epoch time in milliseconds\n     * @return Optional value indicating whether the error was handled here and the outcome of\n     *    that handling. Specifically:\n     *\n     *    - Optional.empty means that the response was not handled here and the custom\n     *        API handler should be applied\n     *    - Optional.of(true) indicates that the response was successfully handled here and\n     *        the request does not need to be retried\n     *    - Optional.of(false) indicates that the response was handled here, but that the request\n     *        will need to be retried\n     */\n    private Optional<Boolean> maybeHandleCommonResponse(\n        Errors error,\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    ) {\n        if (epoch < quorum.epoch() || error == Errors.UNKNOWN_LEADER_EPOCH) {\n            // We have a larger epoch, so the response is no longer relevant\n            return Optional.of(true);\n        } else if (epoch > quorum.epoch()\n            || error == Errors.FENCED_LEADER_EPOCH\n            || error == Errors.NOT_LEADER_OR_FOLLOWER) {\n\n            // The response indicates that the request had a stale epoch, but we need\n            // to validate the epoch from the response against our current state.\n            maybeTransition(leaderId, epoch, currentTimeMs);\n            return Optional.of(true);\n        } else if (epoch == quorum.epoch()\n            && leaderId.isPresent()\n            && !quorum.hasLeader()) {\n\n            // Since we are transitioning to Follower, we will only forward the\n            // request to the handler if there is no error. Otherwise, we will let\n            // the request be retried immediately (if needed) after the transition.\n            // This handling allows an observer to discover the leader and append\n            // to the log in the same Fetch request.\n            transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n            if (error == Errors.NONE) {\n                return Optional.empty();\n            } else {\n                return Optional.of(true);\n            }\n        } else if (error == Errors.BROKER_NOT_AVAILABLE) {\n            return Optional.of(false);\n        } else if (error == Errors.INCONSISTENT_GROUP_PROTOCOL) {\n            // For now we treat this as a fatal error. Once we have support for quorum\n            // reassignment, this error could suggest that either we or the recipient of\n            // the request just has stale voter information, which means we can retry\n            // after backing off.\n            throw new IllegalStateException(\"Received error indicating inconsistent voter sets\");\n        } else if (error == Errors.INVALID_REQUEST) {\n            throw new IllegalStateException(\"Received unexpected invalid request error\");\n        }\n\n        return Optional.empty();\n    }\n\n    private void maybeTransition(\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    ) {\n        if (!hasConsistentLeader(epoch, leaderId)) {\n            throw new IllegalStateException(\"Received request or response with leader \" + leaderId +\n                \" and epoch \" + epoch + \" which is inconsistent with current leader \" +\n                quorum.leaderId() + \" and epoch \" + quorum.epoch());\n        } else if (epoch > quorum.epoch()) {\n            if (leaderId.isPresent()) {\n                transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n            } else {\n                transitionToUnattached(epoch);\n            }\n        } else if (leaderId.isPresent() && !quorum.hasLeader()) {\n            // The request or response indicates the leader of the current epoch,\n            // which is currently unknown\n            transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n        }\n    }\n\n    private boolean handleTopLevelError(Errors error, RaftResponse.Inbound response) {\n        if (error == Errors.BROKER_NOT_AVAILABLE) {\n            return false;\n        } else if (error == Errors.CLUSTER_AUTHORIZATION_FAILED) {\n            throw new ClusterAuthorizationException(\"Received cluster authorization error in response \" + response);\n        } else {\n            return handleUnexpectedError(error, response);\n        }\n    }\n\n    private boolean handleUnexpectedError(Errors error, RaftResponse.Inbound response) {\n        logger.error(\"Unexpected error {} in {} response: {}\",\n            error, ApiKeys.forId(response.data.apiKey()), response);\n        return false;\n    }\n\n    private void handleResponse(RaftResponse.Inbound response, long currentTimeMs) {\n        // The response epoch matches the local epoch, so we can handle the response\n        ApiKeys apiKey = ApiKeys.forId(response.data.apiKey());\n        final boolean handledSuccessfully;\n\n        switch (apiKey) {\n            case FETCH:\n                handledSuccessfully = handleFetchResponse(response, currentTimeMs);\n                break;\n\n            case VOTE:\n                handledSuccessfully = handleVoteResponse(response, currentTimeMs);\n                break;\n\n            case BEGIN_QUORUM_EPOCH:\n                handledSuccessfully = handleBeginQuorumEpochResponse(response, currentTimeMs);\n                break;\n\n            case END_QUORUM_EPOCH:\n                handledSuccessfully = handleEndQuorumEpochResponse(response, currentTimeMs);\n                break;\n\n            case FETCH_SNAPSHOT:\n                handledSuccessfully = handleFetchSnapshotResponse(response, currentTimeMs);\n                break;\n\n            default:\n                throw new IllegalArgumentException(\"Received unexpected response type: \" + apiKey);\n        }\n\n        ConnectionState connection = requestManager.getOrCreate(response.sourceId());\n        if (handledSuccessfully) {\n            connection.onResponseReceived(response.correlationId);\n        } else {\n            connection.onResponseError(response.correlationId, currentTimeMs);\n        }\n    }\n\n    /**\n     * Validate common state for requests to establish leadership.\n     *\n     * These include the Vote, BeginQuorumEpoch and EndQuorumEpoch RPCs. If an error is present in\n     * the returned value, it should be returned in the response.\n     */\n    private Optional<Errors> validateVoterOnlyRequest(int remoteNodeId, int requestEpoch) {\n        if (requestEpoch < quorum.epoch()) {\n            return Optional.of(Errors.FENCED_LEADER_EPOCH);\n        } else if (remoteNodeId < 0) {\n            return Optional.of(Errors.INVALID_REQUEST);\n        } else {\n            return Optional.empty();\n        }\n    }\n\n    /**\n     * Validate a request which is intended for the current quorum leader.\n     * If an error is present in the returned value, it should be returned\n     * in the response.\n     */\n    private Optional<Errors> validateLeaderOnlyRequest(int requestEpoch) {\n        if (requestEpoch < quorum.epoch()) {\n            return Optional.of(Errors.FENCED_LEADER_EPOCH);\n        } else if (requestEpoch > quorum.epoch()) {\n            return Optional.of(Errors.UNKNOWN_LEADER_EPOCH);\n        } else if (!quorum.isLeader()) {\n            // In general, non-leaders do not expect to receive requests\n            // matching their own epoch, but it is possible when observers\n            // are using the Fetch API to find the result of an election.\n            return Optional.of(Errors.NOT_LEADER_OR_FOLLOWER);\n        } else if (shutdown.get() != null) {\n            return Optional.of(Errors.BROKER_NOT_AVAILABLE);\n        } else {\n            return Optional.empty();\n        }\n    }\n\n    private void handleRequest(RaftRequest.Inbound request, long currentTimeMs) {\n        ApiKeys apiKey = ApiKeys.forId(request.data.apiKey());\n        final CompletableFuture<? extends ApiMessage> responseFuture;\n\n        switch (apiKey) {\n            case FETCH:\n                responseFuture = handleFetchRequest(request, currentTimeMs);\n                break;\n\n            case VOTE:\n                responseFuture = completedFuture(handleVoteRequest(request));\n                break;\n\n            case BEGIN_QUORUM_EPOCH:\n                responseFuture = completedFuture(handleBeginQuorumEpochRequest(request, currentTimeMs));\n                break;\n\n            case END_QUORUM_EPOCH:\n                responseFuture = completedFuture(handleEndQuorumEpochRequest(request, currentTimeMs));\n                break;\n\n            case DESCRIBE_QUORUM:\n                responseFuture = completedFuture(handleDescribeQuorumRequest(request, currentTimeMs));\n                break;\n\n            case FETCH_SNAPSHOT:\n                responseFuture = completedFuture(handleFetchSnapshotRequest(request, currentTimeMs));\n                break;\n\n            default:\n                throw new IllegalArgumentException(\"Unexpected request type \" + apiKey);\n        }\n\n        responseFuture.whenComplete((response, exception) -> {\n            final ApiMessage message;\n            if (response != null) {\n                message = response;\n            } else {\n                message = RaftUtil.errorResponse(apiKey, Errors.forException(exception));\n            }\n\n            RaftResponse.Outbound responseMessage = new RaftResponse.Outbound(request.correlationId(), message);\n            request.completion.complete(responseMessage);\n            logger.trace(\"Sent response {} to inbound request {}\", responseMessage, request);\n        });\n    }\n\n    private void handleInboundMessage(RaftMessage message, long currentTimeMs) {\n        logger.trace(\"Received inbound message {}\", message);\n\n        if (message instanceof RaftRequest.Inbound) {\n            RaftRequest.Inbound request = (RaftRequest.Inbound) message;\n            handleRequest(request, currentTimeMs);\n        } else if (message instanceof RaftResponse.Inbound) {\n            RaftResponse.Inbound response = (RaftResponse.Inbound) message;\n            ConnectionState connection = requestManager.getOrCreate(response.sourceId());\n            if (connection.isResponseExpected(response.correlationId)) {\n                handleResponse(response, currentTimeMs);\n            } else {\n                logger.debug(\"Ignoring response {} since it is no longer needed\", response);\n            }\n        } else {\n            throw new IllegalArgumentException(\"Unexpected message \" + message);\n        }\n    }\n\n    /**\n     * Attempt to send a request. Return the time to wait before the request can be retried.\n     */\n    private long maybeSendRequest(\n        long currentTimeMs,\n        int destinationId,\n        Supplier<ApiMessage> requestSupplier\n    )  {\n        ConnectionState connection = requestManager.getOrCreate(destinationId);\n\n        if (connection.isBackingOff(currentTimeMs)) {\n            long remainingBackoffMs = connection.remainingBackoffMs(currentTimeMs);\n            logger.debug(\"Connection for {} is backing off for {} ms\", destinationId, remainingBackoffMs);\n            return remainingBackoffMs;\n        }\n\n        if (connection.isReady(currentTimeMs)) {\n            int correlationId = channel.newCorrelationId();\n            ApiMessage request = requestSupplier.get();\n\n            RaftRequest.Outbound requestMessage = new RaftRequest.Outbound(\n                correlationId,\n                request,\n                destinationId,\n                currentTimeMs\n            );\n\n            requestMessage.completion.whenComplete((response, exception) -> {\n                if (exception != null) {\n                    ApiKeys api = ApiKeys.forId(request.apiKey());\n                    Errors error = Errors.forException(exception);\n                    ApiMessage errorResponse = RaftUtil.errorResponse(api, error);\n\n                    response = new RaftResponse.Inbound(\n                        correlationId,\n                        errorResponse,\n                        destinationId\n                    );\n                }\n\n                messageQueue.add(response);\n            });\n\n            channel.send(requestMessage);\n            logger.trace(\"Sent outbound request: {}\", requestMessage);\n            connection.onRequestSent(correlationId, currentTimeMs);\n            return Long.MAX_VALUE;\n        }\n\n        return connection.remainingRequestTimeMs(currentTimeMs);\n    }\n\n    private EndQuorumEpochRequestData buildEndQuorumEpochRequest(\n        ResignedState state\n    ) {\n        return EndQuorumEpochRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow(),\n            state.preferredSuccessors()\n        );\n    }\n\n    private long maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    ) {\n        long minBackoffMs = Long.MAX_VALUE;\n        for (Integer destinationId : destinationIds) {\n            long backoffMs = maybeSendRequest(currentTimeMs, destinationId, requestSupplier);\n            if (backoffMs < minBackoffMs) {\n                minBackoffMs = backoffMs;\n            }\n        }\n        return minBackoffMs;\n    }\n\n    private BeginQuorumEpochRequestData buildBeginQuorumEpochRequest() {\n        return BeginQuorumEpochRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow()\n        );\n    }\n\n    private VoteRequestData buildVoteRequest() {\n        OffsetAndEpoch endOffset = endOffset();\n        return VoteRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow(),\n            endOffset.epoch(),\n            endOffset.offset()\n        );\n    }\n\n    private FetchRequestData buildFetchRequest() {\n        FetchRequestData request = RaftUtil.singletonFetchRequest(log.topicPartition(), log.topicId(), fetchPartition -> {\n            fetchPartition\n                .setCurrentLeaderEpoch(quorum.epoch())\n                .setLastFetchedEpoch(log.lastFetchedEpoch())\n                .setFetchOffset(log.endOffset().offset);\n        });\n        return request\n            .setMaxBytes(MAX_FETCH_SIZE_BYTES)\n            .setMaxWaitMs(fetchMaxWaitMs)\n            .setClusterId(clusterId)\n            .setReplicaState(new FetchRequestData.ReplicaState().setReplicaId(quorum.localIdOrSentinel()));\n    }\n\n    private long maybeSendAnyVoterFetch(long currentTimeMs) {\n        OptionalInt readyVoterIdOpt = requestManager.findReadyVoter(currentTimeMs);\n        if (readyVoterIdOpt.isPresent()) {\n            return maybeSendRequest(\n                currentTimeMs,\n                readyVoterIdOpt.getAsInt(),\n                this::buildFetchRequest\n            );\n        } else {\n            return requestManager.backoffBeforeAvailableVoter(currentTimeMs);\n        }\n    }\n\n    private FetchSnapshotRequestData buildFetchSnapshotRequest(OffsetAndEpoch snapshotId, long snapshotSize) {\n        FetchSnapshotRequestData.SnapshotId requestSnapshotId = new FetchSnapshotRequestData.SnapshotId()\n            .setEpoch(snapshotId.epoch())\n            .setEndOffset(snapshotId.offset());\n\n        FetchSnapshotRequestData request = FetchSnapshotRequest.singleton(\n            clusterId,\n            quorum().localIdOrSentinel(),\n            log.topicPartition(),\n            snapshotPartition -> snapshotPartition\n                .setCurrentLeaderEpoch(quorum.epoch())\n                .setSnapshotId(requestSnapshotId)\n                .setPosition(snapshotSize)\n        );\n\n        return request.setReplicaId(quorum.localIdOrSentinel());\n    }\n\n    private FetchSnapshotResponseData.PartitionSnapshot addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    ) {\n        partitionSnapshot.currentLeader()\n            .setLeaderEpoch(quorum.epoch())\n            .setLeaderId(quorum.leaderIdOrSentinel());\n\n        return partitionSnapshot;\n    }\n\n    public boolean isRunning() {\n        GracefulShutdown gracefulShutdown = shutdown.get();\n        return gracefulShutdown == null || !gracefulShutdown.isFinished();\n    }\n\n    public boolean isShuttingDown() {\n        GracefulShutdown gracefulShutdown = shutdown.get();\n        return gracefulShutdown != null && !gracefulShutdown.isFinished();\n    }\n\n    private void appendBatch(\n        LeaderState<T> state,\n        BatchAccumulator.CompletedBatch<T> batch,\n        long appendTimeMs\n    ) {\n        try {\n            int epoch = state.epoch();\n            LogAppendInfo info = appendAsLeader(batch.data);\n            OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(info.lastOffset, epoch);\n            CompletableFuture<Long> future = appendPurgatory.await(\n                offsetAndEpoch.offset() + 1, Integer.MAX_VALUE);\n\n            future.whenComplete((commitTimeMs, exception) -> {\n                if (exception != null) {\n                    logger.debug(\"Failed to commit {} records up to last offset {}\", batch.numRecords, offsetAndEpoch, exception);\n                } else {\n                    long elapsedTime = Math.max(0, commitTimeMs - appendTimeMs);\n                    double elapsedTimePerRecord = (double) elapsedTime / batch.numRecords;\n                    kafkaRaftMetrics.updateCommitLatency(elapsedTimePerRecord, appendTimeMs);\n                    logger.debug(\"Completed commit of {} records up to last offset {}\", batch.numRecords, offsetAndEpoch);\n                    batch.records.ifPresent(records -> {\n                        maybeFireHandleCommit(batch.baseOffset, epoch, batch.appendTimestamp(), batch.sizeInBytes(), records);\n                    });\n                }\n            });\n        } finally {\n            batch.release();\n        }\n    }\n\n    private long maybeAppendBatches(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        long timeUntilDrain = state.accumulator().timeUntilDrain(currentTimeMs);\n        if (timeUntilDrain <= 0) {\n            List<BatchAccumulator.CompletedBatch<T>> batches = state.accumulator().drain();\n            Iterator<BatchAccumulator.CompletedBatch<T>> iterator = batches.iterator();\n\n            try {\n                while (iterator.hasNext()) {\n                    BatchAccumulator.CompletedBatch<T> batch = iterator.next();\n                    appendBatch(state, batch, currentTimeMs);\n                }\n                flushLeaderLog(state, currentTimeMs);\n            } finally {\n                // Release and discard any batches which failed to be appended\n                while (iterator.hasNext()) {\n                    iterator.next().release();\n                }\n            }\n        }\n        return timeUntilDrain;\n    }\n\n    private long pollResigned(long currentTimeMs) {\n        ResignedState state = quorum.resignedStateOrThrow();\n        long endQuorumBackoffMs = maybeSendRequests(\n            currentTimeMs,\n            state.unackedVoters(),\n            () -> buildEndQuorumEpochRequest(state)\n        );\n\n        GracefulShutdown shutdown = this.shutdown.get();\n        final long stateTimeoutMs;\n        if (shutdown != null) {\n            // If we are shutting down, then we will remain in the resigned state\n            // until either the shutdown expires or an election bumps the epoch\n            stateTimeoutMs = shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            stateTimeoutMs = 0L;\n        } else {\n            stateTimeoutMs = state.remainingElectionTimeMs(currentTimeMs);\n        }\n\n        return Math.min(stateTimeoutMs, endQuorumBackoffMs);\n    }\n\n    private long pollLeader(long currentTimeMs) {\n        LeaderState<T> state = quorum.leaderStateOrThrow();\n        maybeFireLeaderChange(state);\n\n        long timeUntilCheckQuorumExpires = state.timeUntilCheckQuorumExpires(currentTimeMs);\n        if (shutdown.get() != null || state.isResignRequested() || timeUntilCheckQuorumExpires == 0) {\n            transitionToResigned(state.nonLeaderVotersByDescendingFetchOffset());\n            return 0L;\n        }\n\n        long timeUntilFlush = maybeAppendBatches(\n            state,\n            currentTimeMs\n        );\n\n        long timeUntilSend = maybeSendRequests(\n            currentTimeMs,\n            state.nonAcknowledgingVoters(),\n            this::buildBeginQuorumEpochRequest\n        );\n\n        return Math.min(timeUntilFlush, Math.min(timeUntilSend, timeUntilCheckQuorumExpires));\n    }\n\n    private long maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    ) {\n        // Continue sending Vote requests as long as we still have a chance to win the election\n        if (!state.isVoteRejected()) {\n            return maybeSendRequests(\n                currentTimeMs,\n                state.unrecordedVoters(),\n                this::buildVoteRequest\n            );\n        }\n        return Long.MAX_VALUE;\n    }\n\n    private long pollCandidate(long currentTimeMs) {\n        CandidateState state = quorum.candidateStateOrThrow();\n        GracefulShutdown shutdown = this.shutdown.get();\n\n        if (shutdown != null) {\n            // If we happen to shutdown while we are a candidate, we will continue\n            // with the current election until one of the following conditions is met:\n            //  1) we are elected as leader (which allows us to resign)\n            //  2) another leader is elected\n            //  3) the shutdown timer expires\n            long minRequestBackoffMs = maybeSendVoteRequests(state, currentTimeMs);\n            return Math.min(shutdown.remainingTimeMs(), minRequestBackoffMs);\n        } else if (state.isBackingOff()) {\n            if (state.isBackoffComplete(currentTimeMs)) {\n                logger.info(\"Re-elect as candidate after election backoff has completed\");\n                transitionToCandidate(currentTimeMs);\n                return 0L;\n            }\n            return state.remainingBackoffMs(currentTimeMs);\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            long backoffDurationMs = binaryExponentialElectionBackoffMs(state.retries());\n            logger.info(\"Election has timed out, backing off for {}ms before becoming a candidate again\",\n                backoffDurationMs);\n            state.startBackingOff(currentTimeMs, backoffDurationMs);\n            return backoffDurationMs;\n        } else {\n            long minRequestBackoffMs = maybeSendVoteRequests(state, currentTimeMs);\n            return Math.min(minRequestBackoffMs, state.remainingElectionTimeMs(currentTimeMs));\n        }\n    }\n\n    private long pollFollower(long currentTimeMs) {\n        FollowerState state = quorum.followerStateOrThrow();\n        if (quorum.isVoter()) {\n            return pollFollowerAsVoter(state, currentTimeMs);\n        } else {\n            return pollFollowerAsObserver(state, currentTimeMs);\n        }\n    }\n\n    private long pollFollowerAsVoter(FollowerState state, long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown != null) {\n            // If we are a follower, then we can shutdown immediately. We want to\n            // skip the transition to candidate in any case.\n            return 0;\n        } else if (state.hasFetchTimeoutExpired(currentTimeMs)) {\n            logger.info(\"Become candidate due to fetch timeout\");\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            long backoffMs = maybeSendFetchOrFetchSnapshot(state, currentTimeMs);\n\n            return Math.min(backoffMs, state.remainingFetchTimeMs(currentTimeMs));\n        }\n    }\n\n    private long pollFollowerAsObserver(FollowerState state, long currentTimeMs) {\n        if (state.hasFetchTimeoutExpired(currentTimeMs)) {\n            return maybeSendAnyVoterFetch(currentTimeMs);\n        } else {\n            final long backoffMs;\n\n            // If the current leader is backing off due to some failure or if the\n            // request has timed out, then we attempt to send the Fetch to another\n            // voter in order to discover if there has been a leader change.\n            ConnectionState connection = hasAnyInflightRequest(state);\n            if (connection.hasRequestTimedOut(currentTimeMs)) {\n                backoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n                connection.reset();\n            } else if (connection.isBackingOff(currentTimeMs)) {\n                backoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n            } else {\n                backoffMs = maybeSendFetchOrFetchSnapshot(state, currentTimeMs);\n            }\n\n            return Math.min(backoffMs, state.remainingFetchTimeMs(currentTimeMs));\n        }\n    }\n\n    private ConnectionState hasAnyInflightRequest(FollowerState state) {\n        ConnectionState connection = requestManager.getOrCreate(state.leaderId());\n        return connection;\n    }\n\n    private long maybeSendFetchOrFetchSnapshot(FollowerState state, long currentTimeMs) {\n        final Supplier<ApiMessage> requestSupplier;\n\n        if (state.fetchingSnapshot().isPresent()) {\n            RawSnapshotWriter snapshot = state.fetchingSnapshot().get();\n            long snapshotSize = snapshot.sizeInBytes();\n\n            requestSupplier = () -> buildFetchSnapshotRequest(snapshot.snapshotId(), snapshotSize);\n        } else {\n            requestSupplier = this::buildFetchRequest;\n        }\n\n        return maybeSendRequest(currentTimeMs, state.leaderId(), requestSupplier);\n    }\n\n    private long pollVoted(long currentTimeMs) {\n        VotedState state = quorum.votedStateOrThrow();\n        GracefulShutdown shutdown = this.shutdown.get();\n\n        if (shutdown != null) {\n            // If shutting down, then remain in this state until either the\n            // shutdown completes or an epoch bump forces another state transition\n            return shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            return state.remainingElectionTimeMs(currentTimeMs);\n        }\n    }\n\n    private long pollUnattached(long currentTimeMs) {\n        UnattachedState state = quorum.unattachedStateOrThrow();\n        if (quorum.isVoter()) {\n            return pollUnattachedAsVoter(state, currentTimeMs);\n        } else {\n            return pollUnattachedAsObserver(state, currentTimeMs);\n        }\n    }\n\n    private long pollUnattachedAsVoter(UnattachedState state, long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown != null) {\n            // If shutting down, then remain in this state until either the\n            // shutdown completes or an epoch bump forces another state transition\n            return shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            return state.remainingElectionTimeMs(currentTimeMs);\n        }\n    }\n\n    private long pollUnattachedAsObserver(UnattachedState state, long currentTimeMs) {\n        long fetchBackoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n        return Math.min(fetchBackoffMs, state.remainingElectionTimeMs(currentTimeMs));\n    }\n\n    private long pollCurrentState(long currentTimeMs) {\n        if (quorum.isLeader()) {\n            return pollLeader(currentTimeMs);\n        } else if (quorum.isCandidate()) {\n            return pollCandidate(currentTimeMs);\n        } else if (quorum.isFollower()) {\n            return pollFollower(currentTimeMs);\n        } else if (quorum.isVoted()) {\n            return pollVoted(currentTimeMs);\n        } else if (quorum.isUnattached()) {\n            return pollUnattached(currentTimeMs);\n        } else if (quorum.isResigned()) {\n            return pollResigned(currentTimeMs);\n        } else {\n            throw new IllegalStateException(\"Unexpected quorum state \" + quorum);\n        }\n    }\n\n    private void pollListeners() {\n        // Apply all of the pending registration\n        while (true) {\n            Registration<T> registration = pendingRegistrations.poll();\n            if (registration == null) {\n                break;\n            }\n\n            processRegistration(registration);\n        }\n\n        // Check listener progress to see if reads are expected\n        quorum.highWatermark().ifPresent(highWatermarkMetadata -> {\n            updateListenersProgress(highWatermarkMetadata.offset);\n        });\n\n        // Notify the new listeners of the latest leader and epoch\n        Optional<LeaderState<T>> leaderState = quorum.maybeLeaderState();\n        if (leaderState.isPresent()) {\n            maybeFireLeaderChange(leaderState.get());\n        } else {\n            maybeFireLeaderChange();\n        }\n    }\n\n    private void processRegistration(Registration<T> registration) {\n        Listener<T> listener = registration.listener();\n        Registration.Ops ops = registration.ops();\n\n        if (ops == Registration.Ops.REGISTER) {\n            if (listenerContexts.putIfAbsent(listener, new ListenerContext(listener)) != null) {\n                logger.error(\"Attempting to add a listener that already exists: {}\", listenerName(listener));\n            } else {\n                logger.info(\"Registered the listener {}\", listenerName(listener));\n            }\n        } else {\n            if (listenerContexts.remove(listener) == null) {\n                logger.error(\"Attempting to remove a listener that doesn't exists: {}\", listenerName(listener));\n            } else {\n                logger.info(\"Unregistered the listener {}\", listenerName(listener));\n            }\n        }\n    }\n\n    private boolean maybeCompleteShutdown(long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown == null) {\n            return false;\n        }\n\n        shutdown.update(currentTimeMs);\n        if (shutdown.hasTimedOut()) {\n            shutdown.failWithTimeout();\n            return true;\n        }\n\n        if (quorum.isObserver()\n            || quorum.isOnlyVoter()\n            || quorum.hasRemoteLeader()\n        ) {\n            shutdown.complete();\n            return true;\n        }\n\n        return false;\n    }\n\n    /**\n     * A simple timer based log cleaner\n     */\n    private static class RaftMetadataLogCleanerManager {\n        private final Logger logger;\n        private final Timer timer;\n        private final long delayMs;\n        private final Runnable cleaner;\n\n        RaftMetadataLogCleanerManager(Logger logger, Time time, long delayMs, Runnable cleaner) {\n            this.logger = logger;\n            this.timer = time.timer(delayMs);\n            this.delayMs = delayMs;\n            this.cleaner = cleaner;\n        }\n\n        public long maybeClean(long currentTimeMs) {\n            timer.update(currentTimeMs);\n            if (timer.isExpired()) {\n                try {\n                    cleaner.run();\n                } catch (Throwable t) {\n                    logger.error(\"Had an error during log cleaning\", t);\n                }\n                timer.reset(delayMs);\n            }\n            return timer.remainingMs();\n        }\n    }\n\n    private void wakeup() {\n        messageQueue.wakeup();\n    }\n\n    /**\n     * Handle an inbound request. The response will be returned through\n     * {@link RaftRequest.Inbound#completion}.\n     *\n     * @param request The inbound request\n     */\n    public void handle(RaftRequest.Inbound request) {\n        messageQueue.add(Objects.requireNonNull(request));\n    }\n\n    /**\n     * Poll for new events. This allows the client to handle inbound\n     * requests and send any needed outbound requests.\n     */\n    public void poll() {\n        if (!isInitialized()) {\n            throw new IllegalStateException(\"Replica needs to be initialized before polling\");\n        }\n\n        long startPollTimeMs = time.milliseconds();\n        if (maybeCompleteShutdown(startPollTimeMs)) {\n            return;\n        }\n\n        long pollStateTimeoutMs = pollCurrentState(startPollTimeMs);\n        long cleaningTimeoutMs = snapshotCleaner.maybeClean(startPollTimeMs);\n        long pollTimeoutMs = Math.min(pollStateTimeoutMs, cleaningTimeoutMs);\n\n        long startWaitTimeMs = time.milliseconds();\n        kafkaRaftMetrics.updatePollStart(startWaitTimeMs);\n\n        RaftMessage message = messageQueue.poll(pollTimeoutMs);\n\n        long endWaitTimeMs = time.milliseconds();\n        kafkaRaftMetrics.updatePollEnd(endWaitTimeMs);\n\n        if (message != null) {\n            handleInboundMessage(message, endWaitTimeMs);\n        }\n\n        pollListeners();\n    }\n\n    @Override\n    public long scheduleAppend(int epoch, List<T> records) {\n        return append(epoch, records, OptionalLong.empty(), false);\n    }\n\n    @Override\n    public long scheduleAtomicAppend(int epoch, OptionalLong requiredBaseOffset, List<T> records) {\n        return append(epoch, records, requiredBaseOffset, true);\n    }\n\n    private long append(int epoch, List<T> records, OptionalLong requiredBaseOffset, boolean isAtomic) {\n        if (!isInitialized()) {\n            throw new NotLeaderException(\"Append failed because the replica is not the current leader\");\n        }\n\n        LeaderState<T> leaderState = quorum.<T>maybeLeaderState().orElseThrow(\n            () -> new NotLeaderException(\"Append failed because the replica is not the current leader\")\n        );\n\n        BatchAccumulator<T> accumulator = leaderState.accumulator();\n        boolean isFirstAppend = accumulator.isEmpty();\n        final long offset = accumulator.append(epoch, records, requiredBaseOffset, isAtomic);\n\n        // Wakeup the network channel if either this is the first append\n        // or the accumulator is ready to drain now. Checking for the first\n        // append ensures that we give the IO thread a chance to observe\n        // the linger timeout so that it can schedule its own wakeup in case\n        // there are no additional appends.\n        if (isFirstAppend || accumulator.needsDrain(time.milliseconds())) {\n            wakeup();\n        }\n        return offset;\n    }\n\n    @Override\n    public CompletableFuture<Void> shutdown(int timeoutMs) {\n        logger.info(\"Beginning graceful shutdown\");\n        CompletableFuture<Void> shutdownComplete = new CompletableFuture<>();\n        shutdown.set(new GracefulShutdown(timeoutMs, shutdownComplete));\n        wakeup();\n        return shutdownComplete;\n    }\n\n    @Override\n    public void resign(int epoch) {\n        if (epoch < 0) {\n            throw new IllegalArgumentException(\"Attempt to resign from an invalid negative epoch \" + epoch);\n        } else if (!isInitialized()) {\n            throw new IllegalStateException(\"Replica needs to be initialized before resigning\");\n        } else if (!quorum.isVoter()) {\n            throw new IllegalStateException(\"Attempt to resign by a non-voter\");\n        }\n\n        LeaderAndEpoch leaderAndEpoch = leaderAndEpoch();\n        int currentEpoch = leaderAndEpoch.epoch();\n\n        if (epoch > currentEpoch) {\n            throw new IllegalArgumentException(\"Attempt to resign from epoch \" + epoch +\n                \" which is larger than the current epoch \" + currentEpoch);\n        } else if (epoch < currentEpoch) {\n            // If the passed epoch is smaller than the current epoch, then it might mean\n            // that the listener has not been notified about a leader change that already\n            // took place. In this case, we consider the call as already fulfilled and\n            // take no further action.\n            logger.debug(\"Ignoring call to resign from epoch {} since it is smaller than the \" +\n                \"current epoch {}\", epoch, currentEpoch);\n        } else if (!leaderAndEpoch.isLeader(quorum.localIdOrThrow())) {\n            throw new IllegalArgumentException(\"Cannot resign from epoch \" + epoch +\n                \" since we are not the leader\");\n        } else {\n            // Note that if we transition to another state before we have a chance to\n            // request resignation, then we consider the call fulfilled.\n            Optional<LeaderState<Object>> leaderStateOpt = quorum.maybeLeaderState();\n            if (!leaderStateOpt.isPresent()) {\n                logger.debug(\"Ignoring call to resign from epoch {} since this node is \" +\n                    \"no longer the leader\", epoch);\n                return;\n            }\n\n            LeaderState<Object> leaderState = leaderStateOpt.get();\n            if (leaderState.epoch() != epoch) {\n                logger.debug(\"Ignoring call to resign from epoch {} since it is smaller than the \" +\n                    \"current epoch {}\", epoch, leaderState.epoch());\n            } else {\n                logger.info(\"Received user request to resign from the current epoch {}\", currentEpoch);\n                leaderState.requestResign();\n                wakeup();\n            }\n        }\n    }\n\n    @Override\n    public Optional<SnapshotWriter<T>> createSnapshot(\n        OffsetAndEpoch snapshotId,\n        long lastContainedLogTimestamp\n    ) {\n        if (!isInitialized()) {\n            throw new IllegalStateException(\"Cannot create snapshot before the replica has been initialized\");\n        }\n\n        return log.createNewSnapshot(snapshotId).map(writer -> {\n            long lastContainedLogOffset = snapshotId.offset() - 1;\n\n            RawSnapshotWriter wrappedWriter = new NotifyingRawSnapshotWriter(writer, offsetAndEpoch -> {\n                // Trim the state in the internal listener up to the new snapshot\n                partitionState.truncateOldEntries(offsetAndEpoch.offset());\n            });\n\n            return new RecordsSnapshotWriter.Builder()\n                .setLastContainedLogTimestamp(lastContainedLogTimestamp)\n                .setTime(time)\n                .setMaxBatchSize(MAX_BATCH_SIZE_BYTES)\n                .setMemoryPool(memoryPool)\n                .setRawSnapshotWriter(wrappedWriter)\n                .setKraftVersion(partitionState.kraftVersionAtOffset(lastContainedLogOffset))\n                .setVoterSet(partitionState.voterSetAtOffset(lastContainedLogOffset))\n                .build(serde);\n        });\n    }\n\n    @Override\n    public Optional<OffsetAndEpoch> latestSnapshotId() {\n        return log.latestSnapshotId();\n    }\n\n    @Override\n    public long logEndOffset() {\n        return log.endOffset().offset;\n    }\n\n    @Override\n    public void close() {\n        log.flush(true);\n        if (kafkaRaftMetrics != null) {\n            kafkaRaftMetrics.close();\n        }\n        if (memoryPool instanceof BatchMemoryPool) {\n            BatchMemoryPool batchMemoryPool = (BatchMemoryPool) memoryPool;\n            batchMemoryPool.releaseRetained();\n        }\n    }\n\n    @Override\n    public OptionalLong highWatermark() {\n        if (isInitialized() && quorum.highWatermark().isPresent()) {\n            return OptionalLong.of(quorum.highWatermark().get().offset);\n        } else {\n            return OptionalLong.empty();\n        }\n    }\n\n    public Optional<Node> voterNode(int id, String listener) {\n        return partitionState.lastVoterSet().voterNode(id, listener);\n    }\n\n    // Visible only for test\n    QuorumState quorum() {\n        // It's okay to return null since this method is only called by tests\n        return quorum;\n    }\n\n    private boolean isInitialized() {\n        return partitionState != null && quorum != null && requestManager != null && kafkaRaftMetrics != null;\n    }\n\n    private class GracefulShutdown {\n        final Timer finishTimer;\n        final CompletableFuture<Void> completeFuture;\n\n        public GracefulShutdown(long shutdownTimeoutMs,\n                                CompletableFuture<Void> completeFuture) {\n            this.finishTimer = time.timer(shutdownTimeoutMs);\n            this.completeFuture = completeFuture;\n        }\n\n        public void update(long currentTimeMs) {\n            finishTimer.update(currentTimeMs);\n        }\n\n        public boolean hasTimedOut() {\n            return finishTimer.isExpired();\n        }\n\n        public boolean isFinished() {\n            return completeFuture.isDone();\n        }\n\n        public long remainingTimeMs() {\n            return finishTimer.remainingMs();\n        }\n\n        public void failWithTimeout() {\n            logger.warn(\"Graceful shutdown timed out after {}ms\", finishTimer.timeoutMs());\n            completeFuture.completeExceptionally(\n                new TimeoutException(\"Timeout expired before graceful shutdown completed\"));\n        }\n\n        public void complete() {\n            logger.info(\"Graceful shutdown completed\");\n            completeFuture.complete(null);\n        }\n    }\n\n    private static final class Registration<T> {\n        private final Ops ops;\n        private final Listener<T> listener;\n\n        private Registration(Ops ops, Listener<T> listener) {\n            this.ops = ops;\n            this.listener = listener;\n        }\n\n        private Ops ops() {\n            return ops;\n        }\n\n        private Listener<T> listener() {\n            return listener;\n        }\n\n        private enum Ops {\n            REGISTER, UNREGISTER\n        }\n\n        private static <T> Registration<T> register(Listener<T> listener) {\n            return new Registration<>(Ops.REGISTER, listener);\n        }\n\n        private static <T> Registration<T> unregister(Listener<T> listener) {\n            return new Registration<>(Ops.UNREGISTER, listener);\n        }\n    }\n\n    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n        private final RaftClient.Listener<T> listener;\n        // This field is used only by the Raft IO thread\n        private LeaderAndEpoch lastFiredLeaderChange = LeaderAndEpoch.UNKNOWN;\n\n        // These fields are visible to both the Raft IO thread and the listener\n        // and are protected through synchronization on this ListenerContext instance\n        private BatchReader<T> lastSent = null;\n        private long nextOffset = 0;\n\n        private ListenerContext(Listener<T> listener) {\n            this.listener = listener;\n        }\n\n        /**\n         * Get the last acked offset, which is one greater than the offset of the\n         * last record which was acked by the state machine.\n         */\n        private synchronized long nextOffset() {\n            return nextOffset;\n        }\n\n        /**\n         * Get the next expected offset, which might be larger than the last acked\n         * offset if there are inflight batches which have not been acked yet.\n         * Note that when fetching from disk, we may not know the last offset of\n         * inflight data until it has been processed by the state machine. In this case,\n         * we delay sending additional data until the state machine has read to the\n         * end and the last offset is determined.\n         */\n        private synchronized OptionalLong nextExpectedOffset() {\n            if (lastSent != null) {\n                OptionalLong lastSentOffset = lastSent.lastOffset();\n                if (lastSentOffset.isPresent()) {\n                    return OptionalLong.of(lastSentOffset.getAsLong() + 1);\n                } else {\n                    return OptionalLong.empty();\n                }\n            } else {\n                return OptionalLong.of(nextOffset);\n            }\n        }\n\n        /**\n         * This API is used when the Listener needs to be notified of a new snapshot. This happens\n         * when the context's next offset is less than the log start offset.\n         */\n        private void fireHandleSnapshot(SnapshotReader<T> reader) {\n            synchronized (this) {\n                nextOffset = reader.snapshotId().offset();\n                lastSent = null;\n            }\n\n            logger.debug(\"Notifying listener {} of snapshot {}\", listenerName(), reader.snapshotId());\n            listener.handleLoadSnapshot(reader);\n        }\n\n        /**\n         * This API is used for committed records that have been received through\n         * replication. In general, followers will write new data to disk before they\n         * know whether it has been committed. Rather than retaining the uncommitted\n         * data in memory, we let the state machine read the records from disk.\n         */\n        private void fireHandleCommit(long baseOffset, Records records) {\n            fireHandleCommit(\n                RecordsBatchReader.of(\n                    baseOffset,\n                    records,\n                    serde,\n                    BufferSupplier.create(),\n                    MAX_BATCH_SIZE_BYTES,\n                    this,\n                    true /* Validate batch CRC*/\n                )\n            );\n        }\n\n        /**\n         * This API is used for committed records originating from {@link #scheduleAppend(int, List)}\n         * or {@link #scheduleAtomicAppend(int, OptionalLong, List)} on this instance. In this case,\n         * we are able to save the original record objects, which saves the need to read them back\n         * from disk. This is a nice optimization for the leader which is typically doing more work\n         * than all of the * followers.\n         */\n        private void fireHandleCommit(\n            long baseOffset,\n            int epoch,\n            long appendTimestamp,\n            int sizeInBytes,\n            List<T> records\n        ) {\n            Batch<T> batch = Batch.data(baseOffset, epoch, appendTimestamp, sizeInBytes, records);\n            MemoryBatchReader<T> reader = MemoryBatchReader.of(Collections.singletonList(batch), this);\n            fireHandleCommit(reader);\n        }\n\n        private String listenerName() {\n            return KafkaRaftClient.listenerName(listener);\n        }\n\n        private void fireHandleCommit(BatchReader<T> reader) {\n            synchronized (this) {\n                this.lastSent = reader;\n            }\n            logger.debug(\n                \"Notifying listener {} of batch for baseOffset {} and lastOffset {}\",\n                listenerName(),\n                reader.baseOffset(),\n                reader.lastOffset()\n            );\n            listener.handleCommit(reader);\n        }\n\n        private void maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            if (shouldFireLeaderChange(leaderAndEpoch)) {\n                lastFiredLeaderChange = leaderAndEpoch;\n                logger.debug(\"Notifying listener {} of leader change {}\", listenerName(), leaderAndEpoch);\n                listener.handleLeaderChange(leaderAndEpoch);\n            }\n        }\n\n        private boolean shouldFireLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            if (leaderAndEpoch.equals(lastFiredLeaderChange)) {\n                return false;\n            } else if (leaderAndEpoch.epoch() > lastFiredLeaderChange.epoch()) {\n                return true;\n            } else {\n                return leaderAndEpoch.leaderId().isPresent() &&\n                    !lastFiredLeaderChange.leaderId().isPresent();\n            }\n        }\n\n        private void maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch, long epochStartOffset) {\n            // If this node is becoming the leader, then we can fire `handleLeaderChange` as soon\n            // as the listener has caught up to the start of the leader epoch. This guarantees\n            // that the state machine has seen the full committed state before it becomes\n            // leader and begins writing to the log.\n            //\n            // Note that the raft client doesn't need to compare nextOffset against the high-watermark\n            // to guarantee that the listener has caught up to the high-watermark. This is true because\n            // the only way nextOffset can be greater than epochStartOffset is for the leader to have\n            // established the new high-watermark (of at least epochStartOffset + 1) and for the listener\n            // to have consumed up to that new high-watermark.\n            if (shouldFireLeaderChange(leaderAndEpoch) && nextOffset() > epochStartOffset) {\n                lastFiredLeaderChange = leaderAndEpoch;\n                listener.handleLeaderChange(leaderAndEpoch);\n            }\n        }\n\n        public synchronized void onClose(BatchReader<T> reader) {\n            OptionalLong lastOffset = reader.lastOffset();\n\n            if (lastOffset.isPresent()) {\n                nextOffset = lastOffset.getAsLong() + 1;\n            }\n\n            if (lastSent == reader) {\n                lastSent = null;\n                wakeup();\n            }\n        }\n    }\n}",
                "methodCount": 136
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 13,
                "candidates": [
                    {
                        "lineStart": 1668,
                        "lineEnd": 1672,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method handleUnexpectedError to class NetworkChannel",
                        "description": "Move method handleUnexpectedError to org.apache.kafka.raft.NetworkChannel\nRationale: The handleUnexpectedError method processes errors related to RaftResponse.Inbound, which deals with network responses. NetworkChannel is a more suitable class as it encompasses network interface and communication, including the handling of errors that occur during these operations. This move ensures a more cohesive structure by localizing networking-related error handling in the same class, aligning responsibilities more logically.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 579,
                        "lineEnd": 587,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildVoteResponse to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildVoteResponse to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The buildVoteResponse method appears to be related to the process of building a vote response within a Raft context, which fits with the responsibilities of RaftMetadataLogCleanerManager. This class deals with Raft metadata and log cleaning, making it more aligned with quorum-related functionalities. Moving this method to RaftMetadataLogCleanerManager centralizes Raft-related logic and keeps the codebase organized and modular.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 540,
                        "lineEnd": 546,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method transitionToResigned to class RaftMetadataLogCleanerManager",
                        "description": "Move method transitionToResigned to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The transitionToResigned method involves operations related to quorum management and state transitions, which aligns more closely with high-level metadata and log management responsibilities. The RaftMetadataLogCleanerManager manages operations related to the Raft log, and transitioning to a resigned state likely involves metadata changes and clean-ups, making it a suitable class for this method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 569,
                        "lineEnd": 577,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method transitionToFollower to class Time",
                        "description": "Move method transitionToFollower to org.apache.kafka.common.utils.Time\nRationale: The 'transitionToFollower' method involves timestamp and timing-related parameters, such as 'currentTimeMs', which is closely related to the 'Time' class. The 'Time' class provides abstractions for working with time, making it the most relevant target class. The method's reliance on time and its operations align well with the functionalities offered by the 'Time' class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 548,
                        "lineEnd": 552,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method transitionToVoted to class NetworkChannel",
                        "description": "Move method transitionToVoted to org.apache.kafka.raft.NetworkChannel\nRationale: The method `transitionToVoted` involves transitioning a quorum state, potentially triggering leader change events, and resetting connections. These operations are deeply related to network communication and maintaining up-to-date connection states which makes `NetworkChannel` a suitable target class. This interface deals directly with network operations like sending requests and updating connection information, which aligns with the responsibilities encapsulated in `transitionToVoted`. The method relies on connectivity and managing network states, hence it is logical to place it in `NetworkChannel`.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1961,
                        "lineEnd": 1969,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method addQuorumLeader to class QuorumState",
                        "description": "Move method addQuorumLeader to org.apache.kafka.raft.QuorumState\nRationale: The 'addQuorumLeader' function directly interacts with the quorum to set the leader epoch and leader id. The QuorumState class manages the state of the quorum and contains methods related to leader management and election processes. Thus, this class is the most appropriate destination for this method as it logically groups the state management and leader-related functionalities in one place, ensuring that the quorum state management is centralized and cohesive.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1270,
                        "lineEnd": 1282,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method appendAsLeader to class ReplicatedLog",
                        "description": "Move method appendAsLeader to org.apache.kafka.raft.ReplicatedLog\nRationale: The `appendAsLeader` method strongly aligns with the responsibilities of the `ReplicatedLog` class. The method's purpose is to append records as a leader, which directly relates to the `appendAsLeader` function already defined within `ReplicatedLog`. Moreover, the `log.appendAsLeader(records, quorum.epoch())` call within the method indicates that it is performing operations on a replicated log. Moving this method to `ReplicatedLog` enhances cohesion, as the method manipulates log data and updates states pertinent to the log.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 926,
                        "lineEnd": 959,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildFetchResponse to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildFetchResponse to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The buildFetchResponse() method involves constructing a FetchResponseData, which seems highly relevant to log management and processing. Since RaftMetadataLogCleanerManager is already managing logs (i.e., cleaning logs), it would be more appropriate to add the method here, as there's high cohesion with the existing responsibilities of log management within RaftMetadataLogCleanerManager.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2556,
                        "lineEnd": 2558,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method voterNode to class KRaftControlRecordStateMachine",
                        "description": "Move method voterNode to org.apache.kafka.raft.internals.KRaftControlRecordStateMachine\nRationale: The method `voterNode` operates closely with the voter set, similar to other methods in KRaftControlRecordStateMachine like `voterSetAtOffset`, `lastVoterSet`, and `kraftVersionAtOffset`. This class tracks changes to the voter set which suggests that voterNode belongs to it, as the operations deal with managing and retrieving voter nodes, aligning with the responsibilities of KRaftControlRecordStateMachine.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 720,
                        "lineEnd": 728,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method strictExponentialElectionBackoffMs to class QuorumConfig",
                        "description": "Move method strictExponentialElectionBackoffMs to org.apache.kafka.raft.QuorumConfig\nRationale: The method strictExponentialElectionBackoffMs is closely tied to configuration parameters related to elections and backoff settings, which are defined within the QuorumConfig class. Specifically, it uses quorumConfig.electionBackoffMaxMs() indicating that the method is inherently linked to the quorum configuration logic. By moving the method to QuorumConfig, we enhance cohesion as the method and its related configuration settings are encapsulated within the same class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1868,
                        "lineEnd": 1878,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildEndQuorumEpochRequest to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildEndQuorumEpochRequest to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The `buildEndQuorumEpochRequest` method involves operations related to quorum and the state of a log, which are closely tied to the management and cleaning processes of metadata logs in a Raft consensus system. The `RaftMetadataLogCleanerManager` class, which manages and cleans logs based on timers, is a logical place for this method. This ensures that all log-related operations are consolidated and encapsulated within a single manager class, making the codebase more organized and cohesive.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 730,
                        "lineEnd": 737,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildBeginQuorumEpochResponse to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildBeginQuorumEpochResponse to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The buildBeginQuorumEpochResponse method is highly specific to the context of a Raft quorum and log metadata management, as it involves quorum epochs, topic partitions, and error handling which are closely related to Raft log cleaning and management. The RaftMetadataLogCleanerManager class, despite its name, deals with log cleaning within a Raft-based setup, making it an appropriate class to group related functionalities that handle similar concepts such as epochs and logs. This method doesn't fit into the MemoryPool or Time interfaces, which are more generic and unrelated to Raft-specific functionalities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 819,
                        "lineEnd": 826,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildEndQuorumEpochResponse to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildEndQuorumEpochResponse to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The buildEndQuorumEpochResponse method is specific to handling responses related to quorum epochs, which fits closely with the functionality of the RaftMetadataLogCleanerManager class. This class has access to necessary components such as logger and timer, and could easily be extended to manage quorum-related operations, making it a suitable place for the method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "binaryExponentialElectionBackoffMs",
                            "method_signature": "private int binaryExponentialElectionBackoffMs(int retries)",
                            "target_class": "QuorumConfig",
                            "rationale": "This method is highly coupled with the configuration and should be handled within the configuration context."
                        },
                        {
                            "method_name": "strictExponentialElectionBackoffMs",
                            "method_signature": "private int strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                            "target_class": "QuorumConfig",
                            "rationale": "Similar to binaryExponentialElectionBackoffMs, this method is dealing with configuration specific logic and should be within the QuorumConfig class."
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private FetchResponseData buildFetchResponse(Errors error, Records records, ValidOffsetAndEpoch validOffsetAndEpoch, Optional<LogOffsetMetadata> highWatermark)",
                            "target_class": "Log",
                            "rationale": "This method deals with building a FetchResponse based on the log state, making it a better fit for the Log class."
                        },
                        {
                            "method_name": "buildEmptyFetchResponse",
                            "method_signature": "private FetchResponseData buildEmptyFetchResponse(Errors error, Optional<LogOffsetMetadata> highWatermark)",
                            "target_class": "Log",
                            "rationale": "This is a specific case of building a fetch response that is inherently centered around the Log class state."
                        },
                        {
                            "method_name": "validateLeaderOnlyRequest",
                            "method_signature": "private Optional<Errors> validateLeaderOnlyRequest(int requestEpoch)",
                            "target_class": "QuorumState",
                            "rationale": "The validation of leader-specific requests should be part of the QuorumState logic."
                        },
                        {
                            "method_name": "validateVoterOnlyRequest",
                            "method_signature": "private Optional<Errors> validateVoterOnlyRequest(int remoteNodeId, int requestEpoch)",
                            "target_class": "QuorumState",
                            "rationale": "Similar rationale as validateLeaderOnlyRequest; this logic is specific to the voter state of the quorum."
                        },
                        {
                            "method_name": "hasValidClusterId",
                            "method_signature": "private boolean hasValidClusterId(String requestClusterId)",
                            "target_class": "Cluster",
                            "rationale": "Cluster ID validation is a cluster-specific concern and should be handled there."
                        },
                        {
                            "method_name": "maybeCompleteShutdown",
                            "method_signature": "private boolean maybeCompleteShutdown(long currentTimeMs)",
                            "target_class": "ShutdownManager",
                            "rationale": "Shutdown logic is complex enough to be encapsulated in its own class, improving separation of concerns."
                        },
                        {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private boolean handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "ErrorHandler",
                            "rationale": "Error handling can be centralized in a separate ErrorHandler class to improve readability and maintainability."
                        }
                    ],
                    "llm_response_time": 11310
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "handleVoteRequest",
                            "method_signature": "private VoteResponseData handleVoteRequest(RaftRequest.Inbound requestMetadata)",
                            "target_class": "QuorumState",
                            "rationale": "The handleVoteRequest method deals with handling voting requests which are inherently tied to managing the state's voting aspects. It should reside in QuorumState for better encapsulation and maintainability."
                        },
                        {
                            "method_name": "handleBeginQuorumEpochRequest",
                            "method_signature": "private BeginQuorumEpochResponseData handleBeginQuorumEpochRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "QuorumState",
                            "rationale": "The handleBeginQuorumEpochRequest method is responsible for processing quorum epoch initiation requests. This type of operation pertains to quorum state management and would be more appropriately placed within the QuorumState class."
                        },
                        {
                            "method_name": "handleEndQuorumEpochRequest",
                            "method_signature": "private EndQuorumEpochResponseData handleEndQuorumEpochRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "QuorumState",
                            "rationale": "The handleEndQuorumEpochRequest method addresses the end of quorum epochs which are fundamental to the management of quorum states. It should be included in the QuorumState class for better logical separation and code management."
                        },
                        {
                            "method_name": "handleFetchRequest",
                            "method_signature": "private CompletableFuture<FetchResponseData> handleFetchRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "QuorumState",
                            "rationale": "The handleFetchRequest method deals with the fetching of data which is crucial in the state management of the quorum. It should be part of the QuorumState class to maintain a clear separation of concerns and effective state management."
                        },
                        {
                            "method_name": "handleFetchSnapshotRequest",
                            "method_signature": "private FetchSnapshotResponseData handleFetchSnapshotRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "QuorumState",
                            "rationale": "The handleFetchSnapshotRequest method processes snapshot requests, closely related to the quorum's data state and integrity. Moving this method to the QuorumState class would centralize state-related operations and improve code cohesion."
                        }
                    ],
                    "llm_response_time": 7475
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private VoteResponseData buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "VoteRequestHandler",
                            "rationale": "The buildVoteResponse method is tightly coupled with the logic for handling Vote requests. Moving it to a dedicated handler class for Vote requests will improve separation of concerns and make the KafkaRaftClient class more cohesive."
                        },
                        {
                            "method_name": "handleVoteRequest",
                            "method_signature": "private VoteResponseData handleVoteRequest(RaftRequest.Inbound requestMetadata)",
                            "target_class": "VoteRequestHandler",
                            "rationale": "Handling vote requests can be encapsulated within a dedicated handler class to enhance modularity and focus the KafkaRaftClient class on coordination tasks."
                        },
                        {
                            "method_name": "handleVoteResponse",
                            "method_signature": "private boolean handleVoteResponse(RaftResponse.Inbound responseMetadata, long currentTimeMs)",
                            "target_class": "VoteResponseHandler",
                            "rationale": "Similar to request handlers, response handlers can be moved to a dedicated class to improve encapsulation and separation of concerns."
                        },
                        {
                            "method_name": "buildBeginQuorumEpochResponse",
                            "method_signature": "private BeginQuorumEpochResponseData buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "BeginQuorumEpochRequestHandler",
                            "rationale": "Creating responses for a specific request type should be handled by a corresponding request handler class. This will help maintain the single responsibility principle."
                        },
                        {
                            "method_name": "handleBeginQuorumEpochRequest",
                            "method_signature": "private BeginQuorumEpochResponseData handleBeginQuorumEpochRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "BeginQuorumEpochRequestHandler",
                            "rationale": "Handling BeginQuorumEpoch requests can be encapsulated within a dedicated handler class to improve code maintainability and separation of concerns."
                        },
                        {
                            "method_name": "handleBeginQuorumEpochResponse",
                            "method_signature": "private boolean handleBeginQuorumEpochResponse(RaftResponse.Inbound responseMetadata, long currentTimeMs)",
                            "target_class": "BeginQuorumEpochResponseHandler",
                            "rationale": "Moving response handling logic to a dedicated class will enhance readability and make the main class more manageable by reducing its size."
                        },
                        {
                            "method_name": "buildEndQuorumEpochResponse",
                            "method_signature": "private EndQuorumEpochResponseData buildEndQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "EndQuorumEpochRequestHandler",
                            "rationale": "Segregating response-building logic into a dedicated handler class makes the KafkaRaftClient class more cohesive by focusing it on high-level coordination."
                        },
                        {
                            "method_name": "handleEndQuorumEpochRequest",
                            "method_signature": "private EndQuorumEpochResponseData handleEndQuorumEpochRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "EndQuorumEpochRequestHandler",
                            "rationale": "Encapsulating specific request handling in a dedicated handler class improves code organization by adhering to the single responsibility principle."
                        },
                        {
                            "method_name": "handleEndQuorumEpochResponse",
                            "method_signature": "private boolean handleEndQuorumEpochResponse(RaftResponse.Inbound responseMetadata, long currentTimeMs)",
                            "target_class": "EndQuorumEpochResponseHandler",
                            "rationale": "Moving the logic for handling EndQuorumEpoch responses to a dedicated class will improve the modularity and testability of the code."
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private FetchResponseData buildFetchResponse(Errors error, Records records, ValidOffsetAndEpoch validOffsetAndEpoch, Optional<LogOffsetMetadata> highWatermark)",
                            "target_class": "FetchRequestHandler",
                            "rationale": "Response building for fetch operations can be encapsulated within a dedicated request handler class to improve the modularity and readability of the code."
                        },
                        {
                            "method_name": "handleFetchRequest",
                            "method_signature": "private CompletableFuture<FetchResponseData> handleFetchRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "FetchRequestHandler",
                            "rationale": "Handling Fetch requests can be encapsulated within a dedicated handler class to enhance the modularity and maintainability of the KafkaRaftClient class."
                        },
                        {
                            "method_name": "handleFetchResponse",
                            "method_signature": "private boolean handleFetchResponse(RaftResponse.Inbound responseMetadata, long currentTimeMs)",
                            "target_class": "FetchResponseHandler",
                            "rationale": "Handling Fetch responses in a dedicated class will separate concerns and reduce the size of KafkaRaftClient, making it easier to manage."
                        },
                        {
                            "method_name": "buildFetchSnapshotRequest",
                            "method_signature": "private FetchSnapshotRequestData buildFetchSnapshotRequest(OffsetAndEpoch snapshotId, long snapshotSize)",
                            "target_class": "FetchSnapshotRequestHandler",
                            "rationale": "Building snapshot fetch requests is better encapsulated in a dedicated handler class, aligning with the single responsibility principle."
                        },
                        {
                            "method_name": "handleFetchSnapshotRequest",
                            "method_signature": "private FetchSnapshotResponseData handleFetchSnapshotRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "FetchSnapshotRequestHandler",
                            "rationale": "Handling FetchSnapshot requests in a dedicated handler class helps in maintaining the modularity and clarity of the KafkaRaftClient class."
                        },
                        {
                            "method_name": "handleFetchSnapshotResponse",
                            "method_signature": "private boolean handleFetchSnapshotResponse(RaftResponse.Inbound responseMetadata, long currentTimeMs)",
                            "target_class": "FetchSnapshotResponseHandler",
                            "rationale": "Handling FetchSnapshot responses in a dedicated class will help keep the KafkaRaftClient class more focused on higher-level functionality."
                        },
                        {
                            "method_name": "handleDescribeQuorumRequest",
                            "method_signature": "private DescribeQuorumResponseData handleDescribeQuorumRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "DescribeQuorumRequestHandler",
                            "rationale": "Handling DescribeQuorum requests can be encapsulated within a dedicated handler class, adhering to best practices for modularity and separation of concerns."
                        }
                    ],
                    "llm_response_time": 19886
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "updateFollowerHighWatermark",
                            "method_signature": "private updateFollowerHighWatermark(\n        FollowerState state,\n        OptionalLong highWatermarkOpt\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateLeaderEndOffsetAndTimestamp",
                            "method_signature": "private updateLeaderEndOffsetAndTimestamp(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUpdateLeaderHighWatermark",
                            "method_signature": "private onUpdateLeaderHighWatermark(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateListenersProgress",
                            "method_signature": "private updateListenersProgress(long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "latestSnapshot",
                            "method_signature": "private latestSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireHandleCommit",
                            "method_signature": "private maybeFireHandleCommit(long baseOffset, int epoch, long appendTimestamp, int sizeInBytes, List<T> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderState<T> state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": "public initialize(\n        Map<Integer, InetSocketAddress> voterAddresses,\n        String listenerName,\n        QuorumStateStore quorumStateStore,\n        Metrics metrics\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endOffset",
                            "method_signature": "private endOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetConnections",
                            "method_signature": "private resetConnections()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeLeader",
                            "method_signature": "private onBecomeLeader(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flushLeaderLog",
                            "method_signature": "private flushLeaderLog(LeaderState<T> state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeTransitionToLeader",
                            "method_signature": "private maybeTransitionToLeader(CandidateState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeCandidate",
                            "method_signature": "private onBecomeCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToCandidate",
                            "method_signature": "private transitionToCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToUnattached",
                            "method_signature": "private transitionToUnattached(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToResigned",
                            "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToVoted",
                            "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeFollower",
                            "method_signature": "private onBecomeFollower(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToFollower",
                            "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleVoteRequest",
                            "method_signature": "private handleVoteRequest(\n        RaftRequest.Inbound requestMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleVoteResponse",
                            "method_signature": "private handleVoteResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "binaryExponentialElectionBackoffMs",
                            "method_signature": "private binaryExponentialElectionBackoffMs(int retries)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "strictExponentialElectionBackoffMs",
                            "method_signature": "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBeginQuorumEpochResponse",
                            "method_signature": "private buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleBeginQuorumEpochRequest",
                            "method_signature": "private handleBeginQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleBeginQuorumEpochResponse",
                            "method_signature": "private handleBeginQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochResponse",
                            "method_signature": "private buildEndQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleEndQuorumEpochRequest",
                            "method_signature": "private handleEndQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endEpochElectionBackoff",
                            "method_signature": "private endEpochElectionBackoff(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleEndQuorumEpochResponse",
                            "method_signature": "private handleEndQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchRequest",
                            "method_signature": "private handleFetchRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tryCompleteFetchRequest",
                            "method_signature": "private tryCompleteFetchRequest(\n        int replicaId,\n        FetchRequestData.FetchPartition request,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "listenerName",
                            "method_signature": "private static listenerName(Listener<?> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchResponse",
                            "method_signature": "private handleFetchResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsFollower",
                            "method_signature": "private appendAsFollower(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsLeader",
                            "method_signature": "private appendAsLeader(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleDescribeQuorumRequest",
                            "method_signature": "private handleDescribeQuorumRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchSnapshotRequest",
                            "method_signature": "private handleFetchSnapshotRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchSnapshotResponse",
                            "method_signature": "private handleFetchSnapshotResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasConsistentLeader",
                            "method_signature": "private hasConsistentLeader(int epoch, OptionalInt leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeHandleCommonResponse",
                            "method_signature": "private maybeHandleCommonResponse(\n        Errors error,\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeTransition",
                            "method_signature": "private maybeTransition(\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleResponse",
                            "method_signature": "private handleResponse(RaftResponse.Inbound response, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateVoterOnlyRequest",
                            "method_signature": "private validateVoterOnlyRequest(int remoteNodeId, int requestEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateLeaderOnlyRequest",
                            "method_signature": "private validateLeaderOnlyRequest(int requestEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleRequest",
                            "method_signature": "private handleRequest(RaftRequest.Inbound request, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleInboundMessage",
                            "method_signature": "private handleInboundMessage(RaftMessage message, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendRequest",
                            "method_signature": "private maybeSendRequest(\n        long currentTimeMs,\n        int destinationId,\n        Supplier<ApiMessage> requestSupplier\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochRequest",
                            "method_signature": "private buildEndQuorumEpochRequest(\n        ResignedState state\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBeginQuorumEpochRequest",
                            "method_signature": "private buildBeginQuorumEpochRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteRequest",
                            "method_signature": "private buildVoteRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchRequest",
                            "method_signature": "private buildFetchRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendAnyVoterFetch",
                            "method_signature": "private maybeSendAnyVoterFetch(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchSnapshotRequest",
                            "method_signature": "private buildFetchSnapshotRequest(OffsetAndEpoch snapshotId, long snapshotSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addQuorumLeader",
                            "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendBatch",
                            "method_signature": "private appendBatch(\n        LeaderState<T> state,\n        BatchAccumulator.CompletedBatch<T> batch,\n        long appendTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAppendBatches",
                            "method_signature": "private maybeAppendBatches(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollResigned",
                            "method_signature": "private pollResigned(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollLeader",
                            "method_signature": "private pollLeader(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendVoteRequests",
                            "method_signature": "private maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollCandidate",
                            "method_signature": "private pollCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollower",
                            "method_signature": "private pollFollower(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollowerAsVoter",
                            "method_signature": "private pollFollowerAsVoter(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollowerAsObserver",
                            "method_signature": "private pollFollowerAsObserver(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasAnyInflightRequest",
                            "method_signature": "private hasAnyInflightRequest(FollowerState state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendFetchOrFetchSnapshot",
                            "method_signature": "private maybeSendFetchOrFetchSnapshot(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollVoted",
                            "method_signature": "private pollVoted(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattached",
                            "method_signature": "private pollUnattached(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattachedAsVoter",
                            "method_signature": "private pollUnattachedAsVoter(UnattachedState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattachedAsObserver",
                            "method_signature": "private pollUnattachedAsObserver(UnattachedState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollCurrentState",
                            "method_signature": "private pollCurrentState(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollListeners",
                            "method_signature": "private pollListeners()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processRegistration",
                            "method_signature": "private processRegistration(Registration<T> registration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCompleteShutdown",
                            "method_signature": "private maybeCompleteShutdown(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeClean",
                            "method_signature": "public maybeClean(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "wakeup",
                            "method_signature": "private wakeup()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handle",
                            "method_signature": "public handle(RaftRequest.Inbound request)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "poll",
                            "method_signature": "public poll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "append",
                            "method_signature": "private append(int epoch, List<T> records, OptionalLong requiredBaseOffset, boolean isAtomic)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int id, String listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "update",
                            "method_signature": "public update(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasTimedOut",
                            "method_signature": "public hasTimedOut()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "remainingTimeMs",
                            "method_signature": "public remainingTimeMs()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "nextExpectedOffset",
                            "method_signature": "private synchronized nextExpectedOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleSnapshot",
                            "method_signature": "private fireHandleSnapshot(SnapshotReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shouldFireLeaderChange",
                            "method_signature": "private shouldFireLeaderChange(LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch, long epochStartOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onClose",
                            "method_signature": "public synchronized onClose(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "latestSnapshot",
                            "method_signature": "private latestSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToResigned",
                            "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToFollower",
                            "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToVoted",
                            "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addQuorumLeader",
                            "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsLeader",
                            "method_signature": "private appendAsLeader(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int id, String listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "strictExponentialElectionBackoffMs",
                            "method_signature": "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochRequest",
                            "method_signature": "private buildEndQuorumEpochRequest(\n        ResignedState state\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBeginQuorumEpochResponse",
                            "method_signature": "private buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochResponse",
                            "method_signature": "private buildEndQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "public failWithTimeout()": {
                        "first": {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.24569711685267134
                    },
                    "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)": {
                        "first": {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3148528256741625
                    },
                    "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)": {
                        "first": {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.31784767941305514
                    },
                    "private latestSnapshot()": {
                        "first": {
                            "method_name": "latestSnapshot",
                            "method_signature": "private latestSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.33251439200312055
                    },
                    "private transitionToResigned(List<Integer> preferredSuccessors)": {
                        "first": {
                            "method_name": "transitionToResigned",
                            "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3334349844567339
                    },
                    "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )": {
                        "first": {
                            "method_name": "transitionToFollower",
                            "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3347560484856552
                    },
                    "private transitionToVoted(ReplicaKey candidateKey, int epoch)": {
                        "first": {
                            "method_name": "transitionToVoted",
                            "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.34011180824655607
                    },
                    "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )": {
                        "first": {
                            "method_name": "addQuorumLeader",
                            "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3467371153947286
                    },
                    "private appendAsLeader(\n        Records records\n    )": {
                        "first": {
                            "method_name": "appendAsLeader",
                            "method_signature": "private appendAsLeader(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.34715179557885995
                    },
                    "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )": {
                        "first": {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3543391755961774
                    },
                    "public voterNode(int id, String listener)": {
                        "first": {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int id, String listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3562483791115151
                    },
                    "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)": {
                        "first": {
                            "method_name": "strictExponentialElectionBackoffMs",
                            "method_signature": "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3582431588920858
                    },
                    "private buildEndQuorumEpochRequest(\n        ResignedState state\n    )": {
                        "first": {
                            "method_name": "buildEndQuorumEpochRequest",
                            "method_signature": "private buildEndQuorumEpochRequest(\n        ResignedState state\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.36543137607483317
                    },
                    "private buildBeginQuorumEpochResponse(Errors partitionLevelError)": {
                        "first": {
                            "method_name": "buildBeginQuorumEpochResponse",
                            "method_signature": "private buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.36685984817252193
                    },
                    "private buildEndQuorumEpochResponse(Errors partitionLevelError)": {
                        "first": {
                            "method_name": "buildEndQuorumEpochResponse",
                            "method_signature": "private buildEndQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.36685984817252193
                    }
                },
                "voyage": {
                    "private resetConnections()": {
                        "first": {
                            "method_name": "resetConnections",
                            "method_signature": "private resetConnections()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.17540149800828203
                    },
                    "private static listenerName(Listener<?> listener)": {
                        "first": {
                            "method_name": "listenerName",
                            "method_signature": "private static listenerName(Listener<?> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.19033238744230452
                    },
                    "public update(long currentTimeMs)": {
                        "first": {
                            "method_name": "update",
                            "method_signature": "public update(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.20761149361012948
                    },
                    "public hasTimedOut()": {
                        "first": {
                            "method_name": "hasTimedOut",
                            "method_signature": "public hasTimedOut()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.22310116274419084
                    },
                    "private processRegistration(Registration<T> registration)": {
                        "first": {
                            "method_name": "processRegistration",
                            "method_signature": "private processRegistration(Registration<T> registration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.25672210762679315
                    },
                    "public maybeClean(long currentTimeMs)": {
                        "first": {
                            "method_name": "maybeClean",
                            "method_signature": "public maybeClean(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.26162611641950567
                    },
                    "public remainingTimeMs()": {
                        "first": {
                            "method_name": "remainingTimeMs",
                            "method_signature": "public remainingTimeMs()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.27279216315095045
                    },
                    "private wakeup()": {
                        "first": {
                            "method_name": "wakeup",
                            "method_signature": "private wakeup()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.28922855137625214
                    },
                    "public failWithTimeout()": {
                        "first": {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.29968437340175774
                    },
                    "private binaryExponentialElectionBackoffMs(int retries)": {
                        "first": {
                            "method_name": "binaryExponentialElectionBackoffMs",
                            "method_signature": "private binaryExponentialElectionBackoffMs(int retries)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3603877871947545
                    },
                    "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)": {
                        "first": {
                            "method_name": "strictExponentialElectionBackoffMs",
                            "method_signature": "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3607823774335509
                    },
                    "public synchronized onClose(BatchReader<T> reader)": {
                        "first": {
                            "method_name": "onClose",
                            "method_signature": "public synchronized onClose(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3756203448055128
                    },
                    "private maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    )": {
                        "first": {
                            "method_name": "maybeSendVoteRequests",
                            "method_signature": "private maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.38504126149491874
                    },
                    "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)": {
                        "first": {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4080798695317416
                    },
                    "private fireHandleCommit(BatchReader<T> reader)": {
                        "first": {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.40830015817287396
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private latestSnapshot()",
                        "private transitionToResigned(List<Integer> preferredSuccessors)",
                        "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                        "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                        "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                        "private buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                        "private buildEndQuorumEpochResponse(Errors partitionLevelError)",
                        "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                        "public voterNode(int id, String listener)",
                        "public failWithTimeout()"
                    ],
                    "llm_response_time": 7619
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                        "private transitionToResigned(List<Integer> preferredSuccessors)",
                        "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                        "private latestSnapshot()",
                        "public failWithTimeout()"
                    ],
                    "llm_response_time": 6368
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                        "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                        "public failWithTimeout()"
                    ],
                    "llm_response_time": 6560
                },
                "voyage": {
                    "priority_method_names": [
                        "private binaryExponentialElectionBackoffMs(int retries)",
                        "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                        "private maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    )",
                        "private fireHandleCommit(BatchReader<T> reader)",
                        "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                        "private wakeup()",
                        "private processRegistration(Registration<T> registration)",
                        "private resetConnections()",
                        "private static listenerName(Listener<?> listener)",
                        "public synchronized onClose(BatchReader<T> reader)",
                        "public update(long currentTimeMs)",
                        "public hasTimedOut()",
                        "public remainingTimeMs()",
                        "public failWithTimeout()",
                        "public maybeClean(long currentTimeMs)"
                    ],
                    "llm_response_time": 6834
                },
                "voyage-5": {
                    "priority_method_names": [],
                    "llm_response_time": 6640
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private resetConnections()",
                        "public update(long currentTimeMs)"
                    ],
                    "llm_response_time": 2746
                }
            },
            "targetClassMap": {
                "failWithTimeout": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3461,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "handleUnexpectedError": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10577027387372219
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.05594542388644595
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04352434006443845
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14159846508095775
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.042691922456284
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "NetworkChannel",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 3557,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "buildVoteResponse": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.08956419427010578
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03230010554076729
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.039564477010326005
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14987850520743612
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.03286425722981093
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2963188789948769
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 3376,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "latestSnapshot": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2976,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "transitionToResigned": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.13841739114470894
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.1615005277038364
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.08233688458905682
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14306584587982538
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.057512450152169124
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2798567190507171
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "NetworkChannel",
                        "MemoryPool"
                    ],
                    "llm_response_time": 3283,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "transitionToFollower": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.11399079270740736
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.11305036939268549
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.045980338147135626
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14306584587982538
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.03286425722981093
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.3292431988831966
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 2895,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "transitionToVoted": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.09972117053875637
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.11867816581938534
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04583712596673114
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14184408272449836
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.04025033049430489
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.3427530813818426
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "NetworkChannel",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 3680,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "addQuorumLeader": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10257232541543393
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03538299282895168
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04216934244541642
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14925788761321498
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.02700074253062953
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.3246010684802756
                        },
                        {
                            "class_name": "QuorumState",
                            "similarity_score": 0.275797464034139
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "QuorumState",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 3315,
                    "similarity_computation_time": 6,
                    "similarity_metric": "cosine"
                },
                "appendAsLeader": {
                    "target_classes": [
                        {
                            "class_name": "Records",
                            "similarity_score": 0.11837617651007314
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1077851268021598
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03800743117926434
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.1434409434297192
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.10020543130080872
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.03383735833851262
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ReplicatedLog",
                        "Records",
                        "Time"
                    ],
                    "llm_response_time": 3793,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "buildFetchResponse": {
                    "target_classes": [
                        {
                            "class_name": "Records",
                            "similarity_score": 0.026094068112009733
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10249173027591259
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.04065846966124125
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04361097561378978
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14750006307045013
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.024821164819372063
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2983983352366476
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool",
                        "Time"
                    ],
                    "llm_response_time": 2852,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "voterNode": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1197786705541612
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.08263686767584087
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.047875316664207634
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.24401392662080099
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.04204009910334443
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2948185205998629
                        },
                        {
                            "class_name": "KRaftControlRecordStateMachine",
                            "similarity_score": 0.2590633629193959
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "KRaftControlRecordStateMachine",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 3450,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "strictExponentialElectionBackoffMs": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.12533231025314606
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.0806259369670165
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.08763740600172468
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.17288849356223998
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.044435161714545535
                        },
                        {
                            "class_name": "QuorumConfig",
                            "similarity_score": 0.4315435875337899
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2397035379720598
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "QuorumConfig",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 3772,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "buildEndQuorumEpochRequest": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.09931522702893585
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03425943549137658
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.040830290252903835
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14451832825402
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.02614335653909062
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.3142936330963102
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool",
                        "Time"
                    ],
                    "llm_response_time": 3444,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "buildBeginQuorumEpochResponse": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1053899017621118
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03800743117926434
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04278063225096888
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.16032869008129397
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.029003450004439386
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.34867747990172687
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool",
                        "Time"
                    ],
                    "llm_response_time": 3727,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "buildEndQuorumEpochResponse": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1053899017621118
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03800743117926434
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04278063225096888
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.16032869008129397
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.029003450004439386
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.34867747990172687
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool",
                        "Time"
                    ],
                    "llm_response_time": 2603,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "459da4795a511f6933e940fcf105a824bd9e589c",
        "url": "https://github.com/apache/kafka/commit/459da4795a511f6933e940fcf105a824bd9e589c",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public onResponseResult(node Node, correlationId long, success boolean, timeMs long) : void extracted from private handleResponse(response RaftResponse.Inbound, currentTimeMs long) : void in class org.apache.kafka.raft.KafkaRaftClient & moved to class org.apache.kafka.raft.RequestManager",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1675,
                    "endLine": 1711,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private handleResponse(response RaftResponse.Inbound, currentTimeMs long) : void"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1709,
                    "endLine": 1709,
                    "startColumn": 13,
                    "endColumn": 79,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1706,
                    "endLine": 1710,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1706,
                    "endLine": 1708,
                    "startColumn": 34,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1708,
                    "endLine": 1710,
                    "startColumn": 16,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 234,
                    "endLine": 252,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public onResponseResult(node Node, correlationId long, success boolean, timeMs long) : void"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 249,
                    "endLine": 249,
                    "startColumn": 17,
                    "endColumn": 89,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 244,
                    "endLine": 250,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 244,
                    "endLine": 247,
                    "startColumn": 26,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 247,
                    "endLine": 250,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1736,
                    "endLine": 1772,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private handleResponse(response RaftResponse.Inbound, currentTimeMs long) : void"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1766,
                    "endLine": 1771,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "requestManager.onResponseResult(response.source(),response.correlationId(),handledSuccessfully,currentTimeMs)"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 246,
                    "endLine": 246,
                    "startColumn": 17,
                    "endColumn": 29,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 243,
                    "endLine": 251,
                    "startColumn": 54,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 243,
                    "endLine": 251,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 582,
        "extraction_results": {
            "success": true,
            "newCommitHash": "c1a446eb827fb6a57b6199f41beca07a2b229185",
            "newBranchName": "extract-onResponseResult-handleResponse-8a882a7"
        },
        "telemetry": {
            "id": "11e54767-956d-43a9-9c86-45b53f950fa4",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2692,
                "lineStart": 109,
                "lineEnd": 2800,
                "bodyLineStart": 109,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                "sourceCode": "/**\n * This class implements a Kafkaesque version of the Raft protocol. Leader election\n * is more or less pure Raft, but replication is driven by replica fetching and we use Kafka's\n * log reconciliation protocol to truncate the log to a common point following each leader\n * election.\n *\n * Like Zookeeper, this protocol distinguishes between voters and observers. Voters are\n * the only ones who are eligible to handle protocol requests and they are the only ones\n * who take part in elections. The protocol does not yet support dynamic quorum changes.\n *\n * These are the APIs in this protocol:\n *\n * 1) {@link VoteRequestData}: Sent by valid voters when their election timeout expires and they\n *    become a candidate. This request includes the last offset in the log which electors use\n *    to tell whether or not to grant the vote.\n *\n * 2) {@link BeginQuorumEpochRequestData}: Sent by the leader of an epoch only to valid voters to\n *    assert its leadership of the new epoch. This request will be retried indefinitely for\n *    each voter until it acknowledges the request or a new election occurs.\n *\n *    This is not needed in usual Raft because the leader can use an empty data push\n *    to achieve the same purpose. The Kafka Raft implementation, however, is driven by\n *    fetch requests from followers, so there must be a way to find the new leader after\n *    an election has completed.\n *\n * 3) {@link EndQuorumEpochRequestData}: Sent by the leader of an epoch to valid voters in order to\n *    gracefully resign from the current epoch. This causes remaining voters to immediately\n *    begin a new election.\n *\n * 4) {@link FetchRequestData}: This is the same as the usual Fetch API in Kafka, but we add snapshot\n *    check before responding, and we also piggyback some additional metadata on responses (i.e. current\n *    leader and epoch). Unlike partition replication, we also piggyback truncation detection on this API\n *    rather than through a separate truncation state.\n *\n * 5) {@link FetchSnapshotRequestData}: Sent by the follower to the epoch leader in order to fetch a snapshot.\n *    This happens when a FetchResponse includes a snapshot ID due to the follower's log end offset being less\n *    than the leader's log start offset. This API is similar to the Fetch API since the snapshot is stored\n *    as FileRecords, but we use {@link UnalignedRecords} in FetchSnapshotResponse because the records\n *    are not necessarily offset-aligned.\n */\nfinal public class KafkaRaftClient<T> implements RaftClient<T> {\n    private static final int RETRY_BACKOFF_BASE_MS = 100;\n    public static final int MAX_FETCH_WAIT_MS = 500;\n    public static final int MAX_BATCH_SIZE_BYTES = 8 * 1024 * 1024;\n    public static final int MAX_FETCH_SIZE_BYTES = MAX_BATCH_SIZE_BYTES;\n\n    private final OptionalInt nodeId;\n    private final Uuid nodeDirectoryId;\n    private final AtomicReference<GracefulShutdown> shutdown = new AtomicReference<>();\n    private final LogContext logContext;\n    private final Logger logger;\n    private final Time time;\n    private final int fetchMaxWaitMs;\n    private final String clusterId;\n    private final NetworkChannel channel;\n    private final ReplicatedLog log;\n    private final Random random;\n    private final FuturePurgatory<Long> appendPurgatory;\n    private final FuturePurgatory<Long> fetchPurgatory;\n    private final RecordSerde<T> serde;\n    private final MemoryPool memoryPool;\n    private final RaftMessageQueue messageQueue;\n    private final QuorumConfig quorumConfig;\n    private final RaftMetadataLogCleanerManager snapshotCleaner;\n\n    private final Map<Listener<T>, ListenerContext> listenerContexts = new IdentityHashMap<>();\n    private final ConcurrentLinkedQueue<Registration<T>> pendingRegistrations = new ConcurrentLinkedQueue<>();\n\n    // These components need to be initialized by the method initialize() because they depend on\n    // the voter set\n    /*\n     * The key invariant for the kraft control record state machine is that it has always read to\n     * the LEO. This is achieved by:\n     *\n     * 1. reading the entire partition (snapshot and log) at start up,\n     * 2. updating the state when a snapshot is replaced, because of FETCH_SNAPSHOT, on the\n     *    followers\n     * 3. updating the state when the leader (call to append()) or follower (FETCH) appends to the\n     *    log\n     * 4. truncate new entries when a follower truncates their log\n     * 5. truncate old entries when a snapshot gets generated\n     */\n    private volatile KRaftControlRecordStateMachine partitionState;\n    private volatile KafkaRaftMetrics kafkaRaftMetrics;\n    private volatile QuorumState quorum;\n    private volatile RequestManager requestManager;\n\n    /**\n     * Create a new instance.\n     *\n     * Note that if the node ID is empty, then the client will behave as a\n     * non-participating observer.\n     */\n    public KafkaRaftClient(\n        OptionalInt nodeId,\n        Uuid nodeDirectoryId,\n        RecordSerde<T> serde,\n        NetworkChannel channel,\n        ReplicatedLog log,\n        Time time,\n        ExpirationService expirationService,\n        LogContext logContext,\n        String clusterId,\n        QuorumConfig quorumConfig\n    ) {\n        this(\n            nodeId,\n            nodeDirectoryId,\n            serde,\n            channel,\n            new BlockingMessageQueue(),\n            log,\n            new BatchMemoryPool(5, MAX_BATCH_SIZE_BYTES),\n            time,\n            expirationService,\n            MAX_FETCH_WAIT_MS,\n            clusterId,\n            logContext,\n            new Random(),\n            quorumConfig\n        );\n    }\n\n    KafkaRaftClient(\n        OptionalInt nodeId,\n        Uuid nodeDirectoryId,\n        RecordSerde<T> serde,\n        NetworkChannel channel,\n        RaftMessageQueue messageQueue,\n        ReplicatedLog log,\n        MemoryPool memoryPool,\n        Time time,\n        ExpirationService expirationService,\n        int fetchMaxWaitMs,\n        String clusterId,\n        LogContext logContext,\n        Random random,\n        QuorumConfig quorumConfig\n    ) {\n        this.nodeId = nodeId;\n        this.nodeDirectoryId = nodeDirectoryId;\n        this.logContext = logContext;\n        this.serde = serde;\n        this.channel = channel;\n        this.messageQueue = messageQueue;\n        this.log = log;\n        this.memoryPool = memoryPool;\n        this.fetchPurgatory = new ThresholdPurgatory<>(expirationService);\n        this.appendPurgatory = new ThresholdPurgatory<>(expirationService);\n        this.time = time;\n        this.clusterId = clusterId;\n        this.fetchMaxWaitMs = fetchMaxWaitMs;\n        this.logger = logContext.logger(KafkaRaftClient.class);\n        this.random = random;\n        this.quorumConfig = quorumConfig;\n        this.snapshotCleaner = new RaftMetadataLogCleanerManager(logger, time, 60000, log::maybeClean);\n    }\n\n    private void updateFollowerHighWatermark(\n        FollowerState state,\n        OptionalLong highWatermarkOpt\n    ) {\n        highWatermarkOpt.ifPresent(highWatermark -> {\n            long newHighWatermark = Math.min(endOffset().offset(), highWatermark);\n            if (state.updateHighWatermark(OptionalLong.of(newHighWatermark))) {\n                logger.debug(\"Follower high watermark updated to {}\", newHighWatermark);\n                log.updateHighWatermark(new LogOffsetMetadata(newHighWatermark));\n                updateListenersProgress(newHighWatermark);\n            }\n        });\n    }\n\n    private void updateLeaderEndOffsetAndTimestamp(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n\n        if (state.updateLocalState(endOffsetMetadata)) {\n            onUpdateLeaderHighWatermark(state, currentTimeMs);\n        }\n\n        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n    }\n\n    private void onUpdateLeaderHighWatermark(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        state.highWatermark().ifPresent(highWatermark -> {\n            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n            log.updateHighWatermark(highWatermark);\n\n            // After updating the high watermark, we first clear the append\n            // purgatory so that we have an opportunity to route the pending\n            // records still held in memory directly to the listener\n            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n\n            // It is also possible that the high watermark is being updated\n            // for the first time following the leader election, so we need\n            // to give lagging listeners an opportunity to catch up as well\n            updateListenersProgress(highWatermark.offset);\n        });\n    }\n\n    private void updateListenersProgress(long highWatermark) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.nextExpectedOffset().ifPresent(nextExpectedOffset -> {\n                // Send snapshot to the listener, if the listener is at the beginning of the log and there is a snapshot,\n                // or the listener is trying to read an offset for which there isn't a segment in the log.\n                if (nextExpectedOffset < highWatermark &&\n                    ((nextExpectedOffset == 0 && latestSnapshot().isPresent()) ||\n                     nextExpectedOffset < log.startOffset())\n                ) {\n                    SnapshotReader<T> snapshot = latestSnapshot().orElseThrow(() -> new IllegalStateException(\n                        String.format(\n                            \"Snapshot expected since next offset of %s is %d, log start offset is %d and high-watermark is %d\",\n                            listenerContext.listenerName(),\n                            nextExpectedOffset,\n                            log.startOffset(),\n                            highWatermark\n                        )\n                    ));\n                    listenerContext.fireHandleSnapshot(snapshot);\n                }\n            });\n\n            // Re-read the expected offset in case the snapshot had to be reloaded\n            listenerContext.nextExpectedOffset().ifPresent(nextExpectedOffset -> {\n                if (nextExpectedOffset < highWatermark) {\n                    LogFetchInfo readInfo = log.read(nextExpectedOffset, Isolation.COMMITTED);\n                    listenerContext.fireHandleCommit(nextExpectedOffset, readInfo.records);\n                }\n            });\n        }\n    }\n\n    private Optional<SnapshotReader<T>> latestSnapshot() {\n        return log.latestSnapshot().map(reader ->\n            RecordsSnapshotReader.of(reader,\n                serde,\n                BufferSupplier.create(),\n                MAX_BATCH_SIZE_BYTES,\n                true /* Validate batch CRC*/\n            )\n        );\n    }\n\n    private void maybeFireHandleCommit(long baseOffset, int epoch, long appendTimestamp, int sizeInBytes, List<T> records) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.nextExpectedOffset().ifPresent(nextOffset -> {\n                if (nextOffset == baseOffset) {\n                    listenerContext.fireHandleCommit(baseOffset, epoch, appendTimestamp, sizeInBytes, records);\n                }\n            });\n        }\n    }\n\n    private void maybeFireLeaderChange(LeaderState<T> state) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.maybeFireLeaderChange(quorum.leaderAndEpoch(), state.epochStartOffset());\n        }\n    }\n\n    private void maybeFireLeaderChange() {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.maybeFireLeaderChange(quorum.leaderAndEpoch());\n        }\n    }\n\n    public void initialize(\n        Map<Integer, InetSocketAddress> voterAddresses,\n        String listenerName,\n        QuorumStateStore quorumStateStore,\n        Metrics metrics\n    ) {\n        partitionState = new KRaftControlRecordStateMachine(\n            Optional.of(VoterSet.fromInetSocketAddresses(listenerName, voterAddresses)),\n            log,\n            serde,\n            BufferSupplier.create(),\n            MAX_BATCH_SIZE_BYTES,\n            logContext\n        );\n        // Read the entire log\n        logger.info(\"Reading KRaft snapshot and log as part of the initialization\");\n        partitionState.updateState();\n\n        VoterSet lastVoterSet = partitionState.lastVoterSet();\n        requestManager = new RequestManager(\n            lastVoterSet.voterIds(),\n            quorumConfig.retryBackoffMs(),\n            quorumConfig.requestTimeoutMs(),\n            random\n        );\n\n        quorum = new QuorumState(\n            nodeId,\n            nodeDirectoryId,\n            partitionState::lastVoterSet,\n            partitionState::lastKraftVersion,\n            quorumConfig.electionTimeoutMs(),\n            quorumConfig.fetchTimeoutMs(),\n            quorumStateStore,\n            time,\n            logContext,\n            random\n        );\n\n        kafkaRaftMetrics = new KafkaRaftMetrics(metrics, \"raft\", quorum);\n        // All Raft voters are statically configured and known at startup\n        // so there are no unknown voter connections. Report this metric as 0.\n        kafkaRaftMetrics.updateNumUnknownVoterConnections(0);\n\n        for (Integer voterId : lastVoterSet.voterIds()) {\n            channel.updateEndpoint(voterId, lastVoterSet.voterAddress(voterId, listenerName).get());\n        }\n\n        quorum.initialize(new OffsetAndEpoch(log.endOffset().offset, log.lastFetchedEpoch()));\n\n        long currentTimeMs = time.milliseconds();\n        if (quorum.isLeader()) {\n            throw new IllegalStateException(\"Voter cannot initialize as a Leader\");\n        } else if (quorum.isCandidate()) {\n            onBecomeCandidate(currentTimeMs);\n        } else if (quorum.isFollower()) {\n            onBecomeFollower(currentTimeMs);\n        }\n\n        // When there is only a single voter, become candidate immediately\n        if (quorum.isOnlyVoter() && !quorum.isCandidate()) {\n            transitionToCandidate(currentTimeMs);\n        }\n    }\n\n    @Override\n    public void register(Listener<T> listener) {\n        pendingRegistrations.add(Registration.register(listener));\n        wakeup();\n    }\n\n    @Override\n    public void unregister(Listener<T> listener) {\n        pendingRegistrations.add(Registration.unregister(listener));\n        // No need to wake up the polling thread. It is a removal so the updates can be\n        // delayed until the polling thread wakes up for other reasons.\n    }\n\n    @Override\n    public LeaderAndEpoch leaderAndEpoch() {\n        if (isInitialized()) {\n            return quorum.leaderAndEpoch();\n        } else {\n            return LeaderAndEpoch.UNKNOWN;\n        }\n    }\n\n    @Override\n    public OptionalInt nodeId() {\n        return nodeId;\n    }\n\n    private OffsetAndEpoch endOffset() {\n        return new OffsetAndEpoch(log.endOffset().offset, log.lastFetchedEpoch());\n    }\n\n    private void resetConnections() {\n        requestManager.resetAll();\n    }\n\n    private void onBecomeLeader(long currentTimeMs) {\n        long endOffset = log.endOffset().offset;\n\n        BatchAccumulator<T> accumulator = new BatchAccumulator<>(\n            quorum.epoch(),\n            endOffset,\n            quorumConfig.appendLingerMs(),\n            MAX_BATCH_SIZE_BYTES,\n            memoryPool,\n            time,\n            Compression.NONE,\n            serde\n        );\n\n        LeaderState<T> state = quorum.transitionToLeader(endOffset, accumulator);\n\n        log.initializeLeaderEpoch(quorum.epoch());\n\n        // The high watermark can only be advanced once we have written a record\n        // from the new leader's epoch. Hence we write a control message immediately\n        // to ensure there is no delay committing pending data.\n        state.appendLeaderChangeMessage(currentTimeMs);\n\n        resetConnections();\n        kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n    }\n\n    private void flushLeaderLog(LeaderState<T> state, long currentTimeMs) {\n        // We update the end offset before flushing so that parked fetches can return sooner.\n        updateLeaderEndOffsetAndTimestamp(state, currentTimeMs);\n        log.flush(false);\n    }\n\n    private boolean maybeTransitionToLeader(CandidateState state, long currentTimeMs) {\n        if (state.isVoteGranted()) {\n            onBecomeLeader(currentTimeMs);\n            return true;\n        } else {\n            return false;\n        }\n    }\n\n    private void onBecomeCandidate(long currentTimeMs) {\n        CandidateState state = quorum.candidateStateOrThrow();\n        if (!maybeTransitionToLeader(state, currentTimeMs)) {\n            resetConnections();\n            kafkaRaftMetrics.updateElectionStartMs(currentTimeMs);\n        }\n    }\n\n    private void transitionToCandidate(long currentTimeMs) {\n        quorum.transitionToCandidate();\n        maybeFireLeaderChange();\n        onBecomeCandidate(currentTimeMs);\n    }\n\n    private void transitionToUnattached(int epoch) {\n        quorum.transitionToUnattached(epoch);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void transitionToResigned(List<Integer> preferredSuccessors) {\n        fetchPurgatory.completeAllExceptionally(\n            Errors.NOT_LEADER_OR_FOLLOWER.exception(\"Not handling request since this node is resigning\"));\n        quorum.transitionToResigned(preferredSuccessors);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void transitionToVoted(ReplicaKey candidateKey, int epoch) {\n        quorum.transitionToVoted(epoch, candidateKey);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void onBecomeFollower(long currentTimeMs) {\n        kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n\n        resetConnections();\n\n        // After becoming a follower, we need to complete all pending fetches so that\n        // they can be re-sent to the leader without waiting for their expirations\n        fetchPurgatory.completeAllExceptionally(new NotLeaderOrFollowerException(\n            \"Cannot process the fetch request because the node is no longer the leader.\"));\n\n        // Clearing the append purgatory should complete all futures exceptionally since this node is no longer the leader\n        appendPurgatory.completeAllExceptionally(new NotLeaderOrFollowerException(\n            \"Failed to receive sufficient acknowledgments for this append before leader change.\"));\n    }\n\n    private void transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    ) {\n        quorum.transitionToFollower(epoch, leaderId);\n        maybeFireLeaderChange();\n        onBecomeFollower(currentTimeMs);\n    }\n\n    private VoteResponseData buildVoteResponse(Errors partitionLevelError, boolean voteGranted) {\n        return VoteResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel(),\n            voteGranted);\n    }\n\n    /**\n     * Handle a Vote request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#INVALID_REQUEST} if the last epoch or offset are invalid\n     */\n    private VoteResponseData handleVoteRequest(\n        RaftRequest.Inbound requestMetadata\n    ) {\n        VoteRequestData request = (VoteRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new VoteResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat individual topic partition mismatches as invalid requests\n            return new VoteResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        VoteRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int candidateId = partitionRequest.candidateId();\n        int candidateEpoch = partitionRequest.candidateEpoch();\n\n        int lastEpoch = partitionRequest.lastOffsetEpoch();\n        long lastEpochEndOffset = partitionRequest.lastOffset();\n        if (lastEpochEndOffset < 0 || lastEpoch < 0 || lastEpoch >= candidateEpoch) {\n            return buildVoteResponse(Errors.INVALID_REQUEST, false);\n        }\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(candidateId, candidateEpoch);\n        if (errorOpt.isPresent()) {\n            return buildVoteResponse(errorOpt.get(), false);\n        }\n\n        if (candidateEpoch > quorum.epoch()) {\n            transitionToUnattached(candidateEpoch);\n        }\n\n        OffsetAndEpoch lastEpochEndOffsetAndEpoch = new OffsetAndEpoch(lastEpochEndOffset, lastEpoch);\n        ReplicaKey candidateKey = ReplicaKey.of(candidateId, Optional.empty());\n        boolean voteGranted = quorum.canGrantVote(\n            candidateKey,\n            lastEpochEndOffsetAndEpoch.compareTo(endOffset()) >= 0\n        );\n\n        if (voteGranted && quorum.isUnattached()) {\n            transitionToVoted(candidateKey, candidateEpoch);\n        }\n\n        logger.info(\"Vote request {} with epoch {} is {}\", request, candidateEpoch, voteGranted ? \"granted\" : \"rejected\");\n        return buildVoteResponse(Errors.NONE, voteGranted);\n    }\n\n    private boolean handleVoteResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        int remoteNodeId = responseMetadata.sourceId();\n        VoteResponseData response = (VoteResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        VoteResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors error = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (error == Errors.NONE) {\n            if (quorum.isLeader()) {\n                logger.debug(\"Ignoring vote response {} since we already became leader for epoch {}\",\n                    partitionResponse, quorum.epoch());\n            } else if (quorum.isCandidate()) {\n                CandidateState state = quorum.candidateStateOrThrow();\n                if (partitionResponse.voteGranted()) {\n                    state.recordGrantedVote(remoteNodeId);\n                    maybeTransitionToLeader(state, currentTimeMs);\n                } else {\n                    state.recordRejectedVote(remoteNodeId);\n\n                    // If our vote is rejected, we go immediately to the random backoff. This\n                    // ensures that we are not stuck waiting for the election timeout when the\n                    // vote has become gridlocked.\n                    if (state.isVoteRejected() && !state.isBackingOff()) {\n                        logger.info(\"Insufficient remaining votes to become leader (rejected by {}). \" +\n                            \"We will backoff before retrying election again\", state.rejectingVoters());\n\n                        state.startBackingOff(\n                            currentTimeMs,\n                            binaryExponentialElectionBackoffMs(state.retries())\n                        );\n                    }\n                }\n            } else {\n                logger.debug(\"Ignoring vote response {} since we are no longer a candidate in epoch {}\",\n                    partitionResponse, quorum.epoch());\n            }\n            return true;\n        } else {\n            return handleUnexpectedError(error, responseMetadata);\n        }\n    }\n\n    private int binaryExponentialElectionBackoffMs(int retries) {\n        if (retries <= 0) {\n            throw new IllegalArgumentException(\"Retries \" + retries + \" should be larger than zero\");\n        }\n        // upper limit exponential co-efficients at 20 to avoid overflow\n        return Math.min(RETRY_BACKOFF_BASE_MS * random.nextInt(2 << Math.min(20, retries - 1)),\n                quorumConfig.electionBackoffMaxMs());\n    }\n\n    private int strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors) {\n        if (positionInSuccessors <= 0 || positionInSuccessors >= totalNumSuccessors) {\n            throw new IllegalArgumentException(\"Position \" + positionInSuccessors + \" should be larger than zero\" +\n                    \" and smaller than total number of successors \" + totalNumSuccessors);\n        }\n\n        int retryBackOffBaseMs = quorumConfig.electionBackoffMaxMs() >> (totalNumSuccessors - 1);\n        return Math.min(quorumConfig.electionBackoffMaxMs(), retryBackOffBaseMs << (positionInSuccessors - 1));\n    }\n\n    private BeginQuorumEpochResponseData buildBeginQuorumEpochResponse(Errors partitionLevelError) {\n        return BeginQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel());\n    }\n\n    /**\n     * Handle a BeginEpoch request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     */\n    private BeginQuorumEpochResponseData handleBeginQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        BeginQuorumEpochRequestData request = (BeginQuorumEpochRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new BeginQuorumEpochResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return new BeginQuorumEpochResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        BeginQuorumEpochRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int requestLeaderId = partitionRequest.leaderId();\n        int requestEpoch = partitionRequest.leaderEpoch();\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(requestLeaderId, requestEpoch);\n        if (errorOpt.isPresent()) {\n            return buildBeginQuorumEpochResponse(errorOpt.get());\n        }\n\n        maybeTransition(OptionalInt.of(requestLeaderId), requestEpoch, currentTimeMs);\n        return buildBeginQuorumEpochResponse(Errors.NONE);\n    }\n\n    private boolean handleBeginQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        int remoteNodeId = responseMetadata.sourceId();\n        BeginQuorumEpochResponseData response = (BeginQuorumEpochResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        BeginQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors partitionError = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            partitionError, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (partitionError == Errors.NONE) {\n            if (quorum.isLeader()) {\n                LeaderState<T> state = quorum.leaderStateOrThrow();\n                state.addAcknowledgementFrom(remoteNodeId);\n            } else {\n                logger.debug(\"Ignoring BeginQuorumEpoch response {} since \" +\n                    \"this node is not the leader anymore\", response);\n            }\n            return true;\n        } else {\n            return handleUnexpectedError(partitionError, responseMetadata);\n        }\n    }\n\n    private EndQuorumEpochResponseData buildEndQuorumEpochResponse(Errors partitionLevelError) {\n        return EndQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel());\n    }\n\n    /**\n     * Handle an EndEpoch request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     */\n    private EndQuorumEpochResponseData handleEndQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        EndQuorumEpochRequestData request = (EndQuorumEpochRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new EndQuorumEpochResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return new EndQuorumEpochResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        EndQuorumEpochRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int requestEpoch = partitionRequest.leaderEpoch();\n        int requestLeaderId = partitionRequest.leaderId();\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(requestLeaderId, requestEpoch);\n        if (errorOpt.isPresent()) {\n            return buildEndQuorumEpochResponse(errorOpt.get());\n        }\n        maybeTransition(OptionalInt.of(requestLeaderId), requestEpoch, currentTimeMs);\n\n        if (quorum.isFollower()) {\n            FollowerState state = quorum.followerStateOrThrow();\n            if (state.leaderId() == requestLeaderId) {\n                List<Integer> preferredSuccessors = partitionRequest.preferredSuccessors();\n                long electionBackoffMs = endEpochElectionBackoff(preferredSuccessors);\n                logger.debug(\"Overriding follower fetch timeout to {} after receiving \" +\n                    \"EndQuorumEpoch request from leader {} in epoch {}\", electionBackoffMs,\n                    requestLeaderId, requestEpoch);\n                state.overrideFetchTimeout(currentTimeMs, electionBackoffMs);\n            }\n        }\n        return buildEndQuorumEpochResponse(Errors.NONE);\n    }\n\n    private long endEpochElectionBackoff(List<Integer> preferredSuccessors) {\n        // Based on the priority inside the preferred successors, choose the corresponding delayed\n        // election backoff time based on strict exponential mechanism so that the most up-to-date\n        // voter has a higher chance to be elected. If the node's priority is highest, become\n        // candidate immediately instead of waiting for next poll.\n        int position = preferredSuccessors.indexOf(quorum.localIdOrThrow());\n        if (position <= 0) {\n            return 0;\n        } else {\n            return strictExponentialElectionBackoffMs(position, preferredSuccessors.size());\n        }\n    }\n\n    private boolean handleEndQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        EndQuorumEpochResponseData response = (EndQuorumEpochResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        EndQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors partitionError = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            partitionError, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (partitionError == Errors.NONE) {\n            ResignedState resignedState = quorum.resignedStateOrThrow();\n            resignedState.acknowledgeResignation(responseMetadata.sourceId());\n            return true;\n        } else {\n            return handleUnexpectedError(partitionError, responseMetadata);\n        }\n    }\n\n    private FetchResponseData buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    ) {\n        return RaftUtil.singletonFetchResponse(log.topicPartition(), log.topicId(), Errors.NONE, partitionData -> {\n            partitionData\n                .setRecords(records)\n                .setErrorCode(error.code())\n                .setLogStartOffset(log.startOffset())\n                .setHighWatermark(\n                    highWatermark.map(offsetMetadata -> offsetMetadata.offset).orElse(-1L)\n                );\n\n            partitionData.currentLeader()\n                .setLeaderEpoch(quorum.epoch())\n                .setLeaderId(quorum.leaderIdOrSentinel());\n\n            switch (validOffsetAndEpoch.kind()) {\n                case DIVERGING:\n                    partitionData.divergingEpoch()\n                        .setEpoch(validOffsetAndEpoch.offsetAndEpoch().epoch())\n                        .setEndOffset(validOffsetAndEpoch.offsetAndEpoch().offset());\n                    break;\n                case SNAPSHOT:\n                    partitionData.snapshotId()\n                        .setEpoch(validOffsetAndEpoch.offsetAndEpoch().epoch())\n                        .setEndOffset(validOffsetAndEpoch.offsetAndEpoch().offset());\n                    break;\n                default:\n            }\n        });\n    }\n\n    private FetchResponseData buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    ) {\n        return buildFetchResponse(\n            error,\n            MemoryRecords.EMPTY,\n            ValidOffsetAndEpoch.valid(),\n            highWatermark\n        );\n    }\n\n    private boolean hasValidClusterId(String requestClusterId) {\n        // We don't enforce the cluster id if it is not provided.\n        if (requestClusterId == null) {\n            return true;\n        }\n        return clusterId.equals(requestClusterId);\n    }\n\n    /**\n     * Handle a Fetch request. The fetch offset and last fetched epoch are always\n     * validated against the current log. In the case that they do not match, the response will\n     * indicate the diverging offset/epoch. A follower is expected to truncate its log in this\n     * case and resend the fetch.\n     *\n     * This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *     but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INVALID_REQUEST} if the request epoch is larger than the leader's current epoch\n     *     or if either the fetch offset or the last fetched epoch is invalid\n     */\n    private CompletableFuture<FetchResponseData> handleFetchRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        FetchRequestData request = (FetchRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return completedFuture(new FetchResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code()));\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition(), log.topicId())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return completedFuture(new FetchResponseData().setErrorCode(Errors.INVALID_REQUEST.code()));\n        }\n        // If the ID is valid, we can set the topic name.\n        request.topics().get(0).setTopic(log.topicPartition().topic());\n\n        FetchRequestData.FetchPartition fetchPartition = request.topics().get(0).partitions().get(0);\n        if (request.maxWaitMs() < 0\n            || fetchPartition.fetchOffset() < 0\n            || fetchPartition.lastFetchedEpoch() < 0\n            || fetchPartition.lastFetchedEpoch() > fetchPartition.currentLeaderEpoch()) {\n            return completedFuture(\n                buildEmptyFetchResponse(Errors.INVALID_REQUEST, Optional.empty())\n            );\n        }\n\n        int replicaId = FetchRequest.replicaId(request);\n        FetchResponseData response = tryCompleteFetchRequest(replicaId, fetchPartition, currentTimeMs);\n        FetchResponseData.PartitionData partitionResponse =\n            response.responses().get(0).partitions().get(0);\n\n        if (partitionResponse.errorCode() != Errors.NONE.code()\n            || FetchResponse.recordsSize(partitionResponse) > 0\n            || request.maxWaitMs() == 0\n            || isPartitionDiverged(partitionResponse)\n            || isPartitionSnapshotted(partitionResponse)) {\n            // Reply immediately if any of the following is true\n            // 1. The response contains an error\n            // 2. There are records in the response\n            // 3. The fetching replica doesn't want to wait for the partition to contain new data\n            // 4. The fetching replica needs to truncate because the log diverged\n            // 5. The fetching replica needs to fetch a snapshot\n            return completedFuture(response);\n        }\n\n        CompletableFuture<Long> future = fetchPurgatory.await(\n            fetchPartition.fetchOffset(),\n            request.maxWaitMs());\n\n        return future.handle((completionTimeMs, exception) -> {\n            if (exception != null) {\n                Throwable cause = exception instanceof ExecutionException ?\n                    exception.getCause() : exception;\n\n                Errors error = Errors.forException(cause);\n                if (error == Errors.REQUEST_TIMED_OUT) {\n                    // Note that for this case the calling thread is the expiration service thread and not the\n                    // polling thread.\n                    //\n                    // If the fetch request timed out in purgatory, it means no new data is available,\n                    // just return the original fetch response.\n                    return response;\n                } else {\n                    // If there was any error other than REQUEST_TIMED_OUT, return it.\n                    logger.info(\"Failed to handle fetch from {} at {} due to {}\",\n                        replicaId, fetchPartition.fetchOffset(), error);\n                    return buildEmptyFetchResponse(error, Optional.empty());\n                }\n            }\n\n            // FIXME: `completionTimeMs`, which can be null\n            logger.trace(\"Completing delayed fetch from {} starting at offset {} at {}\",\n                replicaId, fetchPartition.fetchOffset(), completionTimeMs);\n\n            // It is safe to call tryCompleteFetchRequest because only the polling thread completes this\n            // future successfully. This is true because only the polling thread appends record batches to\n            // the log from maybeAppendBatches.\n            return tryCompleteFetchRequest(replicaId, fetchPartition, time.milliseconds());\n        });\n    }\n\n    private FetchResponseData tryCompleteFetchRequest(\n        int replicaId,\n        FetchRequestData.FetchPartition request,\n        long currentTimeMs\n    ) {\n        try {\n            Optional<Errors> errorOpt = validateLeaderOnlyRequest(request.currentLeaderEpoch());\n            if (errorOpt.isPresent()) {\n                return buildEmptyFetchResponse(errorOpt.get(), Optional.empty());\n            }\n\n            long fetchOffset = request.fetchOffset();\n            int lastFetchedEpoch = request.lastFetchedEpoch();\n            LeaderState<T> state = quorum.leaderStateOrThrow();\n\n            Optional<OffsetAndEpoch> latestSnapshotId = log.latestSnapshotId();\n            final ValidOffsetAndEpoch validOffsetAndEpoch;\n            if (fetchOffset == 0 && latestSnapshotId.isPresent()) {\n                // If the follower has an empty log and a snapshot exist, it is always more efficient\n                // to reply with a snapshot id (FETCH_SNAPSHOT) instead of fetching from the log segments.\n                validOffsetAndEpoch = ValidOffsetAndEpoch.snapshot(latestSnapshotId.get());\n            } else {\n                validOffsetAndEpoch = log.validateOffsetAndEpoch(fetchOffset, lastFetchedEpoch);\n            }\n\n            final Records records;\n            if (validOffsetAndEpoch.kind() == ValidOffsetAndEpoch.Kind.VALID) {\n                LogFetchInfo info = log.read(fetchOffset, Isolation.UNCOMMITTED);\n\n                if (state.updateReplicaState(replicaId, currentTimeMs, info.startOffsetMetadata)) {\n                    onUpdateLeaderHighWatermark(state, currentTimeMs);\n                }\n\n                records = info.records;\n            } else {\n                records = MemoryRecords.EMPTY;\n            }\n\n            return buildFetchResponse(Errors.NONE, records, validOffsetAndEpoch, state.highWatermark());\n        } catch (Exception e) {\n            logger.error(\"Caught unexpected error in fetch completion of request {}\", request, e);\n            return buildEmptyFetchResponse(Errors.UNKNOWN_SERVER_ERROR, Optional.empty());\n        }\n    }\n\n    private static boolean isPartitionDiverged(FetchResponseData.PartitionData partitionResponseData) {\n        FetchResponseData.EpochEndOffset divergingEpoch = partitionResponseData.divergingEpoch();\n\n        return divergingEpoch.epoch() != -1 || divergingEpoch.endOffset() != -1;\n    }\n\n    private static boolean isPartitionSnapshotted(FetchResponseData.PartitionData partitionResponseData) {\n        FetchResponseData.SnapshotId snapshotId = partitionResponseData.snapshotId();\n\n        return snapshotId.epoch() != -1 || snapshotId.endOffset() != -1;\n    }\n\n    private static OptionalInt optionalLeaderId(int leaderIdOrNil) {\n        if (leaderIdOrNil < 0)\n            return OptionalInt.empty();\n        return OptionalInt.of(leaderIdOrNil);\n    }\n\n    private static String listenerName(Listener<?> listener) {\n        return String.format(\"%s@%d\", listener.getClass().getTypeName(), System.identityHashCode(listener));\n    }\n\n    private boolean handleFetchResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        FetchResponseData response = (FetchResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!RaftUtil.hasValidTopicPartition(response, log.topicPartition(), log.topicId())) {\n            return false;\n        }\n        // If the ID is valid, we can set the topic name.\n        response.responses().get(0).setTopic(log.topicPartition().topic());\n\n        FetchResponseData.PartitionData partitionResponse =\n            response.responses().get(0).partitions().get(0);\n\n        FetchResponseData.LeaderIdAndEpoch currentLeaderIdAndEpoch = partitionResponse.currentLeader();\n        OptionalInt responseLeaderId = optionalLeaderId(currentLeaderIdAndEpoch.leaderId());\n        int responseEpoch = currentLeaderIdAndEpoch.leaderEpoch();\n        Errors error = Errors.forCode(partitionResponse.errorCode());\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        }\n\n        FollowerState state = quorum.followerStateOrThrow();\n        if (error == Errors.NONE) {\n            FetchResponseData.EpochEndOffset divergingEpoch = partitionResponse.divergingEpoch();\n            if (divergingEpoch.epoch() >= 0) {\n                // The leader is asking us to truncate before continuing\n                final OffsetAndEpoch divergingOffsetAndEpoch = new OffsetAndEpoch(\n                    divergingEpoch.endOffset(), divergingEpoch.epoch());\n\n                state.highWatermark().ifPresent(highWatermark -> {\n                    if (divergingOffsetAndEpoch.offset() < highWatermark.offset) {\n                        throw new KafkaException(\"The leader requested truncation to offset \" +\n                            divergingOffsetAndEpoch.offset() + \", which is below the current high watermark\" +\n                            \" \" + highWatermark);\n                    }\n                });\n\n                long truncationOffset = log.truncateToEndOffset(divergingOffsetAndEpoch);\n                logger.info(\n                    \"Truncated to offset {} from Fetch response from leader {}\",\n                    truncationOffset,\n                    quorum.leaderIdOrSentinel()\n                );\n\n                // Update the internal listener to the new end offset\n                partitionState.truncateNewEntries(truncationOffset);\n            } else if (partitionResponse.snapshotId().epoch() >= 0 ||\n                       partitionResponse.snapshotId().endOffset() >= 0) {\n                // The leader is asking us to fetch a snapshot\n\n                if (partitionResponse.snapshotId().epoch() < 0) {\n                    logger.error(\n                        \"The leader sent a snapshot id with a valid end offset {} but with an invalid epoch {}\",\n                        partitionResponse.snapshotId().endOffset(),\n                        partitionResponse.snapshotId().epoch()\n                    );\n                    return false;\n                } else if (partitionResponse.snapshotId().endOffset() < 0) {\n                    logger.error(\n                        \"The leader sent a snapshot id with a valid epoch {} but with an invalid end offset {}\",\n                        partitionResponse.snapshotId().epoch(),\n                        partitionResponse.snapshotId().endOffset()\n                    );\n                    return false;\n                } else {\n                    final OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n                        partitionResponse.snapshotId().endOffset(),\n                        partitionResponse.snapshotId().epoch()\n                    );\n\n                    // Do not validate the snapshot id against the local replicated log since this\n                    // snapshot is expected to reference offsets and epochs greater than the log\n                    // end offset and high-watermark.\n                    state.setFetchingSnapshot(log.createNewSnapshotUnchecked(snapshotId));\n                    logger.info(\n                        \"Fetching snapshot {} from Fetch response from leader {}\",\n                        snapshotId,\n                        quorum.leaderIdOrSentinel()\n                    );\n                }\n            } else {\n                Records records = FetchResponse.recordsOrFail(partitionResponse);\n                if (records.sizeInBytes() > 0) {\n                    appendAsFollower(records);\n                }\n\n                OptionalLong highWatermark = partitionResponse.highWatermark() < 0 ?\n                    OptionalLong.empty() : OptionalLong.of(partitionResponse.highWatermark());\n                updateFollowerHighWatermark(state, highWatermark);\n            }\n\n            state.resetFetchTimeout(currentTimeMs);\n            return true;\n        } else {\n            return handleUnexpectedError(error, responseMetadata);\n        }\n    }\n\n    private void appendAsFollower(\n        Records records\n    ) {\n        LogAppendInfo info = log.appendAsFollower(records);\n        if (quorum.isVoter()) {\n            // the leader only requires that voters have flushed their log before sending\n            // a Fetch request\n            log.flush(false);\n        }\n\n        partitionState.updateState();\n\n        OffsetAndEpoch endOffset = endOffset();\n        kafkaRaftMetrics.updateFetchedRecords(info.lastOffset - info.firstOffset + 1);\n        kafkaRaftMetrics.updateLogEnd(endOffset);\n        logger.trace(\"Follower end offset updated to {} after append\", endOffset);\n    }\n\n    private LogAppendInfo appendAsLeader(\n        Records records\n    ) {\n        LogAppendInfo info = log.appendAsLeader(records, quorum.epoch());\n\n        partitionState.updateState();\n\n        OffsetAndEpoch endOffset = endOffset();\n        kafkaRaftMetrics.updateAppendRecords(info.lastOffset - info.firstOffset + 1);\n        kafkaRaftMetrics.updateLogEnd(endOffset);\n        logger.trace(\"Leader appended records at base offset {}, new end offset is {}\", info.firstOffset, endOffset);\n        return info;\n    }\n\n    private DescribeQuorumResponseData handleDescribeQuorumRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        DescribeQuorumRequestData describeQuorumRequestData = (DescribeQuorumRequestData) requestMetadata.data;\n        if (!hasValidTopicPartition(describeQuorumRequestData, log.topicPartition())) {\n            return DescribeQuorumRequest.getPartitionLevelErrorResponse(\n                describeQuorumRequestData, Errors.UNKNOWN_TOPIC_OR_PARTITION);\n        }\n\n        if (!quorum.isLeader()) {\n            return DescribeQuorumResponse.singletonErrorResponse(\n                log.topicPartition(),\n                Errors.NOT_LEADER_OR_FOLLOWER\n            );\n        }\n\n        LeaderState<T> leaderState = quorum.leaderStateOrThrow();\n        return DescribeQuorumResponse.singletonResponse(\n            log.topicPartition(),\n            leaderState.describeQuorum(currentTimeMs)\n        );\n    }\n\n    /**\n     * Handle a FetchSnapshot request, similar to the Fetch request but we use {@link UnalignedRecords}\n     * in response because the records are not necessarily offset-aligned.\n     *\n     * This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *     but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INVALID_REQUEST} if the request epoch is larger than the leader's current epoch\n     *     or if either the fetch offset or the last fetched epoch is invalid\n     * - {@link Errors#SNAPSHOT_NOT_FOUND} if the request snapshot id does not exists\n     * - {@link Errors#POSITION_OUT_OF_RANGE} if the request snapshot offset out of range\n     */\n    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(data.clusterId())) {\n            return new FetchSnapshotResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n            return FetchSnapshotResponse.withTopLevelError(Errors.INVALID_REQUEST);\n        }\n\n        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n            .forTopicPartition(data, log.topicPartition());\n        if (!partitionSnapshotOpt.isPresent()) {\n            // The Raft client assumes that there is only one topic partition.\n            TopicPartition unknownTopicPartition = new TopicPartition(\n                data.topics().get(0).name(),\n                data.topics().get(0).partitions().get(0).partition()\n            );\n\n            return FetchSnapshotResponse.singleton(\n                unknownTopicPartition,\n                responsePartitionSnapshot -> responsePartitionSnapshot\n                    .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code())\n            );\n        }\n\n        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n                partitionSnapshot.currentLeaderEpoch()\n        );\n        if (leaderValidation.isPresent()) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(leaderValidation.get().code())\n            );\n        }\n\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n            partitionSnapshot.snapshotId().endOffset(),\n            partitionSnapshot.snapshotId().epoch()\n        );\n        Optional<RawSnapshotReader> snapshotOpt = log.readSnapshot(snapshotId);\n        if (!snapshotOpt.isPresent()) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code())\n            );\n        }\n\n        RawSnapshotReader snapshot = snapshotOpt.get();\n        long snapshotSize = snapshot.sizeInBytes();\n        if (partitionSnapshot.position() < 0 || partitionSnapshot.position() >= snapshotSize) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(Errors.POSITION_OUT_OF_RANGE.code())\n            );\n        }\n\n        if (partitionSnapshot.position() > Integer.MAX_VALUE) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Trying to fetch a snapshot with size (%d) and a position (%d) larger than %d\",\n                    snapshotSize,\n                    partitionSnapshot.position(),\n                    Integer.MAX_VALUE\n                )\n            );\n        }\n\n        int maxSnapshotSize;\n        try {\n            maxSnapshotSize = Math.toIntExact(snapshotSize);\n        } catch (ArithmeticException e) {\n            maxSnapshotSize = Integer.MAX_VALUE;\n        }\n\n        UnalignedRecords records = snapshot.slice(partitionSnapshot.position(), Math.min(data.maxBytes(), maxSnapshotSize));\n\n        LeaderState<T> state = quorum.leaderStateOrThrow();\n        state.updateCheckQuorumForFollowingVoter(data.replicaId(), currentTimeMs);\n\n        return FetchSnapshotResponse.singleton(\n            log.topicPartition(),\n            responsePartitionSnapshot -> {\n                addQuorumLeader(responsePartitionSnapshot)\n                    .snapshotId()\n                    .setEndOffset(snapshotId.offset())\n                    .setEpoch(snapshotId.epoch());\n\n                return responsePartitionSnapshot\n                    .setSize(snapshotSize)\n                    .setPosition(partitionSnapshot.position())\n                    .setUnalignedRecords(records);\n            }\n        );\n    }\n\n    private boolean handleFetchSnapshotResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        FetchSnapshotResponseData data = (FetchSnapshotResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(data.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n            return false;\n        }\n\n        Optional<FetchSnapshotResponseData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotResponse\n            .forTopicPartition(data, log.topicPartition());\n        if (!partitionSnapshotOpt.isPresent()) {\n            return false;\n        }\n\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n\n        FetchSnapshotResponseData.LeaderIdAndEpoch currentLeaderIdAndEpoch = partitionSnapshot.currentLeader();\n        OptionalInt responseLeaderId = optionalLeaderId(currentLeaderIdAndEpoch.leaderId());\n        int responseEpoch = currentLeaderIdAndEpoch.leaderEpoch();\n        Errors error = Errors.forCode(partitionSnapshot.errorCode());\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        }\n\n        FollowerState state = quorum.followerStateOrThrow();\n\n        if (Errors.forCode(partitionSnapshot.errorCode()) == Errors.SNAPSHOT_NOT_FOUND ||\n            partitionSnapshot.snapshotId().endOffset() < 0 ||\n            partitionSnapshot.snapshotId().epoch() < 0) {\n\n            /* The leader deleted the snapshot before the follower could download it. Start over by\n             * resetting the fetching snapshot state and sending another fetch request.\n             */\n            logger.info(\n                \"Leader doesn't know about snapshot id {}, returned error {} and snapshot id {}\",\n                state.fetchingSnapshot(),\n                partitionSnapshot.errorCode(),\n                partitionSnapshot.snapshotId()\n            );\n            state.setFetchingSnapshot(Optional.empty());\n            state.resetFetchTimeout(currentTimeMs);\n            return true;\n        }\n\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n            partitionSnapshot.snapshotId().endOffset(),\n            partitionSnapshot.snapshotId().epoch()\n        );\n\n        RawSnapshotWriter snapshot;\n        if (state.fetchingSnapshot().isPresent()) {\n            snapshot = state.fetchingSnapshot().get();\n        } else {\n            throw new IllegalStateException(\n                String.format(\"Received unexpected fetch snapshot response: %s\", partitionSnapshot)\n            );\n        }\n\n        if (!snapshot.snapshotId().equals(snapshotId)) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Received fetch snapshot response with an invalid id. Expected %s; Received %s\",\n                    snapshot.snapshotId(),\n                    snapshotId\n                )\n            );\n        }\n        if (snapshot.sizeInBytes() != partitionSnapshot.position()) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Received fetch snapshot response with an invalid position. Expected %d; Received %d\",\n                    snapshot.sizeInBytes(),\n                    partitionSnapshot.position()\n                )\n            );\n        }\n\n        final UnalignedMemoryRecords records;\n        if (partitionSnapshot.unalignedRecords() instanceof MemoryRecords) {\n            records = new UnalignedMemoryRecords(((MemoryRecords) partitionSnapshot.unalignedRecords()).buffer());\n        } else if (partitionSnapshot.unalignedRecords() instanceof UnalignedMemoryRecords) {\n            records = (UnalignedMemoryRecords) partitionSnapshot.unalignedRecords();\n        } else {\n            throw new IllegalStateException(String.format(\"Received unexpected fetch snapshot response: %s\", partitionSnapshot));\n        }\n        snapshot.append(records);\n\n        if (snapshot.sizeInBytes() == partitionSnapshot.size()) {\n            // Finished fetching the snapshot.\n            snapshot.freeze();\n            state.setFetchingSnapshot(Optional.empty());\n\n            if (log.truncateToLatestSnapshot()) {\n                logger.info(\n                    \"Fully truncated the log at ({}, {}) after downloading snapshot {} from leader {}\",\n                    log.endOffset(),\n                    log.lastFetchedEpoch(),\n                    snapshot.snapshotId(),\n                    quorum.leaderIdOrSentinel()\n                );\n\n                // This will always reload the snapshot because the internal next offset\n                // is always less than the snapshot id just downloaded.\n                partitionState.updateState();\n\n                updateFollowerHighWatermark(state, OptionalLong.of(log.highWatermark().offset));\n            } else {\n                throw new IllegalStateException(\n                    String.format(\n                        \"Full log truncation expected but didn't happen. Snapshot of %s, log end offset %s, last fetched %d\",\n                        snapshot.snapshotId(),\n                        log.endOffset(),\n                        log.lastFetchedEpoch()\n                    )\n                );\n            }\n        }\n\n        state.resetFetchTimeout(currentTimeMs);\n        return true;\n    }\n\n    private boolean hasConsistentLeader(int epoch, OptionalInt leaderId) {\n        // Only elected leaders are sent in the request/response header, so if we have an elected\n        // leaderId, it should be consistent with what is in the message.\n        if (leaderId.isPresent() && leaderId.getAsInt() == quorum.localIdOrSentinel()) {\n            // The response indicates that we should be the leader, so we verify that is the case\n            return quorum.isLeader();\n        } else {\n            return epoch != quorum.epoch()\n                || !leaderId.isPresent()\n                || !quorum.leaderId().isPresent()\n                || leaderId.equals(quorum.leaderId());\n        }\n    }\n\n    /**\n     * Handle response errors that are common across request types.\n     *\n     * @param error Error from the received response\n     * @param leaderId Optional leaderId from the response\n     * @param epoch Epoch received from the response\n     * @param currentTimeMs Current epoch time in milliseconds\n     * @return Optional value indicating whether the error was handled here and the outcome of\n     *    that handling. Specifically:\n     *\n     *    - Optional.empty means that the response was not handled here and the custom\n     *        API handler should be applied\n     *    - Optional.of(true) indicates that the response was successfully handled here and\n     *        the request does not need to be retried\n     *    - Optional.of(false) indicates that the response was handled here, but that the request\n     *        will need to be retried\n     */\n    private Optional<Boolean> maybeHandleCommonResponse(\n        Errors error,\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    ) {\n        if (epoch < quorum.epoch() || error == Errors.UNKNOWN_LEADER_EPOCH) {\n            // We have a larger epoch, so the response is no longer relevant\n            return Optional.of(true);\n        } else if (epoch > quorum.epoch()\n            || error == Errors.FENCED_LEADER_EPOCH\n            || error == Errors.NOT_LEADER_OR_FOLLOWER) {\n\n            // The response indicates that the request had a stale epoch, but we need\n            // to validate the epoch from the response against our current state.\n            maybeTransition(leaderId, epoch, currentTimeMs);\n            return Optional.of(true);\n        } else if (epoch == quorum.epoch()\n            && leaderId.isPresent()\n            && !quorum.hasLeader()) {\n\n            // Since we are transitioning to Follower, we will only forward the\n            // request to the handler if there is no error. Otherwise, we will let\n            // the request be retried immediately (if needed) after the transition.\n            // This handling allows an observer to discover the leader and append\n            // to the log in the same Fetch request.\n            transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n            if (error == Errors.NONE) {\n                return Optional.empty();\n            } else {\n                return Optional.of(true);\n            }\n        } else if (error == Errors.BROKER_NOT_AVAILABLE) {\n            return Optional.of(false);\n        } else if (error == Errors.INCONSISTENT_GROUP_PROTOCOL) {\n            // For now we treat this as a fatal error. Once we have support for quorum\n            // reassignment, this error could suggest that either we or the recipient of\n            // the request just has stale voter information, which means we can retry\n            // after backing off.\n            throw new IllegalStateException(\"Received error indicating inconsistent voter sets\");\n        } else if (error == Errors.INVALID_REQUEST) {\n            throw new IllegalStateException(\"Received unexpected invalid request error\");\n        }\n\n        return Optional.empty();\n    }\n\n    private void maybeTransition(\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    ) {\n        if (!hasConsistentLeader(epoch, leaderId)) {\n            throw new IllegalStateException(\"Received request or response with leader \" + leaderId +\n                \" and epoch \" + epoch + \" which is inconsistent with current leader \" +\n                quorum.leaderId() + \" and epoch \" + quorum.epoch());\n        } else if (epoch > quorum.epoch()) {\n            if (leaderId.isPresent()) {\n                transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n            } else {\n                transitionToUnattached(epoch);\n            }\n        } else if (leaderId.isPresent() && !quorum.hasLeader()) {\n            // The request or response indicates the leader of the current epoch,\n            // which is currently unknown\n            transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n        }\n    }\n\n    private boolean handleTopLevelError(Errors error, RaftResponse.Inbound response) {\n        if (error == Errors.BROKER_NOT_AVAILABLE) {\n            return false;\n        } else if (error == Errors.CLUSTER_AUTHORIZATION_FAILED) {\n            throw new ClusterAuthorizationException(\"Received cluster authorization error in response \" + response);\n        } else {\n            return handleUnexpectedError(error, response);\n        }\n    }\n\n    private boolean handleUnexpectedError(Errors error, RaftResponse.Inbound response) {\n        logger.error(\"Unexpected error {} in {} response: {}\",\n            error, ApiKeys.forId(response.data.apiKey()), response);\n        return false;\n    }\n\n    private void handleResponse(RaftResponse.Inbound response, long currentTimeMs) {\n        // The response epoch matches the local epoch, so we can handle the response\n        ApiKeys apiKey = ApiKeys.forId(response.data.apiKey());\n        final boolean handledSuccessfully;\n\n        switch (apiKey) {\n            case FETCH:\n                handledSuccessfully = handleFetchResponse(response, currentTimeMs);\n                break;\n\n            case VOTE:\n                handledSuccessfully = handleVoteResponse(response, currentTimeMs);\n                break;\n\n            case BEGIN_QUORUM_EPOCH:\n                handledSuccessfully = handleBeginQuorumEpochResponse(response, currentTimeMs);\n                break;\n\n            case END_QUORUM_EPOCH:\n                handledSuccessfully = handleEndQuorumEpochResponse(response, currentTimeMs);\n                break;\n\n            case FETCH_SNAPSHOT:\n                handledSuccessfully = handleFetchSnapshotResponse(response, currentTimeMs);\n                break;\n\n            default:\n                throw new IllegalArgumentException(\"Received unexpected response type: \" + apiKey);\n        }\n\n        ConnectionState connection = requestManager.getOrCreate(response.sourceId());\n        onResponseResult(response, currentTimeMs, handledSuccessfully, connection);\n    }\n\n    private void onResponseResult(RaftResponse.Inbound response, long currentTimeMs, boolean handledSuccessfully, ConnectionState connection) {\n        if (handledSuccessfully) {\n            connection.onResponseReceived(response.correlationId);\n        } else {\n            connection.onResponseError(response.correlationId, currentTimeMs);\n        }\n    }\n\n    /**\n     * Validate common state for requests to establish leadership.\n     *\n     * These include the Vote, BeginQuorumEpoch and EndQuorumEpoch RPCs. If an error is present in\n     * the returned value, it should be returned in the response.\n     */\n    private Optional<Errors> validateVoterOnlyRequest(int remoteNodeId, int requestEpoch) {\n        if (requestEpoch < quorum.epoch()) {\n            return Optional.of(Errors.FENCED_LEADER_EPOCH);\n        } else if (remoteNodeId < 0) {\n            return Optional.of(Errors.INVALID_REQUEST);\n        } else {\n            return Optional.empty();\n        }\n    }\n\n    /**\n     * Validate a request which is intended for the current quorum leader.\n     * If an error is present in the returned value, it should be returned\n     * in the response.\n     */\n    private Optional<Errors> validateLeaderOnlyRequest(int requestEpoch) {\n        if (requestEpoch < quorum.epoch()) {\n            return Optional.of(Errors.FENCED_LEADER_EPOCH);\n        } else if (requestEpoch > quorum.epoch()) {\n            return Optional.of(Errors.UNKNOWN_LEADER_EPOCH);\n        } else if (!quorum.isLeader()) {\n            // In general, non-leaders do not expect to receive requests\n            // matching their own epoch, but it is possible when observers\n            // are using the Fetch API to find the result of an election.\n            return Optional.of(Errors.NOT_LEADER_OR_FOLLOWER);\n        } else if (shutdown.get() != null) {\n            return Optional.of(Errors.BROKER_NOT_AVAILABLE);\n        } else {\n            return Optional.empty();\n        }\n    }\n\n    private void handleRequest(RaftRequest.Inbound request, long currentTimeMs) {\n        ApiKeys apiKey = ApiKeys.forId(request.data.apiKey());\n        final CompletableFuture<? extends ApiMessage> responseFuture;\n\n        switch (apiKey) {\n            case FETCH:\n                responseFuture = handleFetchRequest(request, currentTimeMs);\n                break;\n\n            case VOTE:\n                responseFuture = completedFuture(handleVoteRequest(request));\n                break;\n\n            case BEGIN_QUORUM_EPOCH:\n                responseFuture = completedFuture(handleBeginQuorumEpochRequest(request, currentTimeMs));\n                break;\n\n            case END_QUORUM_EPOCH:\n                responseFuture = completedFuture(handleEndQuorumEpochRequest(request, currentTimeMs));\n                break;\n\n            case DESCRIBE_QUORUM:\n                responseFuture = completedFuture(handleDescribeQuorumRequest(request, currentTimeMs));\n                break;\n\n            case FETCH_SNAPSHOT:\n                responseFuture = completedFuture(handleFetchSnapshotRequest(request, currentTimeMs));\n                break;\n\n            default:\n                throw new IllegalArgumentException(\"Unexpected request type \" + apiKey);\n        }\n\n        responseFuture.whenComplete((response, exception) -> {\n            final ApiMessage message;\n            if (response != null) {\n                message = response;\n            } else {\n                message = RaftUtil.errorResponse(apiKey, Errors.forException(exception));\n            }\n\n            RaftResponse.Outbound responseMessage = new RaftResponse.Outbound(request.correlationId(), message);\n            request.completion.complete(responseMessage);\n            logger.trace(\"Sent response {} to inbound request {}\", responseMessage, request);\n        });\n    }\n\n    private void handleInboundMessage(RaftMessage message, long currentTimeMs) {\n        logger.trace(\"Received inbound message {}\", message);\n\n        if (message instanceof RaftRequest.Inbound) {\n            RaftRequest.Inbound request = (RaftRequest.Inbound) message;\n            handleRequest(request, currentTimeMs);\n        } else if (message instanceof RaftResponse.Inbound) {\n            RaftResponse.Inbound response = (RaftResponse.Inbound) message;\n            ConnectionState connection = requestManager.getOrCreate(response.sourceId());\n            if (connection.isResponseExpected(response.correlationId)) {\n                handleResponse(response, currentTimeMs);\n            } else {\n                logger.debug(\"Ignoring response {} since it is no longer needed\", response);\n            }\n        } else {\n            throw new IllegalArgumentException(\"Unexpected message \" + message);\n        }\n    }\n\n    /**\n     * Attempt to send a request. Return the time to wait before the request can be retried.\n     */\n    private long maybeSendRequest(\n        long currentTimeMs,\n        int destinationId,\n        Supplier<ApiMessage> requestSupplier\n    )  {\n        ConnectionState connection = requestManager.getOrCreate(destinationId);\n\n        if (connection.isBackingOff(currentTimeMs)) {\n            long remainingBackoffMs = connection.remainingBackoffMs(currentTimeMs);\n            logger.debug(\"Connection for {} is backing off for {} ms\", destinationId, remainingBackoffMs);\n            return remainingBackoffMs;\n        }\n\n        if (connection.isReady(currentTimeMs)) {\n            int correlationId = channel.newCorrelationId();\n            ApiMessage request = requestSupplier.get();\n\n            RaftRequest.Outbound requestMessage = new RaftRequest.Outbound(\n                correlationId,\n                request,\n                destinationId,\n                currentTimeMs\n            );\n\n            requestMessage.completion.whenComplete((response, exception) -> {\n                if (exception != null) {\n                    ApiKeys api = ApiKeys.forId(request.apiKey());\n                    Errors error = Errors.forException(exception);\n                    ApiMessage errorResponse = RaftUtil.errorResponse(api, error);\n\n                    response = new RaftResponse.Inbound(\n                        correlationId,\n                        errorResponse,\n                        destinationId\n                    );\n                }\n\n                messageQueue.add(response);\n            });\n\n            channel.send(requestMessage);\n            logger.trace(\"Sent outbound request: {}\", requestMessage);\n            connection.onRequestSent(correlationId, currentTimeMs);\n            return Long.MAX_VALUE;\n        }\n\n        return connection.remainingRequestTimeMs(currentTimeMs);\n    }\n\n    private EndQuorumEpochRequestData buildEndQuorumEpochRequest(\n        ResignedState state\n    ) {\n        return EndQuorumEpochRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow(),\n            state.preferredSuccessors()\n        );\n    }\n\n    private long maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    ) {\n        long minBackoffMs = Long.MAX_VALUE;\n        for (Integer destinationId : destinationIds) {\n            long backoffMs = maybeSendRequest(currentTimeMs, destinationId, requestSupplier);\n            if (backoffMs < minBackoffMs) {\n                minBackoffMs = backoffMs;\n            }\n        }\n        return minBackoffMs;\n    }\n\n    private BeginQuorumEpochRequestData buildBeginQuorumEpochRequest() {\n        return BeginQuorumEpochRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow()\n        );\n    }\n\n    private VoteRequestData buildVoteRequest() {\n        OffsetAndEpoch endOffset = endOffset();\n        return VoteRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow(),\n            endOffset.epoch(),\n            endOffset.offset()\n        );\n    }\n\n    private FetchRequestData buildFetchRequest() {\n        FetchRequestData request = RaftUtil.singletonFetchRequest(log.topicPartition(), log.topicId(), fetchPartition -> {\n            fetchPartition\n                .setCurrentLeaderEpoch(quorum.epoch())\n                .setLastFetchedEpoch(log.lastFetchedEpoch())\n                .setFetchOffset(log.endOffset().offset);\n        });\n        return request\n            .setMaxBytes(MAX_FETCH_SIZE_BYTES)\n            .setMaxWaitMs(fetchMaxWaitMs)\n            .setClusterId(clusterId)\n            .setReplicaState(new FetchRequestData.ReplicaState().setReplicaId(quorum.localIdOrSentinel()));\n    }\n\n    private long maybeSendAnyVoterFetch(long currentTimeMs) {\n        OptionalInt readyVoterIdOpt = requestManager.findReadyVoter(currentTimeMs);\n        if (readyVoterIdOpt.isPresent()) {\n            return maybeSendRequest(\n                currentTimeMs,\n                readyVoterIdOpt.getAsInt(),\n                this::buildFetchRequest\n            );\n        } else {\n            return requestManager.backoffBeforeAvailableVoter(currentTimeMs);\n        }\n    }\n\n    private FetchSnapshotRequestData buildFetchSnapshotRequest(OffsetAndEpoch snapshotId, long snapshotSize) {\n        FetchSnapshotRequestData.SnapshotId requestSnapshotId = new FetchSnapshotRequestData.SnapshotId()\n            .setEpoch(snapshotId.epoch())\n            .setEndOffset(snapshotId.offset());\n\n        FetchSnapshotRequestData request = FetchSnapshotRequest.singleton(\n            clusterId,\n            quorum().localIdOrSentinel(),\n            log.topicPartition(),\n            snapshotPartition -> snapshotPartition\n                .setCurrentLeaderEpoch(quorum.epoch())\n                .setSnapshotId(requestSnapshotId)\n                .setPosition(snapshotSize)\n        );\n\n        return request.setReplicaId(quorum.localIdOrSentinel());\n    }\n\n    private FetchSnapshotResponseData.PartitionSnapshot addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    ) {\n        partitionSnapshot.currentLeader()\n            .setLeaderEpoch(quorum.epoch())\n            .setLeaderId(quorum.leaderIdOrSentinel());\n\n        return partitionSnapshot;\n    }\n\n    public boolean isRunning() {\n        GracefulShutdown gracefulShutdown = shutdown.get();\n        return gracefulShutdown == null || !gracefulShutdown.isFinished();\n    }\n\n    public boolean isShuttingDown() {\n        GracefulShutdown gracefulShutdown = shutdown.get();\n        return gracefulShutdown != null && !gracefulShutdown.isFinished();\n    }\n\n    private void appendBatch(\n        LeaderState<T> state,\n        BatchAccumulator.CompletedBatch<T> batch,\n        long appendTimeMs\n    ) {\n        try {\n            int epoch = state.epoch();\n            LogAppendInfo info = appendAsLeader(batch.data);\n            OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(info.lastOffset, epoch);\n            CompletableFuture<Long> future = appendPurgatory.await(\n                offsetAndEpoch.offset() + 1, Integer.MAX_VALUE);\n\n            future.whenComplete((commitTimeMs, exception) -> {\n                if (exception != null) {\n                    logger.debug(\"Failed to commit {} records up to last offset {}\", batch.numRecords, offsetAndEpoch, exception);\n                } else {\n                    long elapsedTime = Math.max(0, commitTimeMs - appendTimeMs);\n                    double elapsedTimePerRecord = (double) elapsedTime / batch.numRecords;\n                    kafkaRaftMetrics.updateCommitLatency(elapsedTimePerRecord, appendTimeMs);\n                    logger.debug(\"Completed commit of {} records up to last offset {}\", batch.numRecords, offsetAndEpoch);\n                    batch.records.ifPresent(records -> {\n                        maybeFireHandleCommit(batch.baseOffset, epoch, batch.appendTimestamp(), batch.sizeInBytes(), records);\n                    });\n                }\n            });\n        } finally {\n            batch.release();\n        }\n    }\n\n    private long maybeAppendBatches(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        long timeUntilDrain = state.accumulator().timeUntilDrain(currentTimeMs);\n        if (timeUntilDrain <= 0) {\n            List<BatchAccumulator.CompletedBatch<T>> batches = state.accumulator().drain();\n            Iterator<BatchAccumulator.CompletedBatch<T>> iterator = batches.iterator();\n\n            try {\n                while (iterator.hasNext()) {\n                    BatchAccumulator.CompletedBatch<T> batch = iterator.next();\n                    appendBatch(state, batch, currentTimeMs);\n                }\n                flushLeaderLog(state, currentTimeMs);\n            } finally {\n                // Release and discard any batches which failed to be appended\n                while (iterator.hasNext()) {\n                    iterator.next().release();\n                }\n            }\n        }\n        return timeUntilDrain;\n    }\n\n    private long pollResigned(long currentTimeMs) {\n        ResignedState state = quorum.resignedStateOrThrow();\n        long endQuorumBackoffMs = maybeSendRequests(\n            currentTimeMs,\n            state.unackedVoters(),\n            () -> buildEndQuorumEpochRequest(state)\n        );\n\n        GracefulShutdown shutdown = this.shutdown.get();\n        final long stateTimeoutMs;\n        if (shutdown != null) {\n            // If we are shutting down, then we will remain in the resigned state\n            // until either the shutdown expires or an election bumps the epoch\n            stateTimeoutMs = shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            stateTimeoutMs = 0L;\n        } else {\n            stateTimeoutMs = state.remainingElectionTimeMs(currentTimeMs);\n        }\n\n        return Math.min(stateTimeoutMs, endQuorumBackoffMs);\n    }\n\n    private long pollLeader(long currentTimeMs) {\n        LeaderState<T> state = quorum.leaderStateOrThrow();\n        maybeFireLeaderChange(state);\n\n        long timeUntilCheckQuorumExpires = state.timeUntilCheckQuorumExpires(currentTimeMs);\n        if (shutdown.get() != null || state.isResignRequested() || timeUntilCheckQuorumExpires == 0) {\n            transitionToResigned(state.nonLeaderVotersByDescendingFetchOffset());\n            return 0L;\n        }\n\n        long timeUntilFlush = maybeAppendBatches(\n            state,\n            currentTimeMs\n        );\n\n        long timeUntilSend = maybeSendRequests(\n            currentTimeMs,\n            state.nonAcknowledgingVoters(),\n            this::buildBeginQuorumEpochRequest\n        );\n\n        return Math.min(timeUntilFlush, Math.min(timeUntilSend, timeUntilCheckQuorumExpires));\n    }\n\n    private long maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    ) {\n        // Continue sending Vote requests as long as we still have a chance to win the election\n        if (!state.isVoteRejected()) {\n            return maybeSendRequests(\n                currentTimeMs,\n                state.unrecordedVoters(),\n                this::buildVoteRequest\n            );\n        }\n        return Long.MAX_VALUE;\n    }\n\n    private long pollCandidate(long currentTimeMs) {\n        CandidateState state = quorum.candidateStateOrThrow();\n        GracefulShutdown shutdown = this.shutdown.get();\n\n        if (shutdown != null) {\n            // If we happen to shutdown while we are a candidate, we will continue\n            // with the current election until one of the following conditions is met:\n            //  1) we are elected as leader (which allows us to resign)\n            //  2) another leader is elected\n            //  3) the shutdown timer expires\n            long minRequestBackoffMs = maybeSendVoteRequests(state, currentTimeMs);\n            return Math.min(shutdown.remainingTimeMs(), minRequestBackoffMs);\n        } else if (state.isBackingOff()) {\n            if (state.isBackoffComplete(currentTimeMs)) {\n                logger.info(\"Re-elect as candidate after election backoff has completed\");\n                transitionToCandidate(currentTimeMs);\n                return 0L;\n            }\n            return state.remainingBackoffMs(currentTimeMs);\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            long backoffDurationMs = binaryExponentialElectionBackoffMs(state.retries());\n            logger.info(\"Election has timed out, backing off for {}ms before becoming a candidate again\",\n                backoffDurationMs);\n            state.startBackingOff(currentTimeMs, backoffDurationMs);\n            return backoffDurationMs;\n        } else {\n            long minRequestBackoffMs = maybeSendVoteRequests(state, currentTimeMs);\n            return Math.min(minRequestBackoffMs, state.remainingElectionTimeMs(currentTimeMs));\n        }\n    }\n\n    private long pollFollower(long currentTimeMs) {\n        FollowerState state = quorum.followerStateOrThrow();\n        if (quorum.isVoter()) {\n            return pollFollowerAsVoter(state, currentTimeMs);\n        } else {\n            return pollFollowerAsObserver(state, currentTimeMs);\n        }\n    }\n\n    private long pollFollowerAsVoter(FollowerState state, long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown != null) {\n            // If we are a follower, then we can shutdown immediately. We want to\n            // skip the transition to candidate in any case.\n            return 0;\n        } else if (state.hasFetchTimeoutExpired(currentTimeMs)) {\n            logger.info(\"Become candidate due to fetch timeout\");\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            long backoffMs = maybeSendFetchOrFetchSnapshot(state, currentTimeMs);\n\n            return Math.min(backoffMs, state.remainingFetchTimeMs(currentTimeMs));\n        }\n    }\n\n    private long pollFollowerAsObserver(FollowerState state, long currentTimeMs) {\n        if (state.hasFetchTimeoutExpired(currentTimeMs)) {\n            return maybeSendAnyVoterFetch(currentTimeMs);\n        } else {\n            final long backoffMs;\n\n            // If the current leader is backing off due to some failure or if the\n            // request has timed out, then we attempt to send the Fetch to another\n            // voter in order to discover if there has been a leader change.\n            ConnectionState connection = requestManager.getOrCreate(state.leaderId());\n            if (connection.hasRequestTimedOut(currentTimeMs)) {\n                backoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n                connection.reset();\n            } else if (connection.isBackingOff(currentTimeMs)) {\n                backoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n            } else {\n                backoffMs = maybeSendFetchOrFetchSnapshot(state, currentTimeMs);\n            }\n\n            return Math.min(backoffMs, state.remainingFetchTimeMs(currentTimeMs));\n        }\n    }\n\n    private long maybeSendFetchOrFetchSnapshot(FollowerState state, long currentTimeMs) {\n        final Supplier<ApiMessage> requestSupplier;\n\n        if (state.fetchingSnapshot().isPresent()) {\n            RawSnapshotWriter snapshot = state.fetchingSnapshot().get();\n            long snapshotSize = snapshot.sizeInBytes();\n\n            requestSupplier = () -> buildFetchSnapshotRequest(snapshot.snapshotId(), snapshotSize);\n        } else {\n            requestSupplier = this::buildFetchRequest;\n        }\n\n        return maybeSendRequest(currentTimeMs, state.leaderId(), requestSupplier);\n    }\n\n    private long pollVoted(long currentTimeMs) {\n        VotedState state = quorum.votedStateOrThrow();\n        GracefulShutdown shutdown = this.shutdown.get();\n\n        if (shutdown != null) {\n            // If shutting down, then remain in this state until either the\n            // shutdown completes or an epoch bump forces another state transition\n            return shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            return state.remainingElectionTimeMs(currentTimeMs);\n        }\n    }\n\n    private long pollUnattached(long currentTimeMs) {\n        UnattachedState state = quorum.unattachedStateOrThrow();\n        if (quorum.isVoter()) {\n            return pollUnattachedAsVoter(state, currentTimeMs);\n        } else {\n            return pollUnattachedAsObserver(state, currentTimeMs);\n        }\n    }\n\n    private long pollUnattachedAsVoter(UnattachedState state, long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown != null) {\n            // If shutting down, then remain in this state until either the\n            // shutdown completes or an epoch bump forces another state transition\n            return shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            return state.remainingElectionTimeMs(currentTimeMs);\n        }\n    }\n\n    private long pollUnattachedAsObserver(UnattachedState state, long currentTimeMs) {\n        long fetchBackoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n        return Math.min(fetchBackoffMs, state.remainingElectionTimeMs(currentTimeMs));\n    }\n\n    private long pollCurrentState(long currentTimeMs) {\n        if (quorum.isLeader()) {\n            return pollLeader(currentTimeMs);\n        } else if (quorum.isCandidate()) {\n            return pollCandidate(currentTimeMs);\n        } else if (quorum.isFollower()) {\n            return pollFollower(currentTimeMs);\n        } else if (quorum.isVoted()) {\n            return pollVoted(currentTimeMs);\n        } else if (quorum.isUnattached()) {\n            return pollUnattached(currentTimeMs);\n        } else if (quorum.isResigned()) {\n            return pollResigned(currentTimeMs);\n        } else {\n            throw new IllegalStateException(\"Unexpected quorum state \" + quorum);\n        }\n    }\n\n    private void pollListeners() {\n        // Apply all of the pending registration\n        while (true) {\n            Registration<T> registration = pendingRegistrations.poll();\n            if (registration == null) {\n                break;\n            }\n\n            processRegistration(registration);\n        }\n\n        // Check listener progress to see if reads are expected\n        quorum.highWatermark().ifPresent(highWatermarkMetadata -> {\n            updateListenersProgress(highWatermarkMetadata.offset);\n        });\n\n        // Notify the new listeners of the latest leader and epoch\n        Optional<LeaderState<T>> leaderState = quorum.maybeLeaderState();\n        if (leaderState.isPresent()) {\n            maybeFireLeaderChange(leaderState.get());\n        } else {\n            maybeFireLeaderChange();\n        }\n    }\n\n    private void processRegistration(Registration<T> registration) {\n        Listener<T> listener = registration.listener();\n        Registration.Ops ops = registration.ops();\n\n        if (ops == Registration.Ops.REGISTER) {\n            if (listenerContexts.putIfAbsent(listener, new ListenerContext(listener)) != null) {\n                logger.error(\"Attempting to add a listener that already exists: {}\", listenerName(listener));\n            } else {\n                logger.info(\"Registered the listener {}\", listenerName(listener));\n            }\n        } else {\n            if (listenerContexts.remove(listener) == null) {\n                logger.error(\"Attempting to remove a listener that doesn't exists: {}\", listenerName(listener));\n            } else {\n                logger.info(\"Unregistered the listener {}\", listenerName(listener));\n            }\n        }\n    }\n\n    private boolean maybeCompleteShutdown(long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown == null) {\n            return false;\n        }\n\n        shutdown.update(currentTimeMs);\n        if (shutdown.hasTimedOut()) {\n            shutdown.failWithTimeout();\n            return true;\n        }\n\n        if (quorum.isObserver()\n            || quorum.isOnlyVoter()\n            || quorum.hasRemoteLeader()\n        ) {\n            shutdown.complete();\n            return true;\n        }\n\n        return false;\n    }\n\n    /**\n     * A simple timer based log cleaner\n     */\n    private static class RaftMetadataLogCleanerManager {\n        private final Logger logger;\n        private final Timer timer;\n        private final long delayMs;\n        private final Runnable cleaner;\n\n        RaftMetadataLogCleanerManager(Logger logger, Time time, long delayMs, Runnable cleaner) {\n            this.logger = logger;\n            this.timer = time.timer(delayMs);\n            this.delayMs = delayMs;\n            this.cleaner = cleaner;\n        }\n\n        public long maybeClean(long currentTimeMs) {\n            timer.update(currentTimeMs);\n            if (timer.isExpired()) {\n                try {\n                    cleaner.run();\n                } catch (Throwable t) {\n                    logger.error(\"Had an error during log cleaning\", t);\n                }\n                timer.reset(delayMs);\n            }\n            return timer.remainingMs();\n        }\n    }\n\n    private void wakeup() {\n        messageQueue.wakeup();\n    }\n\n    /**\n     * Handle an inbound request. The response will be returned through\n     * {@link RaftRequest.Inbound#completion}.\n     *\n     * @param request The inbound request\n     */\n    public void handle(RaftRequest.Inbound request) {\n        messageQueue.add(Objects.requireNonNull(request));\n    }\n\n    /**\n     * Poll for new events. This allows the client to handle inbound\n     * requests and send any needed outbound requests.\n     */\n    public void poll() {\n        if (!isInitialized()) {\n            throw new IllegalStateException(\"Replica needs to be initialized before polling\");\n        }\n\n        long startPollTimeMs = time.milliseconds();\n        if (maybeCompleteShutdown(startPollTimeMs)) {\n            return;\n        }\n\n        long pollStateTimeoutMs = pollCurrentState(startPollTimeMs);\n        long cleaningTimeoutMs = snapshotCleaner.maybeClean(startPollTimeMs);\n        long pollTimeoutMs = Math.min(pollStateTimeoutMs, cleaningTimeoutMs);\n\n        long startWaitTimeMs = time.milliseconds();\n        kafkaRaftMetrics.updatePollStart(startWaitTimeMs);\n\n        RaftMessage message = messageQueue.poll(pollTimeoutMs);\n\n        long endWaitTimeMs = time.milliseconds();\n        kafkaRaftMetrics.updatePollEnd(endWaitTimeMs);\n\n        if (message != null) {\n            handleInboundMessage(message, endWaitTimeMs);\n        }\n\n        pollListeners();\n    }\n\n    @Override\n    public long scheduleAppend(int epoch, List<T> records) {\n        return append(epoch, records, OptionalLong.empty(), false);\n    }\n\n    @Override\n    public long scheduleAtomicAppend(int epoch, OptionalLong requiredBaseOffset, List<T> records) {\n        return append(epoch, records, requiredBaseOffset, true);\n    }\n\n    private long append(int epoch, List<T> records, OptionalLong requiredBaseOffset, boolean isAtomic) {\n        if (!isInitialized()) {\n            throw new NotLeaderException(\"Append failed because the replica is not the current leader\");\n        }\n\n        LeaderState<T> leaderState = quorum.<T>maybeLeaderState().orElseThrow(\n            () -> new NotLeaderException(\"Append failed because the replica is not the current leader\")\n        );\n\n        BatchAccumulator<T> accumulator = leaderState.accumulator();\n        boolean isFirstAppend = accumulator.isEmpty();\n        final long offset = accumulator.append(epoch, records, requiredBaseOffset, isAtomic);\n\n        // Wakeup the network channel if either this is the first append\n        // or the accumulator is ready to drain now. Checking for the first\n        // append ensures that we give the IO thread a chance to observe\n        // the linger timeout so that it can schedule its own wakeup in case\n        // there are no additional appends.\n        if (isFirstAppend || accumulator.needsDrain(time.milliseconds())) {\n            wakeup();\n        }\n        return offset;\n    }\n\n    @Override\n    public CompletableFuture<Void> shutdown(int timeoutMs) {\n        logger.info(\"Beginning graceful shutdown\");\n        CompletableFuture<Void> shutdownComplete = new CompletableFuture<>();\n        shutdown.set(new GracefulShutdown(timeoutMs, shutdownComplete));\n        wakeup();\n        return shutdownComplete;\n    }\n\n    @Override\n    public void resign(int epoch) {\n        if (epoch < 0) {\n            throw new IllegalArgumentException(\"Attempt to resign from an invalid negative epoch \" + epoch);\n        } else if (!isInitialized()) {\n            throw new IllegalStateException(\"Replica needs to be initialized before resigning\");\n        } else if (!quorum.isVoter()) {\n            throw new IllegalStateException(\"Attempt to resign by a non-voter\");\n        }\n\n        LeaderAndEpoch leaderAndEpoch = leaderAndEpoch();\n        int currentEpoch = leaderAndEpoch.epoch();\n\n        if (epoch > currentEpoch) {\n            throw new IllegalArgumentException(\"Attempt to resign from epoch \" + epoch +\n                \" which is larger than the current epoch \" + currentEpoch);\n        } else if (epoch < currentEpoch) {\n            // If the passed epoch is smaller than the current epoch, then it might mean\n            // that the listener has not been notified about a leader change that already\n            // took place. In this case, we consider the call as already fulfilled and\n            // take no further action.\n            logger.debug(\"Ignoring call to resign from epoch {} since it is smaller than the \" +\n                \"current epoch {}\", epoch, currentEpoch);\n        } else if (!leaderAndEpoch.isLeader(quorum.localIdOrThrow())) {\n            throw new IllegalArgumentException(\"Cannot resign from epoch \" + epoch +\n                \" since we are not the leader\");\n        } else {\n            // Note that if we transition to another state before we have a chance to\n            // request resignation, then we consider the call fulfilled.\n            Optional<LeaderState<Object>> leaderStateOpt = quorum.maybeLeaderState();\n            if (!leaderStateOpt.isPresent()) {\n                logger.debug(\"Ignoring call to resign from epoch {} since this node is \" +\n                    \"no longer the leader\", epoch);\n                return;\n            }\n\n            LeaderState<Object> leaderState = leaderStateOpt.get();\n            if (leaderState.epoch() != epoch) {\n                logger.debug(\"Ignoring call to resign from epoch {} since it is smaller than the \" +\n                    \"current epoch {}\", epoch, leaderState.epoch());\n            } else {\n                logger.info(\"Received user request to resign from the current epoch {}\", currentEpoch);\n                leaderState.requestResign();\n                wakeup();\n            }\n        }\n    }\n\n    @Override\n    public Optional<SnapshotWriter<T>> createSnapshot(\n        OffsetAndEpoch snapshotId,\n        long lastContainedLogTimestamp\n    ) {\n        if (!isInitialized()) {\n            throw new IllegalStateException(\"Cannot create snapshot before the replica has been initialized\");\n        }\n\n        return log.createNewSnapshot(snapshotId).map(writer -> {\n            long lastContainedLogOffset = snapshotId.offset() - 1;\n\n            RawSnapshotWriter wrappedWriter = new NotifyingRawSnapshotWriter(writer, offsetAndEpoch -> {\n                // Trim the state in the internal listener up to the new snapshot\n                partitionState.truncateOldEntries(offsetAndEpoch.offset());\n            });\n\n            return new RecordsSnapshotWriter.Builder()\n                .setLastContainedLogTimestamp(lastContainedLogTimestamp)\n                .setTime(time)\n                .setMaxBatchSize(MAX_BATCH_SIZE_BYTES)\n                .setMemoryPool(memoryPool)\n                .setRawSnapshotWriter(wrappedWriter)\n                .setKraftVersion(partitionState.kraftVersionAtOffset(lastContainedLogOffset))\n                .setVoterSet(partitionState.voterSetAtOffset(lastContainedLogOffset))\n                .build(serde);\n        });\n    }\n\n    @Override\n    public Optional<OffsetAndEpoch> latestSnapshotId() {\n        return log.latestSnapshotId();\n    }\n\n    @Override\n    public long logEndOffset() {\n        return log.endOffset().offset;\n    }\n\n    @Override\n    public void close() {\n        log.flush(true);\n        if (kafkaRaftMetrics != null) {\n            kafkaRaftMetrics.close();\n        }\n        if (memoryPool instanceof BatchMemoryPool) {\n            BatchMemoryPool batchMemoryPool = (BatchMemoryPool) memoryPool;\n            batchMemoryPool.releaseRetained();\n        }\n    }\n\n    @Override\n    public OptionalLong highWatermark() {\n        if (isInitialized() && quorum.highWatermark().isPresent()) {\n            return OptionalLong.of(quorum.highWatermark().get().offset);\n        } else {\n            return OptionalLong.empty();\n        }\n    }\n\n    public Optional<Node> voterNode(int id, String listener) {\n        return partitionState.lastVoterSet().voterNode(id, listener);\n    }\n\n    // Visible only for test\n    QuorumState quorum() {\n        // It's okay to return null since this method is only called by tests\n        return quorum;\n    }\n\n    private boolean isInitialized() {\n        return partitionState != null && quorum != null && requestManager != null && kafkaRaftMetrics != null;\n    }\n\n    private class GracefulShutdown {\n        final Timer finishTimer;\n        final CompletableFuture<Void> completeFuture;\n\n        public GracefulShutdown(long shutdownTimeoutMs,\n                                CompletableFuture<Void> completeFuture) {\n            this.finishTimer = time.timer(shutdownTimeoutMs);\n            this.completeFuture = completeFuture;\n        }\n\n        public void update(long currentTimeMs) {\n            finishTimer.update(currentTimeMs);\n        }\n\n        public boolean hasTimedOut() {\n            return finishTimer.isExpired();\n        }\n\n        public boolean isFinished() {\n            return completeFuture.isDone();\n        }\n\n        public long remainingTimeMs() {\n            return finishTimer.remainingMs();\n        }\n\n        public void failWithTimeout() {\n            logger.warn(\"Graceful shutdown timed out after {}ms\", finishTimer.timeoutMs());\n            completeFuture.completeExceptionally(\n                new TimeoutException(\"Timeout expired before graceful shutdown completed\"));\n        }\n\n        public void complete() {\n            logger.info(\"Graceful shutdown completed\");\n            completeFuture.complete(null);\n        }\n    }\n\n    private static final class Registration<T> {\n        private final Ops ops;\n        private final Listener<T> listener;\n\n        private Registration(Ops ops, Listener<T> listener) {\n            this.ops = ops;\n            this.listener = listener;\n        }\n\n        private Ops ops() {\n            return ops;\n        }\n\n        private Listener<T> listener() {\n            return listener;\n        }\n\n        private enum Ops {\n            REGISTER, UNREGISTER\n        }\n\n        private static <T> Registration<T> register(Listener<T> listener) {\n            return new Registration<>(Ops.REGISTER, listener);\n        }\n\n        private static <T> Registration<T> unregister(Listener<T> listener) {\n            return new Registration<>(Ops.UNREGISTER, listener);\n        }\n    }\n\n    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n        private final RaftClient.Listener<T> listener;\n        // This field is used only by the Raft IO thread\n        private LeaderAndEpoch lastFiredLeaderChange = LeaderAndEpoch.UNKNOWN;\n\n        // These fields are visible to both the Raft IO thread and the listener\n        // and are protected through synchronization on this ListenerContext instance\n        private BatchReader<T> lastSent = null;\n        private long nextOffset = 0;\n\n        private ListenerContext(Listener<T> listener) {\n            this.listener = listener;\n        }\n\n        /**\n         * Get the last acked offset, which is one greater than the offset of the\n         * last record which was acked by the state machine.\n         */\n        private synchronized long nextOffset() {\n            return nextOffset;\n        }\n\n        /**\n         * Get the next expected offset, which might be larger than the last acked\n         * offset if there are inflight batches which have not been acked yet.\n         * Note that when fetching from disk, we may not know the last offset of\n         * inflight data until it has been processed by the state machine. In this case,\n         * we delay sending additional data until the state machine has read to the\n         * end and the last offset is determined.\n         */\n        private synchronized OptionalLong nextExpectedOffset() {\n            if (lastSent != null) {\n                OptionalLong lastSentOffset = lastSent.lastOffset();\n                if (lastSentOffset.isPresent()) {\n                    return OptionalLong.of(lastSentOffset.getAsLong() + 1);\n                } else {\n                    return OptionalLong.empty();\n                }\n            } else {\n                return OptionalLong.of(nextOffset);\n            }\n        }\n\n        /**\n         * This API is used when the Listener needs to be notified of a new snapshot. This happens\n         * when the context's next offset is less than the log start offset.\n         */\n        private void fireHandleSnapshot(SnapshotReader<T> reader) {\n            synchronized (this) {\n                nextOffset = reader.snapshotId().offset();\n                lastSent = null;\n            }\n\n            logger.debug(\"Notifying listener {} of snapshot {}\", listenerName(), reader.snapshotId());\n            listener.handleLoadSnapshot(reader);\n        }\n\n        /**\n         * This API is used for committed records that have been received through\n         * replication. In general, followers will write new data to disk before they\n         * know whether it has been committed. Rather than retaining the uncommitted\n         * data in memory, we let the state machine read the records from disk.\n         */\n        private void fireHandleCommit(long baseOffset, Records records) {\n            fireHandleCommit(\n                RecordsBatchReader.of(\n                    baseOffset,\n                    records,\n                    serde,\n                    BufferSupplier.create(),\n                    MAX_BATCH_SIZE_BYTES,\n                    this,\n                    true /* Validate batch CRC*/\n                )\n            );\n        }\n\n        /**\n         * This API is used for committed records originating from {@link #scheduleAppend(int, List)}\n         * or {@link #scheduleAtomicAppend(int, OptionalLong, List)} on this instance. In this case,\n         * we are able to save the original record objects, which saves the need to read them back\n         * from disk. This is a nice optimization for the leader which is typically doing more work\n         * than all of the * followers.\n         */\n        private void fireHandleCommit(\n            long baseOffset,\n            int epoch,\n            long appendTimestamp,\n            int sizeInBytes,\n            List<T> records\n        ) {\n            Batch<T> batch = Batch.data(baseOffset, epoch, appendTimestamp, sizeInBytes, records);\n            MemoryBatchReader<T> reader = MemoryBatchReader.of(Collections.singletonList(batch), this);\n            fireHandleCommit(reader);\n        }\n\n        private String listenerName() {\n            return KafkaRaftClient.listenerName(listener);\n        }\n\n        private void fireHandleCommit(BatchReader<T> reader) {\n            synchronized (this) {\n                this.lastSent = reader;\n            }\n            logger.debug(\n                \"Notifying listener {} of batch for baseOffset {} and lastOffset {}\",\n                listenerName(),\n                reader.baseOffset(),\n                reader.lastOffset()\n            );\n            listener.handleCommit(reader);\n        }\n\n        private void maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            if (shouldFireLeaderChange(leaderAndEpoch)) {\n                lastFiredLeaderChange = leaderAndEpoch;\n                logger.debug(\"Notifying listener {} of leader change {}\", listenerName(), leaderAndEpoch);\n                listener.handleLeaderChange(leaderAndEpoch);\n            }\n        }\n\n        private boolean shouldFireLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            if (leaderAndEpoch.equals(lastFiredLeaderChange)) {\n                return false;\n            } else if (leaderAndEpoch.epoch() > lastFiredLeaderChange.epoch()) {\n                return true;\n            } else {\n                return leaderAndEpoch.leaderId().isPresent() &&\n                    !lastFiredLeaderChange.leaderId().isPresent();\n            }\n        }\n\n        private void maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch, long epochStartOffset) {\n            // If this node is becoming the leader, then we can fire `handleLeaderChange` as soon\n            // as the listener has caught up to the start of the leader epoch. This guarantees\n            // that the state machine has seen the full committed state before it becomes\n            // leader and begins writing to the log.\n            //\n            // Note that the raft client doesn't need to compare nextOffset against the high-watermark\n            // to guarantee that the listener has caught up to the high-watermark. This is true because\n            // the only way nextOffset can be greater than epochStartOffset is for the leader to have\n            // established the new high-watermark (of at least epochStartOffset + 1) and for the listener\n            // to have consumed up to that new high-watermark.\n            if (shouldFireLeaderChange(leaderAndEpoch) && nextOffset() > epochStartOffset) {\n                lastFiredLeaderChange = leaderAndEpoch;\n                listener.handleLeaderChange(leaderAndEpoch);\n            }\n        }\n\n        public synchronized void onClose(BatchReader<T> reader) {\n            OptionalLong lastOffset = reader.lastOffset();\n\n            if (lastOffset.isPresent()) {\n                nextOffset = lastOffset.getAsLong() + 1;\n            }\n\n            if (lastSent == reader) {\n                lastSent = null;\n                wakeup();\n            }\n        }\n    }\n}",
                "methodCount": 136
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 13,
                "candidates": [
                    {
                        "lineStart": 1668,
                        "lineEnd": 1672,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method handleUnexpectedError to class NetworkChannel",
                        "description": "Move method handleUnexpectedError to org.apache.kafka.raft.NetworkChannel\nRationale: The handleUnexpectedError method processes errors related to RaftResponse.Inbound, which deals with network responses. NetworkChannel is a more suitable class as it encompasses network interface and communication, including the handling of errors that occur during these operations. This move ensures a more cohesive structure by localizing networking-related error handling in the same class, aligning responsibilities more logically.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 579,
                        "lineEnd": 587,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildVoteResponse to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildVoteResponse to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The buildVoteResponse method appears to be related to the process of building a vote response within a Raft context, which fits with the responsibilities of RaftMetadataLogCleanerManager. This class deals with Raft metadata and log cleaning, making it more aligned with quorum-related functionalities. Moving this method to RaftMetadataLogCleanerManager centralizes Raft-related logic and keeps the codebase organized and modular.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 540,
                        "lineEnd": 546,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method transitionToResigned to class RaftMetadataLogCleanerManager",
                        "description": "Move method transitionToResigned to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The transitionToResigned method involves operations related to quorum management and state transitions, which aligns more closely with high-level metadata and log management responsibilities. The RaftMetadataLogCleanerManager manages operations related to the Raft log, and transitioning to a resigned state likely involves metadata changes and clean-ups, making it a suitable class for this method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 569,
                        "lineEnd": 577,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method transitionToFollower to class Time",
                        "description": "Move method transitionToFollower to org.apache.kafka.common.utils.Time\nRationale: The 'transitionToFollower' method involves timestamp and timing-related parameters, such as 'currentTimeMs', which is closely related to the 'Time' class. The 'Time' class provides abstractions for working with time, making it the most relevant target class. The method's reliance on time and its operations align well with the functionalities offered by the 'Time' class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 548,
                        "lineEnd": 552,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method transitionToVoted to class NetworkChannel",
                        "description": "Move method transitionToVoted to org.apache.kafka.raft.NetworkChannel\nRationale: The method `transitionToVoted` involves transitioning a quorum state, potentially triggering leader change events, and resetting connections. These operations are deeply related to network communication and maintaining up-to-date connection states which makes `NetworkChannel` a suitable target class. This interface deals directly with network operations like sending requests and updating connection information, which aligns with the responsibilities encapsulated in `transitionToVoted`. The method relies on connectivity and managing network states, hence it is logical to place it in `NetworkChannel`.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1965,
                        "lineEnd": 1973,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method addQuorumLeader to class QuorumState",
                        "description": "Move method addQuorumLeader to org.apache.kafka.raft.QuorumState\nRationale: The 'addQuorumLeader' function directly interacts with the quorum to set the leader epoch and leader id. The QuorumState class manages the state of the quorum and contains methods related to leader management and election processes. Thus, this class is the most appropriate destination for this method as it logically groups the state management and leader-related functionalities in one place, ensuring that the quorum state management is centralized and cohesive.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1270,
                        "lineEnd": 1282,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method appendAsLeader to class ReplicatedLog",
                        "description": "Move method appendAsLeader to org.apache.kafka.raft.ReplicatedLog\nRationale: The `appendAsLeader` method strongly aligns with the responsibilities of the `ReplicatedLog` class. The method's purpose is to append records as a leader, which directly relates to the `appendAsLeader` function already defined within `ReplicatedLog`. Moreover, the `log.appendAsLeader(records, quorum.epoch())` call within the method indicates that it is performing operations on a replicated log. Moving this method to `ReplicatedLog` enhances cohesion, as the method manipulates log data and updates states pertinent to the log.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 926,
                        "lineEnd": 959,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildFetchResponse to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildFetchResponse to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The buildFetchResponse() method involves constructing a FetchResponseData, which seems highly relevant to log management and processing. Since RaftMetadataLogCleanerManager is already managing logs (i.e., cleaning logs), it would be more appropriate to add the method here, as there's high cohesion with the existing responsibilities of log management within RaftMetadataLogCleanerManager.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2555,
                        "lineEnd": 2557,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method voterNode to class KRaftControlRecordStateMachine",
                        "description": "Move method voterNode to org.apache.kafka.raft.internals.KRaftControlRecordStateMachine\nRationale: The method `voterNode` operates closely with the voter set, similar to other methods in KRaftControlRecordStateMachine like `voterSetAtOffset`, `lastVoterSet`, and `kraftVersionAtOffset`. This class tracks changes to the voter set which suggests that voterNode belongs to it, as the operations deal with managing and retrieving voter nodes, aligning with the responsibilities of KRaftControlRecordStateMachine.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 720,
                        "lineEnd": 728,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method strictExponentialElectionBackoffMs to class QuorumConfig",
                        "description": "Move method strictExponentialElectionBackoffMs to org.apache.kafka.raft.QuorumConfig\nRationale: The method strictExponentialElectionBackoffMs is closely tied to configuration parameters related to elections and backoff settings, which are defined within the QuorumConfig class. Specifically, it uses quorumConfig.electionBackoffMaxMs() indicating that the method is inherently linked to the quorum configuration logic. By moving the method to QuorumConfig, we enhance cohesion as the method and its related configuration settings are encapsulated within the same class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1872,
                        "lineEnd": 1882,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildEndQuorumEpochRequest to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildEndQuorumEpochRequest to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The `buildEndQuorumEpochRequest` method involves operations related to quorum and the state of a log, which are closely tied to the management and cleaning processes of metadata logs in a Raft consensus system. The `RaftMetadataLogCleanerManager` class, which manages and cleans logs based on timers, is a logical place for this method. This ensures that all log-related operations are consolidated and encapsulated within a single manager class, making the codebase more organized and cohesive.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 730,
                        "lineEnd": 737,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildBeginQuorumEpochResponse to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildBeginQuorumEpochResponse to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The buildBeginQuorumEpochResponse method is highly specific to the context of a Raft quorum and log metadata management, as it involves quorum epochs, topic partitions, and error handling which are closely related to Raft log cleaning and management. The RaftMetadataLogCleanerManager class, despite its name, deals with log cleaning within a Raft-based setup, making it an appropriate class to group related functionalities that handle similar concepts such as epochs and logs. This method doesn't fit into the MemoryPool or Time interfaces, which are more generic and unrelated to Raft-specific functionalities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 819,
                        "lineEnd": 826,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildEndQuorumEpochResponse to class RaftMetadataLogCleanerManager",
                        "description": "Move method buildEndQuorumEpochResponse to org.apache.kafka.raft.KafkaRaftClient.RaftMetadataLogCleanerManager\nRationale: The buildEndQuorumEpochResponse method is specific to handling responses related to quorum epochs, which fits closely with the functionality of the RaftMetadataLogCleanerManager class. This class has access to necessary components such as logger and timer, and could easily be extended to manage quorum-related operations, making it a suitable place for the method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "latestSnapshot",
                            "method_signature": "private Optional<SnapshotReader<T>> latestSnapshot()",
                            "target_class": "ReplicatedLog",
                            "rationale": "The latestSnapshot method is accessing log-specific functionality, particularly reading the latest snapshot from the log. This behavior is more appropriate in the ReplicatedLog class."
                        },
                        {
                            "method_name": "hasValidTopicPartition",
                            "method_signature": "private boolean hasValidTopicPartition(Object request, TopicPartition topicPartition)",
                            "target_class": "RaftUtil",
                            "rationale": "The hasValidTopicPartition method is a validation utility that checks the validity of the topic partition in the requests. Such a utility method is better placed in a utility or helper class like RaftUtil, which can be accessed globally without being attached to a specific class like KafkaRaftClient."
                        },
                        {
                            "method_name": "buildBeginQuorumEpochResponse",
                            "method_signature": "private BeginQuorumEpochResponseData buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "ResponseBuilderUtil",
                            "rationale": "The buildBeginQuorumEpochResponse method is responsible for building response data, which qualifies it more as a utility method. Therefore, it should be moved to a utility class aimed at constructing response data, such as ResponseBuilderUtil."
                        },
                        {
                            "method_name": "buildVoteRequest",
                            "method_signature": "private VoteRequestData buildVoteRequest()",
                            "target_class": "RequestBuilderUtil",
                            "rationale": "The buildVoteRequest method constructs a specific request object. This is more appropriately placed in a utility class responsible for building various kinds of requests, like RequestBuilderUtil."
                        },
                        {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private boolean handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "ErrorHandlingUtil",
                            "rationale": "The handleUnexpectedError method processes error responses. Such error handling capabilities are commonly found in a dedicated utility class for error handling, making it more rational to place this method in ErrorHandlingUtil."
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": "public void initialize(Map<Integer, InetSocketAddress> voterAddresses, String listenerName, QuorumStateStore quorumStateStore, Metrics metrics)",
                            "target_class": "QuorumState",
                            "rationale": "The initialize method primarily deals with setting up the QuorumState based on various parameters like voter addresses and state stores. Therefore, it should be moved to the QuorumState class, which it is initializing."
                        }
                    ],
                    "llm_response_time": 8786
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "handleVoteRequest",
                            "method_signature": "private VoteResponseData handleVoteRequest(RaftRequest.Inbound requestMetadata)",
                            "target_class": "VoteHandler",
                            "rationale": "This method handles vote requests and should be grouped with other voting logic for better modularity, testability, and maintainability."
                        },
                        {
                            "method_name": "handleBeginQuorumEpochRequest",
                            "method_signature": "private BeginQuorumEpochResponseData handleBeginQuorumEpochRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "QuorumEpochHandler",
                            "rationale": "This method is specific to handling BeginQuorumEpoch requests, and it would be better placed in a specialized handler for clearer separation of concerns."
                        },
                        {
                            "method_name": "handleEndQuorumEpochRequest",
                            "method_signature": "private EndQuorumEpochResponseData handleEndQuorumEpochRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "QuorumEpochHandler",
                            "rationale": "This method is specific to handling EndQuorumEpoch requests, and placing it in a dedicated handler class improves clarity and separation of concerns."
                        },
                        {
                            "method_name": "handleFetchRequest",
                            "method_signature": "private CompletableFuture<FetchResponseData> handleFetchRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "FetchHandler",
                            "rationale": "This method deals with fetching data and belongs in a class dedicated to handling fetch logic, improving modularity and maintainability."
                        },
                        {
                            "method_name": "handleFetchSnapshotRequest",
                            "method_signature": "private FetchSnapshotResponseData handleFetchSnapshotRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "FetchSnapshotHandler",
                            "rationale": "This method specifically handles snapshot fetch requests, and moving it to a dedicated handler class will encapsulate snapshot logic better."
                        },
                        {
                            "method_name": "handleDescribeQuorumRequest",
                            "method_signature": "private DescribeQuorumResponseData handleDescribeQuorumRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "QuorumHandler",
                            "rationale": "This method is specialized for handling quorum description requests, and should reside in a class focused on quorum operations for better encapsulation."
                        },
                        {
                            "method_name": "handleRequest",
                            "method_signature": "private void handleRequest(RaftRequest.Inbound request, long currentTimeMs)",
                            "target_class": "RequestRouter",
                            "rationale": "This method routes various inbound requests to their respective handlers. It should be moved to a dedicated request routing class to improve code organization and separation of concerns."
                        },
                        {
                            "method_name": "maybeHandleCommonResponse",
                            "method_signature": "private Optional<Boolean> maybeHandleCommonResponse(Errors error, OptionalInt leaderId, int epoch, long currentTimeMs)",
                            "target_class": "ResponseHandler",
                            "rationale": "This method handles common response errors and logic. It should be grouped with other response handling logic in a specialized class to improve modularity and maintainability."
                        }
                    ],
                    "llm_response_time": 9731
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "handleVoteRequest",
                            "method_signature": "private VoteResponseData handleVoteRequest(RaftRequest.Inbound requestMetadata)",
                            "target_class": "ElectionManager",
                            "rationale": "Method is related to handling vote requests, which is generally an activity that ElectionManager should oversee."
                        },
                        {
                            "method_name": "handleVoteResponse",
                            "method_signature": "private boolean handleVoteResponse(RaftResponse.Inbound responseMetadata, long currentTimeMs)",
                            "target_class": "ElectionManager",
                            "rationale": "This method handles the response to vote requests. It's more appropriate to keep vote-related logic within ElectionManager."
                        },
                        {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private VoteResponseData buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "MessageBuilder",
                            "rationale": "Method constructs vote responses. Such operation should be encapsulated in a MessageBuilder to encapsulate message construction."
                        },
                        {
                            "method_name": "buildBeginQuorumEpochResponse",
                            "method_signature": "private BeginQuorumEpochResponseData buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "MessageBuilder",
                            "rationale": "Method builds a quorum epoch response message, relevant for MessageBuilder that deals with creating specific responses."
                        },
                        {
                            "method_name": "handleBeginQuorumEpochRequest",
                            "method_signature": "private BeginQuorumEpochResponseData handleBeginQuorumEpochRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "QuorumManager",
                            "rationale": "Handling quorum epoch requests is a specific activity related to quorum management, so this should belong to QuorumManager."
                        },
                        {
                            "method_name": "handleFetchRequest",
                            "method_signature": "private CompletableFuture<FetchResponseData> handleFetchRequest(RaftRequest.Inbound requestMetadata, long currentTimeMs)",
                            "target_class": "ReplicationManager",
                            "rationale": "This method primarily deals with fetching log entries, which is typical logic found within the domain of replication tasks."
                        },
                        {
                            "method_name": "tryCompleteFetchRequest",
                            "method_signature": "private FetchResponseData tryCompleteFetchRequest(int replicaId, FetchRequestData.FetchPartition request, long currentTimeMs)",
                            "target_class": "ReplicationManager",
                            "rationale": "Attempting to complete a fetch request pertains to replication tasks, making ReplicationManager a more appropriate class."
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private FetchResponseData buildFetchResponse(Errors error, Records records, ValidOffsetAndEpoch validOffsetAndEpoch, Optional<LogOffsetMetadata> highWatermark)",
                            "target_class": "MessageBuilder",
                            "rationale": "Building fetch responses should belong to a class responsible for constructing various types of messages."
                        },
                        {
                            "method_name": "updateFollowerHighWatermark",
                            "method_signature": "private void updateFollowerHighWatermark(FollowerState state, OptionalLong highWatermarkOpt)",
                            "target_class": "FollowerManager",
                            "rationale": "Updating the high watermark for followers falls under the management of a follower's state, which is better handled by FollowerManager."
                        },
                        {
                            "method_name": "updateLeaderEndOffsetAndTimestamp",
                            "method_signature": "private void updateLeaderEndOffsetAndTimestamp(LeaderState<T> state, long currentTimeMs)",
                            "target_class": "LeaderManager",
                            "rationale": "This method is specific to maintaining leader state, which should be orchestrated by a LeaderManager class."
                        },
                        {
                            "method_name": "onUpdateLeaderHighWatermark",
                            "method_signature": "private void onUpdateLeaderHighWatermark(LeaderState<T> state, long currentTimeMs)",
                            "target_class": "LeaderManager",
                            "rationale": "Managing the leader's high watermark is a specific domain of the leader's activities, best handled by LeaderManager."
                        }
                    ],
                    "llm_response_time": 10878
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "updateFollowerHighWatermark",
                            "method_signature": "private updateFollowerHighWatermark(\n        FollowerState state,\n        OptionalLong highWatermarkOpt\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateLeaderEndOffsetAndTimestamp",
                            "method_signature": "private updateLeaderEndOffsetAndTimestamp(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUpdateLeaderHighWatermark",
                            "method_signature": "private onUpdateLeaderHighWatermark(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateListenersProgress",
                            "method_signature": "private updateListenersProgress(long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "latestSnapshot",
                            "method_signature": "private latestSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireHandleCommit",
                            "method_signature": "private maybeFireHandleCommit(long baseOffset, int epoch, long appendTimestamp, int sizeInBytes, List<T> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderState<T> state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": "public initialize(\n        Map<Integer, InetSocketAddress> voterAddresses,\n        String listenerName,\n        QuorumStateStore quorumStateStore,\n        Metrics metrics\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endOffset",
                            "method_signature": "private endOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetConnections",
                            "method_signature": "private resetConnections()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeLeader",
                            "method_signature": "private onBecomeLeader(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flushLeaderLog",
                            "method_signature": "private flushLeaderLog(LeaderState<T> state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeTransitionToLeader",
                            "method_signature": "private maybeTransitionToLeader(CandidateState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeCandidate",
                            "method_signature": "private onBecomeCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToCandidate",
                            "method_signature": "private transitionToCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToUnattached",
                            "method_signature": "private transitionToUnattached(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToResigned",
                            "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToVoted",
                            "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeFollower",
                            "method_signature": "private onBecomeFollower(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToFollower",
                            "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleVoteRequest",
                            "method_signature": "private handleVoteRequest(\n        RaftRequest.Inbound requestMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleVoteResponse",
                            "method_signature": "private handleVoteResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "binaryExponentialElectionBackoffMs",
                            "method_signature": "private binaryExponentialElectionBackoffMs(int retries)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "strictExponentialElectionBackoffMs",
                            "method_signature": "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBeginQuorumEpochResponse",
                            "method_signature": "private buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleBeginQuorumEpochRequest",
                            "method_signature": "private handleBeginQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleBeginQuorumEpochResponse",
                            "method_signature": "private handleBeginQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochResponse",
                            "method_signature": "private buildEndQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleEndQuorumEpochRequest",
                            "method_signature": "private handleEndQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endEpochElectionBackoff",
                            "method_signature": "private endEpochElectionBackoff(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleEndQuorumEpochResponse",
                            "method_signature": "private handleEndQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchRequest",
                            "method_signature": "private handleFetchRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tryCompleteFetchRequest",
                            "method_signature": "private tryCompleteFetchRequest(\n        int replicaId,\n        FetchRequestData.FetchPartition request,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "listenerName",
                            "method_signature": "private static listenerName(Listener<?> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchResponse",
                            "method_signature": "private handleFetchResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsFollower",
                            "method_signature": "private appendAsFollower(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsLeader",
                            "method_signature": "private appendAsLeader(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleDescribeQuorumRequest",
                            "method_signature": "private handleDescribeQuorumRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchSnapshotRequest",
                            "method_signature": "private handleFetchSnapshotRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchSnapshotResponse",
                            "method_signature": "private handleFetchSnapshotResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasConsistentLeader",
                            "method_signature": "private hasConsistentLeader(int epoch, OptionalInt leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeHandleCommonResponse",
                            "method_signature": "private maybeHandleCommonResponse(\n        Errors error,\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeTransition",
                            "method_signature": "private maybeTransition(\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleResponse",
                            "method_signature": "private handleResponse(RaftResponse.Inbound response, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onResponseResult",
                            "method_signature": "private onResponseResult(RaftResponse.Inbound response, long currentTimeMs, boolean handledSuccessfully, ConnectionState connection)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateVoterOnlyRequest",
                            "method_signature": "private validateVoterOnlyRequest(int remoteNodeId, int requestEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateLeaderOnlyRequest",
                            "method_signature": "private validateLeaderOnlyRequest(int requestEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleRequest",
                            "method_signature": "private handleRequest(RaftRequest.Inbound request, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleInboundMessage",
                            "method_signature": "private handleInboundMessage(RaftMessage message, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendRequest",
                            "method_signature": "private maybeSendRequest(\n        long currentTimeMs,\n        int destinationId,\n        Supplier<ApiMessage> requestSupplier\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochRequest",
                            "method_signature": "private buildEndQuorumEpochRequest(\n        ResignedState state\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBeginQuorumEpochRequest",
                            "method_signature": "private buildBeginQuorumEpochRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteRequest",
                            "method_signature": "private buildVoteRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchRequest",
                            "method_signature": "private buildFetchRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendAnyVoterFetch",
                            "method_signature": "private maybeSendAnyVoterFetch(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchSnapshotRequest",
                            "method_signature": "private buildFetchSnapshotRequest(OffsetAndEpoch snapshotId, long snapshotSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addQuorumLeader",
                            "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendBatch",
                            "method_signature": "private appendBatch(\n        LeaderState<T> state,\n        BatchAccumulator.CompletedBatch<T> batch,\n        long appendTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAppendBatches",
                            "method_signature": "private maybeAppendBatches(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollResigned",
                            "method_signature": "private pollResigned(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollLeader",
                            "method_signature": "private pollLeader(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendVoteRequests",
                            "method_signature": "private maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollCandidate",
                            "method_signature": "private pollCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollower",
                            "method_signature": "private pollFollower(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollowerAsVoter",
                            "method_signature": "private pollFollowerAsVoter(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollowerAsObserver",
                            "method_signature": "private pollFollowerAsObserver(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendFetchOrFetchSnapshot",
                            "method_signature": "private maybeSendFetchOrFetchSnapshot(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollVoted",
                            "method_signature": "private pollVoted(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattached",
                            "method_signature": "private pollUnattached(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattachedAsVoter",
                            "method_signature": "private pollUnattachedAsVoter(UnattachedState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattachedAsObserver",
                            "method_signature": "private pollUnattachedAsObserver(UnattachedState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollCurrentState",
                            "method_signature": "private pollCurrentState(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollListeners",
                            "method_signature": "private pollListeners()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processRegistration",
                            "method_signature": "private processRegistration(Registration<T> registration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCompleteShutdown",
                            "method_signature": "private maybeCompleteShutdown(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeClean",
                            "method_signature": "public maybeClean(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "wakeup",
                            "method_signature": "private wakeup()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handle",
                            "method_signature": "public handle(RaftRequest.Inbound request)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "poll",
                            "method_signature": "public poll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "append",
                            "method_signature": "private append(int epoch, List<T> records, OptionalLong requiredBaseOffset, boolean isAtomic)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int id, String listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "update",
                            "method_signature": "public update(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasTimedOut",
                            "method_signature": "public hasTimedOut()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "remainingTimeMs",
                            "method_signature": "public remainingTimeMs()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "nextExpectedOffset",
                            "method_signature": "private synchronized nextExpectedOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleSnapshot",
                            "method_signature": "private fireHandleSnapshot(SnapshotReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shouldFireLeaderChange",
                            "method_signature": "private shouldFireLeaderChange(LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch, long epochStartOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onClose",
                            "method_signature": "public synchronized onClose(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "latestSnapshot",
                            "method_signature": "private latestSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToResigned",
                            "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToFollower",
                            "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToVoted",
                            "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addQuorumLeader",
                            "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsLeader",
                            "method_signature": "private appendAsLeader(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int id, String listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "strictExponentialElectionBackoffMs",
                            "method_signature": "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochRequest",
                            "method_signature": "private buildEndQuorumEpochRequest(\n        ResignedState state\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBeginQuorumEpochResponse",
                            "method_signature": "private buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochResponse",
                            "method_signature": "private buildEndQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "public failWithTimeout()": {
                        "first": {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.24602961244206256
                    },
                    "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)": {
                        "first": {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3149728190830739
                    },
                    "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)": {
                        "first": {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3179687607578866
                    },
                    "private latestSnapshot()": {
                        "first": {
                            "method_name": "latestSnapshot",
                            "method_signature": "private latestSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3323838068538221
                    },
                    "private transitionToResigned(List<Integer> preferredSuccessors)": {
                        "first": {
                            "method_name": "transitionToResigned",
                            "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3338263348997506
                    },
                    "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )": {
                        "first": {
                            "method_name": "transitionToFollower",
                            "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3354122236081435
                    },
                    "private transitionToVoted(ReplicaKey candidateKey, int epoch)": {
                        "first": {
                            "method_name": "transitionToVoted",
                            "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.34056486345018444
                    },
                    "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )": {
                        "first": {
                            "method_name": "addQuorumLeader",
                            "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.34657970773074437
                    },
                    "private appendAsLeader(\n        Records records\n    )": {
                        "first": {
                            "method_name": "appendAsLeader",
                            "method_signature": "private appendAsLeader(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3468167180000903
                    },
                    "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )": {
                        "first": {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3543411053406282
                    },
                    "public voterNode(int id, String listener)": {
                        "first": {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int id, String listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.356046106067574
                    },
                    "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)": {
                        "first": {
                            "method_name": "strictExponentialElectionBackoffMs",
                            "method_signature": "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3581593106420744
                    },
                    "private buildEndQuorumEpochRequest(\n        ResignedState state\n    )": {
                        "first": {
                            "method_name": "buildEndQuorumEpochRequest",
                            "method_signature": "private buildEndQuorumEpochRequest(\n        ResignedState state\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3652902447243953
                    },
                    "private buildBeginQuorumEpochResponse(Errors partitionLevelError)": {
                        "first": {
                            "method_name": "buildBeginQuorumEpochResponse",
                            "method_signature": "private buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.36668864144936714
                    },
                    "private buildEndQuorumEpochResponse(Errors partitionLevelError)": {
                        "first": {
                            "method_name": "buildEndQuorumEpochResponse",
                            "method_signature": "private buildEndQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.36668864144936714
                    }
                },
                "voyage": {
                    "private resetConnections()": {
                        "first": {
                            "method_name": "resetConnections",
                            "method_signature": "private resetConnections()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.17377593182990644
                    },
                    "private static listenerName(Listener<?> listener)": {
                        "first": {
                            "method_name": "listenerName",
                            "method_signature": "private static listenerName(Listener<?> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.19122205430980083
                    },
                    "public update(long currentTimeMs)": {
                        "first": {
                            "method_name": "update",
                            "method_signature": "public update(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.20462006414933237
                    },
                    "public hasTimedOut()": {
                        "first": {
                            "method_name": "hasTimedOut",
                            "method_signature": "public hasTimedOut()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.22183578846651397
                    },
                    "private processRegistration(Registration<T> registration)": {
                        "first": {
                            "method_name": "processRegistration",
                            "method_signature": "private processRegistration(Registration<T> registration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.25642377535248373
                    },
                    "public maybeClean(long currentTimeMs)": {
                        "first": {
                            "method_name": "maybeClean",
                            "method_signature": "public maybeClean(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.2604874486729821
                    },
                    "public remainingTimeMs()": {
                        "first": {
                            "method_name": "remainingTimeMs",
                            "method_signature": "public remainingTimeMs()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.27250738409945147
                    },
                    "private wakeup()": {
                        "first": {
                            "method_name": "wakeup",
                            "method_signature": "private wakeup()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.2892190156348646
                    },
                    "public failWithTimeout()": {
                        "first": {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.2990096310222643
                    },
                    "private binaryExponentialElectionBackoffMs(int retries)": {
                        "first": {
                            "method_name": "binaryExponentialElectionBackoffMs",
                            "method_signature": "private binaryExponentialElectionBackoffMs(int retries)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3597007810466155
                    },
                    "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)": {
                        "first": {
                            "method_name": "strictExponentialElectionBackoffMs",
                            "method_signature": "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3621830937015345
                    },
                    "private onResponseResult(RaftResponse.Inbound response, long currentTimeMs, boolean handledSuccessfully, ConnectionState connection)": {
                        "first": {
                            "method_name": "onResponseResult",
                            "method_signature": "private onResponseResult(RaftResponse.Inbound response, long currentTimeMs, boolean handledSuccessfully, ConnectionState connection)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3690633050953012
                    },
                    "public synchronized onClose(BatchReader<T> reader)": {
                        "first": {
                            "method_name": "onClose",
                            "method_signature": "public synchronized onClose(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3722942058132097
                    },
                    "private maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    )": {
                        "first": {
                            "method_name": "maybeSendVoteRequests",
                            "method_signature": "private maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3856404561833576
                    },
                    "private fireHandleCommit(BatchReader<T> reader)": {
                        "first": {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4074451956890105
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private transitionToResigned(List<Integer> preferredSuccessors)",
                        "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                        "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                        "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                        "private buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                        "private buildEndQuorumEpochResponse(Errors partitionLevelError)",
                        "private latestSnapshot()",
                        "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                        "public voterNode(int id, String listener)",
                        "public failWithTimeout()"
                    ],
                    "llm_response_time": 11095
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                        "private transitionToResigned(List<Integer> preferredSuccessors)",
                        "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                        "public failWithTimeout()",
                        "private latestSnapshot()"
                    ],
                    "llm_response_time": 6717
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                        "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                        "public failWithTimeout()"
                    ],
                    "llm_response_time": 4557
                },
                "voyage": {
                    "priority_method_names": [
                        "private processRegistration(Registration<T> registration)",
                        "private fireHandleCommit(BatchReader<T> reader)",
                        "private onResponseResult(RaftResponse.Inbound response, long currentTimeMs, boolean handledSuccessfully, ConnectionState connection)",
                        "private wakeup()",
                        "private maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    )",
                        "private static listenerName(Listener<?> listener)",
                        "private resetConnections()",
                        "private binaryExponentialElectionBackoffMs(int retries)",
                        "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                        "public synchronized onClose(BatchReader<T> reader)",
                        "public update(long currentTimeMs)",
                        "public hasTimedOut()",
                        "public remainingTimeMs()",
                        "public failWithTimeout()",
                        "public maybeClean(long currentTimeMs)"
                    ],
                    "llm_response_time": 12274
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private processRegistration(Registration<T> registration)",
                        "private resetConnections()",
                        "private static listenerName(Listener<?> listener)",
                        "public hasTimedOut()",
                        "public update(long currentTimeMs)"
                    ],
                    "llm_response_time": 6793
                },
                "voyage-3": {
                    "priority_method_names": [
                        "public update(long currentTimeMs)",
                        "private resetConnections()",
                        "private static listenerName(Listener<?> listener)"
                    ],
                    "llm_response_time": 5461
                }
            },
            "targetClassMap": {
                "failWithTimeout": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "handleUnexpectedError": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10577027387372219
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.05594542388644595
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04352434006443845
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14159846508095775
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.042691922456284
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "NetworkChannel",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "buildVoteResponse": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.08956419427010578
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03230010554076729
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.039564477010326005
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14987850520743612
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.03286425722981093
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2963188789948769
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "Time",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "latestSnapshot": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "transitionToResigned": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.13841739114470894
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.1615005277038364
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.08233688458905682
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14306584587982538
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.057512450152169124
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2798567190507171
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "NetworkChannel",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "transitionToFollower": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.11399079270740736
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.11305036939268549
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.045980338147135626
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14306584587982538
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.03286425722981093
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.3292431988831966
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "transitionToVoted": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.09972117053875637
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.11867816581938534
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04583712596673114
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14184408272449836
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.04025033049430489
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.3427530813818426
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "NetworkChannel",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "addQuorumLeader": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10257232541543393
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03538299282895168
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04216934244541642
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14925788761321498
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.02700074253062953
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.3246010684802756
                        },
                        {
                            "class_name": "QuorumState",
                            "similarity_score": 0.275797464034139
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "QuorumState",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "appendAsLeader": {
                    "target_classes": [
                        {
                            "class_name": "Records",
                            "similarity_score": 0.11837617651007314
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1077851268021598
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03800743117926434
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.1434409434297192
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.10020543130080872
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.03383735833851262
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ReplicatedLog",
                        "Records",
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "buildFetchResponse": {
                    "target_classes": [
                        {
                            "class_name": "Records",
                            "similarity_score": 0.026094068112009733
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.10249173027591259
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.04065846966124125
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04361097561378978
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14750006307045013
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.024821164819372063
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2983983352366476
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool",
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "voterNode": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1197786705541612
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.08263686767584087
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.047875316664207634
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.24401392662080099
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.04204009910334443
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2948185205998629
                        },
                        {
                            "class_name": "KRaftControlRecordStateMachine",
                            "similarity_score": 0.2590633629193959
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "KRaftControlRecordStateMachine",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "strictExponentialElectionBackoffMs": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.12533231025314606
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.0806259369670165
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.08763740600172468
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.17288849356223998
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.044435161714545535
                        },
                        {
                            "class_name": "QuorumConfig",
                            "similarity_score": 0.4315435875337899
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.2397035379720598
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "QuorumConfig",
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "buildEndQuorumEpochRequest": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.09931522702893585
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03425943549137658
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.040830290252903835
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14451832825402
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.02614335653909062
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.3142936330963102
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool",
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "buildBeginQuorumEpochResponse": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1053899017621118
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03800743117926434
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04278063225096888
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.16032869008129397
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.029003450004439386
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.34867747990172687
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool",
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "buildEndQuorumEpochResponse": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1053899017621118
                        },
                        {
                            "class_name": "NetworkChannel",
                            "similarity_score": 0.03800743117926434
                        },
                        {
                            "class_name": "ReplicatedLog",
                            "similarity_score": 0.04278063225096888
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.16032869008129397
                        },
                        {
                            "class_name": "RaftMessageQueue",
                            "similarity_score": 0.029003450004439386
                        },
                        {
                            "class_name": "RaftMetadataLogCleanerManager",
                            "similarity_score": 0.34867747990172687
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RaftMetadataLogCleanerManager",
                        "MemoryPool",
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 3,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "638844f833b165d6f9ca52c173858d26b7254fac",
        "url": "https://github.com/apache/kafka/commit/638844f833b165d6f9ca52c173858d26b7254fac",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method private append(producerId long, producerEpoch short, verificationGuard VerificationGuard, records List<U>, replay boolean, event DeferredEvent) : void extracted from public run() : void in class org.apache.kafka.coordinator.group.runtime.CoordinatorRuntime.CoordinatorWriteEvent & moved to class org.apache.kafka.coordinator.group.runtime.CoordinatorRuntime.CoordinatorContext",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 773,
                    "endLine": 883,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public run() : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 788,
                    "endLine": 788,
                    "startColumn": 25,
                    "endColumn": 104,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 790,
                    "endLine": 790,
                    "startColumn": 29,
                    "endColumn": 93,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 792,
                    "endLine": 792,
                    "startColumn": 29,
                    "endColumn": 44,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 802,
                    "endLine": 802,
                    "startColumn": 25,
                    "endColumn": 66,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 849,
                    "endLine": 851,
                    "startColumn": 37,
                    "endColumn": 85,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 830,
                    "endLine": 835,
                    "startColumn": 37,
                    "endColumn": 39,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 789,
                    "endLine": 793,
                    "startColumn": 25,
                    "endColumn": 26,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 826,
                    "endLine": 836,
                    "startColumn": 33,
                    "endColumn": 34,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 823,
                    "endLine": 853,
                    "startColumn": 29,
                    "endColumn": 30,
                    "codeElementType": "FOR_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 805,
                    "endLine": 877,
                    "startColumn": 25,
                    "endColumn": 26,
                    "codeElementType": "TRY_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 872,
                    "endLine": 875,
                    "startColumn": 27,
                    "endColumn": 26,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 784,
                    "endLine": 878,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 789,
                    "endLine": 791,
                    "startColumn": 56,
                    "endColumn": 26,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 791,
                    "endLine": 793,
                    "startColumn": 32,
                    "endColumn": 26,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 826,
                    "endLine": 836,
                    "startColumn": 61,
                    "endColumn": 34,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 823,
                    "endLine": 853,
                    "startColumn": 79,
                    "endColumn": 30,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 784,
                    "endLine": 794,
                    "startColumn": 53,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 794,
                    "endLine": 878,
                    "startColumn": 28,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 848,
                    "endLine": 852,
                    "startColumn": 40,
                    "endColumn": 34,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 869,
                    "endLine": 871,
                    "startColumn": 36,
                    "endColumn": 30,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 871,
                    "endLine": 1008,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "private append(producerId long, producerEpoch short, verificationGuard VerificationGuard, records List<U>, replay boolean, event DeferredEvent) : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 902,
                    "endLine": 902,
                    "startColumn": 21,
                    "endColumn": 92,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 904,
                    "endLine": 904,
                    "startColumn": 25,
                    "endColumn": 82,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 906,
                    "endLine": 906,
                    "startColumn": 25,
                    "endColumn": 46,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 912,
                    "endLine": 912,
                    "startColumn": 17,
                    "endColumn": 58,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 951,
                    "endLine": 953,
                    "startColumn": 21,
                    "endColumn": 82,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 980,
                    "endLine": 985,
                    "startColumn": 29,
                    "endColumn": 31,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 903,
                    "endLine": 907,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 975,
                    "endLine": 987,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 976,
                    "endLine": 986,
                    "startColumn": 25,
                    "endColumn": 26,
                    "codeElementType": "FOR_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 973,
                    "endLine": 1000,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "TRY_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 994,
                    "endLine": 1000,
                    "startColumn": 19,
                    "endColumn": 18,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 895,
                    "endLine": 1007,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 903,
                    "endLine": 905,
                    "startColumn": 52,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 905,
                    "endLine": 907,
                    "startColumn": 28,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 975,
                    "endLine": 987,
                    "startColumn": 33,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 976,
                    "endLine": 986,
                    "startColumn": 66,
                    "endColumn": 26,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 895,
                    "endLine": 909,
                    "startColumn": 36,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 909,
                    "endLine": 1007,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 950,
                    "endLine": 954,
                    "startColumn": 77,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 956,
                    "endLine": 968,
                    "startColumn": 70,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1246,
                    "endLine": 1276,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public run() : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1258,
                    "endLine": 1265,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "context.append(producerId,producerEpoch,verificationGuard,result.records(),result.replayRecords(),this)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 892,
                    "endLine": 892,
                    "startColumn": 17,
                    "endColumn": 97,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 919,
                    "endLine": 919,
                    "startColumn": 21,
                    "endColumn": 41,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 923,
                    "endLine": 928,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 931,
                    "endLine": 931,
                    "startColumn": 17,
                    "endColumn": 86,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 933,
                    "endLine": 937,
                    "startColumn": 21,
                    "endColumn": 24,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 941,
                    "endLine": 945,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 961,
                    "endLine": 961,
                    "startColumn": 21,
                    "endColumn": 41,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 962,
                    "endLine": 967,
                    "startColumn": 21,
                    "endColumn": 23,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 971,
                    "endLine": 971,
                    "startColumn": 17,
                    "endColumn": 56,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 991,
                    "endLine": 991,
                    "startColumn": 25,
                    "endColumn": 61,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 992,
                    "endLine": 992,
                    "startColumn": 25,
                    "endColumn": 51,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 995,
                    "endLine": 995,
                    "startColumn": 21,
                    "endColumn": 97,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 999,
                    "endLine": 999,
                    "startColumn": 21,
                    "endColumn": 41,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1006,
                    "endLine": 1006,
                    "startColumn": 17,
                    "endColumn": 55,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 900,
                    "endLine": 900,
                    "startColumn": 21,
                    "endColumn": 60,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 891,
                    "endLine": 893,
                    "startColumn": 51,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 891,
                    "endLine": 893,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 899,
                    "endLine": 908,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 916,
                    "endLine": 920,
                    "startColumn": 63,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 916,
                    "endLine": 920,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 932,
                    "endLine": 938,
                    "startColumn": 42,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 932,
                    "endLine": 938,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "ENHANCED_FOR_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 950,
                    "endLine": 954,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 990,
                    "endLine": 993,
                    "startColumn": 65,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 990,
                    "endLine": 993,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "ENHANCED_FOR_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 956,
                    "endLine": 968,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 899,
                    "endLine": 901,
                    "startColumn": 43,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 901,
                    "endLine": 908,
                    "startColumn": 24,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 583,
        "extraction_results": {
            "success": true,
            "newCommitHash": "009fda06cd29dc3f6fa3a6a31a7b34eccf3695a2",
            "newBranchName": "extract-append-run-39ffdea"
        },
        "telemetry": {
            "id": "513bfa98-4f57-4039-9c0d-5b51892e2cb1",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 287,
                "lineStart": 636,
                "lineEnd": 922,
                "bodyLineStart": 636,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                "sourceCode": "/**\n     * A coordinator event that modifies the coordinator state.\n     *\n     * @param <T> The type of the response.\n     */\n    class CoordinatorWriteEvent<T> implements CoordinatorEvent, DeferredEvent {\n        /**\n         * The topic partition that this write event is applied to.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The operation name.\n         */\n        final String name;\n\n        /**\n         * The transactional id.\n         */\n        final String transactionalId;\n\n        /**\n         * The producer id.\n         */\n        final long producerId;\n\n        /**\n         * The producer epoch.\n         */\n        final short producerEpoch;\n\n        /**\n         * The verification guard.\n         */\n        final VerificationGuard verificationGuard;\n\n        /**\n         * The write operation to execute.\n         */\n        final CoordinatorWriteOperation<S, T, U> op;\n\n        /**\n         * The future that will be completed with the response\n         * generated by the write operation or an error.\n         */\n        final CompletableFuture<T> future;\n\n        /**\n         * Timeout value for the write operation.\n         */\n        final Duration writeTimeout;\n\n        /**\n         * The operation timeout.\n         */\n        private OperationTimeout operationTimeout = null;\n\n        /**\n         * The result of the write operation. It could be null\n         * if an exception is thrown before it is assigned.\n         */\n        CoordinatorResult<T, U> result;\n\n        /**\n         * The time this event was created.\n         */\n        private final long createdTimeMs;\n\n        /**\n         * Constructor.\n         *\n         * @param name                  The operation name.\n         * @param tp                    The topic partition that the operation is applied to.\n         * @param writeTimeout          The write operation timeout\n         * @param op                    The write operation.\n         */\n        CoordinatorWriteEvent(\n            String name,\n            TopicPartition tp,\n            Duration writeTimeout,\n            CoordinatorWriteOperation<S, T, U> op\n        ) {\n            this(\n                name,\n                tp,\n                null,\n                RecordBatch.NO_PRODUCER_ID,\n                RecordBatch.NO_PRODUCER_EPOCH,\n                VerificationGuard.SENTINEL,\n                writeTimeout,\n                op\n            );\n        }\n\n        /**\n         * Constructor.\n         *\n         * @param name                      The operation name.\n         * @param tp                        The topic partition that the operation is applied to.\n         * @param transactionalId           The transactional id.\n         * @param producerId                The producer id.\n         * @param producerEpoch             The producer epoch.\n         * @param verificationGuard         The verification guard.\n         * @param writeTimeout              The write operation timeout\n         * @param op                        The write operation.\n         */\n        CoordinatorWriteEvent(\n            String name,\n            TopicPartition tp,\n            String transactionalId,\n            long producerId,\n            short producerEpoch,\n            VerificationGuard verificationGuard,\n            Duration writeTimeout,\n            CoordinatorWriteOperation<S, T, U> op\n        ) {\n            this.tp = tp;\n            this.name = name;\n            this.op = op;\n            this.transactionalId = transactionalId;\n            this.producerId = producerId;\n            this.producerEpoch = producerEpoch;\n            this.verificationGuard = verificationGuard;\n            this.future = new CompletableFuture<>();\n            this.createdTimeMs = time.milliseconds();\n            this.writeTimeout = writeTimeout;\n        }\n\n        /**\n         * @return The key used by the CoordinatorEventProcessor to ensure\n         * that events with the same key are not processed concurrently.\n         */\n        @Override\n        public TopicPartition key() {\n            return tp;\n        }\n\n        /**\n         * Called by the CoordinatorEventProcessor when the event is executed.\n         */\n        @Override\n        public void run() {\n            try {\n                // Get the context of the coordinator or fail if the coordinator is not in active state.\n                withActiveContextOrThrow(tp, context -> {\n                    // Execute the operation.\n                    result = op.generateRecordsAndResult(context.coordinator.coordinator());\n\n                    append(context);\n                });\n            } catch (Throwable t) {\n                complete(t);\n            }\n        }\n\n        private void append(CoordinatorContext context) {\n            if (result.records().isEmpty()) {\n                // If the records are empty, it was a read operation after all. In this case,\n                // the response can be returned directly iff there are no pending write operations;\n                // otherwise, the read needs to wait on the last write operation to be completed.\n                OptionalLong pendingOffset = context.deferredEventQueue.highestPendingOffset();\n                if (pendingOffset.isPresent()) {\n                    context.deferredEventQueue.add(pendingOffset.getAsLong(), this);\n                } else {\n                    complete(null);\n                }\n            } else {\n                // If the records are not empty, first, they are applied to the state machine,\n                // second, then are written to the partition/log, and finally, the response\n                // is put into the deferred event queue.\n                long prevLastWrittenOffset = context.coordinator.lastWrittenOffset();\n                LogConfig logConfig = partitionWriter.config(tp);\n                byte magic = logConfig.recordVersion().value;\n                int maxBatchSize = logConfig.maxMessageSize();\n                long currentTimeMs = time.milliseconds();\n                ByteBuffer buffer = context.bufferSupplier.get(Math.min(MIN_BUFFER_SIZE, maxBatchSize));\n\n                try {\n                    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(\n                        buffer,\n                        magic,\n                        compression,\n                        TimestampType.CREATE_TIME,\n                        0L,\n                        currentTimeMs,\n                        producerId,\n                        producerEpoch,\n                        0,\n                        producerId != RecordBatch.NO_PRODUCER_ID,\n                        false,\n                        RecordBatch.NO_PARTITION_LEADER_EPOCH,\n                        maxBatchSize\n                    );\n\n                    // Apply the records to the state machine and add them to the batch.\n                    for (int i = 0; i < result.records().size(); i++) {\n                        U record = result.records().get(i);\n\n                        if (result.replayRecords()) {\n                            // We compute the offset of the record based on the last written offset. The\n                            // coordinator is the single writer to the underlying partition so we can\n                            // deduce it like this.\n                            context.coordinator.replay(\n                                prevLastWrittenOffset + i,\n                                producerId,\n                                producerEpoch,\n                                record\n                            );\n                        }\n\n                        byte[] keyBytes = serializer.serializeKey(record);\n                        byte[] valBytes = serializer.serializeValue(record);\n\n                        if (builder.hasRoomFor(currentTimeMs, keyBytes, valBytes, EMPTY_HEADERS)) {\n                            builder.append(\n                                currentTimeMs,\n                                keyBytes,\n                                valBytes,\n                                EMPTY_HEADERS\n                            );\n                        } else {\n                            throw new RecordTooLargeException(\"Message batch size is \" + builder.estimatedSizeInBytes() +\n                                \" bytes in append to partition \" + tp + \" which exceeds the maximum \" +\n                                \"configured size of \" + maxBatchSize + \".\");\n                        }\n                    }\n\n                    // Write the records to the log and update the last written\n                    // offset.\n                    long offset = partitionWriter.append(\n                        tp,\n                        verificationGuard,\n                        builder.build()\n                    );\n                    context.coordinator.updateLastWrittenOffset(offset);\n\n                    // Add the response to the deferred queue.\n                    if (!future.isDone()) {\n                        context.deferredEventQueue.add(offset, this);\n                        operationTimeout = new OperationTimeout(tp, this, writeTimeout.toMillis());\n                        timer.add(operationTimeout);\n                    } else {\n                        complete(null);\n                    }\n                } catch (Throwable t) {\n                    context.coordinator.revertLastWrittenOffset(prevLastWrittenOffset);\n                    complete(t);\n                } finally {\n                    context.bufferSupplier.release(buffer);\n                }\n            }\n        }\n\n        /**\n         * Completes the future with either the result of the write operation\n         * or the provided exception.\n         *\n         * @param exception The exception to complete the future with.\n         */\n        @Override\n        public void complete(Throwable exception) {\n            CompletableFuture<Void> appendFuture = result != null ? result.appendFuture() : null;\n\n            if (exception == null) {\n                if (appendFuture != null) result.appendFuture().complete(null);\n                future.complete(result.response());\n            } else {\n                if (appendFuture != null) result.appendFuture().completeExceptionally(exception);\n                future.completeExceptionally(exception);\n            }\n\n            if (operationTimeout != null) {\n                operationTimeout.cancel();\n                operationTimeout = null;\n            }\n        }\n\n        @Override\n        public long createdTimeMs() {\n            return this.createdTimeMs;\n        }\n\n        @Override\n        public String toString() {\n            return \"CoordinatorWriteEvent(name=\" + name + \")\";\n        }\n    }",
                "methodCount": 8
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "processingTime": {
                "llmResponseTime": -1,
                "pluginProcessingTime": -1,
                "totalTime": 12317
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "append",
                            "method_signature": "private void append(CoordinatorContext context)",
                            "target_class": "CoordinatorContext",
                            "rationale": "The append method primarily involves interactions with the CoordinatorContext. Moving it to the CoordinatorContext class will better align the method with its dependencies and reduce cohesion within CoordinatorWriteEvent."
                        },
                        {
                            "method_name": "complete",
                            "method_signature": "public void complete(Throwable exception)",
                            "target_class": "CoordinatorResult",
                            "rationale": "The complete method deals with the result and its append future, which are elements of the CoordinatorResult. Moving it to the CoordinatorResult class would place it closer to the data it manipulates, increasing cohesion."
                        }
                    ],
                    "llm_response_time": 3362
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "complete",
                            "method_signature": "public void complete(Throwable exception)",
                            "target_class": "CoordinatorResult",
                            "rationale": "The complete() method is closely tied to the CoordinatorResult class because it deals with setting the outcome of an operation (success or failure) and acting upon the result's state. Moving it to CoordinatorResult makes the class more cohesive and enables easier management of operation results."
                        }
                    ],
                    "llm_response_time": 1848
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "append",
                            "method_signature": "private void append(CoordinatorContext context)",
                            "target_class": "CoordinatorContext",
                            "rationale": "The append method interacts heavily with the CoordinatorContext, executing operations related to state machine updates and partitions. Moving this method to CoordinatorContext would encapsulate these operations within the class primarily responsible for handling coordinator contexts."
                        },
                        {
                            "method_name": "complete",
                            "method_signature": "public void complete(Throwable exception)",
                            "target_class": "CoordinatorResult",
                            "rationale": "The complete method involves the result object derived from CoordinatorResult and deals with finalizing the outcome of the operation. Moving this method to the CoordinatorResult class would align the task of completing future operations with the entity that holds the operation outcomes."
                        }
                    ],
                    "llm_response_time": 2718
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "append",
                            "method_signature": "private append(CoordinatorContext context)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "append",
                            "method_signature": "private append(CoordinatorContext context)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private append(CoordinatorContext context)": {
                        "first": {
                            "method_name": "append",
                            "method_signature": "private append(CoordinatorContext context)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5562959740107732
                    }
                },
                "voyage": {
                    "private append(CoordinatorContext context)": {
                        "first": {
                            "method_name": "append",
                            "method_signature": "private append(CoordinatorContext context)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5719484665620506
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private append(CoordinatorContext context)"
                    ],
                    "llm_response_time": 1125
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private append(CoordinatorContext context)"
                    ],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private append(CoordinatorContext context)"
                    ],
                    "llm_response_time": 0
                },
                "voyage": {
                    "priority_method_names": [
                        "private append(CoordinatorContext context)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private append(CoordinatorContext context)"
                    ],
                    "llm_response_time": 1
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private append(CoordinatorContext context)"
                    ],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "append": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2727,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "638844f833b165d6f9ca52c173858d26b7254fac",
        "url": "https://github.com/apache/kafka/commit/638844f833b165d6f9ca52c173858d26b7254fac",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method private completeTransaction(producerId long, producerEpoch short, coordinatorEpoch int, result TransactionResult, event DeferredEvent) : void extracted from public run() : void in class org.apache.kafka.coordinator.group.runtime.CoordinatorRuntime.CoordinatorCompleteTransactionEvent & moved to class org.apache.kafka.coordinator.group.runtime.CoordinatorRuntime.CoordinatorContext",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1138,
                    "endLine": 1184,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public run() : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1145,
                    "endLine": 1145,
                    "startColumn": 21,
                    "endColumn": 90,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1148,
                    "endLine": 1152,
                    "startColumn": 25,
                    "endColumn": 27,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1154,
                    "endLine": 1166,
                    "startColumn": 25,
                    "endColumn": 27,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1167,
                    "endLine": 1167,
                    "startColumn": 25,
                    "endColumn": 77,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1170,
                    "endLine": 1170,
                    "startColumn": 29,
                    "endColumn": 74,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1177,
                    "endLine": 1177,
                    "startColumn": 25,
                    "endColumn": 92,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1178,
                    "endLine": 1178,
                    "startColumn": 25,
                    "endColumn": 37,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1147,
                    "endLine": 1179,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "TRY_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1176,
                    "endLine": 1179,
                    "startColumn": 23,
                    "endColumn": 22,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1010,
                    "endLine": 1063,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "private completeTransaction(producerId long, producerEpoch short, coordinatorEpoch int, result TransactionResult, event DeferredEvent) : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1035,
                    "endLine": 1035,
                    "startColumn": 13,
                    "endColumn": 74,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1037,
                    "endLine": 1041,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1043,
                    "endLine": 1055,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1056,
                    "endLine": 1056,
                    "startColumn": 17,
                    "endColumn": 61,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1058,
                    "endLine": 1058,
                    "startColumn": 17,
                    "endColumn": 55,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1060,
                    "endLine": 1060,
                    "startColumn": 17,
                    "endColumn": 76,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1061,
                    "endLine": 1061,
                    "startColumn": 17,
                    "endColumn": 35,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1036,
                    "endLine": 1062,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "TRY_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1059,
                    "endLine": 1062,
                    "startColumn": 15,
                    "endColumn": 14,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1531,
                    "endLine": 1554,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public run() : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1538,
                    "endLine": 1544,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "context.completeTransaction(producerId,producerEpoch,coordinatorEpoch,result,this)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1028,
                    "endLine": 1028,
                    "startColumn": 17,
                    "endColumn": 105,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1033,
                    "endLine": 1033,
                    "startColumn": 13,
                    "endColumn": 33,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1027,
                    "endLine": 1029,
                    "startColumn": 51,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1027,
                    "endLine": 1029,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 584,
        "extraction_results": {
            "success": true,
            "newCommitHash": "fe9c6822775e39d22c74b77d5df030ccc5b570b0",
            "newBranchName": "extract-completeTransaction-run-39ffdea"
        },
        "telemetry": {
            "id": "7284ef49-7e6b-45b5-8c59-12c9d8fdf407",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 166,
                "lineStart": 1054,
                "lineEnd": 1219,
                "bodyLineStart": 1054,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                "sourceCode": "/**\n     * A coordinator event that applies and writes a transaction end marker.\n     */\n    class CoordinatorCompleteTransactionEvent implements CoordinatorEvent, DeferredEvent {\n        /**\n         * The topic partition that this write event is applied to.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The operation name.\n         */\n        final String name;\n\n        /**\n         * The producer id.\n         */\n        final long producerId;\n\n        /**\n         * The producer epoch.\n         */\n        final short producerEpoch;\n\n        /**\n         * The coordinator epoch of the transaction coordinator.\n         */\n        final int coordinatorEpoch;\n\n        /**\n         * The transaction result.\n         */\n        final TransactionResult result;\n\n        /**\n         * Timeout value for the write operation.\n         */\n        final Duration writeTimeout;\n\n        /**\n         * The operation timeout.\n         */\n        private OperationTimeout operationTimeout = null;\n\n        /**\n         * The future that will be completed with the response\n         * generated by the write operation or an error.\n         */\n        final CompletableFuture<Void> future;\n\n        /**\n         * The time this event was created.\n         */\n        private final long createdTimeMs;\n\n        CoordinatorCompleteTransactionEvent(\n            String name,\n            TopicPartition tp,\n            long producerId,\n            short producerEpoch,\n            int coordinatorEpoch,\n            TransactionResult result,\n            Duration writeTimeout\n        ) {\n            this.name = name;\n            this.tp = tp;\n            this.producerId = producerId;\n            this.producerEpoch = producerEpoch;\n            this.coordinatorEpoch = coordinatorEpoch;\n            this.result = result;\n            this.writeTimeout = writeTimeout;\n            this.future = new CompletableFuture<>();\n            this.createdTimeMs = time.milliseconds();\n        }\n\n        /**\n         * @return The key used by the CoordinatorEventProcessor to ensure\n         * that events with the same key are not processed concurrently.\n         */\n        @Override\n        public TopicPartition key() {\n            return tp;\n        }\n\n        /**\n         * Called by the CoordinatorEventProcessor when the event is executed.\n         */\n        @Override\n        public void run() {\n            try {\n                withActiveContextOrThrow(tp, context -> {\n                    long prevLastWrittenOffset = context.coordinator.lastWrittenOffset();\n\n                    completeTransaction(context, prevLastWrittenOffset);\n                });\n            } catch (Throwable t) {\n                complete(t);\n            }\n        }\n\n        private void completeTransaction(CoordinatorContext context, long prevLastWrittenOffset) {\n            try {\n                context.coordinator.replayEndTransactionMarker(\n                    producerId,\n                    producerEpoch,\n                    result\n                );\n\n                long offset = partitionWriter.append(\n                    tp,\n                    VerificationGuard.SENTINEL,\n                    MemoryRecords.withEndTransactionMarker(\n                        time.milliseconds(),\n                        producerId,\n                        producerEpoch,\n                        new EndTransactionMarker(\n                            result == TransactionResult.COMMIT ? ControlRecordType.COMMIT : ControlRecordType.ABORT,\n                            coordinatorEpoch\n                        )\n                    )\n                );\n                context.coordinator.updateLastWrittenOffset(offset);\n\n                if (!future.isDone()) {\n                    context.deferredEventQueue.add(offset, this);\n                    operationTimeout = new OperationTimeout(tp, this, writeTimeout.toMillis());\n                    timer.add(operationTimeout);\n                } else {\n                    complete(null);\n                }\n            } catch (Throwable t) {\n                context.coordinator.revertLastWrittenOffset(prevLastWrittenOffset);\n                complete(t);\n            }\n        }\n\n        /**\n         * Completes the future with either the result of the write operation\n         * or the provided exception.\n         *\n         * @param exception The exception to complete the future with.\n         */\n        @Override\n        public void complete(Throwable exception) {\n            if (exception == null) {\n                future.complete(null);\n            } else {\n                future.completeExceptionally(exception);\n            }\n\n            if (operationTimeout != null) {\n                operationTimeout.cancel();\n                operationTimeout = null;\n            }\n        }\n\n        @Override\n        public long createdTimeMs() {\n            return createdTimeMs;\n        }\n\n        @Override\n        public String toString() {\n            return \"CoordinatorCompleteTransactionEvent(name=\" + name + \")\";\n        }\n    }",
                "methodCount": 7
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "processingTime": {
                "llmResponseTime": -1,
                "pluginProcessingTime": -1,
                "totalTime": 12525
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [],
                    "llm_response_time": 1116
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "completeTransaction",
                            "method_signature": "private void completeTransaction(CoordinatorContext context, long prevLastWrittenOffset)",
                            "target_class": "CoordinatorContext",
                            "rationale": "The method 'completeTransaction' heavily interacts with the 'CoordinatorContext', performing operations on 'context.coordinator'. It would make more sense to move this method into the 'CoordinatorContext' class where these operations are relevant."
                        },
                        {
                            "method_name": "complete",
                            "method_signature": "public void complete(Throwable exception)",
                            "target_class": "CoordinatorContext",
                            "rationale": "The 'complete' method is performing operations related to the state and behavior of the 'CoordinatorContext'. Moving this method there will encapsulate the state management within the relevant class."
                        }
                    ],
                    "llm_response_time": 2642
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "completeTransaction",
                            "method_signature": "private void completeTransaction(CoordinatorContext context, long prevLastWrittenOffset)",
                            "target_class": "Coordinator",
                            "rationale": "The completeTransaction method deals specifically with operations related to the Coordinator such as replaying the end transaction marker, updating offset, and reverting the offset on error. This logic is directly related to the Coordinator's responsibilities and state management, making it more appropriate for this method to reside in the Coordinator class."
                        }
                    ],
                    "llm_response_time": 3243
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "completeTransaction",
                            "method_signature": "private completeTransaction(CoordinatorContext context, long prevLastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "completeTransaction",
                            "method_signature": "private completeTransaction(CoordinatorContext context, long prevLastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private completeTransaction(CoordinatorContext context, long prevLastWrittenOffset)": {
                        "first": {
                            "method_name": "completeTransaction",
                            "method_signature": "private completeTransaction(CoordinatorContext context, long prevLastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.35350741187006834
                    }
                },
                "voyage": {
                    "private completeTransaction(CoordinatorContext context, long prevLastWrittenOffset)": {
                        "first": {
                            "method_name": "completeTransaction",
                            "method_signature": "private completeTransaction(CoordinatorContext context, long prevLastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.7607440403617549
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [],
                    "llm_response_time": 1618
                },
                "tf-idf-5": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "voyage": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "voyage-5": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "voyage-3": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "completeTransaction": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3560,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "9a239c6142a8f2eb36f1600d1012224c31e58e71",
        "url": "https://github.com/apache/kafka/commit/9a239c6142a8f2eb36f1600d1012224c31e58e71",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method private registerStreamThread(streamThread StreamThread) : void extracted from private createAndAddStreamThread(cacheSizePerThread long, threadIdx int) : StreamThread in class org.apache.kafka.streams.KafkaStreams & moved to class org.apache.kafka.streams.KafkaStreams.StreamStateListener",
            "leftSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 1068,
                    "endLine": 1092,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private createAndAddStreamThread(cacheSizePerThread long, threadIdx int) : StreamThread"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 1089,
                    "endLine": 1089,
                    "startColumn": 9,
                    "endColumn": 69,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 699,
                    "endLine": 701,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "private registerStreamThread(streamThread StreamThread) : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 700,
                    "endLine": 700,
                    "startColumn": 13,
                    "endColumn": 73,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 1064,
                    "endLine": 1088,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private createAndAddStreamThread(cacheSizePerThread long, threadIdx int) : StreamThread"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 1083,
                    "endLine": 1083,
                    "startColumn": 9,
                    "endColumn": 63,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "streamStateListener.registerStreamThread(streamThread)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 585,
        "extraction_results": {
            "success": true,
            "newCommitHash": "cc2ec73ca63c3856ab31a8907b66ffd90bf3a1f8",
            "newBranchName": "extract-registerStreamThread-createAndAddStreamThread-fc6f8b6"
        },
        "telemetry": {
            "id": "088a6bfa-ffcd-462b-97df-d1052e9ede11",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2147,
                "lineStart": 118,
                "lineEnd": 2264,
                "bodyLineStart": 118,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                "sourceCode": "/**\n * A Kafka client that allows for performing continuous computation on input coming from one or more input topics and\n * sends output to zero, one, or more output topics.\n * <p>\n * The computational logic can be specified either by using the {@link Topology} to define a DAG topology of\n * {@link org.apache.kafka.streams.processor.api.Processor}s or by using the {@link StreamsBuilder} which provides the high-level DSL to define\n * transformations.\n * <p>\n * One {@code KafkaStreams} instance can contain one or more threads specified in the configs for the processing work.\n * <p>\n * A {@code KafkaStreams} instance can co-ordinate with any other instances with the same\n * {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} (whether in the same process, on other processes on this\n * machine, or on remote machines) as a single (possibly distributed) stream processing application.\n * These instances will divide up the work based on the assignment of the input topic partitions so that all partitions\n * are being consumed.\n * If instances are added or fail, all (remaining) instances will rebalance the partition assignment among themselves\n * to balance processing load and ensure that all input topic partitions are processed.\n * <p>\n * Internally a {@code KafkaStreams} instance contains a normal {@link KafkaProducer} and {@link KafkaConsumer} instance\n * that is used for reading input and writing output.\n * <p>\n * A simple example might look like this:\n * <pre>{@code\n * Properties props = new Properties();\n * props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"my-stream-processing-application\");\n * props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n * props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n * props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n *\n * StreamsBuilder builder = new StreamsBuilder();\n * builder.<String, String>stream(\"my-input-topic\").mapValues(value -> String.valueOf(value.length())).to(\"my-output-topic\");\n *\n * KafkaStreams streams = new KafkaStreams(builder.build(), props);\n * streams.start();\n * }</pre>\n *\n * @see org.apache.kafka.streams.StreamsBuilder\n * @see org.apache.kafka.streams.Topology\n */\npublic class KafkaStreams implements AutoCloseable {\n\n    private static final String JMX_PREFIX = \"kafka.streams\";\n\n    // processId is expected to be unique across JVMs and to be used\n    // in userData of the subscription request to allow assignor be aware\n    // of the co-location of stream thread's consumers. It is for internal\n    // usage only and should not be exposed to users at all.\n    private final Time time;\n    private final Logger log;\n    protected final String clientId;\n    private final Metrics metrics;\n    protected final StreamsConfig applicationConfigs;\n    protected final List<StreamThread> threads;\n    protected final StreamsMetadataState streamsMetadataState;\n    private final ScheduledExecutorService stateDirCleaner;\n    private final ScheduledExecutorService rocksDBMetricsRecordingService;\n    protected final Admin adminClient;\n    private final StreamsMetricsImpl streamsMetrics;\n    private final long totalCacheSize;\n    private final StreamStateListener streamStateListener;\n    private final DelegatingStateRestoreListener delegatingStateRestoreListener;\n    private final Map<Long, StreamThread.State> threadState;\n    private final UUID processId;\n    private final KafkaClientSupplier clientSupplier;\n    protected final TopologyMetadata topologyMetadata;\n    private final QueryableStoreProvider queryableStoreProvider;\n    private final DelegatingStandbyUpdateListener delegatingStandbyUpdateListener;\n\n    GlobalStreamThread globalStreamThread;\n    protected StateDirectory stateDirectory = null;\n    private KafkaStreams.StateListener stateListener;\n    private boolean oldHandler;\n    private BiConsumer<Throwable, Boolean> streamsUncaughtExceptionHandler;\n    private final Object changeThreadCount = new Object();\n\n    // container states\n    /**\n     * Kafka Streams states are the possible state that a Kafka Streams instance can be in.\n     * An instance must only be in one state at a time.\n     * The expected state transition with the following defined states is:\n     *\n     * <pre>\n     *                 +--------------+\n     *         +&lt;----- | Created (0)  |\n     *         |       +-----+--------+\n     *         |             |\n     *         |             v\n     *         |       +----+--+------+\n     *         |       | Re-          |\n     *         +&lt;----- | Balancing (1)| --------&gt;+\n     *         |       +-----+-+------+          |\n     *         |             | ^                 |\n     *         |             v |                 |\n     *         |       +--------------+          v\n     *         |       | Running (2)  | --------&gt;+\n     *         |       +------+-------+          |\n     *         |              |                  |\n     *         |              v                  |\n     *         |       +------+-------+     +----+-------+\n     *         +-----&gt; | Pending      |     | Pending    |\n     *                 | Shutdown (3) |     | Error (5)  |\n     *                 +------+-------+     +-----+------+\n     *                        |                   |\n     *                        v                   v\n     *                 +------+-------+     +-----+--------+\n     *                 | Not          |     | Error (6)    |\n     *                 | Running (4)  |     +--------------+\n     *                 +--------------+\n     *\n     *\n     * </pre>\n     * Note the following:\n     * - RUNNING state will transit to REBALANCING if any of its threads is in PARTITION_REVOKED or PARTITIONS_ASSIGNED state\n     * - REBALANCING state will transit to RUNNING if all of its threads are in RUNNING state\n     * - Any state except NOT_RUNNING, PENDING_ERROR or ERROR can go to PENDING_SHUTDOWN (whenever close is called)\n     * - Of special importance: If the global stream thread dies, or all stream threads die (or both) then\n     *   the instance will be in the ERROR state. The user will not need to close it.\n     */\n    public enum State {\n        // Note: if you add a new state, check the below methods and how they are used within Streams to see if\n        // any of them should be updated to include the new state. For example a new shutdown path or terminal\n        // state would likely need to be included in methods like isShuttingDown(), hasCompletedShutdown(), etc.\n        CREATED(1, 3),          // 0\n        REBALANCING(2, 3, 5),   // 1\n        RUNNING(1, 2, 3, 5),    // 2\n        PENDING_SHUTDOWN(4),    // 3\n        NOT_RUNNING,            // 4\n        PENDING_ERROR(6),       // 5\n        ERROR;                  // 6\n\n        private final Set<Integer> validTransitions = new HashSet<>();\n\n        State(final Integer... validTransitions) {\n            this.validTransitions.addAll(Arrays.asList(validTransitions));\n        }\n\n        public boolean hasNotStarted() {\n            return equals(CREATED);\n        }\n\n        public boolean isRunningOrRebalancing() {\n            return equals(RUNNING) || equals(REBALANCING);\n        }\n\n        public boolean isShuttingDown() {\n            return equals(PENDING_SHUTDOWN) || equals(PENDING_ERROR);\n        }\n\n        public boolean hasCompletedShutdown() {\n            return equals(NOT_RUNNING) || equals(ERROR);\n        }\n\n        public boolean hasStartedOrFinishedShuttingDown() {\n            return isShuttingDown() || hasCompletedShutdown();\n        }\n\n        public boolean isValidTransition(final State newState) {\n            return validTransitions.contains(newState.ordinal());\n        }\n    }\n\n\n\n    private final Object stateLock = new Object();\n    protected volatile State state = State.CREATED;\n\n    private boolean waitOnState(final State targetState, final long waitMs) {\n        final long begin = time.milliseconds();\n        synchronized (stateLock) {\n            boolean interrupted = false;\n            long elapsedMs = 0L;\n            try {\n                while (state != targetState) {\n                    if (waitMs > elapsedMs) {\n                        final long remainingMs = waitMs - elapsedMs;\n                        try {\n                            stateLock.wait(remainingMs);\n                        } catch (final InterruptedException e) {\n                            interrupted = true;\n                        }\n                    } else {\n                        log.debug(\"Cannot transit to {} within {}ms\", targetState, waitMs);\n                        return false;\n                    }\n                    elapsedMs = time.milliseconds() - begin;\n                }\n            } finally {\n                // Make sure to restore the interruption status before returning.\n                // We do not always own the current thread that executes this method, i.e., we do not know the\n                // interruption policy of the thread. The least we can do is restore the interruption status before\n                // the current thread exits this method.\n                if (interrupted) {\n                    Thread.currentThread().interrupt();\n                }\n            }\n            return true;\n        }\n    }\n\n    /**\n     * Sets the state\n     * @param newState New state\n     */\n    private boolean setState(final State newState) {\n        final State oldState;\n\n        synchronized (stateLock) {\n            oldState = state;\n\n            if (state == State.PENDING_SHUTDOWN && newState != State.NOT_RUNNING) {\n                // when the state is already in PENDING_SHUTDOWN, all other transitions than NOT_RUNNING (due to thread dying) will be\n                // refused but we do not throw exception here, to allow appropriate error handling\n                return false;\n            } else if (state == State.NOT_RUNNING && (newState == State.PENDING_SHUTDOWN || newState == State.NOT_RUNNING)) {\n                // when the state is already in NOT_RUNNING, its transition to PENDING_SHUTDOWN or NOT_RUNNING (due to consecutive close calls)\n                // will be refused but we do not throw exception here, to allow idempotent close calls\n                return false;\n            } else if (state == State.REBALANCING && newState == State.REBALANCING) {\n                // when the state is already in REBALANCING, it should not transit to REBALANCING again\n                return false;\n            } else if (state == State.ERROR && (newState == State.PENDING_ERROR || newState == State.ERROR)) {\n                // when the state is already in ERROR, its transition to PENDING_ERROR or ERROR (due to consecutive close calls)\n                return false;\n            } else if (state == State.PENDING_ERROR && newState != State.ERROR) {\n                // when the state is already in PENDING_ERROR, all other transitions than ERROR (due to thread dying) will be\n                // refused but we do not throw exception here, to allow appropriate error handling\n                return false;\n            } else if (!state.isValidTransition(newState)) {\n                throw new IllegalStateException(\"Stream-client \" + clientId + \": Unexpected state transition from \" + oldState + \" to \" + newState);\n            } else {\n                log.info(\"State transition from {} to {}\", oldState, newState);\n            }\n            state = newState;\n            stateLock.notifyAll();\n        }\n\n        // we need to call the user customized state listener outside the state lock to avoid potential deadlocks\n        if (stateListener != null) {\n            stateListener.onChange(newState, oldState);\n        }\n\n        return true;\n    }\n\n    /**\n     * Return the current {@link State} of this {@code KafkaStreams} instance.\n     *\n     * @return the current state of this Kafka Streams instance\n     */\n    public State state() {\n        return state;\n    }\n\n    protected boolean isRunningOrRebalancing() {\n        synchronized (stateLock) {\n            return state.isRunningOrRebalancing();\n        }\n    }\n\n    protected boolean hasStartedOrFinishedShuttingDown() {\n        synchronized (stateLock) {\n            return state.hasStartedOrFinishedShuttingDown();\n        }\n    }\n\n    protected void validateIsRunningOrRebalancing() {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                throw new StreamsNotStartedException(\"KafkaStreams has not been started, you can retry after calling start()\");\n            }\n            if (!state.isRunningOrRebalancing()) {\n                throw new IllegalStateException(\"KafkaStreams is not running. State is \" + state + \".\");\n            }\n        }\n    }\n\n    public Map<Long, StreamThread.State> getThreadState() {\n        return threadState;\n    }\n\n    /**\n     * Listen to {@link State} change events.\n     */\n    public interface StateListener {\n\n        /**\n         * Called when state changes.\n         *\n         * @param newState new state\n         * @param oldState previous state\n         */\n        void onChange(final State newState, final State oldState);\n    }\n\n    /**\n     * An app can set a single {@link KafkaStreams.StateListener} so that the app is notified when state changes.\n     *\n     * @param listener a new state listener\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     */\n    public void setStateListener(final KafkaStreams.StateListener listener) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                stateListener = listener;\n            } else {\n                throw new IllegalStateException(\"Can only set StateListener before calling start(). Current state is: \" + state);\n            }\n        }\n    }\n\n    /**\n     * Set the handler invoked when an internal {@link StreamsConfig#NUM_STREAM_THREADS_CONFIG stream thread} abruptly\n     * terminates due to an uncaught exception.\n     *\n     * @param uncaughtExceptionHandler the uncaught exception handler for all internal threads; {@code null} deletes the current handler\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     *\n     * @deprecated Since 2.8.0. Use {@link KafkaStreams#setUncaughtExceptionHandler(StreamsUncaughtExceptionHandler)} instead.\n     *\n     */\n    @Deprecated\n    public void setUncaughtExceptionHandler(final Thread.UncaughtExceptionHandler uncaughtExceptionHandler) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                oldHandler = true;\n                processStreamThread(thread -> thread.setUncaughtExceptionHandler(uncaughtExceptionHandler));\n\n                if (globalStreamThread != null) {\n                    globalStreamThread.setUncaughtExceptionHandler(uncaughtExceptionHandler);\n                }\n            } else {\n                throw new IllegalStateException(\"Can only set UncaughtExceptionHandler before calling start(). \" +\n                    \"Current state is: \" + state);\n            }\n        }\n    }\n\n    /**\n     * Set the handler invoked when an internal {@link StreamsConfig#NUM_STREAM_THREADS_CONFIG stream thread}\n     * throws an unexpected exception.\n     * These might be exceptions indicating rare bugs in Kafka Streams, or they\n     * might be exceptions thrown by your code, for example a NullPointerException thrown from your processor logic.\n     * The handler will execute on the thread that produced the exception.\n     * In order to get the thread that threw the exception, use {@code Thread.currentThread()}.\n     * <p>\n     * Note, this handler must be threadsafe, since it will be shared among all threads, and invoked from any\n     * thread that encounters such an exception.\n     *\n     * @param userStreamsUncaughtExceptionHandler the uncaught exception handler of type {@link StreamsUncaughtExceptionHandler} for all internal threads\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     * @throws NullPointerException if userStreamsUncaughtExceptionHandler is null.\n     */\n    public void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler userStreamsUncaughtExceptionHandler) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                Objects.requireNonNull(userStreamsUncaughtExceptionHandler);\n                streamsUncaughtExceptionHandler =\n                    (exception, skipThreadReplacement) ->\n                        handleStreamsUncaughtException(exception, userStreamsUncaughtExceptionHandler, skipThreadReplacement);\n                processStreamThread(thread -> thread.setStreamsUncaughtExceptionHandler(streamsUncaughtExceptionHandler));\n                if (globalStreamThread != null) {\n                    globalStreamThread.setUncaughtExceptionHandler(\n                        exception -> handleStreamsUncaughtException(exception, userStreamsUncaughtExceptionHandler, false)\n                    );\n                }\n                processStreamThread(thread -> thread.setUncaughtExceptionHandler((t, e) -> { }\n                ));\n\n                if (globalStreamThread != null) {\n                    globalStreamThread.setUncaughtExceptionHandler((t, e) -> { }\n                    );\n                }\n            } else {\n                throw new IllegalStateException(\"Can only set UncaughtExceptionHandler before calling start(). \" +\n                    \"Current state is: \" + state);\n            }\n        }\n    }\n\n    private void defaultStreamsUncaughtExceptionHandler(final Throwable throwable, final boolean skipThreadReplacement) {\n        if (oldHandler) {\n            threads.remove(Thread.currentThread());\n            if (throwable instanceof RuntimeException) {\n                throw (RuntimeException) throwable;\n            } else if (throwable instanceof Error) {\n                throw (Error) throwable;\n            } else {\n                throw new RuntimeException(\"Unexpected checked exception caught in the uncaught exception handler\", throwable);\n            }\n        } else {\n            handleStreamsUncaughtException(throwable, t -> SHUTDOWN_CLIENT, skipThreadReplacement);\n        }\n    }\n\n    private void replaceStreamThread(final Throwable throwable) {\n        if (globalStreamThread != null && Thread.currentThread().getName().equals(globalStreamThread.getName())) {\n            log.warn(\"The global thread cannot be replaced. Reverting to shutting down the client.\");\n            log.error(\"Encountered the following exception during processing \" +\n                    \" The streams client is going to shut down now. \", throwable);\n            closeToError();\n        }\n        final StreamThread deadThread = (StreamThread) Thread.currentThread();\n        deadThread.shutdown();\n        addStreamThread();\n        if (throwable instanceof RuntimeException) {\n            throw (RuntimeException) throwable;\n        } else if (throwable instanceof Error) {\n            throw (Error) throwable;\n        } else {\n            throw new RuntimeException(\"Unexpected checked exception caught in the uncaught exception handler\", throwable);\n        }\n    }\n\n    private void handleStreamsUncaughtException(final Throwable throwable,\n                                                final StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler,\n                                                final boolean skipThreadReplacement) {\n        final StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse action = streamsUncaughtExceptionHandler.handle(throwable);\n        if (oldHandler) {\n            log.warn(\"Stream's new uncaught exception handler is set as well as the deprecated old handler.\" +\n                    \"The old handler will be ignored as long as a new handler is set.\");\n        }\n        switch (action) {\n            case REPLACE_THREAD:\n                if (!skipThreadReplacement) {\n                    log.error(\"Replacing thread in the streams uncaught exception handler\", throwable);\n                    replaceStreamThread(throwable);\n                } else {\n                    log.debug(\"Skipping thread replacement for recoverable error\");\n                }\n                break;\n            case SHUTDOWN_CLIENT:\n                log.error(\"Encountered the following exception during processing \" +\n                        \"and the registered exception handler opted to \" + action + \".\" +\n                        \" The streams client is going to shut down now. \", throwable);\n                closeToError();\n                break;\n            case SHUTDOWN_APPLICATION:\n                if (getNumLiveStreamThreads() == 1) {\n                    log.warn(\"Attempt to shut down the application requires adding a thread to communicate the shutdown. No processing will be done on this thread\");\n                    addStreamThread();\n                }\n                if (throwable instanceof Error) {\n                    log.error(\"This option requires running threads to shut down the application.\" +\n                            \"but the uncaught exception was an Error, which means this runtime is no \" +\n                            \"longer in a well-defined state. Attempting to send the shutdown command anyway.\", throwable);\n                }\n                if (Thread.currentThread().equals(globalStreamThread) && getNumLiveStreamThreads() == 0) {\n                    log.error(\"Exception in global thread caused the application to attempt to shutdown.\" +\n                            \" This action will succeed only if there is at least one StreamThread running on this client.\" +\n                            \" Currently there are no running threads so will now close the client.\");\n                    closeToError();\n                    break;\n                }\n                processStreamThread(thread -> thread.sendShutdownRequest(AssignorError.SHUTDOWN_REQUESTED));\n                log.error(\"Encountered the following exception during processing \" +\n                        \"and sent shutdown request for the entire application.\", throwable);\n                break;\n        }\n    }\n\n    /**\n     * Set the listener which is triggered whenever a {@link StateStore} is being restored in order to resume\n     * processing.\n     *\n     * @param globalStateRestoreListener The listener triggered when {@link StateStore} is being restored.\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     */\n    public void setGlobalStateRestoreListener(final StateRestoreListener globalStateRestoreListener) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                delegatingStateRestoreListener.setUserStateRestoreListener(globalStateRestoreListener);\n            } else {\n                throw new IllegalStateException(\"Can only set GlobalStateRestoreListener before calling start(). \" +\n                    \"Current state is: \" + state);\n            }\n        }\n    }\n\n    /**\n     * Set the listener which is triggered whenever a standby task is updated\n     *\n     * @param standbyListener The listener triggered when a standby task is updated.\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     */\n    public void setStandbyUpdateListener(final StandbyUpdateListener standbyListener) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                this.delegatingStandbyUpdateListener.setUserStandbyListener(standbyListener);\n            } else {\n                throw new IllegalStateException(\"Can only set StandbyUpdateListener before calling start(). \" +\n                        \"Current state is: \" + state);\n            }\n        }\n    }\n\n    /**\n     * Get read-only handle on global metrics registry, including streams client's own metrics plus\n     * its embedded producer, consumer and admin clients' metrics.\n     *\n     * @return Map of all metrics.\n     */\n    public Map<MetricName, ? extends Metric> metrics() {\n        final Map<MetricName, Metric> result = new LinkedHashMap<>();\n        // producer and consumer clients are per-thread\n        processStreamThread(thread -> {\n            result.putAll(thread.producerMetrics());\n            result.putAll(thread.consumerMetrics());\n            // admin client is shared, so we can actually move it\n            // to result.putAll(adminClient.metrics()).\n            // we did it intentionally just for flexibility.\n            result.putAll(thread.adminClientMetrics());\n        });\n        // global thread's consumer client\n        if (globalStreamThread != null) {\n            result.putAll(globalStreamThread.consumerMetrics());\n        }\n        // self streams metrics\n        result.putAll(metrics.metrics());\n        return Collections.unmodifiableMap(result);\n    }\n\n    /**\n     * Class that handles stream thread transitions\n     */\n    final class StreamStateListener implements StreamThread.StateListener {\n        private final Map<Long, StreamThread.State> threadState;\n        private GlobalStreamThread.State globalThreadState;\n        // this lock should always be held before the state lock\n        private final Object threadStatesLock;\n\n        StreamStateListener(final Map<Long, StreamThread.State> threadState,\n                            final GlobalStreamThread.State globalThreadState) {\n            this.threadState = threadState;\n            this.globalThreadState = globalThreadState;\n            this.threadStatesLock = new Object();\n        }\n\n        /**\n         * If all threads are up, including the global thread, set to RUNNING\n         */\n        private void maybeSetRunning() {\n            // state can be transferred to RUNNING if\n            // 1) all threads are either RUNNING or DEAD\n            // 2) thread is pending-shutdown and there are still other threads running\n            final boolean hasRunningThread = threadState.values().stream().anyMatch(s -> s == StreamThread.State.RUNNING);\n            for (final StreamThread.State state : threadState.values()) {\n                if (state == StreamThread.State.PENDING_SHUTDOWN && hasRunningThread) continue;\n                if (state != StreamThread.State.RUNNING && state != StreamThread.State.DEAD) {\n                    return;\n                }\n            }\n\n            // the global state thread is relevant only if it is started. There are cases\n            // when we don't have a global state thread at all, e.g., when we don't have global KTables\n            if (globalThreadState != null && globalThreadState != GlobalStreamThread.State.RUNNING) {\n                return;\n            }\n\n            setState(State.RUNNING);\n        }\n\n\n        @Override\n        public synchronized void onChange(final Thread thread,\n                                          final ThreadStateTransitionValidator abstractNewState,\n                                          final ThreadStateTransitionValidator abstractOldState) {\n            synchronized (threadStatesLock) {\n                // StreamThreads first\n                if (thread instanceof StreamThread) {\n                    final StreamThread.State newState = (StreamThread.State) abstractNewState;\n                    threadState.put(thread.getId(), newState);\n\n                    if (newState == StreamThread.State.PARTITIONS_REVOKED || newState == StreamThread.State.PARTITIONS_ASSIGNED) {\n                        setState(State.REBALANCING);\n                    } else if (newState == StreamThread.State.RUNNING) {\n                        maybeSetRunning();\n                    }\n                } else if (thread instanceof GlobalStreamThread) {\n                    // global stream thread has different invariants\n                    final GlobalStreamThread.State newState = (GlobalStreamThread.State) abstractNewState;\n                    globalThreadState = newState;\n\n                    if (newState == GlobalStreamThread.State.RUNNING) {\n                        maybeSetRunning();\n                    } else if (newState == GlobalStreamThread.State.DEAD) {\n                        if (state != State.PENDING_SHUTDOWN) {\n                            log.error(\"Global thread has died. The streams application or client will now close to ERROR.\");\n                            closeToError();\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    static final class DelegatingStateRestoreListener implements StateRestoreListener {\n        private StateRestoreListener userStateRestoreListener;\n\n        private void throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName) {\n            throw new StreamsException(\n                    String.format(\"Fatal user code error in store restore listener for store %s, partition %s.\",\n                            storeName,\n                            topicPartition),\n                    fatalUserException);\n        }\n\n        void setUserStateRestoreListener(final StateRestoreListener userStateRestoreListener) {\n            this.userStateRestoreListener = userStateRestoreListener;\n        }\n\n        @Override\n        public void onRestoreStart(final TopicPartition topicPartition,\n                                   final String storeName,\n                                   final long startingOffset,\n                                   final long endingOffset) {\n            if (userStateRestoreListener != null) {\n                try {\n                    userStateRestoreListener.onRestoreStart(topicPartition, storeName, startingOffset, endingOffset);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onBatchRestored(final TopicPartition topicPartition,\n                                    final String storeName,\n                                    final long batchEndOffset,\n                                    final long numRestored) {\n            if (userStateRestoreListener != null) {\n                try {\n                    userStateRestoreListener.onBatchRestored(topicPartition, storeName, batchEndOffset, numRestored);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onRestoreEnd(final TopicPartition topicPartition, final String storeName, final long totalRestored) {\n            if (userStateRestoreListener != null) {\n                try {\n                    userStateRestoreListener.onRestoreEnd(topicPartition, storeName, totalRestored);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onRestoreSuspended(final TopicPartition topicPartition, final String storeName, final long totalRestored) {\n            if (userStateRestoreListener != null) {\n                try {\n                    userStateRestoreListener.onRestoreSuspended(topicPartition, storeName, totalRestored);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n    }\n\n    static final class DelegatingStandbyUpdateListener implements StandbyUpdateListener {\n        private StandbyUpdateListener userStandbyListener;\n\n        private void throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName) {\n            throw new StreamsException(\n                    String.format(\"Fatal user code error in standby update listener for store %s, partition %s.\",\n                            storeName,\n                            topicPartition),\n                    fatalUserException);\n        }\n\n        @Override\n        public void onUpdateStart(final TopicPartition topicPartition,\n                          final String storeName, \n                          final long startingOffset) {\n            if (userStandbyListener != null) {\n                try {\n                    userStandbyListener.onUpdateStart(topicPartition, storeName, startingOffset);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onBatchLoaded(final TopicPartition topicPartition, final String storeName, final TaskId taskId, final long batchEndOffset, final long batchSize, final long currentEndOffset) {\n            if (userStandbyListener != null) {\n                try {\n                    userStandbyListener.onBatchLoaded(topicPartition, storeName, taskId, batchEndOffset, batchSize, currentEndOffset);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onUpdateSuspended(final TopicPartition topicPartition, final String storeName, final long storeOffset, final long currentEndOffset, final SuspendReason reason) {\n            if (userStandbyListener != null) {\n                try {\n                    userStandbyListener.onUpdateSuspended(topicPartition, storeName, storeOffset, currentEndOffset, reason);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        void setUserStandbyListener(final StandbyUpdateListener userStandbyListener) {\n            this.userStandbyListener = userStandbyListener;\n        }\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology the topology specifying the computational logic\n     * @param props    properties for {@link StreamsConfig}\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final Properties props) {\n        this(topology, new StreamsConfig(props));\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param props          properties for {@link StreamsConfig}\n     * @param clientSupplier the Kafka clients supplier which provides underlying producer and consumer clients\n     *                       for the new {@code KafkaStreams} instance\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final Properties props,\n                        final KafkaClientSupplier clientSupplier) {\n        this(topology, new StreamsConfig(props), clientSupplier, Time.SYSTEM);\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param props          properties for {@link StreamsConfig}\n     * @param time           {@code Time} implementation; cannot be null\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final Properties props,\n                        final Time time) {\n        this(topology, new StreamsConfig(props), time);\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param props          properties for {@link StreamsConfig}\n     * @param clientSupplier the Kafka clients supplier which provides underlying producer and consumer clients\n     *                       for the new {@code KafkaStreams} instance\n     * @param time           {@code Time} implementation; cannot be null\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final Properties props,\n                        final KafkaClientSupplier clientSupplier,\n                        final Time time) {\n        this(topology, new StreamsConfig(props), clientSupplier, time);\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology  the topology specifying the computational logic\n     * @param applicationConfigs    configs for Kafka Streams\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final StreamsConfig applicationConfigs) {\n        this(topology, applicationConfigs, applicationConfigs.getKafkaClientSupplier());\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param applicationConfigs         configs for Kafka Streams\n     * @param clientSupplier the Kafka clients supplier which provides underlying producer and consumer clients\n     *                       for the new {@code KafkaStreams} instance\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final StreamsConfig applicationConfigs,\n                        final KafkaClientSupplier clientSupplier) {\n        this(new TopologyMetadata(topology.internalTopologyBuilder, applicationConfigs), applicationConfigs, clientSupplier);\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param applicationConfigs         configs for Kafka Streams\n     * @param time           {@code Time} implementation; cannot be null\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final StreamsConfig applicationConfigs,\n                        final Time time) {\n        this(new TopologyMetadata(topology.internalTopologyBuilder, applicationConfigs), applicationConfigs, applicationConfigs.getKafkaClientSupplier(), time);\n    }\n\n    private KafkaStreams(final Topology topology,\n                         final StreamsConfig applicationConfigs,\n                         final KafkaClientSupplier clientSupplier,\n                         final Time time) throws StreamsException {\n        this(new TopologyMetadata(topology.internalTopologyBuilder, applicationConfigs), applicationConfigs, clientSupplier, time);\n    }\n\n    protected KafkaStreams(final TopologyMetadata topologyMetadata,\n                           final StreamsConfig applicationConfigs,\n                           final KafkaClientSupplier clientSupplier) throws StreamsException {\n        this(topologyMetadata, applicationConfigs, clientSupplier, Time.SYSTEM);\n    }\n\n    @SuppressWarnings(\"this-escape\")\n    private KafkaStreams(final TopologyMetadata topologyMetadata,\n                         final StreamsConfig applicationConfigs,\n                         final KafkaClientSupplier clientSupplier,\n                         final Time time) throws StreamsException {\n        this.applicationConfigs = applicationConfigs;\n        this.time = time;\n\n        this.topologyMetadata = topologyMetadata;\n        this.topologyMetadata.buildAndRewriteTopology();\n\n        final boolean hasGlobalTopology = topologyMetadata.hasGlobalTopology();\n\n        try {\n            stateDirectory = new StateDirectory(applicationConfigs, time, topologyMetadata.hasPersistentStores(), topologyMetadata.hasNamedTopologies());\n            processId = stateDirectory.initializeProcessId();\n        } catch (final ProcessorStateException fatal) {\n            Utils.closeQuietly(stateDirectory, \"streams state directory\");\n            throw new StreamsException(fatal);\n        }\n\n        // The application ID is a required config and hence should always have value\n        final String userClientId = applicationConfigs.getString(StreamsConfig.CLIENT_ID_CONFIG);\n        final String applicationId = applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n        if (userClientId.length() <= 0) {\n            clientId = applicationId + \"-\" + processId;\n        } else {\n            clientId = userClientId;\n        }\n        final LogContext logContext = new LogContext(String.format(\"stream-client [%s] \", clientId));\n        this.log = logContext.logger(getClass());\n        topologyMetadata.setLog(logContext);\n\n        // use client id instead of thread client id since this admin client may be shared among threads\n        this.clientSupplier = clientSupplier;\n        adminClient = clientSupplier.getAdmin(applicationConfigs.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId)));\n\n        log.info(\"Kafka Streams version: {}\", ClientMetrics.version());\n        log.info(\"Kafka Streams commit ID: {}\", ClientMetrics.commitId());\n\n        metrics = getMetrics(applicationConfigs, time, clientId);\n        streamsMetrics = new StreamsMetricsImpl(\n            metrics,\n            clientId,\n            applicationConfigs.getString(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG),\n            time\n        );\n\n        ClientMetrics.addVersionMetric(streamsMetrics);\n        ClientMetrics.addCommitIdMetric(streamsMetrics);\n        ClientMetrics.addApplicationIdMetric(streamsMetrics, applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG));\n        ClientMetrics.addTopologyDescriptionMetric(streamsMetrics, (metricsConfig, now) -> this.topologyMetadata.topologyDescriptionString());\n        ClientMetrics.addStateMetric(streamsMetrics, (metricsConfig, now) -> state);\n        threads = Collections.synchronizedList(new LinkedList<>());\n        ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) -> getNumLiveStreamThreads());\n\n        streamsMetadataState = new StreamsMetadataState(\n            this.topologyMetadata,\n            parseHostInfo(applicationConfigs.getString(StreamsConfig.APPLICATION_SERVER_CONFIG)),\n            logContext\n        );\n\n        oldHandler = false;\n        streamsUncaughtExceptionHandler = this::defaultStreamsUncaughtExceptionHandler;\n        delegatingStateRestoreListener = new DelegatingStateRestoreListener();\n        delegatingStandbyUpdateListener = new DelegatingStandbyUpdateListener();\n\n        totalCacheSize = getTotalCacheSize(applicationConfigs);\n        final int numStreamThreads = topologyMetadata.getNumStreamThreads(applicationConfigs);\n        final long cacheSizePerThread = getCacheSizePerThread(numStreamThreads);\n\n        GlobalStreamThread.State globalThreadState = null;\n        if (hasGlobalTopology) {\n            final String globalThreadId = clientId + \"-GlobalStreamThread\";\n            globalStreamThread = new GlobalStreamThread(\n                topologyMetadata.globalTaskTopology(),\n                applicationConfigs,\n                clientSupplier.getGlobalConsumer(applicationConfigs.getGlobalConsumerConfigs(clientId)),\n                stateDirectory,\n                cacheSizePerThread,\n                streamsMetrics,\n                time,\n                globalThreadId,\n                delegatingStateRestoreListener,\n                exception -> defaultStreamsUncaughtExceptionHandler(exception, false)\n            );\n            globalThreadState = globalStreamThread.state();\n        }\n\n        threadState = new HashMap<>(numStreamThreads);\n        streamStateListener = new StreamStateListener(threadState, globalThreadState);\n\n        final GlobalStateStoreProvider globalStateStoreProvider = new GlobalStateStoreProvider(this.topologyMetadata.globalStateStores());\n\n        if (hasGlobalTopology) {\n            globalStreamThread.setStateListener(streamStateListener);\n        }\n\n        queryableStoreProvider = new QueryableStoreProvider(globalStateStoreProvider);\n        for (int i = 1; i <= numStreamThreads; i++) {\n            createAndAddStreamThread(cacheSizePerThread, i);\n        }\n\n        stateDirCleaner = setupStateDirCleaner();\n        rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, applicationConfigs);\n    }\n\n    private StreamThread createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx) {\n        final StreamThread streamThread = StreamThread.create(\n            topologyMetadata,\n            applicationConfigs,\n            clientSupplier,\n            adminClient,\n            processId,\n            clientId,\n            streamsMetrics,\n            time,\n            streamsMetadataState,\n            cacheSizePerThread,\n            stateDirectory,\n            delegatingStateRestoreListener,\n            delegatingStandbyUpdateListener,\n            threadIdx,\n            KafkaStreams.this::closeToError,\n            streamsUncaughtExceptionHandler\n        );\n        streamThread.setStateListener(streamStateListener);\n        threads.add(streamThread);\n        registerStreamThread(streamThread);\n        queryableStoreProvider.addStoreProviderForThread(streamThread.getName(), new StreamThreadStateStoreProvider(streamThread));\n        return streamThread;\n    }\n\n    private void registerStreamThread(StreamThread streamThread) {\n        getThreadState().put(streamThread.getId(), streamThread.state());\n    }\n\n    static Metrics getMetrics(final StreamsConfig config, final Time time, final String clientId) {\n        final MetricConfig metricConfig = new MetricConfig()\n            .samples(config.getInt(StreamsConfig.METRICS_NUM_SAMPLES_CONFIG))\n            .recordLevel(Sensor.RecordingLevel.forName(config.getString(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG)))\n            .timeWindow(config.getLong(StreamsConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG), TimeUnit.MILLISECONDS);\n        final List<MetricsReporter> reporters = CommonClientConfigs.metricsReporters(clientId, config);\n\n        final MetricsContext metricsContext = new KafkaMetricsContext(JMX_PREFIX,\n                                                                      config.originalsWithPrefix(CommonClientConfigs.METRICS_CONTEXT_PREFIX));\n        return new Metrics(metricConfig, reporters, time, metricsContext);\n    }\n\n    /**\n     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n     * Kafka Streams client.\n     * <p>\n     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n     * threads does not exceed the total cache size specified in configuration\n     * {@link StreamsConfig#STATESTORE_CACHE_MAX_BYTES_CONFIG}.\n     * <p>\n     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n     *\n     * @return name of the added stream thread or empty if a new stream thread could not be added\n     */\n    public Optional<String> addStreamThread() {\n        if (isRunningOrRebalancing()) {\n            final StreamThread streamThread;\n            synchronized (changeThreadCount) {\n                final int threadIdx = getNextThreadIndex();\n                final int numLiveThreads = getNumLiveStreamThreads();\n                final long cacheSizePerThread = getCacheSizePerThread(numLiveThreads + 1);\n                log.info(\"Adding StreamThread-{}, there will now be {} live threads and the new cache size per thread is {}\",\n                         threadIdx, numLiveThreads + 1, cacheSizePerThread);\n                resizeThreadCache(cacheSizePerThread);\n                // Creating thread should hold the lock in order to avoid duplicate thread index.\n                // If the duplicate index happen, the metadata of thread may be duplicate too.\n                streamThread = createAndAddStreamThread(cacheSizePerThread, threadIdx);\n            }\n\n            synchronized (stateLock) {\n                if (isRunningOrRebalancing()) {\n                    streamThread.start();\n                    return Optional.of(streamThread.getName());\n                } else {\n                    log.warn(\"Terminating the new thread because the Kafka Streams client is in state {}\", state);\n                    streamThread.shutdown();\n                    threads.remove(streamThread);\n                    final long cacheSizePerThread = getCacheSizePerThread(getNumLiveStreamThreads());\n                    log.info(\"Resizing thread cache due to terminating added thread, new cache size per thread is {}\", cacheSizePerThread);\n                    resizeThreadCache(cacheSizePerThread);\n                    return Optional.empty();\n                }\n            }\n        } else {\n            log.warn(\"Cannot add a stream thread when Kafka Streams client is in state {}\", state);\n            return Optional.empty();\n        }\n    }\n\n    /**\n     * Removes one stream thread out of the running stream threads from this Kafka Streams client.\n     * <p>\n     * The removed stream thread is gracefully shut down. This method does not specify which stream\n     * thread is shut down.\n     * <p>\n     * Since the number of stream threads decreases, the sizes of the caches in the remaining stream\n     * threads are adapted so that the sum of the cache sizes over all stream threads equals the total\n     * cache size specified in configuration {@link StreamsConfig#STATESTORE_CACHE_MAX_BYTES_CONFIG}.\n     *\n     * @return name of the removed stream thread or empty if a stream thread could not be removed because\n     *         no stream threads are alive\n     */\n    public Optional<String> removeStreamThread() {\n        return removeStreamThread(Long.MAX_VALUE);\n    }\n\n    /**\n     * Removes one stream thread out of the running stream threads from this Kafka Streams client.\n     * <p>\n     * The removed stream thread is gracefully shut down. This method does not specify which stream\n     * thread is shut down.\n     * <p>\n     * Since the number of stream threads decreases, the sizes of the caches in the remaining stream\n     * threads are adapted so that the sum of the cache sizes over all stream threads equals the total\n     * cache size specified in configuration {@link StreamsConfig#STATESTORE_CACHE_MAX_BYTES_CONFIG}.\n     *\n     * @param timeout The length of time to wait for the thread to shut down\n     * @throws org.apache.kafka.common.errors.TimeoutException if the thread does not stop in time\n     * @return name of the removed stream thread or empty if a stream thread could not be removed because\n     *         no stream threads are alive\n     */\n    public Optional<String> removeStreamThread(final Duration timeout) {\n        final String msgPrefix = prepareMillisCheckFailMsgPrefix(timeout, \"timeout\");\n        final long timeoutMs = validateMillisecondDuration(timeout, msgPrefix);\n        return removeStreamThread(timeoutMs);\n    }\n\n    private Optional<String> removeStreamThread(final long timeoutMs) throws TimeoutException {\n        final long startMs = time.milliseconds();\n\n        if (isRunningOrRebalancing()) {\n            synchronized (changeThreadCount) {\n                // make a copy of threads to avoid holding lock\n                for (final StreamThread streamThread : new ArrayList<>(threads)) {\n                    final boolean callingThreadIsNotCurrentStreamThread = !streamThread.getName().equals(Thread.currentThread().getName());\n                    if (streamThread.isThreadAlive() && (callingThreadIsNotCurrentStreamThread || getNumLiveStreamThreads() == 1)) {\n                        log.info(\"Removing StreamThread \" + streamThread.getName());\n                        final Optional<String> groupInstanceID = streamThread.getGroupInstanceID();\n                        streamThread.requestLeaveGroupDuringShutdown();\n                        streamThread.shutdown();\n                        if (!streamThread.getName().equals(Thread.currentThread().getName())) {\n                            final long remainingTimeMs = timeoutMs - (time.milliseconds() - startMs);\n                            if (remainingTimeMs <= 0 || !streamThread.waitOnThreadState(StreamThread.State.DEAD, remainingTimeMs)) {\n                                log.warn(\"{} did not shutdown in the allotted time.\", streamThread.getName());\n                                // Don't remove from threads until shutdown is complete. We will trim it from the\n                                // list once it reaches DEAD, and if for some reason it's hanging indefinitely in the\n                                // shutdown then we should just consider this thread.id to be burned\n                            } else {\n                                log.info(\"Successfully removed {} in {}ms\", streamThread.getName(), time.milliseconds() - startMs);\n                                threads.remove(streamThread);\n                                queryableStoreProvider.removeStoreProviderForThread(streamThread.getName());\n                            }\n                        } else {\n                            log.info(\"{} is the last remaining thread and must remove itself, therefore we cannot wait \"\n                                + \"for it to complete shutdown as this will result in deadlock.\", streamThread.getName());\n                        }\n\n                        final long cacheSizePerThread = getCacheSizePerThread(getNumLiveStreamThreads());\n                        log.info(\"Resizing thread cache due to thread removal, new cache size per thread is {}\", cacheSizePerThread);\n                        resizeThreadCache(cacheSizePerThread);\n                        if (groupInstanceID.isPresent() && callingThreadIsNotCurrentStreamThread) {\n                            final MemberToRemove memberToRemove = new MemberToRemove(groupInstanceID.get());\n                            final Collection<MemberToRemove> membersToRemove = Collections.singletonList(memberToRemove);\n                            final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = \n                                adminClient.removeMembersFromConsumerGroup(\n                                    applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG),\n                                    new RemoveMembersFromConsumerGroupOptions(membersToRemove)\n                                );\n                            try {\n                                final long remainingTimeMs = timeoutMs - (time.milliseconds() - startMs);\n                                removeMembersFromConsumerGroupResult.memberResult(memberToRemove).get(remainingTimeMs, TimeUnit.MILLISECONDS);\n                            } catch (final java.util.concurrent.TimeoutException exception) {\n                                log.error(\n                                    String.format(\n                                        \"Could not remove static member %s from consumer group %s due to a timeout:\",\n                                        groupInstanceID.get(),\n                                        applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG)\n                                    ),\n                                        exception\n                                );\n                                throw new TimeoutException(exception.getMessage(), exception);\n                            } catch (final InterruptedException e) {\n                                Thread.currentThread().interrupt();\n                            } catch (final ExecutionException exception) {\n                                log.error(\n                                    String.format(\n                                        \"Could not remove static member %s from consumer group %s due to:\",\n                                        groupInstanceID.get(),\n                                        applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG)\n                                    ),\n                                        exception\n                                );\n                                throw new StreamsException(\n                                        \"Could not remove static member \" + groupInstanceID.get()\n                                            + \" from consumer group \" + applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG)\n                                            + \" for the following reason: \",\n                                        exception.getCause()\n                                );\n                            }\n                        }\n                        final long remainingTimeMs = timeoutMs - (time.milliseconds() - startMs);\n                        if (remainingTimeMs <= 0) {\n                            throw new TimeoutException(\"Thread \" + streamThread.getName() + \" did not stop in the allotted time\");\n                        }\n                        return Optional.of(streamThread.getName());\n                    }\n                }\n            }\n            log.warn(\"There are no threads eligible for removal\");\n        } else {\n            log.warn(\"Cannot remove a stream thread when Kafka Streams client is in state  \" + state());\n        }\n        return Optional.empty();\n    }\n\n    /*\n     * Takes a snapshot and counts the number of stream threads which are not in PENDING_SHUTDOWN or DEAD\n     *\n     * note: iteration over SynchronizedList is not thread safe so it must be manually synchronized. However, we may\n     * require other locks when looping threads and it could cause deadlock. Hence, we create a copy to avoid holding\n     * threads lock when looping threads.\n     * @return number of alive stream threads\n     */\n    private int getNumLiveStreamThreads() {\n        final AtomicInteger numLiveThreads = new AtomicInteger(0);\n\n        synchronized (threads) {\n            processStreamThread(thread -> {\n                if (thread.state() == StreamThread.State.DEAD) {\n                    log.debug(\"Trimming thread {} from the threads list since it's state is {}\", thread.getName(), StreamThread.State.DEAD);\n                    threads.remove(thread);\n                } else if (thread.state() == StreamThread.State.PENDING_SHUTDOWN) {\n                    log.debug(\"Skipping thread {} from num live threads computation since it's state is {}\",\n                              thread.getName(), StreamThread.State.PENDING_SHUTDOWN);\n                } else {\n                    numLiveThreads.incrementAndGet();\n                }\n            });\n            return numLiveThreads.get();\n        }\n    }\n\n    private int getNextThreadIndex() {\n        final HashSet<String> allLiveThreadNames = new HashSet<>();\n        final AtomicInteger maxThreadId = new AtomicInteger(1);\n        synchronized (threads) {\n            processStreamThread(thread -> {\n                // trim any DEAD threads from the list so we can reuse the thread.id\n                // this is only safe to do once the thread has fully completed shutdown\n                if (thread.state() == StreamThread.State.DEAD) {\n                    threads.remove(thread);\n                } else {\n                    allLiveThreadNames.add(thread.getName());\n                    // Assume threads are always named with the \"-StreamThread-<threadId>\" suffix\n                    final int threadId = Integer.parseInt(thread.getName().substring(thread.getName().lastIndexOf(\"-\") + 1));\n                    if (threadId > maxThreadId.get()) {\n                        maxThreadId.set(threadId);\n                    }\n                }\n            });\n\n            final String baseName = clientId + \"-StreamThread-\";\n            for (int i = 1; i <= maxThreadId.get(); i++) {\n                final String name = baseName + i;\n                if (!allLiveThreadNames.contains(name)) {\n                    return i;\n                }\n            }\n            // It's safe to use threads.size() rather than getNumLiveStreamThreads() to infer the number of threads\n            // here since we trimmed any DEAD threads earlier in this method while holding the lock\n            return threads.size() + 1;\n        }\n    }\n\n    private long getCacheSizePerThread(final int numStreamThreads) {\n        if (numStreamThreads == 0) {\n            return totalCacheSize;\n        }\n        return totalCacheSize / (numStreamThreads + (topologyMetadata.hasGlobalTopology() ? 1 : 0));\n    }\n\n    private void resizeThreadCache(final long cacheSizePerThread) {\n        processStreamThread(thread -> thread.resizeCache(cacheSizePerThread));\n        if (globalStreamThread != null) {\n            globalStreamThread.resize(cacheSizePerThread);\n        }\n    }\n\n    private ScheduledExecutorService setupStateDirCleaner() {\n        return Executors.newSingleThreadScheduledExecutor(r -> {\n            final Thread thread = new Thread(r, clientId + \"-CleanupThread\");\n            thread.setDaemon(true);\n            return thread;\n        });\n    }\n\n    private static ScheduledExecutorService maybeCreateRocksDBMetricsRecordingService(final String clientId,\n                                                                                      final StreamsConfig config) {\n        if (RecordingLevel.forName(config.getString(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG)) == RecordingLevel.DEBUG) {\n            return Executors.newSingleThreadScheduledExecutor(r -> {\n                final Thread thread = new Thread(r, clientId + \"-RocksDBMetricsRecordingTrigger\");\n                thread.setDaemon(true);\n                return thread;\n            });\n        }\n        return null;\n    }\n\n    private static HostInfo parseHostInfo(final String endPoint) {\n        final HostInfo hostInfo = HostInfo.buildFromEndpoint(endPoint);\n        if (hostInfo == null) {\n            return StreamsMetadataState.UNKNOWN_HOST;\n        } else {\n            return hostInfo;\n        }\n    }\n\n    /**\n     * Start the {@code KafkaStreams} instance by starting all its threads.\n     * This function is expected to be called only once during the life cycle of the client.\n     * <p>\n     * Because threads are started in the background, this method does not block.\n     * However, if you have global stores in your topology, this method blocks until all global stores are restored.\n     * As a consequence, any fatal exception that happens during processing is by default only logged.\n     * If you want to be notified about dying threads, you can\n     * {@link #setUncaughtExceptionHandler(Thread.UncaughtExceptionHandler) register an uncaught exception handler}\n     * before starting the {@code KafkaStreams} instance.\n     * <p>\n     * Note, for brokers with version {@code 0.9.x} or lower, the broker version cannot be checked.\n     * There will be no error and the client will hang and retry to verify the broker version until it\n     * {@link StreamsConfig#REQUEST_TIMEOUT_MS_CONFIG times out}.\n\n     * @throws IllegalStateException if process was already started\n     * @throws StreamsException if the Kafka brokers have version 0.10.0.x or\n     *                          if {@link StreamsConfig#PROCESSING_GUARANTEE_CONFIG exactly-once} is enabled for pre 0.11.0.x brokers\n     */\n    public synchronized void start() throws IllegalStateException, StreamsException {\n        if (setState(State.REBALANCING)) {\n            log.debug(\"Starting Streams client\");\n\n            if (globalStreamThread != null) {\n                globalStreamThread.start();\n            }\n\n            final int numThreads = processStreamThread(StreamThread::start);\n\n            log.info(\"Started {} stream threads\", numThreads);\n\n            final Long cleanupDelay = applicationConfigs.getLong(StreamsConfig.STATE_CLEANUP_DELAY_MS_CONFIG);\n            stateDirCleaner.scheduleAtFixedRate(() -> {\n                // we do not use lock here since we only read on the value and act on it\n                if (state == State.RUNNING) {\n                    stateDirectory.cleanRemovedTasks(cleanupDelay);\n                }\n            }, cleanupDelay, cleanupDelay, TimeUnit.MILLISECONDS);\n\n            final long recordingDelay = 0;\n            final long recordingInterval = 1;\n            if (rocksDBMetricsRecordingService != null) {\n                rocksDBMetricsRecordingService.scheduleAtFixedRate(\n                    streamsMetrics.rocksDBMetricsRecordingTrigger(),\n                    recordingDelay,\n                    recordingInterval,\n                    TimeUnit.MINUTES\n                );\n            }\n        } else {\n            throw new IllegalStateException(\"The client is either already started or already stopped, cannot re-start\");\n        }\n    }\n\n    /**\n     * Class that handles options passed in case of {@code KafkaStreams} instance scale down\n     */\n    public static class CloseOptions {\n        private Duration timeout = Duration.ofMillis(Long.MAX_VALUE);\n        private boolean leaveGroup = false;\n\n        public CloseOptions timeout(final Duration timeout) {\n            this.timeout = timeout;\n            return this;\n        }\n\n        public CloseOptions leaveGroup(final boolean leaveGroup) {\n            this.leaveGroup = leaveGroup;\n            return this;\n        }\n    }\n\n    /**\n     * Shutdown this {@code KafkaStreams} instance by signaling all the threads to stop, and then wait for them to join.\n     * This will block until all threads have stopped.\n     */\n    public void close() {\n        close(Long.MAX_VALUE, false);\n    }\n\n    private Thread shutdownHelper(final boolean error, final long timeoutMs, final boolean leaveGroup) {\n        stateDirCleaner.shutdownNow();\n        if (rocksDBMetricsRecordingService != null) {\n            rocksDBMetricsRecordingService.shutdownNow();\n        }\n\n        // wait for all threads to join in a separate thread;\n        // save the current thread so that if it is a stream thread\n        // we don't attempt to join it and cause a deadlock\n        return new Thread(() -> {\n            // notify all the threads to stop; avoid deadlocks by stopping any\n            // further state reports from the thread since we're shutting down\n            int numStreamThreads = processStreamThread(StreamThread::shutdown);\n\n            log.info(\"Shutting down {} stream threads\", numStreamThreads);\n\n            topologyMetadata.wakeupThreads();\n\n            numStreamThreads = processStreamThread(thread -> {\n                try {\n                    if (!thread.isRunning()) {\n                        log.debug(\"Shutdown {} complete\", thread.getName());\n\n                        thread.join();\n                    }\n                } catch (final InterruptedException ex) {\n                    log.warn(\"Shutdown {} interrupted\", thread.getName());\n\n                    Thread.currentThread().interrupt();\n                }\n            });\n\n            if (leaveGroup) {\n                processStreamThread(streamThreadLeaveConsumerGroup(timeoutMs));\n            }\n\n            log.info(\"Shutdown {} stream threads complete\", numStreamThreads);\n\n            if (globalStreamThread != null) {\n                log.info(\"Shutting down the global stream threads\");\n\n                globalStreamThread.shutdown();\n            }\n\n            if (globalStreamThread != null && !globalStreamThread.stillRunning()) {\n                try {\n                    globalStreamThread.join();\n                } catch (final InterruptedException e) {\n                    log.warn(\"Shutdown the global stream thread interrupted\");\n\n                    Thread.currentThread().interrupt();\n                }\n                globalStreamThread = null;\n\n                log.info(\"Shutdown global stream threads complete\");\n            }\n\n            stateDirectory.close();\n            adminClient.close();\n\n            streamsMetrics.removeAllClientLevelSensorsAndMetrics();\n            metrics.close();\n            if (!error) {\n                setState(State.NOT_RUNNING);\n            } else {\n                setState(State.ERROR);\n            }\n        }, clientId + \"-CloseThread\");\n    }\n\n    private boolean close(final long timeoutMs, final boolean leaveGroup) {\n        if (state.hasCompletedShutdown()) {\n            log.info(\"Streams client is already in the terminal {} state, all resources are closed and the client has stopped.\", state);\n            return true;\n        }\n        if (state.isShuttingDown()) {\n            log.info(\"Streams client is in {}, all resources are being closed and the client will be stopped.\", state);\n            if (state == State.PENDING_ERROR && waitOnState(State.ERROR, timeoutMs)) {\n                log.info(\"Streams client stopped to ERROR completely\");\n                return true;\n            } else if (state == State.PENDING_SHUTDOWN && waitOnState(State.NOT_RUNNING, timeoutMs)) {\n                log.info(\"Streams client stopped to NOT_RUNNING completely\");\n                return true;\n            } else {\n                log.warn(\"Streams client cannot transition to {} completely within the timeout\",\n                         state == State.PENDING_SHUTDOWN ? State.NOT_RUNNING : State.ERROR);\n                return false;\n            }\n        }\n\n        if (!setState(State.PENDING_SHUTDOWN)) {\n            // if we can't transition to PENDING_SHUTDOWN but not because we're already shutting down, then it must be fatal\n            log.error(\"Failed to transition to PENDING_SHUTDOWN, current state is {}\", state);\n            throw new StreamsException(\"Failed to shut down while in state \" + state);\n        } else {\n\n            final Thread shutdownThread = shutdownHelper(false, timeoutMs, leaveGroup);\n\n            shutdownThread.setDaemon(true);\n            shutdownThread.start();\n        }\n\n        if (waitOnState(State.NOT_RUNNING, timeoutMs)) {\n            log.info(\"Streams client stopped completely\");\n            return true;\n        } else {\n            log.info(\"Streams client cannot stop completely within the {}ms timeout\", timeoutMs);\n            return false;\n        }\n    }\n\n    private void closeToError() {\n        if (!setState(State.PENDING_ERROR)) {\n            log.info(\"Skipping shutdown since we are already in \" + state());\n        } else {\n            final Thread shutdownThread = shutdownHelper(true, -1, false);\n\n            shutdownThread.setDaemon(true);\n            shutdownThread.start();\n        }\n    }\n\n    /**\n     * Shutdown this {@code KafkaStreams} by signaling all the threads to stop, and then wait up to the timeout for the\n     * threads to join.\n     * A {@code timeout} of Duration.ZERO (or any other zero duration) makes the close operation asynchronous.\n     * Negative-duration timeouts are rejected.\n     *\n     * @param timeout  how long to wait for the threads to shutdown\n     * @return {@code true} if all threads were successfully stopped&mdash;{@code false} if the timeout was reached\n     * before all threads stopped\n     * Note that this method must not be called in the {@link StateListener#onChange(KafkaStreams.State, KafkaStreams.State)} callback of {@link StateListener}.\n     * @throws IllegalArgumentException if {@code timeout} can't be represented as {@code long milliseconds}\n     */\n    public synchronized boolean close(final Duration timeout) throws IllegalArgumentException {\n        final String msgPrefix = prepareMillisCheckFailMsgPrefix(timeout, \"timeout\");\n        final long timeoutMs = validateMillisecondDuration(timeout, msgPrefix);\n        if (timeoutMs < 0) {\n            throw new IllegalArgumentException(\"Timeout can't be negative.\");\n        }\n\n        log.debug(\"Stopping Streams client with timeoutMillis = {} ms.\", timeoutMs);\n\n        return close(timeoutMs, false);\n    }\n\n    /**\n     * Shutdown this {@code KafkaStreams} by signaling all the threads to stop, and then wait up to the timeout for the\n     * threads to join.\n     * @param options  contains timeout to specify how long to wait for the threads to shutdown, and a flag leaveGroup to\n     *                 trigger consumer leave call\n     * @return {@code true} if all threads were successfully stopped&mdash;{@code false} if the timeout was reached\n     * before all threads stopped\n     * Note that this method must not be called in the {@link StateListener#onChange(KafkaStreams.State, KafkaStreams.State)} callback of {@link StateListener}.\n     * @throws IllegalArgumentException if {@code timeout} can't be represented as {@code long milliseconds}\n     */\n    public synchronized boolean close(final CloseOptions options) throws IllegalArgumentException {\n        Objects.requireNonNull(options, \"options cannot be null\");\n        final String msgPrefix = prepareMillisCheckFailMsgPrefix(options.timeout, \"timeout\");\n        final long timeoutMs = validateMillisecondDuration(options.timeout, msgPrefix);\n        if (timeoutMs < 0) {\n            throw new IllegalArgumentException(\"Timeout can't be negative.\");\n        }\n        log.debug(\"Stopping Streams client with timeoutMillis = {} ms.\", timeoutMs);\n        return close(timeoutMs, options.leaveGroup);\n    }\n\n    private Consumer<StreamThread> streamThreadLeaveConsumerGroup(final long remainingTimeMs) {\n        return thread -> {\n            final Optional<String> groupInstanceId = thread.getGroupInstanceID();\n            if (groupInstanceId.isPresent()) {\n                log.debug(\"Sending leave group trigger to removing instance from consumer group: {}.\",\n                    groupInstanceId.get());\n                final MemberToRemove memberToRemove = new MemberToRemove(groupInstanceId.get());\n                final Collection<MemberToRemove> membersToRemove = Collections.singletonList(memberToRemove);\n\n                final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = adminClient\n                    .removeMembersFromConsumerGroup(\n                        applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG),\n                        new RemoveMembersFromConsumerGroupOptions(membersToRemove)\n                    );\n\n                try {\n                    removeMembersFromConsumerGroupResult.memberResult(memberToRemove)\n                        .get(remainingTimeMs, TimeUnit.MILLISECONDS);\n                } catch (final Exception e) {\n                    final String msg = String.format(\"Could not remove static member %s from consumer group %s.\",\n                                                     groupInstanceId.get(), applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG));\n                    log.error(msg, e);\n                }\n            }\n        };\n    }\n\n    /**\n     * Do a clean up of the local {@link StateStore} directory ({@link StreamsConfig#STATE_DIR_CONFIG}) by deleting all\n     * data with regard to the {@link StreamsConfig#APPLICATION_ID_CONFIG application ID}.\n     * <p>\n     * May only be called either before this {@code KafkaStreams} instance is {@link #start() started} or after the\n     * instance is {@link #close() closed}.\n     * <p>\n     * Calling this method triggers a restore of local {@link StateStore}s on the next {@link #start() application start}.\n     *\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has been started and hasn't fully shut down\n     * @throws StreamsException if cleanup failed\n     */\n    public void cleanUp() {\n        if (!(state.hasNotStarted() || state.hasCompletedShutdown())) {\n            throw new IllegalStateException(\"Cannot clean up while running.\");\n        }\n        stateDirectory.clean();\n    }\n\n    /**\n     * Find all currently running {@code KafkaStreams} instances (potentially remotely) that use the same\n     * {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} as this instance (i.e., all instances that belong to\n     * the same Kafka Streams application) and return {@link StreamsMetadata} for each discovered instance.\n     * <p>\n     * Note: this is a point in time view and it may change due to partition reassignment.\n     *\n     * @return {@link StreamsMetadata} for each {@code KafkaStreams} instances of this application\n     * @deprecated since 3.0.0 use {@link KafkaStreams#metadataForAllStreamsClients}\n     */\n    @Deprecated\n    public Collection<org.apache.kafka.streams.state.StreamsMetadata> allMetadata() {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getAllMetadata().stream().map(streamsMetadata ->\n                new org.apache.kafka.streams.state.StreamsMetadata(streamsMetadata.hostInfo(),\n                        streamsMetadata.stateStoreNames(),\n                        streamsMetadata.topicPartitions(),\n                        streamsMetadata.standbyStateStoreNames(),\n                        streamsMetadata.standbyTopicPartitions()))\n                .collect(Collectors.toSet());\n    }\n\n    /**\n     * Find all currently running {@code KafkaStreams} instances (potentially remotely) that use the same\n     * {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} as this instance (i.e., all instances that belong to\n     * the same Kafka Streams application) and return {@link StreamsMetadata} for each discovered instance.\n     * <p>\n     * Note: this is a point in time view and it may change due to partition reassignment.\n     *\n     * @return {@link StreamsMetadata} for each {@code KafkaStreams} instances of this application\n     */\n    public Collection<StreamsMetadata> metadataForAllStreamsClients() {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getAllMetadata();\n    }\n\n    /**\n     * Find all currently running {@code KafkaStreams} instances (potentially remotely) that\n     * <ul>\n     *   <li>use the same {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} as this instance (i.e., all\n     *       instances that belong to the same Kafka Streams application)</li>\n     *   <li>and that contain a {@link StateStore} with the given {@code storeName}</li>\n     * </ul>\n     * and return {@link StreamsMetadata} for each discovered instance.\n     * <p>\n     * Note: this is a point in time view and it may change due to partition reassignment.\n     *\n     * @param storeName the {@code storeName} to find metadata for\n     * @return {@link StreamsMetadata} for each {@code KafkaStreams} instances with the provide {@code storeName} of\n     * this application\n     * @deprecated since 3.0.0 use {@link KafkaStreams#streamsMetadataForStore} instead\n     */\n    @Deprecated\n    public Collection<org.apache.kafka.streams.state.StreamsMetadata> allMetadataForStore(final String storeName) {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getAllMetadataForStore(storeName).stream().map(streamsMetadata ->\n                new org.apache.kafka.streams.state.StreamsMetadata(streamsMetadata.hostInfo(),\n                        streamsMetadata.stateStoreNames(),\n                        streamsMetadata.topicPartitions(),\n                        streamsMetadata.standbyStateStoreNames(),\n                        streamsMetadata.standbyTopicPartitions()))\n                .collect(Collectors.toSet());\n    }\n\n    /**\n     * Find all currently running {@code KafkaStreams} instances (potentially remotely) that\n     * <ul>\n     *   <li>use the same {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} as this instance (i.e., all\n     *       instances that belong to the same Kafka Streams application)</li>\n     *   <li>and that contain a {@link StateStore} with the given {@code storeName}</li>\n     * </ul>\n     * and return {@link StreamsMetadata} for each discovered instance.\n     * <p>\n     * Note: this is a point in time view and it may change due to partition reassignment.\n     *\n     * @param storeName the {@code storeName} to find metadata for\n     * @return {@link StreamsMetadata} for each {@code KafkaStreams} instances with the provide {@code storeName} of\n     * this application\n     */\n    public Collection<StreamsMetadata> streamsMetadataForStore(final String storeName) {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getAllMetadataForStore(storeName);\n    }\n\n    /**\n     * Finds the metadata containing the active hosts and standby hosts where the key being queried would reside.\n     *\n     * @param storeName     the {@code storeName} to find metadata for\n     * @param key           the key to find metadata for\n     * @param keySerializer serializer for the key\n     * @param <K>           key type\n     * Returns {@link KeyQueryMetadata} containing all metadata about hosting the given key for the given store,\n     * or {@code null} if no matching metadata could be found.\n     */\n    public <K> KeyQueryMetadata queryMetadataForKey(final String storeName,\n                                                    final K key,\n                                                    final Serializer<K> keySerializer) {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getKeyQueryMetadataForKey(storeName, key, keySerializer);\n    }\n\n    /**\n     * Finds the metadata containing the active hosts and standby hosts where the key being queried would reside.\n     *\n     * @param storeName     the {@code storeName} to find metadata for\n     * @param key           the key to find metadata for\n     * @param partitioner the partitioner to be use to locate the host for the key\n     * @param <K>           key type\n     * Returns {@link KeyQueryMetadata} containing all metadata about hosting the given key for the given store, using the\n     * the supplied partitioner, or {@code null} if no matching metadata could be found.\n     */\n    public <K> KeyQueryMetadata queryMetadataForKey(final String storeName,\n                                                    final K key,\n                                                    final StreamPartitioner<? super K, ?> partitioner) {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getKeyQueryMetadataForKey(storeName, key, partitioner);\n    }\n\n    /**\n     * Get a facade wrapping the local {@link StateStore} instances with the provided {@link StoreQueryParameters}.\n     * The returned object can be used to query the {@link StateStore} instances.\n     *\n     * @param storeQueryParameters   the parameters used to fetch a queryable store\n     * @return A facade wrapping the local {@link StateStore} instances\n     * @throws StreamsNotStartedException If Streams has not yet been started. Just call {@link KafkaStreams#start()}\n     *                                    and then retry this call.\n     * @throws UnknownStateStoreException If the specified store name does not exist in the topology.\n     * @throws InvalidStateStorePartitionException If the specified partition does not exist.\n     * @throws InvalidStateStoreException If the Streams instance isn't in a queryable state.\n     *                                    If the store's type does not match the QueryableStoreType,\n     *                                    the Streams instance is not in a queryable state with respect\n     *                                    to the parameters, or if the store is not available locally, then\n     *                                    an InvalidStateStoreException is thrown upon store access.\n     */\n    public <T> T store(final StoreQueryParameters<T> storeQueryParameters) {\n        validateIsRunningOrRebalancing();\n        final String storeName = storeQueryParameters.storeName();\n        if (!topologyMetadata.hasStore(storeName)) {\n            throw new UnknownStateStoreException(\n                \"Cannot get state store \" + storeName + \" because no such store is registered in the topology.\"\n            );\n        }\n        return queryableStoreProvider.getStore(storeQueryParameters);\n    }\n\n    /**\n     *  This method pauses processing for the KafkaStreams instance.\n     *\n     *  <p>Paused topologies will only skip over a) processing, b) punctuation, and c) standby tasks.\n     *  Notably, paused topologies will still poll Kafka consumers, and commit offsets.\n     *  This method sets transient state that is not maintained or managed among instances.\n     *  Note that pause() can be called before start() in order to start a KafkaStreams instance\n     *  in a manner where the processing is paused as described, but the consumers are started up.\n     */\n    public void pause() {\n        if (topologyMetadata.hasNamedTopologies()) {\n            for (final NamedTopology namedTopology : topologyMetadata.getAllNamedTopologies()) {\n                topologyMetadata.pauseTopology(namedTopology.name());\n            }\n        } else {\n            topologyMetadata.pauseTopology(UNNAMED_TOPOLOGY);\n        }\n    }\n\n    /**\n     * @return true when the KafkaStreams instance has its processing paused.\n     */\n    public boolean isPaused() {\n        if (topologyMetadata.hasNamedTopologies()) {\n            return topologyMetadata.getAllNamedTopologies().stream()\n                .map(NamedTopology::name)\n                .allMatch(topologyMetadata::isPaused);\n        } else {\n            return topologyMetadata.isPaused(UNNAMED_TOPOLOGY);\n        }\n    }\n\n    /**\n     * This method resumes processing for the KafkaStreams instance.\n     */\n    public void resume() {\n        if (topologyMetadata.hasNamedTopologies()) {\n            for (final NamedTopology namedTopology : topologyMetadata.getAllNamedTopologies()) {\n                topologyMetadata.resumeTopology(namedTopology.name());\n            }\n        } else {\n            topologyMetadata.resumeTopology(UNNAMED_TOPOLOGY);\n        }\n        threads.forEach(StreamThread::signalResume);\n    }\n\n    /**\n     * handle each stream thread in a snapshot of threads.\n     * noted: iteration over SynchronizedList is not thread safe so it must be manually synchronized. However, we may\n     * require other locks when looping threads and it could cause deadlock. Hence, we create a copy to avoid holding\n     * threads lock when looping threads.\n     * @param consumer handler\n     */\n    protected int processStreamThread(final Consumer<StreamThread> consumer) {\n        final List<StreamThread> copy = new ArrayList<>(threads);\n        for (final StreamThread thread : copy) consumer.accept(thread);\n\n        return copy.size();\n    }\n\n    /**\n     * Returns the internal clients' assigned {@code client instance ids}.\n     *\n     * @return The internal clients' assigned instance ids used for metrics collection.\n     *\n     * @throws IllegalArgumentException If {@code timeout} is negative.\n     * @throws IllegalStateException If {@code KafkaStreams} is not running.\n     * @throws TimeoutException Indicates that a request timed out.\n     * @throws StreamsException For any other error that might occur.\n     */\n    public synchronized ClientInstanceIds clientInstanceIds(final Duration timeout) {\n        if (timeout.isNegative()) {\n            throw new IllegalArgumentException(\"The timeout cannot be negative.\");\n        }\n        if (state().hasNotStarted()) {\n            throw new IllegalStateException(\"KafkaStreams has not been started, you can retry after calling start().\");\n        }\n        if (state().isShuttingDown() || state.hasCompletedShutdown()) {\n            throw new IllegalStateException(\"KafkaStreams has been stopped (\" + state + \").\");\n        }\n\n        final Timer remainingTime = time.timer(timeout.toMillis());\n        final ClientInstanceIdsImpl clientInstanceIds = new ClientInstanceIdsImpl();\n\n        // (1) fan-out calls to threads\n\n        // StreamThread for main/restore consumers and producer(s)\n        final Map<String, KafkaFuture<Uuid>> consumerFutures = new HashMap<>();\n        final Map<String, KafkaFuture<Map<String, KafkaFuture<Uuid>>>> producerFutures = new HashMap<>();\n        synchronized (changeThreadCount) {\n            for (final StreamThread streamThread : threads) {\n                consumerFutures.putAll(streamThread.consumerClientInstanceIds(timeout));\n                producerFutures.put(streamThread.getName(), streamThread.producersClientInstanceIds(timeout));\n            }\n        }\n\n        // GlobalThread\n        KafkaFuture<Uuid> globalThreadFuture = null;\n        if (globalStreamThread != null) {\n            globalThreadFuture = globalStreamThread.globalConsumerInstanceId(timeout);\n        }\n\n        // (2) get admin client instance id in a blocking fashion, while Stream/GlobalThreads work in parallel\n        try {\n            clientInstanceIds.setAdminInstanceId(adminClient.clientInstanceId(timeout));\n            remainingTime.update(time.milliseconds());\n        } catch (final IllegalStateException telemetryDisabledError) {\n            // swallow\n            log.debug(\"Telemetry is disabled on the admin client.\");\n        } catch (final TimeoutException timeoutException) {\n            throw timeoutException;\n        } catch (final Exception error) {\n            throw new StreamsException(\"Could not retrieve admin client instance id.\", error);\n        }\n\n        // (3) collect client instance ids from threads\n\n        // (3a) collect consumers from StreamsThread\n        for (final Map.Entry<String, KafkaFuture<Uuid>> consumerFuture : consumerFutures.entrySet()) {\n            final Uuid instanceId = getOrThrowException(\n                consumerFuture.getValue(),\n                remainingTime.remainingMs(),\n                () -> String.format(\n                    \"Could not retrieve consumer instance id for %s.\",\n                    consumerFuture.getKey()\n                )\n            );\n            remainingTime.update(time.milliseconds());\n\n            // could be `null` if telemetry is disabled on the consumer itself\n            if (instanceId != null) {\n                clientInstanceIds.addConsumerInstanceId(\n                    consumerFuture.getKey(),\n                    instanceId\n                );\n            } else {\n                log.debug(String.format(\"Telemetry is disabled for %s.\", consumerFuture.getKey()));\n            }\n        }\n\n        // (3b) collect producers from StreamsThread\n        for (final Map.Entry<String, KafkaFuture<Map<String, KafkaFuture<Uuid>>>> threadProducerFuture : producerFutures.entrySet()) {\n            final Map<String, KafkaFuture<Uuid>> streamThreadProducerFutures = getOrThrowException(\n                threadProducerFuture.getValue(),\n                remainingTime.remainingMs(),\n                () -> String.format(\n                    \"Could not retrieve producer instance id for %s.\",\n                    threadProducerFuture.getKey()\n                )\n            );\n            remainingTime.update(time.milliseconds());\n\n            for (final Map.Entry<String, KafkaFuture<Uuid>> producerFuture : streamThreadProducerFutures.entrySet()) {\n                final Uuid instanceId = getOrThrowException(\n                    producerFuture.getValue(),\n                    remainingTime.remainingMs(),\n                    () -> String.format(\n                        \"Could not retrieve producer instance id for %s.\",\n                        producerFuture.getKey()\n                    )\n                );\n                remainingTime.update(time.milliseconds());\n\n                // could be `null` if telemetry is disabled on the producer itself\n                if (instanceId != null) {\n                    clientInstanceIds.addProducerInstanceId(\n                        producerFuture.getKey(),\n                        instanceId\n                    );\n                } else {\n                    log.debug(String.format(\"Telemetry is disabled for %s.\", producerFuture.getKey()));\n                }\n            }\n        }\n\n        // (3c) collect from GlobalThread\n        if (globalThreadFuture != null) {\n            final Uuid instanceId = getOrThrowException(\n                globalThreadFuture,\n                remainingTime.remainingMs(),\n                () -> \"Could not retrieve global consumer client instance id.\"\n            );\n            remainingTime.update(time.milliseconds());\n\n            // could be `null` if telemetry is disabled on the client itself\n            if (instanceId != null) {\n                clientInstanceIds.addConsumerInstanceId(\n                    globalStreamThread.getName(),\n                    instanceId\n                );\n            } else {\n                log.debug(\"Telemetry is disabled for the global consumer.\");\n            }\n        }\n\n        return clientInstanceIds;\n    }\n\n    private <T> T getOrThrowException(\n        final KafkaFuture<T> future,\n        final long timeoutMs,\n        final Supplier<String> errorMessage) {\n        final Throwable cause;\n\n        try {\n            return future.get(timeoutMs, TimeUnit.MILLISECONDS);\n        } catch (final java.util.concurrent.TimeoutException timeout) {\n            throw new TimeoutException(errorMessage.get(), timeout);\n        } catch (final ExecutionException exception) {\n            cause = exception.getCause();\n            if (cause instanceof TimeoutException) {\n                throw (TimeoutException) cause;\n            }\n        } catch (final InterruptedException error) {\n            cause = error;\n        }\n\n        throw new StreamsException(errorMessage.get(), cause);\n    }\n\n    /**\n     * Returns runtime information about the local threads of this {@link KafkaStreams} instance.\n     *\n     * @return the set of {@link org.apache.kafka.streams.processor.ThreadMetadata}.\n     * @deprecated since 3.0 use {@link #metadataForLocalThreads()}\n     */\n    @Deprecated\n    public Set<org.apache.kafka.streams.processor.ThreadMetadata> localThreadsMetadata() {\n        return metadataForLocalThreads().stream().map(threadMetadata -> new org.apache.kafka.streams.processor.ThreadMetadata(\n                threadMetadata.threadName(),\n                threadMetadata.threadState(),\n                threadMetadata.consumerClientId(),\n                threadMetadata.restoreConsumerClientId(),\n                threadMetadata.producerClientIds(),\n                threadMetadata.adminClientId(),\n                threadMetadata.activeTasks().stream().map(taskMetadata -> new org.apache.kafka.streams.processor.TaskMetadata(\n                        taskMetadata.taskId().toString(),\n                        taskMetadata.topicPartitions(),\n                        taskMetadata.committedOffsets(),\n                        taskMetadata.endOffsets(),\n                        taskMetadata.timeCurrentIdlingStarted())\n                ).collect(Collectors.toSet()),\n                threadMetadata.standbyTasks().stream().map(taskMetadata -> new org.apache.kafka.streams.processor.TaskMetadata(\n                        taskMetadata.taskId().toString(),\n                        taskMetadata.topicPartitions(),\n                        taskMetadata.committedOffsets(),\n                        taskMetadata.endOffsets(),\n                        taskMetadata.timeCurrentIdlingStarted())\n                ).collect(Collectors.toSet())))\n                .collect(Collectors.toSet());\n    }\n\n    /**\n     * Returns runtime information about the local threads of this {@link KafkaStreams} instance.\n     *\n     * @return the set of {@link ThreadMetadata}.\n     */\n    public Set<ThreadMetadata> metadataForLocalThreads() {\n        final Set<ThreadMetadata> threadMetadata = new HashSet<>();\n        processStreamThread(thread -> {\n            synchronized (thread.getStateLock()) {\n                if (thread.state() != StreamThread.State.DEAD) {\n                    threadMetadata.add(thread.threadMetadata());\n                }\n            }\n        });\n        return threadMetadata;\n    }\n\n    /**\n     * Returns {@link LagInfo}, for all store partitions (active or standby) local to this Streams instance. Note that the\n     * values returned are just estimates and meant to be used for making soft decisions on whether the data in the store\n     * partition is fresh enough for querying.\n     *\n     * <p>Note: Each invocation of this method issues a call to the Kafka brokers. Thus, it's advisable to limit the frequency\n     * of invocation to once every few seconds.\n     *\n     * @return map of store names to another map of partition to {@link LagInfo}s\n     * @throws StreamsException if the admin client request throws exception\n     */\n    public Map<String, Map<Integer, LagInfo>> allLocalStorePartitionLags() {\n        final List<Task> allTasks = new ArrayList<>();\n        processStreamThread(thread -> allTasks.addAll(thread.readyOnlyAllTasks()));\n        return allLocalStorePartitionLags(allTasks);\n    }\n\n    protected Map<String, Map<Integer, LagInfo>> allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor) {\n        final Map<String, Map<Integer, LagInfo>> localStorePartitionLags = new TreeMap<>();\n        final Collection<TopicPartition> allPartitions = new LinkedList<>();\n        final Map<TopicPartition, Long> allChangelogPositions = new HashMap<>();\n\n        // Obtain the current positions, of all the active-restoring and standby tasks\n        for (final Task task : tasksToCollectLagFor) {\n            allPartitions.addAll(task.changelogPartitions());\n            // Note that not all changelog partitions, will have positions; since some may not have started\n            allChangelogPositions.putAll(task.changelogOffsets());\n        }\n\n        log.debug(\"Current changelog positions: {}\", allChangelogPositions);\n        final Map<TopicPartition, ListOffsetsResultInfo> allEndOffsets;\n        allEndOffsets = fetchEndOffsets(allPartitions, adminClient);\n        log.debug(\"Current end offsets :{}\", allEndOffsets);\n\n        for (final Map.Entry<TopicPartition, ListOffsetsResultInfo> entry : allEndOffsets.entrySet()) {\n            // Avoiding an extra admin API lookup by computing lags for not-yet-started restorations\n            // from zero instead of the real \"earliest offset\" for the changelog.\n            // This will yield the correct relative order of lagginess for the tasks in the cluster,\n            // but it is an over-estimate of how much work remains to restore the task from scratch.\n            final long earliestOffset = 0L;\n            final long changelogPosition = allChangelogPositions.getOrDefault(entry.getKey(), earliestOffset);\n            final long latestOffset = entry.getValue().offset();\n            final LagInfo lagInfo = new LagInfo(changelogPosition == Task.LATEST_OFFSET ? latestOffset : changelogPosition, latestOffset);\n            final String storeName = streamsMetadataState.getStoreForChangelogTopic(entry.getKey().topic());\n            localStorePartitionLags.computeIfAbsent(storeName, ignored -> new TreeMap<>())\n                .put(entry.getKey().partition(), lagInfo);\n        }\n\n        return Collections.unmodifiableMap(localStorePartitionLags);\n    }\n\n    /**\n     * Run an interactive query against a state store.\n     * <p>\n     * This method allows callers outside of the Streams runtime to access the internal state of\n     * stateful processors. See <a href=\"https://kafka.apache.org/documentation/streams/developer-guide/interactive-queries.html\">IQ docs</a>\n     * for more information.\n     * <p>\n     * NOTICE: This functionality is {@link Evolving} and subject to change in minor versions.\n     * Once it is stabilized, this notice and the evolving annotation will be removed.\n     *\n     * @param <R> The result type specified by the query.\n     * @throws StreamsNotStartedException If Streams has not yet been started. Just call {@link\n     *                                    KafkaStreams#start()} and then retry this call.\n     * @throws StreamsStoppedException    If Streams is in a terminal state like PENDING_SHUTDOWN,\n     *                                    NOT_RUNNING, PENDING_ERROR, or ERROR. The caller should\n     *                                    discover a new instance to query.\n     * @throws UnknownStateStoreException If the specified store name does not exist in the\n     *                                    topology.\n     */\n    @Evolving\n    public <R> StateQueryResult<R> query(final StateQueryRequest<R> request) {\n        final String storeName = request.getStoreName();\n        if (!topologyMetadata.hasStore(storeName)) {\n            throw new UnknownStateStoreException(\n                \"Cannot get state store \"\n                    + storeName\n                    + \" because no such store is registered in the topology.\"\n            );\n        }\n        if (state().hasNotStarted()) {\n            throw new StreamsNotStartedException(\n                \"KafkaStreams has not been started, you can retry after calling start().\"\n            );\n        }\n        if (state().isShuttingDown() || state.hasCompletedShutdown()) {\n            throw new StreamsStoppedException(\n                \"KafkaStreams has been stopped (\" + state + \").\"\n                    + \" This instance can no longer serve queries.\"\n            );\n        }\n        final StateQueryResult<R> result = new StateQueryResult<>();\n\n        final Map<String, StateStore> globalStateStores = topologyMetadata.globalStateStores();\n        if (globalStateStores.containsKey(storeName)) {\n            // See KAFKA-13523\n            result.setGlobalResult(\n                QueryResult.forFailure(\n                    FailureReason.UNKNOWN_QUERY_TYPE,\n                    \"Global stores do not yet support the KafkaStreams#query API. Use KafkaStreams#store instead.\"\n                )\n            );\n        } else {\n            for (final StreamThread thread : threads) {\n                final Set<Task> tasks = thread.readyOnlyAllTasks();\n                for (final Task task : tasks) {\n\n                    final TaskId taskId = task.id();\n                    final int partition = taskId.partition();\n                    if (request.isAllPartitions() || request.getPartitions().contains(partition)) {\n                        final StateStore store = task.getStore(storeName);\n                        if (store != null) {\n                            final StreamThread.State state = thread.state();\n                            final boolean active = task.isActive();\n                            if (request.isRequireActive()\n                                && (state != StreamThread.State.RUNNING || !active)) {\n\n                                result.addResult(\n                                    partition,\n                                    QueryResult.forFailure(\n                                        FailureReason.NOT_ACTIVE,\n                                        \"Query requires a running active task,\"\n                                            + \" but partition was in state \"\n                                            + state + \" and was \"\n                                            + (active ? \"active\" : \"not active\") + \".\"\n                                    )\n                                );\n                            } else {\n                                final QueryResult<R> r = store.query(\n                                    request.getQuery(),\n                                    request.isRequireActive()\n                                        ? PositionBound.unbounded()\n                                        : request.getPositionBound(),\n                                    new QueryConfig(request.executionInfoEnabled())\n                                );\n                                result.addResult(partition, r);\n                            }\n\n\n                            // optimization: if we have handled all the requested partitions,\n                            // we can return right away.\n                            if (!request.isAllPartitions()\n                                && result.getPartitionResults().keySet().containsAll(request.getPartitions())) {\n                                return result;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        if (!request.isAllPartitions()) {\n            for (final Integer partition : request.getPartitions()) {\n                if (!result.getPartitionResults().containsKey(partition)) {\n                    result.addResult(partition, QueryResult.forFailure(\n                        FailureReason.NOT_PRESENT,\n                        \"The requested partition was not present at the time of the query.\"\n                    ));\n                }\n            }\n        }\n\n        return result;\n    }\n\n}",
                "methodCount": 91
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 14,
                "candidates": [
                    {
                        "lineStart": 1073,
                        "lineEnd": 1097,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method createAndAddStreamThread to class StreamStateListener",
                        "description": "Move method createAndAddStreamThread to org.apache.kafka.streams.KafkaStreams.StreamStateListener\nRationale: The createAndAddStreamThread method primarily deals with the creation and management of StreamThread objects, including setting state listeners and adding them to a collection of threads. The StreamStateListener class is directly involved in handling stream thread transitions and maintaining the state of these threads. Moving the method to the StreamStateListener class will have close proximity to similar functionality, and ensure better encapsulation and easier maintenance by keeping related methods in a single class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1099,
                        "lineEnd": 1101,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method registerStreamThread to class TopologyMetadata",
                        "description": "Move method registerStreamThread to org.apache.kafka.streams.processor.internals.TopologyMetadata\nRationale: The method registerStreamThread is primarily concerned with the state of StreamThread instances and their registration, which aligns closely with the functionality handled by TopologyMetadata. This class already manages aspects of thread registration and versioning, such as in methods like registerThread and unregisterThread, and maintains a comprehensive view of the application's topology and thread states. Moving the method to TopologyMetadata ensures that thread state management is centralized and coherent.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1103,
                        "lineEnd": 1113,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method getMetrics to class DelegatingStateRestoreListener",
                        "description": "move method getMetrics to PsiClass:DelegatingStateRestoreListener\nRationale: The method 'getMetrics' is related to configuring and retrieving metrics for a Kafka Streams application. The 'DelegatingStateRestoreListener' class is already associated with the restoration process of state stores in Kafka Streams, which involves metrics and state information. Placing the method in this class aids in centralizing state and metrics handling. Additionally, the 'DelegatingStateRestoreListener' deals with states at a granular level, which aligns well with the requirement to gather and configure metrics.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1348,
                        "lineEnd": 1353,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method getCacheSizePerThread to class StreamStateListener",
                        "description": "Move method getCacheSizePerThread to org.apache.kafka.streams.KafkaStreams.StreamStateListener\nRationale: The getCacheSizePerThread() method is concerned with calculating cache size, a function that is more relevant to managing stream threads and their associated cache. StreamStateListener already manages different states and threading concerns, making it a more appropriate location for such a method. The topologyMetadata.hasGlobalTopology() condition further links this method to the overall streaming architecture, which StreamStateListener is designed to handle.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 370,
                        "lineEnd": 374,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method isRunningOrRebalancing to class StreamStateListener",
                        "description": "Move method isRunningOrRebalancing to org.apache.kafka.streams.KafkaStreams.StreamStateListener\nRationale: The method isRunningOrRebalancing heavily relies on checking the state of a system, which involves synchronization and state transitions that are closely related to stream thread management. The StreamStateListener class is responsible for handling stream thread transitions and maintaining the states of individual threads and the global thread. Thus, this method is most cohesively aligned with the responsibilities of StreamStateListener.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 376,
                        "lineEnd": 380,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method hasStartedOrFinishedShuttingDown to class State",
                        "description": "Move method hasStartedOrFinishedShuttingDown to org.apache.kafka.streams.KafkaStreams.State\nRationale: The method `hasStartedOrFinishedShuttingDown()` directly invokes `state.hasStartedOrFinishedShuttingDown()`, which is a method of the `State` class. This indicates that the method is intended to query the state of an object that reflects the state transitions of a system, most likely related to a Kafka Streams instance. Therefore, it would be most appropriate to move this method to the `State` class where it can directly access and manage state transitions.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1355,
                        "lineEnd": 1360,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method resizeThreadCache to class StreamStateListener",
                        "description": "Move method resizeThreadCache to org.apache.kafka.streams.KafkaStreams.StreamStateListener\nRationale: The method resizeThreadCache() deals with thread caches and thread-specific operations, which aligns closely with the purpose of StreamStateListener. StreamStateListener is responsible for handling stream thread transitions and managing states, making it the most appropriate place for thread-related operations such as resizing thread caches. By moving resizeThreadCache() to StreamStateListener, we ensure that all thread-specific logic is centralized within a relevant class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2049,
                        "lineEnd": 2079,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method localThreadsMetadata to class KafkaClientSupplier",
                        "description": "Move method localThreadsMetadata to org.apache.kafka.streams.KafkaClientSupplier\nRationale: The method localThreadsMetadata() returns runtime information about Kafka Streams' local threads, which involves retrieving metadata about threads and tasks within a Kafka Streams instance. Since KafkaClientSupplier provides various Kafka client instances to KafkaStreams, it is logical to include methods that relate to providing and managing runtime information about those instances here. This placement ensures that all client-related metadata and their runtime information can be centrally accessed and managed within KafkaClientSupplier.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1370,
                        "lineEnd": 1380,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method maybeCreateRocksDBMetricsRecordingService to class ClientUtils",
                        "description": "move method maybeCreateRocksDBMetricsRecordingService to PsiClass:ClientUtils\nRationale: The maybeCreateRocksDBMetricsRecordingService() method is related to the configuration and scheduling of metrics recording, which fits within the utility functions and configuration management tasks handled by the ClientUtils class. This class already deals with client-related settings, metrics, and utilities, therefore it makes sense to place this method here to keep related functionality together.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 612,
                        "lineEnd": 636,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method metrics to class Admin",
                        "description": "Move method metrics to org.apache.kafka.clients.admin.Admin\nRationale: The metrics() method provides access to the global metrics registry, including the metrics of Kafka clients such as producer, consumer, and admin clients. The Admin class is responsible for managing and inspecting Kafka configurations, topics, brokers, and ACLs. Additionally, it includes methods involving metrics and operations on Kafka clusters. Consequently, metrics related to Kafka clients should logically reside within the Admin class, as it aligns with the function and responsibilities of managing and accessing comprehensive Kafka metrics.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2115,
                        "lineEnd": 2147,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method allLocalStorePartitionLags to class Admin",
                        "description": "Move method allLocalStorePartitionLags to org.apache.kafka.clients.admin.Admin\nRationale: The method `allLocalStorePartitionLags` revolves around administrative details involving tasks, partitions, and changelogs, which align closely with the responsibilities of the Admin class. Moving it here makes sense because the Admin class is inherently designed to handle Kafka administrative operations, including managing partitions and offsets. Furthermore, the method has dependencies on admin client operations (fetchEndOffsets), making it a natural fit for the Admin class, which is already set up to handle such client interactions efficiently.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 382,
                        "lineEnd": 391,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method validateIsRunningOrRebalancing to class State",
                        "description": "Move method validateIsRunningOrRebalancing to org.apache.kafka.streams.KafkaStreams.State\nRationale: The validateIsRunningOrRebalancing() method heavily relies on the state of KafkaStreams, checking if it has started and whether it is running or rebalancing. The State class provides methods like hasNotStarted() and isRunningOrRebalancing(), which are crucial for this validation. Therefore, it is logical to move validateIsRunningOrRebalancing() to the State class where it can directly utilize these methods and enhance cohesion.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1638,
                        "lineEnd": 1663,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method streamThreadLeaveConsumerGroup to class StreamStateListener",
                        "description": "Move method streamThreadLeaveConsumerGroup to org.apache.kafka.streams.KafkaStreams.StreamStateListener\nRationale: The method `streamThreadLeaveConsumerGroup` deals primarily with the interaction of `StreamThread` objects and the consumer group they are a part of. The `StreamStateListener` class aligns best with this functionality, as it handles stream thread transitions and already maintains state information for `StreamThread` instances. Moving the method here will centralize thread state management and consumer group interaction logic, making it coherent and easier to manage. Additionally, this aligns well with the class\u2019s responsibility of monitoring and reacting to thread state changes, which fits naturally with leaving the consumer group.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 2027,
                        "lineEnd": 2047,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method getOrThrowException to class StateDirectory",
                        "description": "Move method getOrThrowException to org.apache.kafka.streams.processor.internals.StateDirectory\nRationale: The `getOrThrowException` method manages exceptions related to future results with timeouts and is closely related to managing states and tasks. The `StateDirectory` class has overall responsibility for state management and error handling, making it a suitable place for such utility methods. Additionally, the `StateDirectory` class can reuse this method for handling various state-related asynchronous operations and error management, aligning it well with the existing functionalities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "getMetrics",
                            "method_signature": "private static Metrics getMetrics(StreamsConfig config, Time time, String clientId)",
                            "target_class": "MetricsUtil",
                            "rationale": "This method deals specifically with creating and configuring a Metrics object, which is more relevant to a utility class focused on metrics configuration and initialization."
                        },
                        {
                            "method_name": "parseHostInfo",
                            "method_signature": "private static HostInfo parseHostInfo(String endPoint)",
                            "target_class": "HostInfoUtil",
                            "rationale": "This method deals with parsing and constructing HostInfo objects based on an endpoint string, which is more appropriate for a utility class focused on host information management."
                        },
                        {
                            "method_name": "maybeCreateRocksDBMetricsRecordingService",
                            "method_signature": "private static ScheduledExecutorService maybeCreateRocksDBMetricsRecordingService(String clientId, StreamsConfig config)",
                            "target_class": "MetricsUtil",
                            "rationale": "This method is specifically concerned with creating a metrics recording service for RocksDB, which fits better in a utility class focused on metrics management."
                        },
                        {
                            "method_name": "getNumLiveStreamThreads",
                            "method_signature": "private int getNumLiveStreamThreads()",
                            "target_class": "StreamThreadUtil",
                            "rationale": "This method deals with counting live stream threads, which is more relevant for a utility class or a helper class focused on stream thread management."
                        },
                        {
                            "method_name": "getNextThreadIndex",
                            "method_signature": "private int getNextThreadIndex()",
                            "target_class": "StreamThreadUtil",
                            "rationale": "This method is responsible for calculating the next available thread index, which is better placed in a utility class dealing with stream thread operations."
                        }
                    ],
                    "llm_response_time": 7411
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "setGlobalStateRestoreListener",
                            "method_signature": "public void setGlobalStateRestoreListener(final StateRestoreListener globalStateRestoreListener)",
                            "target_class": "StateRestoreListener",
                            "rationale": "The method setGlobalStateRestoreListener delegates functionality to the StateRestoreListener and operates on related logic, suggesting it would be more cohesive within the StateRestoreListener class."
                        },
                        {
                            "method_name": "setStandbyUpdateListener",
                            "method_signature": "public void setStandbyUpdateListener(final StandbyUpdateListener standbyListener)",
                            "target_class": "StandbyUpdateListener",
                            "rationale": "The method setStandbyUpdateListener deals with functionality closely tied to the StandbyUpdateListener. It would be more cohesive directly within the StandbyUpdateListener class."
                        },
                        {
                            "method_name": "replaceStreamThread",
                            "method_signature": "private void replaceStreamThread(final Throwable throwable)",
                            "target_class": "StreamThread",
                            "rationale": "The replaceStreamThread method handles the creation and management of a StreamThread, making it more appropriate to reside within the StreamThread class."
                        },
                        {
                            "method_name": "defaultStreamsUncaughtExceptionHandler",
                            "method_signature": "private void defaultStreamsUncaughtExceptionHandler(final Throwable throwable, final boolean skipThreadReplacement)",
                            "target_class": "StreamsUncaughtExceptionHandler",
                            "rationale": "The method defaultStreamsUncaughtExceptionHandler deals with handling exceptions specifically for streams, making the StreamsUncaughtExceptionHandler a more appropriate class for it."
                        },
                        {
                            "method_name": "globalThreadFuture",
                            "method_signature": "KafkaFuture<Uuid> globalThreadFuture(final long timeout)",
                            "target_class": "GlobalStreamThread",
                            "rationale": "The method session involves handling an aspect specifically related to the GlobalStreamThread, suggesting it would be more cohesive within the GlobalStreamThread class."
                        }
                    ],
                    "llm_response_time": 7169
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "defaultStreamsUncaughtExceptionHandler",
                            "method_signature": "private void defaultStreamsUncaughtExceptionHandler(final Throwable throwable, final boolean skipThreadReplacement)",
                            "target_class": "StreamsUncaughtExceptionHandler",
                            "rationale": "This method handles uncaught exceptions in streams and does not specifically require access to `KafkaStreams` internals. It could be moved to a dedicated exception handler class to reduce the size and complexity of the `KafkaStreams` class and to encapsulate exception handling logic."
                        },
                        {
                            "method_name": "handleStreamsUncaughtException",
                            "method_signature": "private void handleStreamsUncaughtException(final Throwable throwable, final StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler, final boolean skipThreadReplacement)",
                            "target_class": "StreamsUncaughtExceptionHandler",
                            "rationale": "Similar to `defaultStreamsUncaughtExceptionHandler`, this method handles streams uncaught exceptions and can be moved to a dedicated handler class, encapsulating all related exception handling logic in one place."
                        },
                        {
                            "method_name": "replaceStreamThread",
                            "method_signature": "private void replaceStreamThread(final Throwable throwable)",
                            "target_class": "StreamThreadManager",
                            "rationale": "This method deals with replacing stream threads and involves operations that coordinate between multiple threads. Moving it to a class responsible for managing stream threads would separate concerns and make it easier to maintain."
                        },
                        {
                            "method_name": "createAndAddStreamThread",
                            "method_signature": "private StreamThread createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)",
                            "target_class": "StreamThreadManager",
                            "rationale": "This method is responsible for creating and adding StreamThread instances, which would be more appropriately placed in a class dedicated to managing threads, making the `KafkaStreams` class cleaner and more focused."
                        },
                        {
                            "method_name": "getMetrics",
                            "method_signature": "static Metrics getMetrics(final StreamsConfig config, final Time time, final String clientId)",
                            "target_class": "MetricsManager",
                            "rationale": "This method fetches metrics and is more related to metrics management. Moving it to a `MetricsManager` class would segregate the responsibility of handling metrics and would simplify the `KafkaStreams` class."
                        },
                        {
                            "method_name": "fetchEndOffsets",
                            "method_signature": "private Map<TopicPartition, ListOffsetsResultInfo> fetchEndOffsets(final Collection<TopicPartition> partitions, final Admin adminClient)",
                            "target_class": "OffsetFetcher",
                            "rationale": "This method deals with fetching end offsets for partitions and can be moved to a dedicated class responsible for fetching offsets, thus promoting the Single Responsibility Principle."
                        },
                        {
                            "method_name": "queryMetadataForKey",
                            "method_signature": "public <K> KeyQueryMetadata queryMetadataForKey(final String storeName, final K key, final Serializer<K> keySerializer)",
                            "target_class": "MetadataQueryService",
                            "rationale": "This method queries metadata for a specific key and moving it to a `MetadataQueryService` class would centralize key metadata query logic, making it easier to manage and test."
                        }
                    ],
                    "llm_response_time": 9114
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "isValidTransition",
                            "method_signature": "public isValidTransition(final State newState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "waitOnState",
                            "method_signature": "private waitOnState(final State targetState, final long waitMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunningOrRebalancing",
                            "method_signature": "protected isRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasStartedOrFinishedShuttingDown",
                            "method_signature": "protected hasStartedOrFinishedShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateIsRunningOrRebalancing",
                            "method_signature": "protected validateIsRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replaceStreamThread",
                            "method_signature": "private replaceStreamThread(final Throwable throwable)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleStreamsUncaughtException",
                            "method_signature": "private handleStreamsUncaughtException(final Throwable throwable,\n                                                final StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler,\n                                                final boolean skipThreadReplacement)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metrics",
                            "method_signature": "public metrics()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createAndAddStreamThread",
                            "method_signature": "private createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "registerStreamThread",
                            "method_signature": "private registerStreamThread(StreamThread streamThread)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getMetrics",
                            "method_signature": "static getMetrics(final StreamsConfig config, final Time time, final String clientId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeStreamThread",
                            "method_signature": "private removeStreamThread(final long timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getCacheSizePerThread",
                            "method_signature": "private getCacheSizePerThread(final int numStreamThreads)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resizeThreadCache",
                            "method_signature": "private resizeThreadCache(final long cacheSizePerThread)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCreateRocksDBMetricsRecordingService",
                            "method_signature": "private static maybeCreateRocksDBMetricsRecordingService(final String clientId,\n                                                                                      final StreamsConfig config)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "start",
                            "method_signature": "public synchronized start()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shutdownHelper",
                            "method_signature": "private shutdownHelper(final boolean error, final long timeoutMs, final boolean leaveGroup)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "private close(final long timeoutMs, final boolean leaveGroup)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public synchronized close(final CloseOptions options)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "streamThreadLeaveConsumerGroup",
                            "method_signature": "private streamThreadLeaveConsumerGroup(final long remainingTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanUp",
                            "method_signature": "public cleanUp()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allMetadata",
                            "method_signature": "@Deprecated\n    public allMetadata()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metadataForAllStreamsClients",
                            "method_signature": "public metadataForAllStreamsClients()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allMetadataForStore",
                            "method_signature": "@Deprecated\n    public allMetadataForStore(final String storeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "streamsMetadataForStore",
                            "method_signature": "public streamsMetadataForStore(final String storeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "queryMetadataForKey",
                            "method_signature": "public queryMetadataForKey(final String storeName,\n                                                    final K key,\n                                                    final Serializer<K> keySerializer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "queryMetadataForKey",
                            "method_signature": "public queryMetadataForKey(final String storeName,\n                                                    final K key,\n                                                    final StreamPartitioner<? super K, ?> partitioner)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "store",
                            "method_signature": "public store(final StoreQueryParameters<T> storeQueryParameters)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pause",
                            "method_signature": "public pause()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isPaused",
                            "method_signature": "public isPaused()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resume",
                            "method_signature": "public resume()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clientInstanceIds",
                            "method_signature": "public synchronized clientInstanceIds(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrThrowException",
                            "method_signature": "private getOrThrowException(\n        final KafkaFuture<T> future,\n        final long timeoutMs,\n        final Supplier<String> errorMessage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "localThreadsMetadata",
                            "method_signature": "@Deprecated\n    public localThreadsMetadata()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metadataForLocalThreads",
                            "method_signature": "public metadataForLocalThreads()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allLocalStorePartitionLags",
                            "method_signature": "public allLocalStorePartitionLags()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allLocalStorePartitionLags",
                            "method_signature": "protected allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "query",
                            "method_signature": "@Evolving\n    public query(final StateQueryRequest<R> request)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "createAndAddStreamThread",
                            "method_signature": "private createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "registerStreamThread",
                            "method_signature": "private registerStreamThread(StreamThread streamThread)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getMetrics",
                            "method_signature": "static getMetrics(final StreamsConfig config, final Time time, final String clientId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isValidTransition",
                            "method_signature": "public isValidTransition(final State newState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getCacheSizePerThread",
                            "method_signature": "private getCacheSizePerThread(final int numStreamThreads)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunningOrRebalancing",
                            "method_signature": "protected isRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasStartedOrFinishedShuttingDown",
                            "method_signature": "protected hasStartedOrFinishedShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resizeThreadCache",
                            "method_signature": "private resizeThreadCache(final long cacheSizePerThread)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "localThreadsMetadata",
                            "method_signature": "@Deprecated\n    public localThreadsMetadata()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCreateRocksDBMetricsRecordingService",
                            "method_signature": "private static maybeCreateRocksDBMetricsRecordingService(final String clientId,\n                                                                                      final StreamsConfig config)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metrics",
                            "method_signature": "public metrics()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allLocalStorePartitionLags",
                            "method_signature": "protected allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateIsRunningOrRebalancing",
                            "method_signature": "protected validateIsRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "streamThreadLeaveConsumerGroup",
                            "method_signature": "private streamThreadLeaveConsumerGroup(final long remainingTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrThrowException",
                            "method_signature": "private getOrThrowException(\n        final KafkaFuture<T> future,\n        final long timeoutMs,\n        final Supplier<String> errorMessage)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)": {
                        "first": {
                            "method_name": "createAndAddStreamThread",
                            "method_signature": "private createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.2571815189979113
                    },
                    "private registerStreamThread(StreamThread streamThread)": {
                        "first": {
                            "method_name": "registerStreamThread",
                            "method_signature": "private registerStreamThread(StreamThread streamThread)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.2981393697176576
                    },
                    "static getMetrics(final StreamsConfig config, final Time time, final String clientId)": {
                        "first": {
                            "method_name": "getMetrics",
                            "method_signature": "static getMetrics(final StreamsConfig config, final Time time, final String clientId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.32526175251504386
                    },
                    "public isValidTransition(final State newState)": {
                        "first": {
                            "method_name": "isValidTransition",
                            "method_signature": "public isValidTransition(final State newState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.33799190003990914
                    },
                    "private getCacheSizePerThread(final int numStreamThreads)": {
                        "first": {
                            "method_name": "getCacheSizePerThread",
                            "method_signature": "private getCacheSizePerThread(final int numStreamThreads)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.37371302739403467
                    },
                    "protected isRunningOrRebalancing()": {
                        "first": {
                            "method_name": "isRunningOrRebalancing",
                            "method_signature": "protected isRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.42735110601435355
                    },
                    "protected hasStartedOrFinishedShuttingDown()": {
                        "first": {
                            "method_name": "hasStartedOrFinishedShuttingDown",
                            "method_signature": "protected hasStartedOrFinishedShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.42735168334771934
                    },
                    "private resizeThreadCache(final long cacheSizePerThread)": {
                        "first": {
                            "method_name": "resizeThreadCache",
                            "method_signature": "private resizeThreadCache(final long cacheSizePerThread)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.43239025210123583
                    },
                    "@Deprecated\n    public localThreadsMetadata()": {
                        "first": {
                            "method_name": "localThreadsMetadata",
                            "method_signature": "@Deprecated\n    public localThreadsMetadata()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4416467371374306
                    },
                    "private static maybeCreateRocksDBMetricsRecordingService(final String clientId,\n                                                                                      final StreamsConfig config)": {
                        "first": {
                            "method_name": "maybeCreateRocksDBMetricsRecordingService",
                            "method_signature": "private static maybeCreateRocksDBMetricsRecordingService(final String clientId,\n                                                                                      final StreamsConfig config)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.48120751659127103
                    },
                    "public metrics()": {
                        "first": {
                            "method_name": "metrics",
                            "method_signature": "public metrics()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4856458385717108
                    },
                    "protected allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor)": {
                        "first": {
                            "method_name": "allLocalStorePartitionLags",
                            "method_signature": "protected allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5076397670991507
                    },
                    "protected validateIsRunningOrRebalancing()": {
                        "first": {
                            "method_name": "validateIsRunningOrRebalancing",
                            "method_signature": "protected validateIsRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5100240936846383
                    },
                    "private streamThreadLeaveConsumerGroup(final long remainingTimeMs)": {
                        "first": {
                            "method_name": "streamThreadLeaveConsumerGroup",
                            "method_signature": "private streamThreadLeaveConsumerGroup(final long remainingTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5186829967845279
                    },
                    "private getOrThrowException(\n        final KafkaFuture<T> future,\n        final long timeoutMs,\n        final Supplier<String> errorMessage)": {
                        "first": {
                            "method_name": "getOrThrowException",
                            "method_signature": "private getOrThrowException(\n        final KafkaFuture<T> future,\n        final long timeoutMs,\n        final Supplier<String> errorMessage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5259309696655758
                    }
                },
                "voyage": {
                    "public isValidTransition(final State newState)": {
                        "first": {
                            "method_name": "isValidTransition",
                            "method_signature": "public isValidTransition(final State newState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.20036525708490893
                    },
                    "private waitOnState(final State targetState, final long waitMs)": {
                        "first": {
                            "method_name": "waitOnState",
                            "method_signature": "private waitOnState(final State targetState, final long waitMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.29466591753759364
                    },
                    "protected hasStartedOrFinishedShuttingDown()": {
                        "first": {
                            "method_name": "hasStartedOrFinishedShuttingDown",
                            "method_signature": "protected hasStartedOrFinishedShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.33857826263934254
                    },
                    "protected allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor)": {
                        "first": {
                            "method_name": "allLocalStorePartitionLags",
                            "method_signature": "protected allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3694160289212117
                    },
                    "protected isRunningOrRebalancing()": {
                        "first": {
                            "method_name": "isRunningOrRebalancing",
                            "method_signature": "protected isRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3786477594072846
                    },
                    "private static maybeCreateRocksDBMetricsRecordingService(final String clientId,\n                                                                                      final StreamsConfig config)": {
                        "first": {
                            "method_name": "maybeCreateRocksDBMetricsRecordingService",
                            "method_signature": "private static maybeCreateRocksDBMetricsRecordingService(final String clientId,\n                                                                                      final StreamsConfig config)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.43065716867264103
                    },
                    "private resizeThreadCache(final long cacheSizePerThread)": {
                        "first": {
                            "method_name": "resizeThreadCache",
                            "method_signature": "private resizeThreadCache(final long cacheSizePerThread)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4609096916081689
                    },
                    "private getOrThrowException(\n        final KafkaFuture<T> future,\n        final long timeoutMs,\n        final Supplier<String> errorMessage)": {
                        "first": {
                            "method_name": "getOrThrowException",
                            "method_signature": "private getOrThrowException(\n        final KafkaFuture<T> future,\n        final long timeoutMs,\n        final Supplier<String> errorMessage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.47667805443338496
                    },
                    "private streamThreadLeaveConsumerGroup(final long remainingTimeMs)": {
                        "first": {
                            "method_name": "streamThreadLeaveConsumerGroup",
                            "method_signature": "private streamThreadLeaveConsumerGroup(final long remainingTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4770131591548925
                    },
                    "private registerStreamThread(StreamThread streamThread)": {
                        "first": {
                            "method_name": "registerStreamThread",
                            "method_signature": "private registerStreamThread(StreamThread streamThread)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.521174383009029
                    },
                    "private getCacheSizePerThread(final int numStreamThreads)": {
                        "first": {
                            "method_name": "getCacheSizePerThread",
                            "method_signature": "private getCacheSizePerThread(final int numStreamThreads)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5369699815156913
                    },
                    "private replaceStreamThread(final Throwable throwable)": {
                        "first": {
                            "method_name": "replaceStreamThread",
                            "method_signature": "private replaceStreamThread(final Throwable throwable)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5380310143545074
                    },
                    "static getMetrics(final StreamsConfig config, final Time time, final String clientId)": {
                        "first": {
                            "method_name": "getMetrics",
                            "method_signature": "static getMetrics(final StreamsConfig config, final Time time, final String clientId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5430820465679188
                    },
                    "private handleStreamsUncaughtException(final Throwable throwable,\n                                                final StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler,\n                                                final boolean skipThreadReplacement)": {
                        "first": {
                            "method_name": "handleStreamsUncaughtException",
                            "method_signature": "private handleStreamsUncaughtException(final Throwable throwable,\n                                                final StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler,\n                                                final boolean skipThreadReplacement)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5522887107208821
                    },
                    "protected validateIsRunningOrRebalancing()": {
                        "first": {
                            "method_name": "validateIsRunningOrRebalancing",
                            "method_signature": "protected validateIsRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5557238959665479
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "protected validateIsRunningOrRebalancing()",
                        "protected isRunningOrRebalancing()",
                        "protected hasStartedOrFinishedShuttingDown()",
                        "private createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)",
                        "public metrics()",
                        "protected allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor)",
                        "public isValidTransition(final State newState)",
                        "private resizeThreadCache(final long cacheSizePerThread)",
                        "private getCacheSizePerThread(final int numStreamThreads)",
                        "static getMetrics(final StreamsConfig config, final Time time, final String clientId)",
                        "private registerStreamThread(StreamThread streamThread)",
                        "private streamThreadLeaveConsumerGroup(final long remainingTimeMs)",
                        "@Deprecated\n    public localThreadsMetadata()"
                    ],
                    "llm_response_time": 6281
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)",
                        "private getCacheSizePerThread(final int numStreamThreads)",
                        "static getMetrics(final StreamsConfig config, final Time time, final String clientId)",
                        "private registerStreamThread(StreamThread streamThread)",
                        "public isValidTransition(final State newState)"
                    ],
                    "llm_response_time": 6215
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "static getMetrics(final StreamsConfig config, final Time time, final String clientId)",
                        "private createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)",
                        "private registerStreamThread(StreamThread streamThread)"
                    ],
                    "llm_response_time": 4339
                },
                "voyage": {
                    "priority_method_names": [
                        "protected validateIsRunningOrRebalancing()",
                        "protected hasStartedOrFinishedShuttingDown()",
                        "protected isRunningOrRebalancing()",
                        "public isValidTransition(final State newState)",
                        "protected allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor)",
                        "private waitOnState(final State targetState, final long waitMs)",
                        "private registerStreamThread(StreamThread streamThread)",
                        "private resizeThreadCache(final long cacheSizePerThread)",
                        "private getCacheSizePerThread(final int numStreamThreads)",
                        "private replaceStreamThread(final Throwable throwable)",
                        "private streamThreadLeaveConsumerGroup(final long remainingTimeMs)",
                        "static getMetrics(final StreamsConfig config, final Time time, final String clientId)"
                    ],
                    "llm_response_time": 5192
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private waitOnState(final State targetState, final long waitMs)",
                        "public isValidTransition(final State newState)",
                        "protected hasStartedOrFinishedShuttingDown()",
                        "protected isRunningOrRebalancing()",
                        "protected allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor)"
                    ],
                    "llm_response_time": 9515
                },
                "voyage-3": {
                    "priority_method_names": [
                        "protected hasStartedOrFinishedShuttingDown()",
                        "private waitOnState(final State targetState, final long waitMs)",
                        "public isValidTransition(final State newState)"
                    ],
                    "llm_response_time": 5199
                }
            },
            "targetClassMap": {
                "createAndAddStreamThread": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.08800141237968083
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.025348039886047998
                        },
                        {
                            "class_name": "StreamStateListener",
                            "similarity_score": 0.22924821597026102
                        },
                        {
                            "class_name": "DelegatingStateRestoreListener",
                            "similarity_score": 0.273138984094458
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.014173351371926584
                        },
                        {
                            "class_name": "DelegatingStandbyUpdateListener",
                            "similarity_score": 0.2980564646168641
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.07669649888473705
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.060090203007582506
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "StreamStateListener",
                        "DelegatingStateRestoreListener",
                        "DelegatingStandbyUpdateListener"
                    ],
                    "llm_response_time": 3397,
                    "similarity_computation_time": 5,
                    "similarity_metric": "cosine"
                },
                "registerStreamThread": {
                    "target_classes": [
                        {
                            "class_name": "StreamThread",
                            "similarity_score": 0.3462033749496624
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.12222418386066781
                        },
                        {
                            "class_name": "Metrics",
                            "similarity_score": 0.17320688312717197
                        },
                        {
                            "class_name": "StreamsConfig",
                            "similarity_score": 0.12430917839775033
                        },
                        {
                            "class_name": "StreamsMetadataState",
                            "similarity_score": 0.29989623155024736
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.028656331856469708
                        },
                        {
                            "class_name": "StreamsMetricsImpl",
                            "similarity_score": 0.23013624865685428
                        },
                        {
                            "class_name": "StreamStateListener",
                            "similarity_score": 0.35342433295415243
                        },
                        {
                            "class_name": "DelegatingStateRestoreListener",
                            "similarity_score": 0.4244727455521982
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.02834670274385317
                        },
                        {
                            "class_name": "TopologyMetadata",
                            "similarity_score": 0.40987310819113176
                        },
                        {
                            "class_name": "QueryableStoreProvider",
                            "similarity_score": 0.2136564362550534
                        },
                        {
                            "class_name": "DelegatingStandbyUpdateListener",
                            "similarity_score": 0.40544445554499886
                        },
                        {
                            "class_name": "GlobalStreamThread",
                            "similarity_score": 0.38938788743880465
                        },
                        {
                            "class_name": "StateDirectory",
                            "similarity_score": 0.383028743868051
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.11504474832710555
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.08512778759407522
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "TopologyMetadata",
                        "DelegatingStateRestoreListener",
                        "DelegatingStandbyUpdateListener"
                    ],
                    "llm_response_time": 3946,
                    "similarity_computation_time": 19,
                    "similarity_metric": "cosine"
                },
                "getMetrics": {
                    "target_classes": [
                        {
                            "class_name": "CloseOptions",
                            "similarity_score": 0.2302277300680952
                        },
                        {
                            "class_name": "DelegatingStandbyUpdateListener",
                            "similarity_score": 0.38463838326272454
                        },
                        {
                            "class_name": "DelegatingStateRestoreListener",
                            "similarity_score": 0.3355751701525133
                        },
                        {
                            "class_name": "StreamStateListener",
                            "similarity_score": 0.29904043619843756
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "DelegatingStateRestoreListener",
                        "StreamStateListener",
                        "DelegatingStandbyUpdateListener"
                    ],
                    "llm_response_time": 3677,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "isValidTransition": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2036,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "getCacheSizePerThread": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.168501278681587
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.05448752981302172
                        },
                        {
                            "class_name": "StreamStateListener",
                            "similarity_score": 0.39976215003186305
                        },
                        {
                            "class_name": "DelegatingStateRestoreListener",
                            "similarity_score": 0.39981830569128424
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.02791390324071063
                        },
                        {
                            "class_name": "DelegatingStandbyUpdateListener",
                            "similarity_score": 0.3830681232979435
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.0755254912049361
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.12574207558471995
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "StreamStateListener",
                        "DelegatingStateRestoreListener",
                        "DelegatingStandbyUpdateListener"
                    ],
                    "llm_response_time": 3328,
                    "similarity_computation_time": 3,
                    "similarity_metric": "cosine"
                },
                "isRunningOrRebalancing": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.17851984686172434
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.05206384962798438
                        },
                        {
                            "class_name": "StreamStateListener",
                            "similarity_score": 0.4394754670415481
                        },
                        {
                            "class_name": "DelegatingStateRestoreListener",
                            "similarity_score": 0.5121591487439655
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.041403009396538584
                        },
                        {
                            "class_name": "DelegatingStandbyUpdateListener",
                            "similarity_score": 0.48015367376734364
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.1120224067222408
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.16456379836158858
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "StreamStateListener",
                        "DelegatingStandbyUpdateListener",
                        "DelegatingStateRestoreListener"
                    ],
                    "llm_response_time": 2660,
                    "similarity_computation_time": 3,
                    "similarity_metric": "cosine"
                },
                "hasStartedOrFinishedShuttingDown": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.17851984686172434
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.05206384962798438
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.041403009396538584
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.1120224067222408
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.16456379836158858
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "State",
                        "StateListener",
                        "Time"
                    ],
                    "llm_response_time": 4156,
                    "similarity_computation_time": 4,
                    "similarity_metric": "cosine"
                },
                "resizeThreadCache": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.18105216975440347
                        },
                        {
                            "class_name": "StreamsConfig",
                            "similarity_score": 0.15596159077403313
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.04932248536767448
                        },
                        {
                            "class_name": "StreamStateListener",
                            "similarity_score": 0.5011283757081292
                        },
                        {
                            "class_name": "DelegatingStateRestoreListener",
                            "similarity_score": 0.5809349385951049
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.03499192955004934
                        },
                        {
                            "class_name": "DelegatingStandbyUpdateListener",
                            "similarity_score": 0.5545986910875981
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.11834526708278774
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.11744684857031391
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "StreamStateListener",
                        "DelegatingStandbyUpdateListener",
                        "DelegatingStateRestoreListener"
                    ],
                    "llm_response_time": 5974,
                    "similarity_computation_time": 7,
                    "similarity_metric": "cosine"
                },
                "localThreadsMetadata": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.43813282945983006
                        },
                        {
                            "class_name": "Metrics",
                            "similarity_score": 0.4230720203054956
                        },
                        {
                            "class_name": "StreamsConfig",
                            "similarity_score": 0.355179662567981
                        },
                        {
                            "class_name": "StreamsMetadataState",
                            "similarity_score": 0.37677877612332294
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.4634008073948765
                        },
                        {
                            "class_name": "StreamsMetricsImpl",
                            "similarity_score": 0.12163369009448917
                        },
                        {
                            "class_name": "StreamStateListener",
                            "similarity_score": 0.15552578830563635
                        },
                        {
                            "class_name": "DelegatingStateRestoreListener",
                            "similarity_score": 0.10904869969167096
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.455957791527208
                        },
                        {
                            "class_name": "TopologyMetadata",
                            "similarity_score": 0.2960043666893756
                        },
                        {
                            "class_name": "QueryableStoreProvider",
                            "similarity_score": 0.40328292721871917
                        },
                        {
                            "class_name": "DelegatingStandbyUpdateListener",
                            "similarity_score": 0.103597229540172
                        },
                        {
                            "class_name": "GlobalStreamThread",
                            "similarity_score": 0.29029986365187116
                        },
                        {
                            "class_name": "StateDirectory",
                            "similarity_score": 0.30075558132313296
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.33233454148568886
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.307718978062947
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "KafkaClientSupplier",
                        "Admin",
                        "Time"
                    ],
                    "llm_response_time": 5537,
                    "similarity_computation_time": 13,
                    "similarity_metric": "cosine"
                },
                "maybeCreateRocksDBMetricsRecordingService": {
                    "target_classes": [
                        {
                            "class_name": "ClientUtils",
                            "similarity_score": 0.6363761478138487
                        },
                        {
                            "class_name": "ConsumerProtocolUtils",
                            "similarity_score": 0.5561913804948607
                        },
                        {
                            "class_name": "WrappingNullableUtils",
                            "similarity_score": 0.5948179165159316
                        },
                        {
                            "class_name": "StreamsConfigUtils",
                            "similarity_score": 0.5929423750737217
                        },
                        {
                            "class_name": "TaskAssignmentUtils",
                            "similarity_score": 0.5363259151889292
                        },
                        {
                            "class_name": "StoreQueryUtils",
                            "similarity_score": 0.6432859512882007
                        },
                        {
                            "class_name": "ProcessorContextUtils",
                            "similarity_score": 0.5553157971080658
                        },
                        {
                            "class_name": "ApiUtils",
                            "similarity_score": 0.509910938812825
                        },
                        {
                            "class_name": "RackUtils",
                            "similarity_score": 0.5506303343482575
                        },
                        {
                            "class_name": "GraphGraceSearchUtil",
                            "similarity_score": 0.6061430104032589
                        },
                        {
                            "class_name": "InternalQueryResultUtil",
                            "similarity_score": 0.35177015130504713
                        },
                        {
                            "class_name": "StreamStreamJoinUtil",
                            "similarity_score": 0.37229039552496995
                        },
                        {
                            "class_name": "RackAwareOptimizationParams",
                            "similarity_score": 0.43078044650082564
                        },
                        {
                            "class_name": "QuietConsumerConfig",
                            "similarity_score": 0.3611111111111111
                        },
                        {
                            "class_name": "QuietStreamsConfig",
                            "similarity_score": 0.3888888888888889
                        },
                        {
                            "class_name": "PrefixedSessionKeySchemas",
                            "similarity_score": 0.5966355048232669
                        },
                        {
                            "class_name": "PrefixedWindowKeySchemas",
                            "similarity_score": 0.5898373685276698
                        },
                        {
                            "class_name": "QueryableStoreTypes",
                            "similarity_score": 0.41762718756147377
                        },
                        {
                            "class_name": "RackAwareGraphConstructorFactory",
                            "similarity_score": 0.5793654595023211
                        },
                        {
                            "class_name": "Murmur3",
                            "similarity_score": 0.29219262892704784
                        },
                        {
                            "class_name": "ChangelogRecordDeserializationHelper",
                            "similarity_score": 0.473029498104873
                        },
                        {
                            "class_name": "WindowedSerdes",
                            "similarity_score": 0.4533433244808839
                        },
                        {
                            "class_name": "InternalConfig",
                            "similarity_score": 0.5123364584770754
                        },
                        {
                            "class_name": "TopicMetrics",
                            "similarity_score": 0.4166313044871611
                        },
                        {
                            "class_name": "Stores",
                            "similarity_score": 0.18395927710463306
                        },
                        {
                            "class_name": "StoreSerdeInitializer",
                            "similarity_score": 0.46651238559414226
                        },
                        {
                            "class_name": "RocksDBMetrics",
                            "similarity_score": 0.4559012997344206
                        },
                        {
                            "class_name": "StateStoreMetrics",
                            "similarity_score": 0.44348143806264056
                        },
                        {
                            "class_name": "ThreadMetrics",
                            "similarity_score": 0.46830060816682373
                        },
                        {
                            "class_name": "TaskMetrics",
                            "similarity_score": 0.453133104516924
                        },
                        {
                            "class_name": "ClientMetrics",
                            "similarity_score": 0.47313460773423693
                        },
                        {
                            "class_name": "ProcessorNodeMetrics",
                            "similarity_score": 0.4425800404536507
                        },
                        {
                            "class_name": "HighAvailabilityTaskAssignor",
                            "similarity_score": 0.5522057797425258
                        },
                        {
                            "class_name": "WindowKeySchema",
                            "similarity_score": 0.5709910771002444
                        },
                        {
                            "class_name": "SessionKeySchema",
                            "similarity_score": 0.585566680091152
                        },
                        {
                            "class_name": "PositionSerde",
                            "similarity_score": 0.4317863215472255
                        },
                        {
                            "class_name": "RecordConverters",
                            "similarity_score": 0.5081594604019434
                        },
                        {
                            "class_name": "KeyFirstWindowKeySchema",
                            "similarity_score": 0.5636915661355181
                        },
                        {
                            "class_name": "KeyFirstSessionKeySchema",
                            "similarity_score": 0.5830223584490942
                        },
                        {
                            "class_name": "TimeFirstSessionKeySchema",
                            "similarity_score": 0.5775833932125598
                        },
                        {
                            "class_name": "TimeFirstWindowKeySchema",
                            "similarity_score": 0.5819040784557356
                        },
                        {
                            "class_name": "Branched",
                            "similarity_score": 0.14193336882546118
                        },
                        {
                            "class_name": "Grouped",
                            "similarity_score": 0.12619126709472495
                        },
                        {
                            "class_name": "Named",
                            "similarity_score": 0.45240064141108344
                        },
                        {
                            "class_name": "NamedCacheMetrics",
                            "similarity_score": 0.4047182202143189
                        },
                        {
                            "class_name": "InternalFixedKeyRecordFactory",
                            "similarity_score": 0.28571979712497314
                        },
                        {
                            "class_name": "StateRestoreCallbackAdapter",
                            "similarity_score": 0.5492104702344917
                        },
                        {
                            "class_name": "StickyTaskAssignor",
                            "similarity_score": 0.627876926975293
                        },
                        {
                            "class_name": "Produced",
                            "similarity_score": 0.24295237879573942
                        },
                        {
                            "class_name": "RangeQuery",
                            "similarity_score": 0.2023230598208423
                        },
                        {
                            "class_name": "ValueAndTimestampSerializer",
                            "similarity_score": 0.5586676457821598
                        },
                        {
                            "class_name": "TimestampedRangeQuery",
                            "similarity_score": 0.20692546544094456
                        },
                        {
                            "class_name": "Joined",
                            "similarity_score": 0.16341383023007774
                        },
                        {
                            "class_name": "TaskAndAction",
                            "similarity_score": 0.5199246459121749
                        },
                        {
                            "class_name": "Repartitioned",
                            "similarity_score": 0.18667207529604893
                        },
                        {
                            "class_name": "Consumed",
                            "similarity_score": 0.18228275608199845
                        },
                        {
                            "class_name": "ProcessorAdapter",
                            "similarity_score": 0.48632012170267613
                        },
                        {
                            "class_name": "FullChangeSerde",
                            "similarity_score": 0.6069561149494899
                        },
                        {
                            "class_name": "Materialized",
                            "similarity_score": 0.2031577576775274
                        },
                        {
                            "class_name": "ValueAndTimestamp",
                            "similarity_score": 0.37921584941153763
                        },
                        {
                            "class_name": "SlidingWindows",
                            "similarity_score": 0.22524151929429678
                        },
                        {
                            "class_name": "StreamThreadStateStoreProvider",
                            "similarity_score": 0.5741950067703714
                        },
                        {
                            "class_name": "StreamsConfig",
                            "similarity_score": 0.37598747921620507
                        },
                        {
                            "class_name": "SessionWindows",
                            "similarity_score": 0.21934017064956476
                        },
                        {
                            "class_name": "AssignmentInfo",
                            "similarity_score": 0.5491295915743736
                        },
                        {
                            "class_name": "Printed",
                            "similarity_score": 0.25572414712882685
                        },
                        {
                            "class_name": "KTableKTableJoinMerger",
                            "similarity_score": 0.5475896925319554
                        },
                        {
                            "class_name": "CopartitionedTopicsEnforcer",
                            "similarity_score": 0.556958523831428
                        },
                        {
                            "class_name": "TableJoined",
                            "similarity_score": 0.09692991360611036
                        },
                        {
                            "class_name": "WindowRangeQuery",
                            "similarity_score": 0.5471357425594605
                        },
                        {
                            "class_name": "PositionBound",
                            "similarity_score": 0.4744237363580554
                        },
                        {
                            "class_name": "HostInfo",
                            "similarity_score": 0.5249343311601774
                        },
                        {
                            "class_name": "AssignmentConfigs",
                            "similarity_score": 0.5346004195960328
                        },
                        {
                            "class_name": "LeftOrRightValue",
                            "similarity_score": 0.40256463418908783
                        },
                        {
                            "class_name": "RecordDeserializer",
                            "similarity_score": 0.4897653872837505
                        },
                        {
                            "class_name": "RecordTimeDefinition",
                            "similarity_score": 0.5287010571454294
                        },
                        {
                            "class_name": "UnoptimizableRepartitionNode",
                            "similarity_score": 0.45061505869656254
                        },
                        {
                            "class_name": "WindowEndTimeDefinition",
                            "similarity_score": 0.5143444998736397
                        },
                        {
                            "class_name": "InternalTopicProperties",
                            "similarity_score": 0.5334215094657592
                        },
                        {
                            "class_name": "TimeWindows",
                            "similarity_score": 0.24696165110939777
                        },
                        {
                            "class_name": "StreamsMetricsImpl",
                            "similarity_score": 0.5548752545571328
                        },
                        {
                            "class_name": "StreamJoined",
                            "similarity_score": 0.1932221472993633
                        },
                        {
                            "class_name": "Position",
                            "similarity_score": 0.4462549783036793
                        },
                        {
                            "class_name": "NamedInternal",
                            "similarity_score": 0.5329412328258994
                        },
                        {
                            "class_name": "OffsetCheckpoint",
                            "similarity_score": 0.45872049302918066
                        },
                        {
                            "class_name": "Maybe",
                            "similarity_score": 0.5684604193057915
                        },
                        {
                            "class_name": "To",
                            "similarity_score": 0.4322649674336255
                        },
                        {
                            "class_name": "TimestampedKeyAndJoinSide",
                            "similarity_score": 0.3871471347966618
                        },
                        {
                            "class_name": "JoinWindows",
                            "similarity_score": 0.21845982595858748
                        },
                        {
                            "class_name": "TaskId",
                            "similarity_score": 0.554001299833452
                        },
                        {
                            "class_name": "SubscriptionInfo",
                            "similarity_score": 0.6615311647542882
                        },
                        {
                            "class_name": "KeyQuery",
                            "similarity_score": 0.27344899741289763
                        },
                        {
                            "class_name": "KeyValue",
                            "similarity_score": 0.32586429191166727
                        },
                        {
                            "class_name": "TableSourceNode",
                            "similarity_score": 0.5572868173263924
                        },
                        {
                            "class_name": "VersionedKeyQuery",
                            "similarity_score": 0.1687054477417948
                        },
                        {
                            "class_name": "TimestampedKeyQuery",
                            "similarity_score": 0.2713198526571542
                        },
                        {
                            "class_name": "BufferValue",
                            "similarity_score": 0.5883477936944631
                        },
                        {
                            "class_name": "WindowKeyQuery",
                            "similarity_score": 0.5384943209092843
                        },
                        {
                            "class_name": "NamedTopologyStoreQueryParameters",
                            "similarity_score": 0.6129462186552845
                        },
                        {
                            "class_name": "ProcessId",
                            "similarity_score": 0.5103910194317351
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ClientUtils",
                        "StoreQueryUtils",
                        "SubscriptionInfo"
                    ],
                    "llm_response_time": 4171,
                    "similarity_computation_time": 25,
                    "similarity_metric": "cosine"
                },
                "metrics": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.33131155305704724
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.3167858457127419
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.28666876148460396
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.26387337116547055
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.33308048642935495
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Admin",
                        "State",
                        "Time"
                    ],
                    "llm_response_time": 5213,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "allLocalStorePartitionLags": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.35498353295575297
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.27617082428866535
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.16758020340017685
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.0976585747993176
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.1754954299627584
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Admin",
                        "Time",
                        "State"
                    ],
                    "llm_response_time": 7344,
                    "similarity_computation_time": 3,
                    "similarity_metric": "cosine"
                },
                "validateIsRunningOrRebalancing": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1997390471159653
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.09964972298064143
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.08685796484931753
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.25308553412176554
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.1809767353109946
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "State",
                        "StateListener",
                        "Time"
                    ],
                    "llm_response_time": 3310,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "streamThreadLeaveConsumerGroup": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.18670052476523674
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.06766808631037334
                        },
                        {
                            "class_name": "StreamStateListener",
                            "similarity_score": 0.5127671358013038
                        },
                        {
                            "class_name": "DelegatingStateRestoreListener",
                            "similarity_score": 0.5618062089605499
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.08041484968283592
                        },
                        {
                            "class_name": "DelegatingStandbyUpdateListener",
                            "similarity_score": 0.5786758528284401
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.1422606594884729
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.12566392435660173
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "StreamStateListener",
                        "DelegatingStandbyUpdateListener",
                        "DelegatingStateRestoreListener"
                    ],
                    "llm_response_time": 3605,
                    "similarity_computation_time": 3,
                    "similarity_metric": "cosine"
                },
                "getOrThrowException": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.2030811437477814
                        },
                        {
                            "class_name": "Metrics",
                            "similarity_score": 0.2650980427779992
                        },
                        {
                            "class_name": "StreamsConfig",
                            "similarity_score": 0.30012932844166157
                        },
                        {
                            "class_name": "StreamsMetadataState",
                            "similarity_score": 0.600969726743205
                        },
                        {
                            "class_name": "Admin",
                            "similarity_score": 0.05403975351285659
                        },
                        {
                            "class_name": "StreamsMetricsImpl",
                            "similarity_score": 0.5058345207091284
                        },
                        {
                            "class_name": "StreamStateListener",
                            "similarity_score": 0.537501438451034
                        },
                        {
                            "class_name": "DelegatingStateRestoreListener",
                            "similarity_score": 0.6684838548147903
                        },
                        {
                            "class_name": "KafkaClientSupplier",
                            "similarity_score": 0.04552938116202748
                        },
                        {
                            "class_name": "TopologyMetadata",
                            "similarity_score": 0.6561455991347379
                        },
                        {
                            "class_name": "QueryableStoreProvider",
                            "similarity_score": 0.36091673465185664
                        },
                        {
                            "class_name": "DelegatingStandbyUpdateListener",
                            "similarity_score": 0.6675956940941349
                        },
                        {
                            "class_name": "GlobalStreamThread",
                            "similarity_score": 0.6292538048243945
                        },
                        {
                            "class_name": "StateDirectory",
                            "similarity_score": 0.6575630527066008
                        },
                        {
                            "class_name": "StateListener",
                            "similarity_score": 0.15292174320642082
                        },
                        {
                            "class_name": "State",
                            "similarity_score": 0.13645162998669955
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "StateDirectory",
                        "DelegatingStateRestoreListener",
                        "DelegatingStandbyUpdateListener"
                    ],
                    "llm_response_time": 3963,
                    "similarity_computation_time": 13,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public isVoter(replicaKey ReplicaKey) : boolean extracted from public isVoter(nodeKey ReplicaKey) : boolean in class org.apache.kafka.raft.internals.VoterSet & moved to class org.apache.kafka.raft.internals.VoterSet.VoterNode",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 97,
                    "endLine": 122,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public isVoter(nodeKey ReplicaKey) : boolean"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 117,
                    "endLine": 117,
                    "startColumn": 17,
                    "endColumn": 29,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 113,
                    "endLine": 113,
                    "startColumn": 17,
                    "endColumn": 84,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 112,
                    "endLine": 118,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 112,
                    "endLine": 114,
                    "startColumn": 60,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 114,
                    "endLine": 118,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 303,
                    "endLine": 325,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public isVoter(replicaKey ReplicaKey) : boolean"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 323,
                    "endLine": 323,
                    "startColumn": 17,
                    "endColumn": 29,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 319,
                    "endLine": 319,
                    "startColumn": 17,
                    "endColumn": 80,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 318,
                    "endLine": 324,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 318,
                    "endLine": 320,
                    "startColumn": 53,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 320,
                    "endLine": 324,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 98,
                    "endLine": 114,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public isVoter(replicaKey ReplicaKey) : boolean"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 112,
                    "endLine": 112,
                    "startColumn": 26,
                    "endColumn": 50,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "node.isVoter(replicaKey)"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 316,
                    "endLine": 316,
                    "startColumn": 51,
                    "endColumn": 64,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 316,
                    "endLine": 316,
                    "startColumn": 13,
                    "endColumn": 64,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 598,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a59dce86eeab4914e3eae243b556d5c0ee07379a",
            "newBranchName": "extract-isVoter-isVoter-5b0e96d"
        },
        "telemetry": {
            "id": "0ddf72a7-67ef-4a23-9118-237c6e425349",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 367,
                "lineStart": 40,
                "lineEnd": 406,
                "bodyLineStart": 40,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                "sourceCode": "/**\n * A type for representing the set of voters for a topic partition.\n *\n * It encapsulates static information like a voter's endpoint and their supported kraft.version.\n *\n * It provides functionality for converting to and from {@code VotersRecord} and for converting\n * from the static configuration.\n */\npublic final class VoterSet {\n    private final Map<Integer, VoterNode> voters;\n\n    VoterSet(Map<Integer, VoterNode> voters) {\n        if (voters.isEmpty()) {\n            throw new IllegalArgumentException(\"Voters cannot be empty\");\n        }\n\n        this.voters = voters;\n    }\n\n    /**\n     * Returns the node information for all the given voter ids and listener.\n     *\n     * @param voterIds the ids of the voters\n     * @param listenerName the name of the listener\n     * @return the node information for all of the voter ids\n     * @throws IllegalArgumentException if there are missing endpoints\n     */\n    public Set<Node> voterNodes(Stream<Integer> voterIds, ListenerName listenerName) {\n        return voterIds\n            .map(voterId ->\n                voterNode(voterId, listenerName).orElseThrow(() ->\n                    new IllegalArgumentException(\n                        String.format(\n                            \"Unable to find endpoint for voter %d and listener %s in %s\",\n                            voterId,\n                            listenerName,\n                            voters\n                        )\n                    )\n                )\n            )\n            .collect(Collectors.toSet());\n    }\n\n    /**\n     * Returns the node information for a given voter id and listener.\n     *\n     * @param voterId the id of the voter\n     * @param listenerName the name of the listener\n     * @return the node information if it exists, otherwise {@code Optional.empty()}\n     */\n    public Optional<Node> voterNode(int voterId, ListenerName listenerName) {\n        return Optional.ofNullable(voters.get(voterId))\n            .flatMap(voterNode -> voterNode.address(listenerName))\n            .map(address -> new Node(voterId, address.getHostString(), address.getPort()));\n    }\n\n    /**\n     * Returns if the node is a voter in the set of voters.\n     *\n     * If the voter set includes the directory id, the {@code nodeKey} directory id must match the\n     * directory id specified by the voter set.\n     *\n     * If the voter set doesn't include the directory id ({@code Optional.empty()}), a node is in\n     * the voter set as long as the node id matches. The directory id is not checked.\n     *\n     * @param nodeKey the node's id and directory id\n     * @return true if the node is a voter in the voter set, otherwise false\n     */\n    public boolean isVoter(ReplicaKey nodeKey) {\n        VoterNode node = voters.get(nodeKey.id());\n        if (node != null) {\n            return isVoter(nodeKey, node);\n        } else {\n            return false;\n        }\n    }\n\n    private boolean isVoter(ReplicaKey nodeKey, VoterNode node) {\n        if (node.voterKey().directoryId().isPresent()) {\n            return node.voterKey().directoryId().equals(nodeKey.directoryId());\n        } else {\n            // configured voter set doesn't include a directory id so it is a voter as long as the node id\n            // matches\n            return true;\n        }\n    }\n\n    /**\n     * Returns if the node is the only voter in the set of voters.\n     *\n     * @param nodeKey the node's id and directory id\n     * @return true if the node is the only voter in the voter set, otherwise false\n     */\n    public boolean isOnlyVoter(ReplicaKey nodeKey) {\n        return voters.size() == 1 && isVoter(nodeKey);\n    }\n\n    /**\n     * Returns all of the voter ids.\n     */\n    public Set<Integer> voterIds() {\n        return voters.keySet();\n    }\n\n    public Map<Integer, VoterNode> voters() {\n        return voters;\n    }\n\n    /**\n     * Adds a voter to the voter set.\n     *\n     * This object is immutable. A new voter set is returned if the voter was added.\n     *\n     * A new voter can be added to a voter set if its id doesn't already exist in the voter set.\n     *\n     * @param voter the new voter to add\n     * @return a new voter set if the voter was added, otherwise {@code Optional.empty()}\n     */\n    public Optional<VoterSet> addVoter(VoterNode voter) {\n        if (voters.containsKey(voter.voterKey().id())) {\n            return Optional.empty();\n        }\n\n        HashMap<Integer, VoterNode> newVoters = new HashMap<>(voters);\n        newVoters.put(voter.voterKey().id(), voter);\n\n        return Optional.of(new VoterSet(newVoters));\n    }\n\n    /**\n     * Remove a voter from the voter set.\n     *\n     * This object is immutable. A new voter set is returned if the voter was removed.\n     *\n     * A voter can be removed from the voter set if its id and directory id match.\n     *\n     * @param voterKey the voter key\n     * @return a new voter set if the voter was removed, otherwise {@code Optional.empty()}\n     */\n    public Optional<VoterSet> removeVoter(ReplicaKey voterKey) {\n        VoterNode oldVoter = voters.get(voterKey.id());\n        if (oldVoter != null && Objects.equals(oldVoter.voterKey(), voterKey)) {\n            HashMap<Integer, VoterNode> newVoters = new HashMap<>(voters);\n            newVoters.remove(voterKey.id());\n\n            return Optional.of(new VoterSet(newVoters));\n        }\n\n        return Optional.empty();\n    }\n\n    /**\n     * Converts a voter set to a voters record for a given version.\n     *\n     * @param version the version of the voters record\n     */\n    public VotersRecord toVotersRecord(short version) {\n        Function<VoterNode, VotersRecord.Voter> voterConvertor = voter -> {\n            Iterator<VotersRecord.Endpoint> endpoints = voter\n                .listeners()\n                .entrySet()\n                .stream()\n                .map(entry ->\n                    new VotersRecord.Endpoint()\n                        .setName(entry.getKey().value())\n                        .setHost(entry.getValue().getHostString())\n                        .setPort(entry.getValue().getPort())\n                )\n                .iterator();\n\n            VotersRecord.KRaftVersionFeature kraftVersionFeature = new VotersRecord.KRaftVersionFeature()\n                .setMinSupportedVersion(voter.supportedKRaftVersion().min())\n                .setMaxSupportedVersion(voter.supportedKRaftVersion().max());\n\n            return new VotersRecord.Voter()\n                .setVoterId(voter.voterKey().id())\n                .setVoterDirectoryId(voter.voterKey().directoryId().orElse(Uuid.ZERO_UUID))\n                .setEndpoints(new VotersRecord.EndpointCollection(endpoints))\n                .setKRaftVersionFeature(kraftVersionFeature);\n        };\n\n        List<VotersRecord.Voter> voterRecordVoters = voters\n            .values()\n            .stream()\n            .map(voterConvertor)\n            .collect(Collectors.toList());\n\n        return new VotersRecord()\n            .setVersion(version)\n            .setVoters(voterRecordVoters);\n    }\n\n    /**\n     * Determines if two sets of voters have an overlapping majority.\n     *\n     * An overlapping majority means that for all majorities in {@code this} set of voters and for\n     * all majority in {@code that} set of voters, they have at least one voter in common.\n     *\n     * If this function returns true, it means that if one of the set of voters commits an offset,\n     * the other set of voters cannot commit a conflicting offset.\n     *\n     * @param that the other voter set to compare\n     * @return true if they have an overlapping majority, false otherwise\n     */\n    public boolean hasOverlappingMajority(VoterSet that) {\n        Set<ReplicaKey> thisReplicaKeys = voters\n            .values()\n            .stream()\n            .map(VoterNode::voterKey)\n            .collect(Collectors.toSet());\n\n        Set<ReplicaKey> thatReplicaKeys = that.voters\n            .values()\n            .stream()\n            .map(VoterNode::voterKey)\n            .collect(Collectors.toSet());\n\n        if (Utils.diff(HashSet::new, thisReplicaKeys, thatReplicaKeys).size() > 1) return false;\n        return Utils.diff(HashSet::new, thatReplicaKeys, thisReplicaKeys).size() <= 1;\n    }\n\n    @Override\n    public boolean equals(Object o) {\n        if (this == o) return true;\n        if (o == null || getClass() != o.getClass()) return false;\n\n        VoterSet that = (VoterSet) o;\n\n        return voters.equals(that.voters);\n    }\n\n    @Override\n    public int hashCode() {\n        return Objects.hashCode(voters);\n    }\n\n    @Override\n    public String toString() {\n        return String.format(\"VoterSet(voters=%s)\", voters);\n    }\n\n    public static final class VoterNode {\n        private final ReplicaKey voterKey;\n        private final Map<ListenerName, InetSocketAddress> listeners;\n        private final SupportedVersionRange supportedKRaftVersion;\n\n        public VoterNode(\n            ReplicaKey voterKey,\n            Map<ListenerName, InetSocketAddress> listeners,\n            SupportedVersionRange supportedKRaftVersion\n        ) {\n            this.voterKey = voterKey;\n            this.listeners = listeners;\n            this.supportedKRaftVersion = supportedKRaftVersion;\n        }\n\n        public ReplicaKey voterKey() {\n            return voterKey;\n        }\n\n        Map<ListenerName, InetSocketAddress> listeners() {\n            return listeners;\n        }\n\n        SupportedVersionRange supportedKRaftVersion() {\n            return supportedKRaftVersion;\n        }\n\n\n        Optional<InetSocketAddress> address(ListenerName listener) {\n            return Optional.ofNullable(listeners.get(listener));\n        }\n\n        @Override\n        public boolean equals(Object o) {\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n\n            VoterNode that = (VoterNode) o;\n\n            if (!Objects.equals(voterKey, that.voterKey)) return false;\n            if (!Objects.equals(supportedKRaftVersion, that.supportedKRaftVersion)) return false;\n            return Objects.equals(listeners, that.listeners);\n        }\n\n        @Override\n        public int hashCode() {\n            return Objects.hash(voterKey, listeners, supportedKRaftVersion);\n        }\n\n        @Override\n        public String toString() {\n            return String.format(\n                \"VoterNode(voterKey=%s, listeners=%s, supportedKRaftVersion=%s)\",\n                voterKey,\n                listeners,\n                supportedKRaftVersion\n            );\n        }\n    }\n\n    /**\n     * Converts a {@code VotersRecord} to a {@code VoterSet}.\n     *\n     * @param voters the set of voters control record\n     * @return the voter set\n     */\n    public static VoterSet fromVotersRecord(VotersRecord voters) {\n        HashMap<Integer, VoterNode> voterNodes = new HashMap<>(voters.voters().size());\n        for (VotersRecord.Voter voter: voters.voters()) {\n            final Optional<Uuid> directoryId;\n            if (!voter.voterDirectoryId().equals(Uuid.ZERO_UUID)) {\n                directoryId = Optional.of(voter.voterDirectoryId());\n            } else {\n                directoryId = Optional.empty();\n            }\n\n            Map<ListenerName, InetSocketAddress> listeners = new HashMap<>(voter.endpoints().size());\n            for (VotersRecord.Endpoint endpoint : voter.endpoints()) {\n                listeners.put(\n                    ListenerName.normalised(endpoint.name()),\n                    InetSocketAddress.createUnresolved(endpoint.host(), endpoint.port())\n                );\n            }\n\n            voterNodes.put(\n                voter.voterId(),\n                new VoterNode(\n                    ReplicaKey.of(voter.voterId(), directoryId),\n                    listeners,\n                    new SupportedVersionRange(\n                        voter.kRaftVersionFeature().minSupportedVersion(),\n                        voter.kRaftVersionFeature().maxSupportedVersion()\n                    )\n                )\n            );\n        }\n\n        return new VoterSet(voterNodes);\n    }\n\n    /**\n     * Creates a voter set from a map of socket addresses.\n     *\n     * @param listener the listener name for all of the endpoints\n     * @param voters the socket addresses by voter id\n     * @return the voter set\n     */\n    public static VoterSet fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters) {\n        Map<Integer, VoterNode> voterNodes = voters\n            .entrySet()\n            .stream()\n            .collect(\n                Collectors.toMap(\n                    Map.Entry::getKey,\n                    entry -> new VoterNode(\n                        ReplicaKey.of(entry.getKey(), Optional.empty()),\n                        Collections.singletonMap(listener, entry.getValue()),\n                        new SupportedVersionRange((short) 0, (short) 0)\n                    )\n                )\n            );\n\n        return new VoterSet(voterNodes);\n    }\n}",
                "methodCount": 25
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 117,
                        "lineEnd": 125,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method isVoter to class VoterNode",
                        "description": "Move method isVoter to org.apache.kafka.raft.internals.VoterSet.VoterNode\nRationale: The method `isVoter` predominantly relies on the `VoterNode` class to perform its logic, specifically the `voterKey()` method of `VoterNode`. This indicates that the behavior being checked (whether a node is a voter) is closely tied to the `VoterNode` itself. By placing this method inside the `VoterNode` class, we keep related functionality together, improving cohesion and making the code easier to understand and maintain. Additionally, it respects the principle of encapsulating behavior within the domain entity that it most directly acts upon.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "fromVotersRecord",
                            "method_signature": "public static VoterSet fromVotersRecord(VotersRecord voters)",
                            "target_class": "VotersRecord",
                            "rationale": "The method converts a VotersRecord to a VoterSet, making it more appropriate to reside in the VotersRecord class as a factory method."
                        },
                        {
                            "method_name": "fromInetSocketAddresses",
                            "method_signature": "public static VoterSet fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters)",
                            "target_class": "VotersRecord",
                            "rationale": "The method constructs a VoterSet from Map of socket addresses, this functionality might fit better in the VotersRecord class, possibly as a utility or conversion method."
                        }
                    ],
                    "llm_response_time": 3502
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "toVotersRecord",
                            "method_signature": "public VotersRecord toVotersRecord(short version)",
                            "target_class": "VotersRecord",
                            "rationale": "The method toVotersRecord is responsible for converting a VoterSet to a VotersRecord, making it a better fit for the VotersRecord class which is directly related to the conversion."
                        },
                        {
                            "method_name": "fromVotersRecord",
                            "method_signature": "public static VoterSet fromVotersRecord(VotersRecord voters)",
                            "target_class": "VotersRecord",
                            "rationale": "The method fromVotersRecord is responsible for creating a VoterSet from a VotersRecord, which makes it a better fit for the VotersRecord class because it directly deals with the details of VotersRecord."
                        },
                        {
                            "method_name": "fromInetSocketAddresses",
                            "method_signature": "public static VoterSet fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters)",
                            "target_class": "VoterNode",
                            "rationale": "The method fromInetSocketAddresses is responsible for creating VoterNodes from a map of socket addresses, making it a more logical part of the VoterNode class which handles these kinds of operations."
                        }
                    ],
                    "llm_response_time": 3644
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "toVotersRecord",
                            "method_signature": "public VotersRecord toVotersRecord(short version)",
                            "target_class": "VotersRecord",
                            "rationale": "The conversion from VoterSet to VotersRecord should logically be handled within the VotersRecord class because it is specific to the creation of a VotersRecord instance."
                        },
                        {
                            "method_name": "fromVotersRecord",
                            "method_signature": "public static VoterSet fromVotersRecord(VotersRecord voters)",
                            "target_class": "VotersRecord",
                            "rationale": "The conversion from a VotersRecord to a VoterSet should logically be handled within the VotersRecord class because it is specific to interpreting VotersRecord data into a VoterSet."
                        },
                        {
                            "method_name": "fromInetSocketAddresses",
                            "method_signature": "public static VoterSet fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters)",
                            "target_class": "InetSocketAddress",
                            "rationale": "This method converts from a collection of InetSocketAddresses to a VoterSet, hence it belongs within an InetSocketAddress-based utility or helper class."
                        }
                    ],
                    "llm_response_time": 3302
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int voterId, ListenerName listenerName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isVoter",
                            "method_signature": "public isVoter(ReplicaKey nodeKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isVoter",
                            "method_signature": "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addVoter",
                            "method_signature": "public addVoter(VoterNode voter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeVoter",
                            "method_signature": "public removeVoter(ReplicaKey voterKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toVotersRecord",
                            "method_signature": "public toVotersRecord(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOverlappingMajority",
                            "method_signature": "public hasOverlappingMajority(VoterSet that)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "isVoter",
                            "method_signature": "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOverlappingMajority",
                            "method_signature": "public hasOverlappingMajority(VoterSet that)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toVotersRecord",
                            "method_signature": "public toVotersRecord(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int voterId, ListenerName listenerName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isVoter",
                            "method_signature": "public isVoter(ReplicaKey nodeKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addVoter",
                            "method_signature": "public addVoter(VoterNode voter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeVoter",
                            "method_signature": "public removeVoter(ReplicaKey voterKey)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private isVoter(ReplicaKey nodeKey, VoterNode node)": {
                        "first": {
                            "method_name": "isVoter",
                            "method_signature": "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5026545630796335
                    },
                    "public hasOverlappingMajority(VoterSet that)": {
                        "first": {
                            "method_name": "hasOverlappingMajority",
                            "method_signature": "public hasOverlappingMajority(VoterSet that)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5910342605513871
                    },
                    "public toVotersRecord(short version)": {
                        "first": {
                            "method_name": "toVotersRecord",
                            "method_signature": "public toVotersRecord(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5969578432683813
                    },
                    "public voterNode(int voterId, ListenerName listenerName)": {
                        "first": {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int voterId, ListenerName listenerName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.7281878658067636
                    },
                    "public isVoter(ReplicaKey nodeKey)": {
                        "first": {
                            "method_name": "isVoter",
                            "method_signature": "public isVoter(ReplicaKey nodeKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.7438990651278814
                    },
                    "public addVoter(VoterNode voter)": {
                        "first": {
                            "method_name": "addVoter",
                            "method_signature": "public addVoter(VoterNode voter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.756175773318763
                    },
                    "public removeVoter(ReplicaKey voterKey)": {
                        "first": {
                            "method_name": "removeVoter",
                            "method_signature": "public removeVoter(ReplicaKey voterKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.7980538348554548
                    }
                },
                "voyage": {
                    "public hasOverlappingMajority(VoterSet that)": {
                        "first": {
                            "method_name": "hasOverlappingMajority",
                            "method_signature": "public hasOverlappingMajority(VoterSet that)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5133125854160105
                    },
                    "public isVoter(ReplicaKey nodeKey)": {
                        "first": {
                            "method_name": "isVoter",
                            "method_signature": "public isVoter(ReplicaKey nodeKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6275568570368207
                    },
                    "private isVoter(ReplicaKey nodeKey, VoterNode node)": {
                        "first": {
                            "method_name": "isVoter",
                            "method_signature": "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6431897903994309
                    },
                    "public voterNode(int voterId, ListenerName listenerName)": {
                        "first": {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int voterId, ListenerName listenerName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6460647093026198
                    },
                    "public removeVoter(ReplicaKey voterKey)": {
                        "first": {
                            "method_name": "removeVoter",
                            "method_signature": "public removeVoter(ReplicaKey voterKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6697522585183326
                    },
                    "public addVoter(VoterNode voter)": {
                        "first": {
                            "method_name": "addVoter",
                            "method_signature": "public addVoter(VoterNode voter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6774990165798196
                    },
                    "public toVotersRecord(short version)": {
                        "first": {
                            "method_name": "toVotersRecord",
                            "method_signature": "public toVotersRecord(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.7263314158254318
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "public hasOverlappingMajority(VoterSet that)",
                        "public toVotersRecord(short version)",
                        "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                        "public voterNode(int voterId, ListenerName listenerName)",
                        "public isVoter(ReplicaKey nodeKey)",
                        "public addVoter(VoterNode voter)",
                        "public removeVoter(ReplicaKey voterKey)"
                    ],
                    "llm_response_time": 6305
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                        "public isVoter(ReplicaKey nodeKey)",
                        "public voterNode(int voterId, ListenerName listenerName)",
                        "public hasOverlappingMajority(VoterSet that)",
                        "public toVotersRecord(short version)"
                    ],
                    "llm_response_time": 5005
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "public toVotersRecord(short version)",
                        "public hasOverlappingMajority(VoterSet that)",
                        "private isVoter(ReplicaKey nodeKey, VoterNode node)"
                    ],
                    "llm_response_time": 3402
                },
                "voyage": {
                    "priority_method_names": [
                        "public hasOverlappingMajority(VoterSet that)",
                        "public toVotersRecord(short version)",
                        "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                        "public voterNode(int voterId, ListenerName listenerName)",
                        "public isVoter(ReplicaKey nodeKey)",
                        "public addVoter(VoterNode voter)",
                        "public removeVoter(ReplicaKey voterKey)"
                    ],
                    "llm_response_time": 5850
                },
                "voyage-5": {
                    "priority_method_names": [
                        "public hasOverlappingMajority(VoterSet that)",
                        "public removeVoter(ReplicaKey voterKey)",
                        "public isVoter(ReplicaKey nodeKey)",
                        "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                        "public voterNode(int voterId, ListenerName listenerName)"
                    ],
                    "llm_response_time": 4719
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                        "public isVoter(ReplicaKey nodeKey)",
                        "public hasOverlappingMajority(VoterSet that)"
                    ],
                    "llm_response_time": 3886
                }
            },
            "targetClassMap": {
                "isVoter": {
                    "target_classes": [
                        {
                            "class_name": "VoterNode",
                            "similarity_score": 0.19311110245460245
                        },
                        {
                            "class_name": "ReplicaKey",
                            "similarity_score": 0.19311110245460245
                        }
                    ],
                    "target_classes_sorted_by_llm": ["VoterNode", "ReplicaKey"],
                    "llm_response_time": 2622,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "hasOverlappingMajority": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2043,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "toVotersRecord": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2901,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "voterNode": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3328,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "addVoter": {
                    "target_classes": [
                        {
                            "class_name": "VoterNode",
                            "similarity_score": 0.19311110245460245
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "VoterNode"
                    ],
                    "llm_response_time": 1953,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "removeVoter": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3442,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public voterKeys() : Set<ReplicaKey> extracted from public advanceLocalLeaderHighWatermarkToLogEndOffset() : void in class org.apache.kafka.raft.RaftClientTestContext & moved to class org.apache.kafka.raft.internals.VoterSet",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 1156,
                    "endLine": 1171,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public advanceLocalLeaderHighWatermarkToLogEndOffset() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 1159,
                    "endLine": 1159,
                    "startColumn": 9,
                    "endColumn": 123,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 133,
                    "endLine": 142,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public voterKeys() : Set<ReplicaKey>"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 137,
                    "endLine": 141,
                    "startColumn": 9,
                    "endColumn": 42,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 1466,
                    "endLine": 1487,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public advanceLocalLeaderHighWatermarkToLogEndOffset() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 1470,
                    "endLine": 1471,
                    "startColumn": 48,
                    "endColumn": 25,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "voters.voterKeys()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 599,
        "extraction_results": {
            "success": true,
            "newCommitHash": "62284eacd3fc8ac2c2d1c96aec657cfc93a1434c",
            "newBranchName": "extract-voterKeys-advanceLocalLeaderHighWatermarkToLogEndOffset-5b0e96d"
        },
        "telemetry": {
            "id": "b0183501-a220-49ac-a7d3-062d78ee7dd8",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1218,
                "lineStart": 94,
                "lineEnd": 1311,
                "bodyLineStart": 94,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                "sourceCode": "public final class RaftClientTestContext {\n    public final RecordSerde<String> serde = Builder.SERDE;\n    final TopicPartition metadataPartition = Builder.METADATA_PARTITION;\n    final Uuid metadataTopicId = Uuid.METADATA_TOPIC_ID;\n    final int electionBackoffMaxMs = Builder.ELECTION_BACKOFF_MAX_MS;\n    final int fetchMaxWaitMs = Builder.FETCH_MAX_WAIT_MS;\n    final int fetchTimeoutMs = Builder.FETCH_TIMEOUT_MS;\n    final int checkQuorumTimeoutMs = (int) (fetchTimeoutMs * CHECK_QUORUM_TIMEOUT_FACTOR);\n    final int retryBackoffMs = Builder.RETRY_BACKOFF_MS;\n\n    private int electionTimeoutMs;\n    private int requestTimeoutMs;\n    private int appendLingerMs;\n\n    private final QuorumStateStore quorumStateStore;\n    final Uuid clusterId;\n    private final OptionalInt localId;\n    public final KafkaRaftClient<String> client;\n    final Metrics metrics;\n    public final MockLog log;\n    final MockNetworkChannel channel;\n    final MockMessageQueue messageQueue;\n    final MockTime time;\n    final MockListener listener;\n    final Set<Integer> voters;\n    final Set<Integer> bootstrapIds;\n\n    private final List<RaftResponse.Outbound> sentResponses = new ArrayList<>();\n\n    public static final class Builder {\n        static final int DEFAULT_ELECTION_TIMEOUT_MS = 10000;\n\n        private static final RecordSerde<String> SERDE = new StringSerde();\n        private static final TopicPartition METADATA_PARTITION = new TopicPartition(\"metadata\", 0);\n        private static final int ELECTION_BACKOFF_MAX_MS = 100;\n        private static final int FETCH_MAX_WAIT_MS = 0;\n        // fetch timeout is usually larger than election timeout\n        private static final int FETCH_TIMEOUT_MS = 50000;\n        private static final int DEFAULT_REQUEST_TIMEOUT_MS = 5000;\n        private static final int RETRY_BACKOFF_MS = 50;\n        private static final int DEFAULT_APPEND_LINGER_MS = 0;\n\n        private final MockMessageQueue messageQueue = new MockMessageQueue();\n        private final MockTime time = new MockTime();\n        private final QuorumStateStore quorumStateStore = new MockQuorumStateStore();\n        private final MockableRandom random = new MockableRandom(1L);\n        private final LogContext logContext = new LogContext();\n        private final MockLog log = new MockLog(METADATA_PARTITION, Uuid.METADATA_TOPIC_ID, logContext);\n        private final Uuid clusterId = Uuid.randomUuid();\n        private final Set<Integer> voters;\n        private final OptionalInt localId;\n        private final Uuid localDirectoryId = Uuid.randomUuid();\n        private final short kraftVersion = 0;\n\n        private int requestTimeoutMs = DEFAULT_REQUEST_TIMEOUT_MS;\n        private int electionTimeoutMs = DEFAULT_ELECTION_TIMEOUT_MS;\n        private int appendLingerMs = DEFAULT_APPEND_LINGER_MS;\n        private MemoryPool memoryPool = MemoryPool.NONE;\n        private List<InetSocketAddress> bootstrapServers = Collections.emptyList();\n\n        public Builder(int localId, Set<Integer> voters) {\n            this(OptionalInt.of(localId), voters);\n        }\n\n        public Builder(OptionalInt localId, Set<Integer> voters) {\n            this.voters = voters;\n            this.localId = localId;\n        }\n\n        Builder withElectedLeader(int epoch, int leaderId) {\n            quorumStateStore.writeElectionState(\n                ElectionState.withElectedLeader(epoch, leaderId, voters),\n                kraftVersion\n            );\n            return this;\n        }\n\n        Builder withUnknownLeader(int epoch) {\n            quorumStateStore.writeElectionState(\n                ElectionState.withUnknownLeader(epoch, voters),\n                kraftVersion\n            );\n            return this;\n        }\n\n        Builder withVotedCandidate(int epoch, ReplicaKey votedKey) {\n            quorumStateStore.writeElectionState(\n                ElectionState.withVotedCandidate(epoch, votedKey, voters),\n                kraftVersion\n            );\n            return this;\n        }\n\n        Builder updateRandom(Consumer<MockableRandom> consumer) {\n            consumer.accept(random);\n            return this;\n        }\n\n        Builder withMemoryPool(MemoryPool pool) {\n            this.memoryPool = pool;\n            return this;\n        }\n\n        Builder withAppendLingerMs(int appendLingerMs) {\n            this.appendLingerMs = appendLingerMs;\n            return this;\n        }\n\n        public Builder appendToLog(int epoch, List<String> records) {\n            MemoryRecords batch = buildBatch(\n                time.milliseconds(),\n                log.endOffset().offset,\n                epoch,\n                records\n            );\n            log.appendAsLeader(batch, epoch);\n            // Need to flush the log to update the last flushed offset. This is always correct\n            // because append operation was done in the Builder which represent the state of the\n            // log before the replica starts.\n            log.flush(false);\n\n            // Reset the value of this method since \"flush\" before the replica start should not\n            // count when checking for flushes by the KRaft client.\n            log.flushedSinceLastChecked();\n            return this;\n        }\n\n        Builder withEmptySnapshot(OffsetAndEpoch snapshotId) {\n            try (RawSnapshotWriter snapshot = log.createNewSnapshotUnchecked(snapshotId).get()) {\n                snapshot.freeze();\n            }\n            return this;\n        }\n\n        Builder deleteBeforeSnapshot(OffsetAndEpoch snapshotId) {\n            if (snapshotId.offset() > log.highWatermark().offset) {\n                log.updateHighWatermark(new LogOffsetMetadata(snapshotId.offset()));\n            }\n            log.deleteBeforeSnapshot(snapshotId);\n\n            return this;\n        }\n\n        Builder withElectionTimeoutMs(int electionTimeoutMs) {\n            this.electionTimeoutMs = electionTimeoutMs;\n            return this;\n        }\n\n        Builder withRequestTimeoutMs(int requestTimeoutMs) {\n            this.requestTimeoutMs = requestTimeoutMs;\n            return this;\n        }\n\n        Builder withBootstrapServers(List<InetSocketAddress> bootstrapServers) {\n            this.bootstrapServers = bootstrapServers;\n            return this;\n        }\n\n        public RaftClientTestContext build() throws IOException {\n            Metrics metrics = new Metrics(time);\n            MockNetworkChannel channel = new MockNetworkChannel();\n            MockListener listener = new MockListener(localId);\n            Map<Integer, InetSocketAddress> voterAddressMap = voters\n                .stream()\n                .collect(Collectors.toMap(Function.identity(), RaftClientTestContext::mockAddress));\n\n            QuorumConfig quorumConfig = new QuorumConfig(\n                requestTimeoutMs,\n                RETRY_BACKOFF_MS,\n                electionTimeoutMs,\n                ELECTION_BACKOFF_MAX_MS,\n                FETCH_TIMEOUT_MS,\n                appendLingerMs\n            );\n\n            KafkaRaftClient<String> client = new KafkaRaftClient<>(\n                localId,\n                localDirectoryId,\n                SERDE,\n                channel,\n                messageQueue,\n                log,\n                memoryPool,\n                time,\n                new MockExpirationService(time),\n                FETCH_MAX_WAIT_MS,\n                clusterId.toString(),\n                bootstrapServers,\n                logContext,\n                random,\n                quorumConfig\n            );\n\n            client.register(listener);\n            client.initialize(\n                voterAddressMap,\n                quorumStateStore,\n                metrics\n            );\n\n            RaftClientTestContext context = new RaftClientTestContext(\n                clusterId,\n                localId,\n                client,\n                log,\n                channel,\n                messageQueue,\n                time,\n                quorumStateStore,\n                voters,\n                IntStream\n                    .iterate(-2, id -> id - 1)\n                    .limit(bootstrapServers.size())\n                    .boxed()\n                    .collect(Collectors.toSet()),\n                metrics,\n                listener\n            );\n\n            context.electionTimeoutMs = electionTimeoutMs;\n            context.requestTimeoutMs = requestTimeoutMs;\n            context.appendLingerMs = appendLingerMs;\n\n            return context;\n        }\n    }\n\n    private RaftClientTestContext(\n        Uuid clusterId,\n        OptionalInt localId,\n        KafkaRaftClient<String> client,\n        MockLog log,\n        MockNetworkChannel channel,\n        MockMessageQueue messageQueue,\n        MockTime time,\n        QuorumStateStore quorumStateStore,\n        Set<Integer> voters,\n        Set<Integer> bootstrapIds,\n        Metrics metrics,\n        MockListener listener\n    ) {\n        this.clusterId = clusterId;\n        this.localId = localId;\n        this.client = client;\n        this.log = log;\n        this.channel = channel;\n        this.messageQueue = messageQueue;\n        this.time = time;\n        this.quorumStateStore = quorumStateStore;\n        this.voters = voters;\n        this.bootstrapIds = bootstrapIds;\n        this.metrics = metrics;\n        this.listener = listener;\n    }\n\n    int electionTimeoutMs() {\n        return electionTimeoutMs;\n    }\n\n    int requestTimeoutMs() {\n        return requestTimeoutMs;\n    }\n\n    int appendLingerMs() {\n        return appendLingerMs;\n    }\n\n    MemoryRecords buildBatch(\n        long baseOffset,\n        int epoch,\n        List<String> records\n    ) {\n        return buildBatch(time.milliseconds(), baseOffset, epoch, records);\n    }\n\n    static MemoryRecords buildBatch(\n        long timestamp,\n        long baseOffset,\n        int epoch,\n        List<String> records\n    ) {\n        ByteBuffer buffer = ByteBuffer.allocate(512);\n        BatchBuilder<String> builder = new BatchBuilder<>(\n            buffer,\n            Builder.SERDE,\n            Compression.NONE,\n            baseOffset,\n            timestamp,\n            false,\n            epoch,\n            512\n        );\n\n        for (String record : records) {\n            builder.appendRecord(record, null);\n        }\n\n        return builder.build();\n    }\n\n    static RaftClientTestContext initializeAsLeader(int localId, Set<Integer> voters, int epoch) throws Exception {\n        if (epoch <= 0) {\n            throw new IllegalArgumentException(\"Cannot become leader in epoch \" + epoch);\n        }\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withUnknownLeader(epoch - 1)\n            .build();\n\n        context.assertUnknownLeader(epoch - 1);\n        context.becomeLeader();\n        return context;\n    }\n\n    public void becomeLeader() throws Exception {\n        int currentEpoch = currentEpoch();\n        time.sleep(electionTimeoutMs * 2L);\n        expectAndGrantVotes(currentEpoch + 1);\n        expectBeginEpoch(currentEpoch + 1);\n    }\n\n    public OptionalInt currentLeader() {\n        return currentLeaderAndEpoch().leaderId();\n    }\n\n    public int currentEpoch() {\n        return currentLeaderAndEpoch().epoch();\n    }\n\n    LeaderAndEpoch currentLeaderAndEpoch() {\n        ElectionState election = quorumStateStore.readElectionState().get();\n        return new LeaderAndEpoch(election.optionalLeaderId(), election.epoch());\n    }\n\n    void expectAndGrantVotes(int epoch) throws Exception {\n        pollUntilRequest();\n\n        List<RaftRequest.Outbound> voteRequests = collectVoteRequests(epoch,\n            log.lastFetchedEpoch(), log.endOffset().offset);\n\n        for (RaftRequest.Outbound request : voteRequests) {\n            VoteResponseData voteResponse = voteResponse(true, Optional.empty(), epoch);\n            deliverResponse(request.correlationId(), request.destination(), voteResponse);\n        }\n\n        client.poll();\n        assertElectedLeader(epoch, localIdOrThrow());\n    }\n\n    private int localIdOrThrow() {\n        return localId.orElseThrow(() -> new AssertionError(\"Required local id is not defined\"));\n    }\n\n    private void expectBeginEpoch(int epoch) throws Exception {\n        pollUntilRequest();\n        for (RaftRequest.Outbound request : collectBeginEpochRequests(epoch)) {\n            BeginQuorumEpochResponseData beginEpochResponse = beginEpochResponse(epoch, localIdOrThrow());\n            deliverResponse(request.correlationId(), request.destination(), beginEpochResponse);\n        }\n        client.poll();\n    }\n\n    public void pollUntil(TestCondition condition) throws InterruptedException {\n        TestUtils.waitForCondition(() -> {\n            client.poll();\n            return condition.conditionMet();\n        }, 5000, \"Condition failed to be satisfied before timeout\");\n    }\n\n    void pollUntilResponse() throws InterruptedException {\n        pollUntil(() -> !sentResponses.isEmpty());\n    }\n\n    void pollUntilRequest() throws InterruptedException {\n        pollUntil(channel::hasSentRequests);\n    }\n\n    void assertVotedCandidate(int epoch, int candidateId) {\n        assertEquals(\n            ElectionState.withVotedCandidate(\n                epoch,\n                ReplicaKey.of(candidateId, Optional.empty()),\n                voters\n            ),\n            quorumStateStore.readElectionState().get()\n        );\n    }\n\n    public void assertElectedLeader(int epoch, int leaderId) {\n        assertEquals(\n            ElectionState.withElectedLeader(epoch, leaderId, voters),\n            quorumStateStore.readElectionState().get()\n        );\n    }\n\n    void assertUnknownLeader(int epoch) {\n        assertEquals(\n            ElectionState.withUnknownLeader(epoch, voters),\n            quorumStateStore.readElectionState().get()\n        );\n    }\n\n    void assertResignedLeader(int epoch, int leaderId) {\n        assertTrue(client.quorum().isResigned());\n        assertEquals(\n            ElectionState.withElectedLeader(epoch, leaderId, voters),\n            quorumStateStore.readElectionState().get()\n        );\n    }\n\n    DescribeQuorumResponseData collectDescribeQuorumResponse() {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.DESCRIBE_QUORUM);\n        assertEquals(1, sentMessages.size());\n        RaftResponse.Outbound raftMessage = sentMessages.get(0);\n        assertInstanceOf(\n            DescribeQuorumResponseData.class,\n            raftMessage.data(),\n            \"Unexpected request type \" + raftMessage.data());\n        return (DescribeQuorumResponseData) raftMessage.data();\n    }\n\n    void assertSentDescribeQuorumResponse(\n        int leaderId,\n        int leaderEpoch,\n        long highWatermark,\n        List<ReplicaState> voterStates,\n        List<ReplicaState> observerStates\n    ) {\n        DescribeQuorumResponseData response = collectDescribeQuorumResponse();\n\n        DescribeQuorumResponseData.PartitionData partitionData = new DescribeQuorumResponseData.PartitionData()\n            .setErrorCode(Errors.NONE.code())\n            .setLeaderId(leaderId)\n            .setLeaderEpoch(leaderEpoch)\n            .setHighWatermark(highWatermark)\n            .setCurrentVoters(voterStates)\n            .setObservers(observerStates);\n\n        DescribeQuorumResponseData.NodeCollection nodes = new DescribeQuorumResponseData.NodeCollection();\n\n        Consumer<DescribeQuorumResponseData.ReplicaState> addToNodes = replicaState -> {\n            if (nodes.find(replicaState.replicaId()) != null)\n                return;\n\n            nodes.add(new DescribeQuorumResponseData.Node()\n                .setNodeId(replicaState.replicaId()));\n        };\n\n        voterStates.forEach(addToNodes);\n        observerStates.forEach(addToNodes);\n\n        DescribeQuorumResponseData expectedResponse = DescribeQuorumResponse.singletonResponse(\n            metadataPartition,\n            partitionData,\n            nodes\n        );\n        assertEquals(expectedResponse, response);\n    }\n\n    RaftRequest.Outbound assertSentVoteRequest(int epoch, int lastEpoch, long lastEpochOffset, int numVoteReceivers) {\n        List<RaftRequest.Outbound> voteRequests = collectVoteRequests(epoch, lastEpoch, lastEpochOffset);\n        assertEquals(numVoteReceivers, voteRequests.size());\n        return voteRequests.iterator().next();\n    }\n\n    void assertSentVoteResponse(Errors error) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.VOTE);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(VoteResponseData.class, raftMessage.data());\n        VoteResponseData response = (VoteResponseData) raftMessage.data();\n\n        assertEquals(error, Errors.forCode(response.errorCode()));\n    }\n\n    void assertSentVoteResponse(\n        Errors error,\n        int epoch,\n        OptionalInt leaderId,\n        boolean voteGranted\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.VOTE);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(VoteResponseData.class, raftMessage.data());\n        VoteResponseData response = (VoteResponseData) raftMessage.data();\n        assertTrue(hasValidTopicPartition(response, metadataPartition));\n\n        VoteResponseData.PartitionData partitionResponse = response.topics().get(0).partitions().get(0);\n\n        assertEquals(voteGranted, partitionResponse.voteGranted());\n        assertEquals(error, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.leaderEpoch());\n        assertEquals(leaderId.orElse(-1), partitionResponse.leaderId());\n    }\n\n    List<RaftRequest.Outbound> collectVoteRequests(\n        int epoch,\n        int lastEpoch,\n        long lastEpochOffset\n    ) {\n        List<RaftRequest.Outbound> voteRequests = new ArrayList<>();\n        for (RaftRequest.Outbound raftMessage : channel.drainSendQueue()) {\n            if (raftMessage.data() instanceof VoteRequestData) {\n                VoteRequestData request = (VoteRequestData) raftMessage.data();\n                VoteRequestData.PartitionData partitionRequest = unwrap(request);\n\n                assertEquals(epoch, partitionRequest.candidateEpoch());\n                assertEquals(localIdOrThrow(), partitionRequest.candidateId());\n                assertEquals(lastEpoch, partitionRequest.lastOffsetEpoch());\n                assertEquals(lastEpochOffset, partitionRequest.lastOffset());\n                voteRequests.add(raftMessage);\n            }\n        }\n        return voteRequests;\n    }\n\n    void deliverRequest(ApiMessage request) {\n        RaftRequest.Inbound inboundRequest = new RaftRequest.Inbound(\n            channel.newCorrelationId(), request.highestSupportedVersion(), request, time.milliseconds());\n        inboundRequest.completion.whenComplete((response, exception) -> {\n            if (exception != null) {\n                throw new RuntimeException(exception);\n            } else {\n                sentResponses.add(response);\n            }\n        });\n        client.handle(inboundRequest);\n    }\n\n    void deliverResponse(int correlationId, Node source, ApiMessage response) {\n        channel.mockReceive(new RaftResponse.Inbound(correlationId, response, source));\n    }\n\n    RaftRequest.Outbound assertSentBeginQuorumEpochRequest(int epoch, int numBeginEpochRequests) {\n        List<RaftRequest.Outbound> requests = collectBeginEpochRequests(epoch);\n        assertEquals(numBeginEpochRequests, requests.size());\n        return requests.get(0);\n    }\n\n    private List<RaftResponse.Outbound> drainSentResponses(\n        ApiKeys apiKey\n    ) {\n        List<RaftResponse.Outbound> res = new ArrayList<>();\n        Iterator<RaftResponse.Outbound> iterator = sentResponses.iterator();\n        while (iterator.hasNext()) {\n            RaftResponse.Outbound response = iterator.next();\n            if (response.data().apiKey() == apiKey.id) {\n                res.add(response);\n                iterator.remove();\n            }\n        }\n        return res;\n    }\n\n    void assertSentBeginQuorumEpochResponse(\n            Errors responseError\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.BEGIN_QUORUM_EPOCH);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(BeginQuorumEpochResponseData.class, raftMessage.data());\n        BeginQuorumEpochResponseData response = (BeginQuorumEpochResponseData) raftMessage.data();\n        assertEquals(responseError, Errors.forCode(response.errorCode()));\n    }\n\n    void assertSentBeginQuorumEpochResponse(\n        Errors partitionError,\n        int epoch,\n        OptionalInt leaderId\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.BEGIN_QUORUM_EPOCH);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(BeginQuorumEpochResponseData.class, raftMessage.data());\n        BeginQuorumEpochResponseData response = (BeginQuorumEpochResponseData) raftMessage.data();\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n\n        BeginQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        assertEquals(epoch, partitionResponse.leaderEpoch());\n        assertEquals(leaderId.orElse(-1), partitionResponse.leaderId());\n        assertEquals(partitionError, Errors.forCode(partitionResponse.errorCode()));\n    }\n\n    RaftRequest.Outbound assertSentEndQuorumEpochRequest(int epoch, int destinationId) {\n        List<RaftRequest.Outbound> endQuorumRequests = collectEndQuorumRequests(\n            epoch,\n            Collections.singleton(destinationId),\n            Optional.empty()\n        );\n        assertEquals(1, endQuorumRequests.size());\n        return endQuorumRequests.get(0);\n    }\n\n    void assertSentEndQuorumEpochResponse(\n        Errors responseError\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.END_QUORUM_EPOCH);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(EndQuorumEpochResponseData.class, raftMessage.data());\n        EndQuorumEpochResponseData response = (EndQuorumEpochResponseData) raftMessage.data();\n        assertEquals(responseError, Errors.forCode(response.errorCode()));\n    }\n\n    void assertSentEndQuorumEpochResponse(\n        Errors partitionError,\n        int epoch,\n        OptionalInt leaderId\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.END_QUORUM_EPOCH);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(EndQuorumEpochResponseData.class, raftMessage.data());\n        EndQuorumEpochResponseData response = (EndQuorumEpochResponseData) raftMessage.data();\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n\n        EndQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        assertEquals(epoch, partitionResponse.leaderEpoch());\n        assertEquals(leaderId.orElse(-1), partitionResponse.leaderId());\n        assertEquals(partitionError, Errors.forCode(partitionResponse.errorCode()));\n    }\n\n    RaftRequest.Outbound assertSentFetchRequest() {\n        List<RaftRequest.Outbound> sentRequests = channel.drainSentRequests(Optional.of(ApiKeys.FETCH));\n        assertEquals(1, sentRequests.size());\n        return sentRequests.get(0);\n    }\n\n    RaftRequest.Outbound assertSentFetchRequest(\n        int epoch,\n        long fetchOffset,\n        int lastFetchedEpoch\n    ) {\n        List<RaftRequest.Outbound> sentMessages = channel.drainSendQueue();\n        assertEquals(1, sentMessages.size());\n\n        RaftRequest.Outbound raftRequest = sentMessages.get(0);\n        assertFetchRequestData(raftRequest, epoch, fetchOffset, lastFetchedEpoch);\n        return raftRequest;\n    }\n\n    FetchResponseData.PartitionData assertSentFetchPartitionResponse() {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.FETCH);\n        assertEquals(\n            1, sentMessages.size(), \"Found unexpected sent messages \" + sentMessages);\n        RaftResponse.Outbound raftMessage = sentMessages.get(0);\n        assertEquals(ApiKeys.FETCH.id, raftMessage.data().apiKey());\n        FetchResponseData response = (FetchResponseData) raftMessage.data();\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n\n        assertEquals(1, response.responses().size());\n        assertEquals(metadataPartition.topic(), response.responses().get(0).topic());\n        assertEquals(1, response.responses().get(0).partitions().size());\n        return response.responses().get(0).partitions().get(0);\n    }\n\n    void assertSentFetchPartitionResponse(Errors topLevelError) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.FETCH);\n        assertEquals(\n            1, sentMessages.size(), \"Found unexpected sent messages \" + sentMessages);\n        RaftResponse.Outbound raftMessage = sentMessages.get(0);\n        assertEquals(ApiKeys.FETCH.id, raftMessage.data().apiKey());\n        FetchResponseData response = (FetchResponseData) raftMessage.data();\n        assertEquals(topLevelError, Errors.forCode(response.errorCode()));\n    }\n\n\n    MemoryRecords assertSentFetchPartitionResponse(\n        Errors error,\n        int epoch,\n        OptionalInt leaderId\n    ) {\n        FetchResponseData.PartitionData partitionResponse = assertSentFetchPartitionResponse();\n        assertEquals(error, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(leaderId.orElse(-1), partitionResponse.currentLeader().leaderId());\n        assertEquals(-1, partitionResponse.divergingEpoch().endOffset());\n        assertEquals(-1, partitionResponse.divergingEpoch().epoch());\n        assertEquals(-1, partitionResponse.snapshotId().endOffset());\n        assertEquals(-1, partitionResponse.snapshotId().epoch());\n        return (MemoryRecords) partitionResponse.records();\n    }\n\n    MemoryRecords assertSentFetchPartitionResponse(\n        long highWatermark,\n        int leaderEpoch\n    ) {\n        FetchResponseData.PartitionData partitionResponse = assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(leaderEpoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(highWatermark, partitionResponse.highWatermark());\n        assertEquals(-1, partitionResponse.divergingEpoch().endOffset());\n        assertEquals(-1, partitionResponse.divergingEpoch().epoch());\n        assertEquals(-1, partitionResponse.snapshotId().endOffset());\n        assertEquals(-1, partitionResponse.snapshotId().epoch());\n        return (MemoryRecords) partitionResponse.records();\n    }\n\n    RaftRequest.Outbound assertSentFetchSnapshotRequest() {\n        List<RaftRequest.Outbound> sentRequests = channel.drainSentRequests(Optional.of(ApiKeys.FETCH_SNAPSHOT));\n        assertEquals(1, sentRequests.size());\n\n        return sentRequests.get(0);\n    }\n\n    void assertSentFetchSnapshotResponse(Errors responseError) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.FETCH_SNAPSHOT);\n        assertEquals(1, sentMessages.size());\n\n        RaftMessage message = sentMessages.get(0);\n        assertInstanceOf(FetchSnapshotResponseData.class, message.data());\n\n        FetchSnapshotResponseData response = (FetchSnapshotResponseData) message.data();\n        assertEquals(responseError, Errors.forCode(response.errorCode()));\n    }\n\n    Optional<FetchSnapshotResponseData.PartitionSnapshot> assertSentFetchSnapshotResponse(TopicPartition topicPartition) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.FETCH_SNAPSHOT);\n        assertEquals(1, sentMessages.size());\n\n        RaftMessage message = sentMessages.get(0);\n        assertInstanceOf(FetchSnapshotResponseData.class, message.data());\n\n        FetchSnapshotResponseData response = (FetchSnapshotResponseData) message.data();\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n\n        return FetchSnapshotResponse.forTopicPartition(response, topicPartition);\n    }\n\n    List<RaftRequest.Outbound> collectEndQuorumRequests(\n        int epoch,\n        Set<Integer> destinationIdSet,\n        Optional<List<Integer>> preferredSuccessorsOpt\n    ) {\n        List<RaftRequest.Outbound> endQuorumRequests = new ArrayList<>();\n        Set<Integer> collectedDestinationIdSet = new HashSet<>();\n        for (RaftRequest.Outbound raftMessage : channel.drainSendQueue()) {\n            if (raftMessage.data() instanceof EndQuorumEpochRequestData) {\n                EndQuorumEpochRequestData request = (EndQuorumEpochRequestData) raftMessage.data();\n\n                EndQuorumEpochRequestData.PartitionData partitionRequest =\n                    request.topics().get(0).partitions().get(0);\n\n                assertEquals(epoch, partitionRequest.leaderEpoch());\n                assertEquals(localIdOrThrow(), partitionRequest.leaderId());\n                preferredSuccessorsOpt.ifPresent(preferredSuccessors -> {\n                    assertEquals(preferredSuccessors, partitionRequest.preferredSuccessors());\n                });\n\n                collectedDestinationIdSet.add(raftMessage.destination().id());\n                endQuorumRequests.add(raftMessage);\n            }\n        }\n        assertEquals(destinationIdSet, collectedDestinationIdSet);\n        return endQuorumRequests;\n    }\n\n    void discoverLeaderAsObserver(\n        int leaderId,\n        int epoch\n    ) throws Exception {\n        pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = assertSentFetchRequest();\n        int destinationId = fetchRequest.destination().id();\n        assertTrue(\n            voters.contains(destinationId) || bootstrapIds.contains(destinationId),\n            String.format(\"id %d is not in sets %s or %s\", destinationId, voters, bootstrapIds)\n        );\n        assertFetchRequestData(fetchRequest, 0, 0L, 0);\n\n        deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            fetchResponse(epoch, leaderId, MemoryRecords.EMPTY, 0L, Errors.NONE)\n        );\n        client.poll();\n        assertElectedLeader(epoch, leaderId);\n    }\n\n    private List<RaftRequest.Outbound> collectBeginEpochRequests(int epoch) {\n        List<RaftRequest.Outbound> requests = new ArrayList<>();\n        for (RaftRequest.Outbound raftRequest : channel.drainSentRequests(Optional.of(ApiKeys.BEGIN_QUORUM_EPOCH))) {\n            assertInstanceOf(BeginQuorumEpochRequestData.class, raftRequest.data());\n            BeginQuorumEpochRequestData request = (BeginQuorumEpochRequestData) raftRequest.data();\n\n            BeginQuorumEpochRequestData.PartitionData partitionRequest =\n                request.topics().get(0).partitions().get(0);\n\n            assertEquals(epoch, partitionRequest.leaderEpoch());\n            assertEquals(localIdOrThrow(), partitionRequest.leaderId());\n            requests.add(raftRequest);\n        }\n        return requests;\n    }\n\n    public static InetSocketAddress mockAddress(int id) {\n        return new InetSocketAddress(\"localhost\", 9990 + id);\n    }\n\n    EndQuorumEpochResponseData endEpochResponse(\n        int epoch,\n        OptionalInt leaderId\n    ) {\n        return EndQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            metadataPartition,\n            Errors.NONE,\n            epoch,\n            leaderId.orElse(-1)\n        );\n    }\n\n    EndQuorumEpochRequestData endEpochRequest(\n        int epoch,\n        int leaderId,\n        List<Integer> preferredSuccessors\n    ) {\n        return EndQuorumEpochRequest.singletonRequest(\n            metadataPartition,\n            epoch,\n            leaderId,\n            preferredSuccessors\n        );\n    }\n\n    EndQuorumEpochRequestData endEpochRequest(\n        String clusterId,\n        int epoch,\n        int leaderId,\n        List<Integer> preferredSuccessors\n    ) {\n        return EndQuorumEpochRequest.singletonRequest(\n            metadataPartition,\n            clusterId,\n            epoch,\n            leaderId,\n            preferredSuccessors\n        );\n    }\n\n    BeginQuorumEpochRequestData beginEpochRequest(String clusterId, int epoch, int leaderId) {\n        return BeginQuorumEpochRequest.singletonRequest(\n            metadataPartition,\n            clusterId,\n            epoch,\n            leaderId\n        );\n    }\n\n    BeginQuorumEpochRequestData beginEpochRequest(int epoch, int leaderId) {\n        return BeginQuorumEpochRequest.singletonRequest(\n            metadataPartition,\n            epoch,\n            leaderId\n        );\n    }\n\n    private BeginQuorumEpochResponseData beginEpochResponse(int epoch, int leaderId) {\n        return BeginQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            metadataPartition,\n            Errors.NONE,\n            epoch,\n            leaderId\n        );\n    }\n\n    VoteRequestData voteRequest(int epoch, int candidateId, int lastEpoch, long lastEpochOffset) {\n        return VoteRequest.singletonRequest(\n            metadataPartition,\n            clusterId.toString(),\n            epoch,\n            candidateId,\n            lastEpoch,\n            lastEpochOffset\n        );\n    }\n\n    VoteRequestData voteRequest(\n        String clusterId,\n        int epoch,\n        int candidateId,\n        int lastEpoch,\n        long lastEpochOffset\n    ) {\n        return VoteRequest.singletonRequest(\n                metadataPartition,\n                clusterId,\n                epoch,\n                candidateId,\n                lastEpoch,\n                lastEpochOffset\n        );\n    }\n\n    VoteResponseData voteResponse(boolean voteGranted, Optional<Integer> leaderId, int epoch) {\n        return VoteResponse.singletonResponse(\n            Errors.NONE,\n            metadataPartition,\n            Errors.NONE,\n            epoch,\n            leaderId.orElse(-1),\n            voteGranted\n        );\n    }\n\n    private VoteRequestData.PartitionData unwrap(VoteRequestData voteRequest) {\n        assertTrue(RaftUtil.hasValidTopicPartition(voteRequest, metadataPartition));\n        return voteRequest.topics().get(0).partitions().get(0);\n    }\n\n    static void assertMatchingRecords(\n        String[] expected,\n        Records actual\n    ) {\n        List<Record> recordList = Utils.toList(actual.records());\n        assertEquals(expected.length, recordList.size());\n        for (int i = 0; i < expected.length; i++) {\n            Record record = recordList.get(i);\n            assertEquals(expected[i], Utils.utf8(record.value()),\n                \"Record at offset \" + record.offset() + \" does not match expected\");\n        }\n    }\n\n    static void verifyLeaderChangeMessage(\n        int leaderId,\n        List<Integer> voters,\n        List<Integer> grantingVoters,\n        ByteBuffer recordKey,\n        ByteBuffer recordValue\n    ) {\n        assertEquals(ControlRecordType.LEADER_CHANGE, ControlRecordType.parse(recordKey));\n\n        LeaderChangeMessage leaderChangeMessage = ControlRecordUtils.deserializeLeaderChangeMessage(recordValue);\n        assertEquals(leaderId, leaderChangeMessage.leaderId());\n        assertEquals(voters.stream().map(voterId -> new Voter().setVoterId(voterId)).collect(Collectors.toList()),\n            leaderChangeMessage.voters());\n        assertEquals(grantingVoters.stream().map(voterId -> new Voter().setVoterId(voterId)).collect(Collectors.toSet()),\n            new HashSet<>(leaderChangeMessage.grantingVoters()));\n    }\n\n    void assertFetchRequestData(\n        RaftRequest.Outbound message,\n        int epoch,\n        long fetchOffset,\n        int lastFetchedEpoch\n    ) {\n        assertInstanceOf(\n            FetchRequestData.class,\n            message.data(),\n            \"unexpected request type \" + message.data());\n        FetchRequestData request = (FetchRequestData) message.data();\n        assertEquals(KafkaRaftClient.MAX_FETCH_SIZE_BYTES, request.maxBytes());\n        assertEquals(fetchMaxWaitMs, request.maxWaitMs());\n\n        assertEquals(1, request.topics().size());\n        assertEquals(metadataPartition.topic(), request.topics().get(0).topic());\n        assertEquals(1, request.topics().get(0).partitions().size());\n\n        FetchRequestData.FetchPartition fetchPartition = request.topics().get(0).partitions().get(0);\n        assertEquals(epoch, fetchPartition.currentLeaderEpoch());\n        assertEquals(fetchOffset, fetchPartition.fetchOffset());\n        assertEquals(lastFetchedEpoch, fetchPartition.lastFetchedEpoch());\n        assertEquals(localId.orElse(-1), request.replicaState().replicaId());\n\n        // Assert that voters have flushed up to the fetch offset\n        if (localId.isPresent() && voters.contains(localId.getAsInt())) {\n            assertEquals(\n                log.firstUnflushedOffset(),\n                fetchOffset,\n                String.format(\n                    \"expected voters have the fetch offset (%s) be the same as the unflushed offset (%s)\",\n                    log.firstUnflushedOffset(),\n                    fetchOffset\n                )\n            );\n        } else {\n            assertFalse(log.flushedSinceLastChecked(), \"KRaft client should not explicitly flush when it is an observer\");\n        }\n    }\n\n    FetchRequestData fetchRequest(\n        int epoch,\n        int replicaId,\n        long fetchOffset,\n        int lastFetchedEpoch,\n        int maxWaitTimeMs\n    ) {\n        return fetchRequest(\n            epoch,\n            clusterId.toString(),\n            replicaId,\n            fetchOffset,\n            lastFetchedEpoch,\n            maxWaitTimeMs\n        );\n    }\n\n    FetchRequestData fetchRequest(\n        int epoch,\n        String clusterId,\n        int replicaId,\n        long fetchOffset,\n        int lastFetchedEpoch,\n        int maxWaitTimeMs\n    ) {\n        FetchRequestData request = RaftUtil.singletonFetchRequest(metadataPartition, metadataTopicId, fetchPartition -> {\n            fetchPartition\n                .setCurrentLeaderEpoch(epoch)\n                .setLastFetchedEpoch(lastFetchedEpoch)\n                .setFetchOffset(fetchOffset);\n        });\n        return request\n            .setMaxWaitMs(maxWaitTimeMs)\n            .setClusterId(clusterId)\n            .setReplicaState(new FetchRequestData.ReplicaState().setReplicaId(replicaId));\n    }\n\n    FetchResponseData fetchResponse(\n        int epoch,\n        int leaderId,\n        Records records,\n        long highWatermark,\n        Errors error\n    ) {\n        return RaftUtil.singletonFetchResponse(metadataPartition, metadataTopicId, Errors.NONE, partitionData -> {\n            partitionData\n                .setRecords(records)\n                .setErrorCode(error.code())\n                .setHighWatermark(highWatermark);\n\n            partitionData.currentLeader()\n                .setLeaderEpoch(epoch)\n                .setLeaderId(leaderId);\n        });\n    }\n\n    FetchResponseData divergingFetchResponse(\n        int epoch,\n        int leaderId,\n        long divergingEpochEndOffset,\n        int divergingEpoch,\n        long highWatermark\n    ) {\n        return RaftUtil.singletonFetchResponse(metadataPartition, metadataTopicId, Errors.NONE, partitionData -> {\n            partitionData.setHighWatermark(highWatermark);\n\n            partitionData.currentLeader()\n                .setLeaderEpoch(epoch)\n                .setLeaderId(leaderId);\n\n            partitionData.divergingEpoch()\n                .setEpoch(divergingEpoch)\n                .setEndOffset(divergingEpochEndOffset);\n        });\n    }\n\n    public void advanceLocalLeaderHighWatermarkToLogEndOffset() throws InterruptedException {\n        assertEquals(localId, currentLeader());\n        long localLogEndOffset = log.endOffset().offset;\n        Set<Integer> followers = voterKeys();\n\n        // Send a request from every follower\n        for (int follower : followers) {\n            deliverRequest(\n                fetchRequest(currentEpoch(), follower, localLogEndOffset, currentEpoch(), 0)\n            );\n            pollUntilResponse();\n            assertSentFetchPartitionResponse(Errors.NONE, currentEpoch(), localId);\n        }\n\n        pollUntil(() -> OptionalLong.of(localLogEndOffset).equals(client.highWatermark()));\n    }\n\n    private Set<Integer> voterKeys() {\n        Set<Integer> followers = voters.stream().filter(voter -> voter != localId.getAsInt()).collect(Collectors.toSet());\n        return followers;\n    }\n\n    static class MockListener implements RaftClient.Listener<String> {\n        private final List<Batch<String>> commits = new ArrayList<>();\n        private final List<BatchReader<String>> savedBatches = new ArrayList<>();\n        private final Map<Integer, Long> claimedEpochStartOffsets = new HashMap<>();\n        private LeaderAndEpoch currentLeaderAndEpoch = new LeaderAndEpoch(OptionalInt.empty(), 0);\n        private final OptionalInt localId;\n        private Optional<SnapshotReader<String>> snapshot = Optional.empty();\n        private boolean readCommit = true;\n\n        MockListener(OptionalInt localId) {\n            this.localId = localId;\n        }\n\n        int numCommittedBatches() {\n            return commits.size();\n        }\n\n        Long claimedEpochStartOffset(int epoch) {\n            return claimedEpochStartOffsets.get(epoch);\n        }\n\n        LeaderAndEpoch currentLeaderAndEpoch() {\n            return currentLeaderAndEpoch;\n        }\n\n        List<Batch<String>> committedBatches() {\n            return commits;\n        }\n\n        Batch<String> lastCommit() {\n            if (commits.isEmpty()) {\n                return null;\n            } else {\n                return commits.get(commits.size() - 1);\n            }\n        }\n\n        OptionalLong lastCommitOffset() {\n            if (commits.isEmpty()) {\n                return OptionalLong.empty();\n            } else {\n                return OptionalLong.of(commits.get(commits.size() - 1).lastOffset());\n            }\n        }\n\n        OptionalInt currentClaimedEpoch() {\n            if (localId.isPresent() && currentLeaderAndEpoch.isLeader(localId.getAsInt())) {\n                return OptionalInt.of(currentLeaderAndEpoch.epoch());\n            } else {\n                return OptionalInt.empty();\n            }\n        }\n\n        List<String> commitWithLastOffset(long lastOffset) {\n            return commits.stream()\n                .filter(batch -> batch.lastOffset() == lastOffset)\n                .findFirst()\n                .map(batch -> batch.records())\n                .orElse(null);\n        }\n\n        Optional<SnapshotReader<String>> drainHandledSnapshot() {\n            Optional<SnapshotReader<String>> temp = snapshot;\n            snapshot = Optional.empty();\n            return temp;\n        }\n\n        void updateReadCommit(boolean readCommit) {\n            this.readCommit = readCommit;\n\n            if (readCommit) {\n                for (BatchReader<String> batch : savedBatches) {\n                    readBatch(batch);\n                }\n\n                savedBatches.clear();\n            }\n        }\n\n        void readBatch(BatchReader<String> reader) {\n            try {\n                while (reader.hasNext()) {\n                    long nextOffset = lastCommitOffset().isPresent() ?\n                        lastCommitOffset().getAsLong() + 1 : 0L;\n                    Batch<String> batch = reader.next();\n                    // We expect monotonic offsets, but not necessarily sequential\n                    // offsets since control records will be filtered.\n                    assertTrue(batch.baseOffset() >= nextOffset,\n                        \"Received non-monotonic commit \" + batch +\n                            \". We expected an offset at least as large as \" + nextOffset);\n                    commits.add(batch);\n                }\n            } finally {\n                reader.close();\n            }\n        }\n\n        @Override\n        public void handleLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            // We record the current committed offset as the claimed epoch's start\n            // offset. This is useful to verify that the `handleLeaderChange` callback\n            // was not received early on the leader.\n            assertTrue(\n                leaderAndEpoch.epoch() >= currentLeaderAndEpoch.epoch(),\n                String.format(\"new epoch (%d) not >= than old epoch (%d)\", leaderAndEpoch.epoch(), currentLeaderAndEpoch.epoch())\n            );\n            assertNotEquals(currentLeaderAndEpoch, leaderAndEpoch);\n            this.currentLeaderAndEpoch = leaderAndEpoch;\n\n            currentClaimedEpoch().ifPresent(claimedEpoch -> {\n                long claimedEpochStartOffset = lastCommitOffset().isPresent() ?\n                    lastCommitOffset().getAsLong() : 0L;\n                this.claimedEpochStartOffsets.put(leaderAndEpoch.epoch(), claimedEpochStartOffset);\n            });\n        }\n\n        @Override\n        public void handleCommit(BatchReader<String> reader) {\n            if (readCommit) {\n                readBatch(reader);\n            } else {\n                savedBatches.add(reader);\n            }\n        }\n\n        @Override\n        public void handleLoadSnapshot(SnapshotReader<String> reader) {\n            snapshot.ifPresent(snapshot -> assertDoesNotThrow(snapshot::close));\n            commits.clear();\n            savedBatches.clear();\n            snapshot = Optional.of(reader);\n        }\n    }\n}",
                "methodCount": 98
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 14,
                "candidates": [
                    {
                        "lineStart": 201,
                        "lineEnd": 218,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method appendToLog to class QuorumStateStore",
                        "description": "Move method appendToLog to org.apache.kafka.raft.QuorumStateStore\nRationale: The method appendToLog seems to handle log-related operations that involve appending records to a log and managing the state of the log, which could be closely tied to the quorum state management. Since QuorumStateStore is responsible for maintaining the quorum state information and handling persistence of election states in a coordinated, atomic manner, it would make sense to contain log management within this class. Log appending and flushing are crucial to ensuring the consistency and durability of the quorum state, aligning with the responsibilities of QuorumStateStore.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 623,
                        "lineEnd": 625,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method deliverResponse to class MockNetworkChannel",
                        "description": "Move method deliverResponse to org.apache.kafka.raft.MockNetworkChannel\nRationale: The deliverResponse() method relies on the mockReceive() method of the MockNetworkChannel class to handle inbound responses. This indicates a close relationship with the functionality of MockNetworkChannel, as it is responsible for managing the receipt of Raft responses. Moving this method to MockNetworkChannel centralizes the handling of outbound requests and their corresponding inbound responses in one class, enhancing cohesiveness and encapsulation of the network communication logic.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 488,
                        "lineEnd": 493,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method assertUnknownLeader to class QuorumStateStore",
                        "description": "Move method assertUnknownLeader to org.apache.kafka.raft.QuorumStateStore\nRationale: The method assertUnknownLeader() heavily relies on the readElectionState() method of QuorumStateStore, which indicates its functionality is closely tied to the operations and state management provided by QuorumStateStore. Additionally, the validation rule enforced by assertUnknownLeader() is directly related to the election state, which is part of QuorumStateStore's domain responsibility. Moving the method to QuorumStateStore would enhance coherence and encapsulation, ensuring that all logic related to election state validation is centralized within the QuorumStateStore class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 455,
                        "lineEnd": 460,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method pollUntil to class MockMessageQueue",
                        "description": "Move method pollUntil to org.apache.kafka.raft.MockMessageQueue\nRationale: The method `pollUntil(TestCondition condition)` closely interacts with the `poll()` method, which is part of `MockMessageQueue`. By moving this method to `MockMessageQueue`, it logically groups related functionalities together, which enhances cohesion and makes the code easier to maintain and understand. Since `MockMessageQueue` is designed to handle message polling and related operations, adding a method that involves polling operations aligns with its purpose.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 178,
                        "lineEnd": 184,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method withVotedCandidate to class QuorumStateStore",
                        "description": "Move method withVotedCandidate to org.apache.kafka.raft.QuorumStateStore\nRationale: The method withVotedCandidate relies heavily on the quorumStateStore.writeElectionState method, which is part of the QuorumStateStore interface. This indicates that the functionality of withVotedCandidate is closely tied to the operations performed by QuorumStateStore. Moving the method to QuorumStateStore ensures that the associated states and operations are encapsulated within the same class, enhancing cohesion and maintainability. It would not make sense to move the method to MemoryPool, as it is unrelated to memory management and buffer allocation.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1079,
                        "lineEnd": 1094,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method fetchRequest to class MockLog",
                        "description": "Move method fetchRequest to org.apache.kafka.raft.MockLog\nRationale: The FetchRequestData fetchRequest(...) method appears to relate to fetching data, which is closely related to the behavior of logs, indicating that MockLog is the most appropriate class. MockLog is responsible for managing log entries, batches, and offsets, which align with the parameters in the method such as fetchOffset, and lastFetchedEpoch.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 170,
                        "lineEnd": 176,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method withUnknownLeader to class QuorumStateStore",
                        "description": "Move method withUnknownLeader to org.apache.kafka.raft.QuorumStateStore\nRationale: The method 'withUnknownLeader(int epoch)' involves writing an election state using 'quorumStateStore.writeElectionState()', clearly interacting with the 'QuorumStateStore' interface. Thus, the method is central to the operations of 'QuorumStateStore', making it the most appropriate class to house this method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 470,
                        "lineEnd": 479,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method assertVotedCandidate to class QuorumStateStore",
                        "description": "Move method assertVotedCandidate to org.apache.kafka.raft.QuorumStateStore\nRationale: The method assertVotedCandidate() is directly involved with reading the election state from the quorumStateStore. By moving this method to the QuorumStateStore class, it will keep related logic together, promoting cohesion. Additionally, this method validates the election state which aligns with the responsibilities defined for the QuorumStateStore class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 965,
                        "lineEnd": 974,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method voteRequest to class MockListener",
                        "description": "Move method voteRequest to org.apache.kafka.raft.RaftClientTestContext.MockListener\nRationale: The `voteRequest` method seems to be highly relevant to handling leader election activities, which is a responsibility of `MockListener`. This class includes functions and states such as `currentLeaderAndEpoch` and `handleLeaderChange` that suggest it deals directly with leadership roles and elections. Moving `voteRequest` to `MockListener` aligns with its role in managing leadership votes and similar events.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 407,
                        "lineEnd": 412,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method becomeLeader to class MockListener",
                        "description": "Move method becomeLeader to org.apache.kafka.raft.RaftClientTestContext.MockListener\nRationale: The method `becomeLeader()` is closely related to the leadership and election mechanism, which is the core functionality of the `MockListener` class. The `MockListener` class already contains methods like `handleLeaderChange` and various epoch-related functionalities, making it a more appropriate place to move `becomeLeader()`. The method logically fits within the context of `MockListener` as it interacts with epochs and voting, elements already managed by this class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 855,
                        "lineEnd": 875,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method discoverLeaderAsObserver to class MockNetworkChannel",
                        "description": "Move method discoverLeaderAsObserver to org.apache.kafka.raft.MockNetworkChannel\nRationale: The method discoverLeaderAsObserver involves significant interaction with network communication, such as sending and polling requests, delivering responses, and dealing with fetch requests. MockNetworkChannel is designed to handle such network interactions, making it the most suitable location for the method. The method does not seem to align closely with the responsibilities of MockListener or MockLog, which focus more on handling batches, commits, and log management rather than network request coordination.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 360,
                        "lineEnd": 366,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method buildBatch to class MockLog",
                        "description": "Move method buildBatch to org.apache.kafka.raft.MockLog\nRationale: The method buildBatch() constructs MemoryRecords based on offsets and epochs, and the majority of the methods within MockLog involve operations on log entries, batches, and epochs. Given MockLog's extensive use of similar methods and its responsibility for managing batches of log entries, moving buildBatch() to MockLog ensures a logical grouping of related functionalities. Moreover, MockLog already handles similar responsibility closely related to this method's operations, indicating that buildBatch() naturally fits within this class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 495,
                        "lineEnd": 501,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method assertResignedLeader to class QuorumStateStore",
                        "description": "Move method assertResignedLeader to org.apache.kafka.raft.QuorumStateStore\nRationale: The method assertResignedLeader is closely related to checking the election state and confirming properties of the quorum state. Since QuorumStateStore is responsible for maintaining the save and retrieval of quorum state information and includes methods for reading and writing election states, it aligns well with the responsibilities of QuorumStateStore. Placing the method here ensures that all functionality related to election state validation remains centralized, making it easier to maintain and extend.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1116,
                        "lineEnd": 1133,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method fetchResponse to class MockLog",
                        "description": "Move method fetchResponse to org.apache.kafka.raft.MockLog\nRationale: The method fetchResponse() is more closely related to the operations and responsibilities of a log, specifically a replicated log like MockLog. It deals with handling records, leader information, and epoch-related details. This aligns well with functionalities that correlate to a log that handles high watermark updates, records, and leader epoch data.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "buildBatch",
                            "method_signature": "MemoryRecords buildBatch(long timestamp, long baseOffset, int epoch, List<String> records)",
                            "target_class": "BatchBuilder",
                            "rationale": "The buildBatch method is generating a batch using the BatchBuilder class. This operation logically belongs in the BatchBuilder class because it directly involves creating a batch object."
                        },
                        {
                            "method_name": "beginEpochRequest",
                            "method_signature": "BeginQuorumEpochRequestData beginEpochRequest(String clusterId, int epoch, int leaderId)",
                            "target_class": "BeginQuorumEpochRequest",
                            "rationale": "The beginEpochRequest methods are directly creating and configuring BeginQuorumEpochRequestData instances. Since this involves constructing request objects related to quorum epochs, it's more appropriate to place them in the BeginQuorumEpochRequest class."
                        },
                        {
                            "method_name": "endEpochRequest",
                            "method_signature": "EndQuorumEpochRequestData endEpochRequest(int epoch, int leaderId, List<Integer> preferredSuccessors)",
                            "target_class": "EndQuorumEpochRequest",
                            "rationale": "The endEpochRequest methods are constructing EndQuorumEpochRequestData instances, making it more appropriate to have them inside the EndQuorumEpochRequest class."
                        },
                        {
                            "method_name": "fetchRequest",
                            "method_signature": "FetchRequestData fetchRequest(int epoch, String clusterId, int replicaId, long fetchOffset, int lastFetchedEpoch, int maxWaitTimeMs)",
                            "target_class": "FetchRequest",
                            "rationale": "The fetchRequest methods are creating fetch request data, which logically belongs in the FetchRequest class as they are concerned with building fetch request objects."
                        },
                        {
                            "method_name": "fetchResponse",
                            "method_signature": "FetchResponseData fetchResponse(int epoch, int leaderId, Records records, long highWatermark, Errors error)",
                            "target_class": "FetchResponse",
                            "rationale": "The fetchResponse methods are constructing FetchResponseData instances, which is specific to the FetchResponse class as it pertains to building response data."
                        },
                        {
                            "method_name": "voteRequest",
                            "method_signature": "VoteRequestData voteRequest(int epoch, int candidateId, int lastEpoch, long lastEpochOffset)",
                            "target_class": "VoteRequest",
                            "rationale": "The voteRequest methods are constructing VoteRequestData instances, making it more appropriate to have them inside the VoteRequest class."
                        },
                        {
                            "method_name": "voteResponse",
                            "method_signature": "VoteResponseData voteResponse(boolean voteGranted, Optional<Integer> leaderId, int epoch)",
                            "target_class": "VoteResponse",
                            "rationale": "The voteResponse methods are creating VoteResponseData instances, and thus, they should be part of the VoteResponse class."
                        },
                        {
                            "method_name": "unwrap",
                            "method_signature": "VoteRequestData.PartitionData unwrap(VoteRequestData voteRequest)",
                            "target_class": "VoteRequest",
                            "rationale": "The unwrap method accesses internal attributes of VoteRequestData in a way that suggests it logically belongs in the VoteRequest class."
                        },
                        {
                            "method_name": "initializeAsLeader",
                            "method_signature": "static RaftClientTestContext initializeAsLeader(int localId, Set<Integer> voters, int epoch) throws Exception",
                            "target_class": "RaftClient",
                            "rationale": "The initializeAsLeader method involves initializing the Raft client context and entails operations which would logically fit within the RaftClient class, as they pertain to client initialization."
                        }
                    ],
                    "llm_response_time": 10168
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "buildBatch",
                            "method_signature": "MemoryRecords buildBatch(long timestamp, long baseOffset, int epoch, List<String> records)",
                            "target_class": "RecordBuilder",
                            "rationale": "The method 'buildBatch' deals specifically with the creation of memory records from a list of strings, which is a distinct responsibility that is independent of the main 'RaftClientTestContext' class. Moving this method to a new class 'RecordBuilder' will adhere to the Single Responsibility Principle and help make the code more modular and maintainable."
                        },
                        {
                            "method_name": "assertMatchingRecords",
                            "method_signature": "static void assertMatchingRecords(String[] expected, Records actual)",
                            "target_class": "RecordAssertions",
                            "rationale": "The method 'assertMatchingRecords' is a utility function for verifying that records match expected values. This type of functionality is better served within a dedicated 'RecordAssertions' class, which will centralize all record-related assertions and make the code easier to test and maintain."
                        },
                        {
                            "method_name": "verifyLeaderChangeMessage",
                            "method_signature": "static void verifyLeaderChangeMessage(int leaderId, List<Integer> voters, List<Integer> grantingVoters, ByteBuffer recordKey, ByteBuffer recordValue)",
                            "target_class": "LeaderChangeVerifier",
                            "rationale": "The method 'verifyLeaderChangeMessage' checks the integrity of leader change messages. This verification logic is separate from the core responsibilities of 'RaftClientTestContext' and should be moved to a new 'LeaderChangeVerifier' class, focusing specifically on leader message verification."
                        }
                    ],
                    "llm_response_time": 5571
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "mockAddress",
                            "method_signature": "public static InetSocketAddress mockAddress(int id)",
                            "target_class": "TestUtils",
                            "rationale": "The method `mockAddress` generates a mock address and is a utility method. It doesn't depend on the state of `RaftClientTestContext` and can be reused by other test contexts and classes."
                        },
                        {
                            "method_name": "buildBatch",
                            "method_signature": "public static MemoryRecords buildBatch(long timestamp, long baseOffset, int epoch, List<String> records)",
                            "target_class": "TestUtils",
                            "rationale": "The method `buildBatch` is a utility for building batches and does not rely specifically on the state of `RaftClientTestContext`. It can be beneficial to move it to a utility class for reuse."
                        },
                        {
                            "method_name": "drainSentResponses",
                            "method_signature": "private List<RaftResponse.Outbound> drainSentResponses(ApiKeys apiKey)",
                            "target_class": "TestUtils",
                            "rationale": "The `drainSentResponses` method is a reusable operation that can be applied more generally than just within `RaftClientTestContext`. Moving it to a utilities class improves reusability."
                        },
                        {
                            "method_name": "assertMatchingRecords",
                            "method_signature": "public static void assertMatchingRecords(String[] expected, Records actual)",
                            "target_class": "TestUtils",
                            "rationale": "The `assertMatchingRecords` method is a utility for asserting records, making it a general-purpose method better suited to a utility class."
                        },
                        {
                            "method_name": "verifyLeaderChangeMessage",
                            "method_signature": "public static void verifyLeaderChangeMessage(int leaderId, List<Integer> voters, List<Integer> grantingVoters, ByteBuffer recordKey, ByteBuffer recordValue)",
                            "target_class": "TestUtils",
                            "rationale": "The `verifyLeaderChangeMessage` method is used to verify leader change messages and is general enough for broader use beyond just `RaftClientTestContext`. Therefore, it's more appropriate to place it in a utility class."
                        }
                    ],
                    "llm_response_time": 6281
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "withElectedLeader",
                            "method_signature": " withElectedLeader(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withUnknownLeader",
                            "method_signature": " withUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withVotedCandidate",
                            "method_signature": " withVotedCandidate(int epoch, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendToLog",
                            "method_signature": "public appendToLog(int epoch, List<String> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withEmptySnapshot",
                            "method_signature": " withEmptySnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteBeforeSnapshot",
                            "method_signature": " deleteBeforeSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBatch",
                            "method_signature": " buildBatch(\n        long baseOffset,\n        int epoch,\n        List<String> records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "becomeLeader",
                            "method_signature": "public becomeLeader()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "currentLeaderAndEpoch",
                            "method_signature": " currentLeaderAndEpoch()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "expectAndGrantVotes",
                            "method_signature": " expectAndGrantVotes(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "expectBeginEpoch",
                            "method_signature": "private expectBeginEpoch(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntil",
                            "method_signature": "public pollUntil(TestCondition condition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertVotedCandidate",
                            "method_signature": " assertVotedCandidate(int epoch, int candidateId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertElectedLeader",
                            "method_signature": "public assertElectedLeader(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertUnknownLeader",
                            "method_signature": " assertUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertResignedLeader",
                            "method_signature": " assertResignedLeader(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentDescribeQuorumResponse",
                            "method_signature": " assertSentDescribeQuorumResponse(\n        int leaderId,\n        int leaderEpoch,\n        long highWatermark,\n        List<ReplicaState> voterStates,\n        List<ReplicaState> observerStates\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "collectVoteRequests",
                            "method_signature": " collectVoteRequests(\n        int epoch,\n        int lastEpoch,\n        long lastEpochOffset\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deliverRequest",
                            "method_signature": " deliverRequest(ApiMessage request)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deliverResponse",
                            "method_signature": " deliverResponse(int correlationId, Node source, ApiMessage response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "drainSentResponses",
                            "method_signature": "private drainSentResponses(\n        ApiKeys apiKey\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchRequest",
                            "method_signature": " assertSentFetchRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchRequest",
                            "method_signature": " assertSentFetchRequest(\n        int epoch,\n        long fetchOffset,\n        int lastFetchedEpoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchPartitionResponse",
                            "method_signature": " assertSentFetchPartitionResponse()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchPartitionResponse",
                            "method_signature": " assertSentFetchPartitionResponse(Errors topLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchSnapshotRequest",
                            "method_signature": " assertSentFetchSnapshotRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "collectEndQuorumRequests",
                            "method_signature": " collectEndQuorumRequests(\n        int epoch,\n        Set<Integer> destinationIdSet,\n        Optional<List<Integer>> preferredSuccessorsOpt\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "discoverLeaderAsObserver",
                            "method_signature": " discoverLeaderAsObserver(\n        int leaderId,\n        int epoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "collectBeginEpochRequests",
                            "method_signature": "private collectBeginEpochRequests(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voteRequest",
                            "method_signature": " voteRequest(int epoch, int candidateId, int lastEpoch, long lastEpochOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertMatchingRecords",
                            "method_signature": "static assertMatchingRecords(\n        String[] expected,\n        Records actual\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertFetchRequestData",
                            "method_signature": " assertFetchRequestData(\n        RaftRequest.Outbound message,\n        int epoch,\n        long fetchOffset,\n        int lastFetchedEpoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchRequest",
                            "method_signature": " fetchRequest(\n        int epoch,\n        int replicaId,\n        long fetchOffset,\n        int lastFetchedEpoch,\n        int maxWaitTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchResponse",
                            "method_signature": " fetchResponse(\n        int epoch,\n        int leaderId,\n        Records records,\n        long highWatermark,\n        Errors error\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "advanceLocalLeaderHighWatermarkToLogEndOffset",
                            "method_signature": "public advanceLocalLeaderHighWatermarkToLogEndOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "currentClaimedEpoch",
                            "method_signature": " currentClaimedEpoch()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitWithLastOffset",
                            "method_signature": " commitWithLastOffset(long lastOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "readBatch",
                            "method_signature": " readBatch(BatchReader<String> reader)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "appendToLog",
                            "method_signature": "public appendToLog(int epoch, List<String> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitWithLastOffset",
                            "method_signature": " commitWithLastOffset(long lastOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deliverResponse",
                            "method_signature": " deliverResponse(int correlationId, Node source, ApiMessage response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertUnknownLeader",
                            "method_signature": " assertUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntil",
                            "method_signature": "public pollUntil(TestCondition condition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withVotedCandidate",
                            "method_signature": " withVotedCandidate(int epoch, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchRequest",
                            "method_signature": " fetchRequest(\n        int epoch,\n        int replicaId,\n        long fetchOffset,\n        int lastFetchedEpoch,\n        int maxWaitTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withUnknownLeader",
                            "method_signature": " withUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertVotedCandidate",
                            "method_signature": " assertVotedCandidate(int epoch, int candidateId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voteRequest",
                            "method_signature": " voteRequest(int epoch, int candidateId, int lastEpoch, long lastEpochOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "becomeLeader",
                            "method_signature": "public becomeLeader()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "discoverLeaderAsObserver",
                            "method_signature": " discoverLeaderAsObserver(\n        int leaderId,\n        int epoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBatch",
                            "method_signature": " buildBatch(\n        long baseOffset,\n        int epoch,\n        List<String> records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertResignedLeader",
                            "method_signature": " assertResignedLeader(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchResponse",
                            "method_signature": " fetchResponse(\n        int epoch,\n        int leaderId,\n        Records records,\n        long highWatermark,\n        Errors error\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "public appendToLog(int epoch, List<String> records)": {
                        "first": {
                            "method_name": "appendToLog",
                            "method_signature": "public appendToLog(int epoch, List<String> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.20037188589374702
                    },
                    " commitWithLastOffset(long lastOffset)": {
                        "first": {
                            "method_name": "commitWithLastOffset",
                            "method_signature": " commitWithLastOffset(long lastOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.2666168079088861
                    },
                    " deliverResponse(int correlationId, Node source, ApiMessage response)": {
                        "first": {
                            "method_name": "deliverResponse",
                            "method_signature": " deliverResponse(int correlationId, Node source, ApiMessage response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.2734065239981689
                    },
                    " assertUnknownLeader(int epoch)": {
                        "first": {
                            "method_name": "assertUnknownLeader",
                            "method_signature": " assertUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3645671106629942
                    },
                    "public pollUntil(TestCondition condition)": {
                        "first": {
                            "method_name": "pollUntil",
                            "method_signature": "public pollUntil(TestCondition condition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3692550031861421
                    },
                    " withVotedCandidate(int epoch, ReplicaKey votedKey)": {
                        "first": {
                            "method_name": "withVotedCandidate",
                            "method_signature": " withVotedCandidate(int epoch, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3781701779397338
                    },
                    " fetchRequest(\n        int epoch,\n        int replicaId,\n        long fetchOffset,\n        int lastFetchedEpoch,\n        int maxWaitTimeMs\n    )": {
                        "first": {
                            "method_name": "fetchRequest",
                            "method_signature": " fetchRequest(\n        int epoch,\n        int replicaId,\n        long fetchOffset,\n        int lastFetchedEpoch,\n        int maxWaitTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.38471215040010603
                    },
                    " withUnknownLeader(int epoch)": {
                        "first": {
                            "method_name": "withUnknownLeader",
                            "method_signature": " withUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3867807490686121
                    },
                    " assertVotedCandidate(int epoch, int candidateId)": {
                        "first": {
                            "method_name": "assertVotedCandidate",
                            "method_signature": " assertVotedCandidate(int epoch, int candidateId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.39636501363757765
                    },
                    " voteRequest(int epoch, int candidateId, int lastEpoch, long lastEpochOffset)": {
                        "first": {
                            "method_name": "voteRequest",
                            "method_signature": " voteRequest(int epoch, int candidateId, int lastEpoch, long lastEpochOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.39910351486728646
                    },
                    "public becomeLeader()": {
                        "first": {
                            "method_name": "becomeLeader",
                            "method_signature": "public becomeLeader()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.40309575995882885
                    },
                    " discoverLeaderAsObserver(\n        int leaderId,\n        int epoch\n    )": {
                        "first": {
                            "method_name": "discoverLeaderAsObserver",
                            "method_signature": " discoverLeaderAsObserver(\n        int leaderId,\n        int epoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4088067704595625
                    },
                    " buildBatch(\n        long baseOffset,\n        int epoch,\n        List<String> records\n    )": {
                        "first": {
                            "method_name": "buildBatch",
                            "method_signature": " buildBatch(\n        long baseOffset,\n        int epoch,\n        List<String> records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4098457504232806
                    },
                    " assertResignedLeader(int epoch, int leaderId)": {
                        "first": {
                            "method_name": "assertResignedLeader",
                            "method_signature": " assertResignedLeader(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4309674369147543
                    },
                    " fetchResponse(\n        int epoch,\n        int leaderId,\n        Records records,\n        long highWatermark,\n        Errors error\n    )": {
                        "first": {
                            "method_name": "fetchResponse",
                            "method_signature": " fetchResponse(\n        int epoch,\n        int leaderId,\n        Records records,\n        long highWatermark,\n        Errors error\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.43595048603902525
                    }
                },
                "voyage": {
                    "public pollUntil(TestCondition condition)": {
                        "first": {
                            "method_name": "pollUntil",
                            "method_signature": "public pollUntil(TestCondition condition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3379495019661108
                    },
                    "private drainSentResponses(\n        ApiKeys apiKey\n    )": {
                        "first": {
                            "method_name": "drainSentResponses",
                            "method_signature": "private drainSentResponses(\n        ApiKeys apiKey\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.346335171839786
                    },
                    " deleteBeforeSnapshot(OffsetAndEpoch snapshotId)": {
                        "first": {
                            "method_name": "deleteBeforeSnapshot",
                            "method_signature": " deleteBeforeSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.37856344976785955
                    },
                    " buildBatch(\n        long baseOffset,\n        int epoch,\n        List<String> records\n    )": {
                        "first": {
                            "method_name": "buildBatch",
                            "method_signature": " buildBatch(\n        long baseOffset,\n        int epoch,\n        List<String> records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.38432351722290675
                    },
                    "static assertMatchingRecords(\n        String[] expected,\n        Records actual\n    )": {
                        "first": {
                            "method_name": "assertMatchingRecords",
                            "method_signature": "static assertMatchingRecords(\n        String[] expected,\n        Records actual\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.39824956804375616
                    },
                    "public appendToLog(int epoch, List<String> records)": {
                        "first": {
                            "method_name": "appendToLog",
                            "method_signature": "public appendToLog(int epoch, List<String> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.41018821367105596
                    },
                    " commitWithLastOffset(long lastOffset)": {
                        "first": {
                            "method_name": "commitWithLastOffset",
                            "method_signature": " commitWithLastOffset(long lastOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.41860481974264513
                    },
                    " withEmptySnapshot(OffsetAndEpoch snapshotId)": {
                        "first": {
                            "method_name": "withEmptySnapshot",
                            "method_signature": " withEmptySnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.42960809977326836
                    },
                    " currentClaimedEpoch()": {
                        "first": {
                            "method_name": "currentClaimedEpoch",
                            "method_signature": " currentClaimedEpoch()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.46602359763059736
                    },
                    " withVotedCandidate(int epoch, ReplicaKey votedKey)": {
                        "first": {
                            "method_name": "withVotedCandidate",
                            "method_signature": " withVotedCandidate(int epoch, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4717485333336645
                    },
                    " readBatch(BatchReader<String> reader)": {
                        "first": {
                            "method_name": "readBatch",
                            "method_signature": " readBatch(BatchReader<String> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4735140088046493
                    },
                    " deliverResponse(int correlationId, Node source, ApiMessage response)": {
                        "first": {
                            "method_name": "deliverResponse",
                            "method_signature": " deliverResponse(int correlationId, Node source, ApiMessage response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.48315157057236124
                    },
                    " withUnknownLeader(int epoch)": {
                        "first": {
                            "method_name": "withUnknownLeader",
                            "method_signature": " withUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5053447630333766
                    },
                    " deliverRequest(ApiMessage request)": {
                        "first": {
                            "method_name": "deliverRequest",
                            "method_signature": " deliverRequest(ApiMessage request)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5071780795550398
                    },
                    " voteRequest(int epoch, int candidateId, int lastEpoch, long lastEpochOffset)": {
                        "first": {
                            "method_name": "voteRequest",
                            "method_signature": " voteRequest(int epoch, int candidateId, int lastEpoch, long lastEpochOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5199800313238467
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [],
                    "llm_response_time": 7775
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "public pollUntil(TestCondition condition)",
                        "public appendToLog(int epoch, List<String> records)"
                    ],
                    "llm_response_time": 5346
                },
                "tf-df-3": {
                    "priority_method_names": [],
                    "llm_response_time": 3886
                },
                "voyage": {
                    "priority_method_names": [
                        "public pollUntil(TestCondition condition)",
                        "public appendToLog(int epoch, List<String> records)"
                    ],
                    "llm_response_time": 6793
                },
                "voyage-5": {
                    "priority_method_names": [
                        "public pollUntil(TestCondition condition)",
                        "static assertMatchingRecords(\n        String[] expected,\n        Records actual\n    )",
                        "private drainSentResponses(\n        ApiKeys apiKey\n    )"
                    ],
                    "llm_response_time": 5698
                },
                "voyage-3": {
                    "priority_method_names": [
                        "public pollUntil(TestCondition condition)",
                        "private drainSentResponses(\n        ApiKeys apiKey\n    )"
                    ],
                    "llm_response_time": 4194
                }
            },
            "targetClassMap": {
                "appendToLog": {
                    "target_classes": [
                        {
                            "class_name": "QuorumStateStore",
                            "similarity_score": 0.3598218313339
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.2725496106466779
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "QuorumStateStore",
                        "MemoryPool"
                    ],
                    "llm_response_time": 2405,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "commitWithLastOffset": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3877,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "deliverResponse": {
                    "target_classes": [
                        {
                            "class_name": "ApiMessage",
                            "similarity_score": 0.1153846153846154
                        },
                        {
                            "class_name": "QuorumStateStore",
                            "similarity_score": 0.04951496274255767
                        },
                        {
                            "class_name": "MockLog",
                            "similarity_score": 0.28932458778573755
                        },
                        {
                            "class_name": "MockNetworkChannel",
                            "similarity_score": 0.3027080505538698
                        },
                        {
                            "class_name": "MockMessageQueue",
                            "similarity_score": 0.2944042959788306
                        },
                        {
                            "class_name": "MockListener",
                            "similarity_score": 0.3437136581065699
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "MockNetworkChannel",
                        "MockListener",
                        "MockMessageQueue"
                    ],
                    "llm_response_time": 3993,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "assertUnknownLeader": {
                    "target_classes": [
                        {
                            "class_name": "QuorumStateStore",
                            "similarity_score": 0.056455743693763674
                        },
                        {
                            "class_name": "MockListener",
                            "similarity_score": 0.4041405495641643
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "QuorumStateStore",
                        "MockListener"
                    ],
                    "llm_response_time": 1944,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "pollUntil": {
                    "target_classes": [
                        {
                            "class_name": "TestCondition",
                            "similarity_score": 0.21574395598823748
                        },
                        {
                            "class_name": "Uuid",
                            "similarity_score": 0.2891081190542408
                        },
                        {
                            "class_name": "QuorumStateStore",
                            "similarity_score": 0.08033793168194132
                        },
                        {
                            "class_name": "Uuid",
                            "similarity_score": 0.2891081190542408
                        },
                        {
                            "class_name": "Metrics",
                            "similarity_score": 0.2080771003652757
                        },
                        {
                            "class_name": "MockLog",
                            "similarity_score": 0.4364023151879356
                        },
                        {
                            "class_name": "MockNetworkChannel",
                            "similarity_score": 0.42151756051718964
                        },
                        {
                            "class_name": "MockMessageQueue",
                            "similarity_score": 0.43521081179610566
                        },
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.4756646198752782
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "MockMessageQueue",
                        "MockTime",
                        "MockLog"
                    ],
                    "llm_response_time": 4449,
                    "similarity_computation_time": 7,
                    "similarity_metric": "cosine"
                },
                "withVotedCandidate": {
                    "target_classes": [
                        {
                            "class_name": "QuorumStateStore",
                            "similarity_score": 0.03457194127476755
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.14925788761321498
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "QuorumStateStore",
                        "MemoryPool"
                    ],
                    "llm_response_time": 2440,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "fetchRequest": {
                    "target_classes": [
                        {
                            "class_name": "QuorumStateStore",
                            "similarity_score": 0.012884201800751829
                        },
                        {
                            "class_name": "MockLog",
                            "similarity_score": 0.24459290663707664
                        },
                        {
                            "class_name": "MockNetworkChannel",
                            "similarity_score": 0.19012756018183888
                        },
                        {
                            "class_name": "MockMessageQueue",
                            "similarity_score": 0.18385542391076867
                        },
                        {
                            "class_name": "MockListener",
                            "similarity_score": 0.2207979133397395
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "MockLog",
                        "MockNetworkChannel",
                        "MockListener"
                    ],
                    "llm_response_time": 3435,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "withUnknownLeader": {
                    "target_classes": [
                        {
                            "class_name": "QuorumStateStore",
                            "similarity_score": 0.038652605402255485
                        },
                        {
                            "class_name": "MemoryPool",
                            "similarity_score": 0.1668753914405863
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "QuorumStateStore",
                        "MemoryPool"
                    ],
                    "llm_response_time": 2789,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "assertVotedCandidate": {
                    "target_classes": [
                        {
                            "class_name": "QuorumStateStore",
                            "similarity_score": 0.04207962687318688
                        },
                        {
                            "class_name": "MockListener",
                            "similarity_score": 0.30122858042986483
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "QuorumStateStore",
                        "MockListener"
                    ],
                    "llm_response_time": 3092,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "voteRequest": {
                    "target_classes": [
                        {
                            "class_name": "QuorumStateStore",
                            "similarity_score": 0.016869380006009026
                        },
                        {
                            "class_name": "MockLog",
                            "similarity_score": 0.30130793255743826
                        },
                        {
                            "class_name": "MockNetworkChannel",
                            "similarity_score": 0.23471052976006615
                        },
                        {
                            "class_name": "MockMessageQueue",
                            "similarity_score": 0.2407232562855118
                        },
                        {
                            "class_name": "MockListener",
                            "similarity_score": 0.28177353576820796
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "MockListener",
                        "MockLog",
                        "MockMessageQueue"
                    ],
                    "llm_response_time": 3755,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "becomeLeader": {
                    "target_classes": [
                        {
                            "class_name": "QuorumStateStore",
                            "similarity_score": 0.16398911634277583
                        },
                        {
                            "class_name": "MockListener",
                            "similarity_score": 0.36759180422471793
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "MockListener",
                        "QuorumStateStore"
                    ],
                    "llm_response_time": 3192,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "discoverLeaderAsObserver": {
                    "target_classes": [
                        {
                            "class_name": "QuorumStateStore",
                            "similarity_score": 0.03271613491439193
                        },
                        {
                            "class_name": "MockLog",
                            "similarity_score": 0.24820983976610245
                        },
                        {
                            "class_name": "MockNetworkChannel",
                            "similarity_score": 0.23909120000447645
                        },
                        {
                            "class_name": "MockMessageQueue",
                            "similarity_score": 0.19452242581092713
                        },
                        {
                            "class_name": "MockListener",
                            "similarity_score": 0.25312489971107205
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "MockNetworkChannel",
                        "MockListener",
                        "MockLog"
                    ],
                    "llm_response_time": 4221,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "buildBatch": {
                    "target_classes": [
                        {
                            "class_name": "QuorumStateStore",
                            "similarity_score": 0.019960119601394977
                        },
                        {
                            "class_name": "MockLog",
                            "similarity_score": 0.3422518595117958
                        },
                        {
                            "class_name": "MockNetworkChannel",
                            "similarity_score": 0.2692976911503764
                        },
                        {
                            "class_name": "MockMessageQueue",
                            "similarity_score": 0.28482759796652474
                        },
                        {
                            "class_name": "MockListener",
                            "similarity_score": 0.33339894367415834
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "MockLog",
                        "MockMessageQueue",
                        "MockListener"
                    ],
                    "llm_response_time": 3840,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "assertResignedLeader": {
                    "target_classes": [
                        {
                            "class_name": "QuorumStateStore",
                            "similarity_score": 0.04771381198664698
                        },
                        {
                            "class_name": "MockListener",
                            "similarity_score": 0.3415611049724744
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "QuorumStateStore",
                        "MockListener"
                    ],
                    "llm_response_time": 3320,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "fetchResponse": {
                    "target_classes": [
                        {
                            "class_name": "Records",
                            "similarity_score": 0.06897961603239507
                        },
                        {
                            "class_name": "QuorumStateStore",
                            "similarity_score": 0.02231609213387259
                        },
                        {
                            "class_name": "MockLog",
                            "similarity_score": 0.3606316974848111
                        },
                        {
                            "class_name": "MockNetworkChannel",
                            "similarity_score": 0.28853880630640955
                        },
                        {
                            "class_name": "MockMessageQueue",
                            "similarity_score": 0.2919096908388514
                        },
                        {
                            "class_name": "MockListener",
                            "similarity_score": 0.36145585536098984
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "MockLog",
                        "MockListener",
                        "MockMessageQueue"
                    ],
                    "llm_response_time": 3686,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "27220d146c5d043da4adc3d636036bd6e7b112d2",
        "url": "https://github.com/apache/kafka/commit/27220d146c5d043da4adc3d636036bd6e7b112d2",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public summarize() : String extracted from public completeRequest(cb FutureCallback<T>) : T in class org.apache.kafka.connect.runtime.rest.HerderRequestHandler & moved to class org.apache.kafka.connect.util.Stage",
            "leftSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 54,
                    "endLine": 89,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public completeRequest(cb FutureCallback<T>) : T"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 70,
                    "endLine": 73,
                    "startColumn": 17,
                    "endColumn": 67,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 71,
                    "endLine": 71,
                    "startColumn": 49,
                    "endColumn": 68,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 72,
                    "endLine": 72,
                    "startColumn": 67,
                    "endColumn": 87,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 75,
                    "endLine": 77,
                    "startColumn": 17,
                    "endColumn": 65,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 76,
                    "endLine": 76,
                    "startColumn": 49,
                    "endColumn": 68,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 69,
                    "endLine": 78,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 69,
                    "endLine": 74,
                    "startColumn": 44,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 74,
                    "endLine": 78,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 64,
                    "endLine": 76,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public summarize() : String"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 67,
                    "endLine": 70,
                    "startColumn": 13,
                    "endColumn": 63,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 68,
                    "endLine": 68,
                    "startColumn": 39,
                    "endColumn": 58,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 69,
                    "endLine": 69,
                    "startColumn": 57,
                    "endColumn": 77,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 72,
                    "endLine": 74,
                    "startColumn": 13,
                    "endColumn": 61,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 73,
                    "endLine": 73,
                    "startColumn": 39,
                    "endColumn": 58,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 66,
                    "endLine": 75,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 66,
                    "endLine": 71,
                    "startColumn": 32,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 71,
                    "endLine": 75,
                    "startColumn": 16,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 53,
                    "endLine": 78,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public completeRequest(cb FutureCallback<T>) : T"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 67,
                    "endLine": 67,
                    "startColumn": 54,
                    "endColumn": 71,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "stage.summarize()"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 65,
                    "endLine": 65,
                    "startColumn": 9,
                    "endColumn": 41,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 601,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a0c65163f3c3b5006b942822829c4c7c4acf3987",
            "newBranchName": "extract-summarize-completeRequest-4550550"
        },
        "telemetry": {
            "id": "5ab4be61-4da0-445f-aba5-ecb93641d6d7",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 133,
                "lineStart": 41,
                "lineEnd": 173,
                "bodyLineStart": 41,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                "sourceCode": "public class HerderRequestHandler {\n\n    private static final Logger log = LoggerFactory.getLogger(HerderRequestHandler.class);\n\n    private final RestClient restClient;\n\n    private final RestRequestTimeout requestTimeout;\n\n    public HerderRequestHandler(RestClient restClient, RestRequestTimeout requestTimeout) {\n        this.restClient = restClient;\n        this.requestTimeout = requestTimeout;\n    }\n\n    /**\n     * Wait for a {@link FutureCallback} to complete and return the result if successful.\n     * @param cb the future callback to wait for\n     * @return the future callback's result if successful\n     * @param <T> the future's result type\n     * @throws Throwable if the future callback isn't successful\n     */\n    public <T> T completeRequest(FutureCallback<T> cb) throws Throwable {\n        try {\n            return cb.get(requestTimeout.timeoutMs(), TimeUnit.MILLISECONDS);\n        } catch (ExecutionException e) {\n            throw e.getCause();\n        } catch (StagedTimeoutException e) {\n            String message;\n            Stage stage = e.stage();\n            message = summarize(stage);\n            // This timeout is for the operation itself. None of the timeout error codes are relevant, so internal server\n            // error is the best option\n            throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), message);\n        } catch (TimeoutException e) {\n            // This timeout is for the operation itself. None of the timeout error codes are relevant, so internal server\n            // error is the best option\n            throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), \"Request timed out\");\n        } catch (InterruptedException e) {\n            throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), \"Request interrupted\");\n        }\n    }\n\n    private String summarize(Stage stage) {\n        String message;\n        if (stage.completed() != null) {\n            message = \"Request timed out. The last operation the worker completed was \"\n                    + stage.description() + \", which began at \"\n                    + Instant.ofEpochMilli(stage.started()) + \" and completed at \"\n                    + Instant.ofEpochMilli(stage.completed());\n        } else {\n            message = \"Request timed out. The worker is currently \"\n                    + stage.description() + \", which began at \"\n                    + Instant.ofEpochMilli(stage.started());\n        }\n        return message;\n    }\n\n    /**\n     * Wait for a {@link FutureCallback} to complete. If it succeeds, return the parsed response. If it fails, try to forward the\n     * request to the indicated target.\n     */\n    public <T, U> T completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward) throws Throwable {\n        try {\n            return completeRequest(cb);\n        } catch (RequestTargetException e) {\n            if (forward == null || forward) {\n                // the only time we allow recursive forwarding is when no forward flag has\n                // been set, which should only be seen by the first worker to handle a user request.\n                // this gives two total hops to resolve the request before giving up.\n                boolean recursiveForward = forward == null;\n                String forwardedUrl = e.forwardUrl();\n                if (forwardedUrl == null) {\n                    // the target didn't know of the leader at this moment.\n                    throw new ConnectRestException(Response.Status.CONFLICT.getStatusCode(),\n                            \"Cannot complete request momentarily due to no known leader URL, \"\n                                    + \"likely because a rebalance was underway.\");\n                }\n                UriBuilder uriBuilder = UriBuilder.fromUri(forwardedUrl)\n                        .path(path)\n                        .queryParam(\"forward\", recursiveForward);\n                if (queryParameters != null) {\n                    queryParameters.forEach(uriBuilder::queryParam);\n                }\n                String forwardUrl = uriBuilder.build().toString();\n                log.debug(\"Forwarding request {} {} {}\", forwardUrl, method, body);\n                return translator.translate(restClient.httpRequest(forwardUrl, method, headers, body, resultType));\n            } else {\n                log.error(\"Request '{} {}' failed because it couldn't find the target Connect worker within two hops (between workers).\",\n                        method, path);\n                // we should find the right target for the query within two hops, so if\n                // we don't, it probably means that a rebalance has taken place.\n                throw new ConnectRestException(Response.Status.CONFLICT.getStatusCode(),\n                        \"Cannot complete request because of a conflicting operation (e.g. worker rebalance)\");\n            }\n        } catch (RebalanceNeededException e) {\n            throw new ConnectRestException(Response.Status.CONFLICT.getStatusCode(),\n                    \"Cannot complete request momentarily due to stale configuration (typically caused by a concurrent config change)\");\n        }\n    }\n\n    public <T, U> T completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                             TypeReference<U> resultType, Translator<T, U> translator, Boolean forward) throws Throwable {\n        return completeOrForwardRequest(cb, path, method, headers, null, body, resultType, translator, forward);\n    }\n\n    public <T> T completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                                 TypeReference<T> resultType, Boolean forward) throws Throwable {\n        return completeOrForwardRequest(cb, path, method, headers, body, resultType, new IdentityTranslator<>(), forward);\n    }\n\n    public void completeOrForwardRequest(FutureCallback<Void> cb, String path, String method, HttpHeaders headers, Object body,\n                                          Boolean forward) throws Throwable {\n        completeOrForwardRequest(cb, path, method, headers, body, new TypeReference<Void>() { }, new IdentityTranslator<>(), forward);\n    }\n\n    public interface Translator<T, U> {\n        T translate(RestClient.HttpResponse<U> response);\n    }\n\n    public static class IdentityTranslator<T> implements Translator<T, T> {\n        @Override\n        public T translate(RestClient.HttpResponse<T> response) {\n            return response.body();\n        }\n    }\n}",
                "methodCount": 9
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 3,
                "candidates": [
                    {
                        "lineStart": 81,
                        "lineEnd": 94,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method summarize to class Stage",
                        "description": "Move method summarize to org.apache.kafka.connect.util.Stage\nRationale: The `summarize(Stage stage)` method relies heavily on the attributes and behavior of the `Stage` class. It uses the `completed()`, `description()`, and `started()` methods of `Stage` for its logic. Moving this method into the `Stage` class would encapsulate this logic within the context of the object it describes, promoting better cohesion and encapsulation. This change would also follow the principle of keeping methods close to the data they operate on.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 53,
                        "lineEnd": 79,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method completeRequest to class RestRequestTimeout",
                        "description": "Move method completeRequest to org.apache.kafka.connect.runtime.rest.RestRequestTimeout\nRationale: The method completeRequest() heavily uses the timeoutMs() method, which belongs to the RestRequestTimeout interface, making RestRequestTimeout a more appropriate home for it. By moving completeRequest() to RestRequestTimeout, the cohesion of the RestRequestTimeout interface is improved, as it will not only provide timeout values but also handle the management of those requests within the context of a specified timeout. This centralizes all timeout-related logic in one place, making the code more modular and maintaining the single responsibility principle. Additionally, the current class with the completeRequest() method likely does not have a direct responsibility for handling request timeouts, hence making RestRequestTimeout the more suitable target.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 96,
                        "lineEnd": 145,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method completeOrForwardRequest to class RestRequestTimeout",
                        "description": "Move method completeOrForwardRequest to org.apache.kafka.connect.runtime.rest.RestRequestTimeout\nRationale: The `completeOrForwardRequest` method does not logically fit in the `RestRequestTimeout` interface, as it focuses on handling REST request completion, error management, and request forwarding. Since `RestRequestTimeout` is an interface focused on retrieving timeout values for REST requests, it does not provide a meaningful place for the listed method. The method deals with complex logic beyond the scope of simply retrieving a timeout value. Therefore, it should not be moved to `RestRequestTimeout`.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "summarize",
                            "method_signature": "private String summarize(Stage stage)",
                            "target_class": "Stage",
                            "rationale": "The summarize method processes details about the Stage object, extracting and converting its information into a string message. It would make more sense to encapsulate this logic within the Stage class itself, enabling Stage to handle its own summarization."
                        },
                        {
                            "method_name": "translate",
                            "method_signature": "public T translate(RestClient.HttpResponse<U> response)",
                            "target_class": "RestClient.HttpResponse",
                            "rationale": "The translate method directly interacts with the RestClient.HttpResponse object. By moving this method to the RestClient.HttpResponse class, we make the method more cohesive with the operations and data of RestClient.HttpResponse."
                        }
                    ],
                    "llm_response_time": 2906
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "summarize",
                            "method_signature": "private String summarize(Stage stage)",
                            "target_class": "Stage",
                            "rationale": "The summarize() method is primarily concerned with the details of a Stage object. By moving it to the Stage class, it encapsulates the logic related to the Stage object within Stage itself, adhering to the Single Responsibility Principle."
                        }
                    ],
                    "llm_response_time": 1759
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "summarize",
                            "method_signature": "private String summarize(Stage stage)",
                            "target_class": "Stage",
                            "rationale": "The summarize method solely processes data related to the Stage class. It would be more cohesive to place this method within the Stage class itself, allowing it to handle its data more efficiently."
                        },
                        {
                            "method_name": "completeRequest",
                            "method_signature": "public <T> T completeRequest(FutureCallback<T> cb) throws Throwable",
                            "target_class": "RestClient",
                            "rationale": "The completeRequest method serves as a mechanism for handling asynchronous client-server communication. Moving it to the RestClient class would centralize all client-related operations, making it easier to manage such processes."
                        }
                    ],
                    "llm_response_time": 2495
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "completeRequest",
                            "method_signature": "public completeRequest(FutureCallback<T> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "summarize",
                            "method_signature": "private summarize(Stage stage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "summarize",
                            "method_signature": "private summarize(Stage stage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeRequest",
                            "method_signature": "public completeRequest(FutureCallback<T> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private summarize(Stage stage)": {
                        "first": {
                            "method_name": "summarize",
                            "method_signature": "private summarize(Stage stage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.356238072502898
                    },
                    "public completeRequest(FutureCallback<T> cb)": {
                        "first": {
                            "method_name": "completeRequest",
                            "method_signature": "public completeRequest(FutureCallback<T> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5309984140082287
                    },
                    "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)": {
                        "first": {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5960443730916112
                    }
                },
                "voyage": {
                    "private summarize(Stage stage)": {
                        "first": {
                            "method_name": "summarize",
                            "method_signature": "private summarize(Stage stage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.34085826567987454
                    },
                    "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)": {
                        "first": {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5861104316496022
                    },
                    "public completeRequest(FutureCallback<T> cb)": {
                        "first": {
                            "method_name": "completeRequest",
                            "method_signature": "public completeRequest(FutureCallback<T> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6347566863505427
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "public completeRequest(FutureCallback<T> cb)"
                    ],
                    "llm_response_time": 3400
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "public completeRequest(FutureCallback<T> cb)"
                    ],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "public completeRequest(FutureCallback<T> cb)"
                    ],
                    "llm_response_time": 0
                },
                "voyage": {
                    "priority_method_names": [
                        "public completeRequest(FutureCallback<T> cb)",
                        "private summarize(Stage stage)"
                    ],
                    "llm_response_time": 1690
                },
                "voyage-5": {
                    "priority_method_names": [
                        "public completeRequest(FutureCallback<T> cb)",
                        "private summarize(Stage stage)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-3": {
                    "priority_method_names": [
                        "public completeRequest(FutureCallback<T> cb)",
                        "private summarize(Stage stage)"
                    ],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "summarize": {
                    "target_classes": [
                        {
                            "class_name": "Stage",
                            "similarity_score": 0.4417011364860841
                        },
                        {
                            "class_name": "RestClient",
                            "similarity_score": 0.30643583166846416
                        },
                        {
                            "class_name": "RestRequestTimeout",
                            "similarity_score": 0.13075098860631257
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Stage",
                        "RestRequestTimeout",
                        "RestClient"
                    ],
                    "llm_response_time": 3508,
                    "similarity_computation_time": 8,
                    "similarity_metric": "cosine"
                },
                "completeRequest": {
                    "target_classes": [
                        {
                            "class_name": "RestRequestTimeout",
                            "similarity_score": 0.3643597788679312
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RestRequestTimeout"
                    ],
                    "llm_response_time": 2003,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "completeOrForwardRequest": {
                    "target_classes": [
                        {
                            "class_name": "RestRequestTimeout",
                            "similarity_score": 0.27177544514381763
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "RestRequestTimeout"
                    ],
                    "llm_response_time": 2056,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package SnapshottableCoordinator(logContext LogContext, snapshotRegistry SnapshotRegistry, coordinator S, tp TopicPartition) in class org.apache.kafka.coordinator.group.runtime.SnapshottableCoordinator & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 65,
                    "endLine": 78,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package SnapshottableCoordinator(logContext LogContext, snapshotRegistry SnapshotRegistry, coordinator S, tp TopicPartition)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 77,
                    "endLine": 77,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 65,
                    "endLine": 78,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package SnapshottableCoordinator(logContext LogContext, snapshotRegistry SnapshotRegistry, coordinator S, tp TopicPartition)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 77,
                    "endLine": 77,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 603,
        "extraction_results": {
            "success": false
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public updateLastWrittenOffset(offset Long) : void in class org.apache.kafka.coordinator.group.runtime.SnapshottableCoordinator & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 134,
                    "endLine": 150,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public updateLastWrittenOffset(offset Long) : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 148,
                    "endLine": 148,
                    "startColumn": 9,
                    "endColumn": 54,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 134,
                    "endLine": 150,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public updateLastWrittenOffset(offset Long) : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 148,
                    "endLine": 148,
                    "startColumn": 9,
                    "endColumn": 58,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(offset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 604,
        "extraction_results": {
            "success": true,
            "newCommitHash": "c20cab42e1d9996007ecc6fa1defd949297eb8be",
            "newBranchName": "extract-idempotentCreateSnapshot-updateLastWrittenOffset-130af38"
        },
        "telemetry": {
            "id": "0d3aab65-1e8c-4bcc-af70-e16baf95d05a",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 213,
                "lineStart": 28,
                "lineEnd": 240,
                "bodyLineStart": 28,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                "sourceCode": "/**\n * SnapshottableCoordinator is a wrapper on top of the coordinator state machine. This object is not accessed concurrently\n * but multiple threads access it while loading the coordinator partition and therefore requires all methods to be\n * synchronized.\n */\nclass SnapshottableCoordinator<S extends CoordinatorShard<U>, U> implements CoordinatorPlayback<U> {\n    /**\n     * The logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry backing the coordinator.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The actual state machine.\n     */\n    private final S coordinator;\n\n    /**\n     * The topic partition.\n     */\n    private final TopicPartition tp;\n\n    /**\n     * The last offset written to the partition.\n     */\n    private long lastWrittenOffset;\n\n    /**\n     * The last offset committed. This represents the high\n     * watermark of the partition.\n     */\n    private long lastCommittedOffset;\n\n    SnapshottableCoordinator(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        S coordinator,\n        TopicPartition tp\n    ) {\n        this.log = logContext.logger(SnapshottableCoordinator.class);\n        this.coordinator = coordinator;\n        this.snapshotRegistry = snapshotRegistry;\n        this.tp = tp;\n        this.lastWrittenOffset = 0;\n        this.lastCommittedOffset = 0;\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    /**\n     * Reverts the last written offset. This also reverts the snapshot\n     * registry to this offset. All the changes applied after the offset\n     * are lost.\n     *\n     * @param offset The offset to revert to.\n     */\n    synchronized void revertLastWrittenOffset(\n        long offset\n    ) {\n        if (offset > lastWrittenOffset) {\n            throw new IllegalStateException(\"New offset \" + offset + \" of \" + tp +\n                \" must be smaller than \" + lastWrittenOffset + \".\");\n        }\n\n        log.debug(\"Revert last written offset of {} to {}.\", tp, offset);\n        lastWrittenOffset = offset;\n        snapshotRegistry.revertToSnapshot(offset);\n    }\n\n    /**\n     * Replays the record onto the state machine.\n     *\n     * @param offset        The offset of the record in the log.\n     * @param producerId    The producer id.\n     * @param producerEpoch The producer epoch.\n     * @param record        A record.\n     */\n    @Override\n    public synchronized void replay(\n        long offset,\n        long producerId,\n        short producerEpoch,\n        U record\n    ) {\n        coordinator.replay(offset, producerId, producerEpoch, record);\n    }\n\n    /**\n     * Applies the end transaction marker.\n     *\n     * @param producerId    The producer id.\n     * @param producerEpoch The producer epoch.\n     * @param result        The result of the transaction.\n     */\n    @Override\n    public synchronized void replayEndTransactionMarker(\n        long producerId,\n        short producerEpoch,\n        TransactionResult result\n    ) {\n        coordinator.replayEndTransactionMarker(producerId, producerEpoch, result);\n    }\n\n    /**\n     * Updates the last written offset. This also create a new snapshot\n     * in the snapshot registry.\n     *\n     * @param offset The new last written offset.\n     */\n    @Override\n    public synchronized void updateLastWrittenOffset(Long offset) {\n        if (offset <= lastWrittenOffset) {\n            throw new IllegalStateException(\"New last written offset \" + offset + \" of \" + tp +\n                \" must be greater than \" + lastWrittenOffset + \".\");\n        }\n\n        lastWrittenOffset = offset;\n        idempotentCreateSnapshot(offset);\n        log.debug(\"Updated last written offset of {} to {}.\", tp, offset);\n    }\n\n    private void idempotentCreateSnapshot(Long offset) {\n        snapshotRegistry.getOrCreateSnapshot(offset);\n    }\n\n    /**\n     * Updates the last committed offset. This completes all the deferred\n     * events waiting on this offset. This also cleanups all the snapshots\n     * prior to this offset.\n     *\n     * @param offset The new last committed offset.\n     */\n    @Override\n    public synchronized void updateLastCommittedOffset(Long offset) {\n        if (offset < lastCommittedOffset) {\n            throw new IllegalStateException(\"New committed offset \" + offset + \" of \" + tp +\n                \" must be greater than or equal to \" + lastCommittedOffset + \".\");\n        }\n\n        if (offset > lastWrittenOffset) {\n            throw new IllegalStateException(\"New committed offset \" + offset + \" of \" + tp +\n                \"must be less than or equal to \" + lastWrittenOffset + \".\");\n        }\n\n        lastCommittedOffset = offset;\n        snapshotRegistry.deleteSnapshotsUpTo(offset);\n        log.debug(\"Updated committed offset of {} to {}.\", tp, offset);\n    }\n\n    /**\n     * The coordinator has been loaded. This is used to apply any\n     * post loading operations.\n     *\n     * @param newImage  The metadata image.\n     */\n    synchronized void onLoaded(MetadataImage newImage) {\n        this.coordinator.onLoaded(newImage);\n    }\n\n    /**\n     * The coordinator has been unloaded. This is used to apply\n     * any post unloading operations.\n     */\n    synchronized void onUnloaded() {\n        if (this.coordinator != null) {\n            this.coordinator.onUnloaded();\n        }\n    }\n\n    /**\n     * @return The last written offset.\n     */\n    synchronized long lastWrittenOffset() {\n        return this.lastWrittenOffset;\n    }\n\n    /**\n     * A new metadata image is available. This is only called after {@link SnapshottableCoordinator#onLoaded(MetadataImage)}\n     * is called to signal that the coordinator has been fully loaded.\n     *\n     * @param newImage  The new metadata image.\n     * @param delta     The delta image.\n     */\n    synchronized void onNewMetadataImage(MetadataImage newImage, MetadataDelta delta) {\n        this.coordinator.onNewMetadataImage(newImage, delta);\n    }\n\n    /**\n     * @return The last committed offset.\n     */\n    synchronized long lastCommittedOffset() {\n        return this.lastCommittedOffset;\n    }\n\n    /**\n     * @return The coordinator.\n     */\n    synchronized S coordinator() {\n        return this.coordinator;\n    }\n\n    /**\n     * @return The snapshot registry.\n     *\n     * Only used for testing.\n     */\n    synchronized SnapshotRegistry snapshotRegistry() {\n        return this.snapshotRegistry;\n    }\n}",
                "methodCount": 14
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 151,
                        "lineEnd": 153,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() is closely related to the operations provided by the SnapshotRegistry class, particularly the getOrCreateSnapshot() method. Moving idempotentCreateSnapshot() to SnapshotRegistry enhances cohesion as it keeps all snapshot-related operations within the same class, making the code more organized and maintainable. The method's primary action involves interacting with the snapshot registry, which aligns with the responsibilities of the SnapshotRegistry class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "onLoaded",
                            "method_signature": "synchronized void onLoaded(MetadataImage newImage)",
                            "target_class": "CoordinatorShard",
                            "rationale": "The method `onLoaded` primarily communicates with the `CoordinatorShard` class. Moving it there aligns responsibilities more closely with where the operation is executed."
                        },
                        {
                            "method_name": "onUnloaded",
                            "method_signature": "synchronized void onUnloaded()",
                            "target_class": "CoordinatorShard",
                            "rationale": "The method `onUnloaded` primarily deals with operations within the `CoordinatorShard` class. Moving it to `CoordinatorShard` ensures clear responsibilities for load/unload operations."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public synchronized void replay(long offset, long producerId, short producerEpoch, U record)",
                            "target_class": "CoordinatorShard",
                            "rationale": "The `replay` method is calling the same method of the `CoordinatorShard` class. Placing it in `CoordinatorShard` ensures the logic for replaying records resides within the right context."
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "public synchronized void replayEndTransactionMarker(long producerId, short producerEpoch, TransactionResult result)",
                            "target_class": "CoordinatorShard",
                            "rationale": "This method is essentially delegating the task to the `CoordinatorShard` class. Moving it to `CoordinatorShard` keeps transaction-related replay logic encapsulated within the proper class."
                        },
                        {
                            "method_name": "onNewMetadataImage",
                            "method_signature": "synchronized void onNewMetadataImage(MetadataImage newImage, MetadataDelta delta)",
                            "target_class": "CoordinatorShard",
                            "rationale": "This method's main purpose is to notify the `CoordinatorShard` about the new metadata image. Its responsibility aligns closely with the `CoordinatorShard` class."
                        }
                    ],
                    "llm_response_time": 4536
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "onLoaded",
                            "method_signature": "synchronized void onLoaded(MetadataImage newImage)",
                            "target_class": "CoordinatorShard",
                            "rationale": "The method `onLoaded` interacts directly with the coordinator state and should be part of the CoordinatorShard class which is responsible for the state machine logic."
                        },
                        {
                            "method_name": "onUnloaded",
                            "method_signature": "synchronized void onUnloaded()",
                            "target_class": "CoordinatorShard",
                            "rationale": "The method `onUnloaded` interacts directly with the coordinator state, indicating that it should reside within the CoordinatorShard class which manages the state machine logic."
                        },
                        {
                            "method_name": "onNewMetadataImage",
                            "method_signature": "synchronized void onNewMetadataImage(MetadataImage newImage, MetadataDelta delta)",
                            "target_class": "CoordinatorShard",
                            "rationale": "The method `onNewMetadataImage` handles changes in metadata images and should be moved to the CoordinatorShard class, as it logically operates on the state machine's metadata."
                        }
                    ],
                    "llm_response_time": 5530
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "revertLastWrittenOffset",
                            "method_signature": "synchronized void revertLastWrittenOffset(long offset)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The method revertLastWrittenOffset primarily deals with managing snapshots in the SnapshotRegistry, making it more appropriate to reside in the SnapshotRegistry class."
                        },
                        {
                            "method_name": "onLoaded",
                            "method_signature": "synchronized void onLoaded(MetadataImage newImage)",
                            "target_class": "CoordinatorShard",
                            "rationale": "The method onLoaded interacts with the coordinator and is concerned with the loading state of the coordinator. Thus, it should be moved to CoordinatorShard."
                        },
                        {
                            "method_name": "onUnloaded",
                            "method_signature": "synchronized void onUnloaded()",
                            "target_class": "CoordinatorShard",
                            "rationale": "The method onUnloaded involves unloading operations related to the coordinator and should therefore be part of the CoordinatorShard class."
                        },
                        {
                            "method_name": "onNewMetadataImage",
                            "method_signature": "synchronized void onNewMetadataImage(MetadataImage newImage, MetadataDelta delta)",
                            "target_class": "CoordinatorShard",
                            "rationale": "The method onNewMetadataImage handles updates related to metadata images and should logically be incorporated within the CoordinatorShard class."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public synchronized void replay(long offset, long producerId, short producerEpoch, U record)",
                            "target_class": "CoordinatorShard",
                            "rationale": "The replay method directly interacts with the state machine inside the CoordinatorShard; thus, it should be implemented in the CoordinatorShard class."
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "public synchronized void replayEndTransactionMarker(long producerId, short producerEpoch, TransactionResult result)",
                            "target_class": "CoordinatorShard",
                            "rationale": "The replayEndTransactionMarker method works closely with the CoordinatorShard's state machine for handling transactions and should be moved there."
                        }
                    ],
                    "llm_response_time": 4345
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "revertLastWrittenOffset",
                            "method_signature": "synchronized revertLastWrittenOffset(\n        long offset\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(Long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "revertLastWrittenOffset",
                            "method_signature": "synchronized revertLastWrittenOffset(\n        long offset\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(Long offset)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.2044794529772991
                    },
                    "synchronized revertLastWrittenOffset(\n        long offset\n    )": {
                        "first": {
                            "method_name": "revertLastWrittenOffset",
                            "method_signature": "synchronized revertLastWrittenOffset(\n        long offset\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.7677929142021896
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(Long offset)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4829411782283535
                    },
                    "synchronized revertLastWrittenOffset(\n        long offset\n    )": {
                        "first": {
                            "method_name": "revertLastWrittenOffset",
                            "method_signature": "synchronized revertLastWrittenOffset(\n        long offset\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5829997779551179
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(Long offset)"
                    ],
                    "llm_response_time": 2880
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(Long offset)"
                    ],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(Long offset)"
                    ],
                    "llm_response_time": 0
                },
                "voyage": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(Long offset)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(Long offset)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(Long offset)"
                    ],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 1916,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                },
                "revertLastWrittenOffset": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2796,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public GroupMetadataManagerTestContext(time MockTime, timer MockCoordinatorTimer<Void,CoordinatorRecord>, snapshotRegistry SnapshotRegistry, metrics GroupCoordinatorMetricsShard, groupMetadataManager GroupMetadataManager, classicGroupInitialRebalanceDelayMs int, classicGroupNewMemberJoinTimeoutMs int) in class org.apache.kafka.coordinator.group.GroupMetadataManagerTestContext & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 519,
                    "endLine": 536,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public GroupMetadataManagerTestContext(time MockTime, timer MockCoordinatorTimer<Void,CoordinatorRecord>, snapshotRegistry SnapshotRegistry, metrics GroupCoordinatorMetricsShard, groupMetadataManager GroupMetadataManager, classicGroupInitialRebalanceDelayMs int, classicGroupNewMemberJoinTimeoutMs int)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 535,
                    "endLine": 535,
                    "startColumn": 9,
                    "endColumn": 65,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 519,
                    "endLine": 536,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public GroupMetadataManagerTestContext(time MockTime, timer MockCoordinatorTimer<Void,CoordinatorRecord>, snapshotRegistry SnapshotRegistry, metrics GroupCoordinatorMetricsShard, groupMetadataManager GroupMetadataManager, classicGroupInitialRebalanceDelayMs int, classicGroupNewMemberJoinTimeoutMs int)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 535,
                    "endLine": 535,
                    "startColumn": 9,
                    "endColumn": 69,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastWrittenOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 605,
        "extraction_results": {
            "success": false
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public replay(record CoordinatorRecord, groupType GroupType) : void in class org.apache.kafka.coordinator.group.GroupMetadataManagerTestContext & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 1430,
                    "endLine": 1516,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public replay(record CoordinatorRecord, groupType GroupType) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 1515,
                    "endLine": 1515,
                    "startColumn": 9,
                    "endColumn": 65,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 1430,
                    "endLine": 1516,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public replay(record CoordinatorRecord, groupType GroupType) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 1515,
                    "endLine": 1515,
                    "startColumn": 9,
                    "endColumn": 69,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastWrittenOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 606,
        "extraction_results": {
            "success": true,
            "newCommitHash": "43fa7a86608100f7e4de70d740725c4d4db8f008",
            "newBranchName": "extract-idempotentCreateSnapshot-replay-130af38-1"
        },
        "telemetry": {
            "id": "bb457f45-824a-454f-93c5-66aaf6d37d66",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1406,
                "lineStart": 120,
                "lineEnd": 1525,
                "bodyLineStart": 120,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                "sourceCode": "public class GroupMetadataManagerTestContext {\n\n    public static void assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts) {\n        assertTrue(timeouts.size() <= 1);\n        timeouts.forEach(timeout -> assertEquals(EMPTY_RESULT, timeout.result));\n    }\n\n    public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toProtocols(String... protocolNames) {\n        JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols = new JoinGroupRequestData.JoinGroupRequestProtocolCollection(0);\n        List<String> topicNames = Arrays.asList(\"foo\", \"bar\", \"baz\");\n        for (int i = 0; i < protocolNames.length; i++) {\n            protocols.add(new JoinGroupRequestData.JoinGroupRequestProtocol()\n                .setName(protocolNames[i])\n                .setMetadata(ConsumerProtocol.serializeSubscription(new ConsumerPartitionAssignor.Subscription(\n                    Collections.singletonList(topicNames.get(i % topicNames.size())))).array())\n            );\n        }\n        return protocols;\n    }\n\n    public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toConsumerProtocol(\n        List<String> topicNames,\n        List<TopicPartition> ownedPartitions\n    ) {\n        return toConsumerProtocol(topicNames, ownedPartitions, ConsumerProtocolSubscription.HIGHEST_SUPPORTED_VERSION);\n    }\n\n    public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toConsumerProtocol(\n        List<String> topicNames,\n        List<TopicPartition> ownedPartitions,\n        short version\n    ) {\n        JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols =\n            new JoinGroupRequestData.JoinGroupRequestProtocolCollection(0);\n        protocols.add(new JoinGroupRequestData.JoinGroupRequestProtocol()\n            .setName(\"range\")\n            .setMetadata(ConsumerProtocol.serializeSubscription(\n                new ConsumerPartitionAssignor.Subscription(\n                    topicNames,\n                    null,\n                    ownedPartitions\n                ),\n                version\n            ).array())\n        );\n        return protocols;\n    }\n\n    public static CoordinatorRecord newGroupMetadataRecord(\n        String groupId,\n        GroupMetadataValue value,\n        MetadataVersion metadataVersion\n    ) {\n        return new CoordinatorRecord(\n            new ApiMessageAndVersion(\n                new GroupMetadataKey()\n                    .setGroup(groupId),\n                (short) 2\n            ),\n            new ApiMessageAndVersion(\n                value,\n                metadataVersion.groupMetadataValueVersion()\n            )\n        );\n    }\n\n    public static class RebalanceResult {\n        int generationId;\n        String leaderId;\n        byte[] leaderAssignment;\n        String followerId;\n        byte[] followerAssignment;\n\n        RebalanceResult(\n            int generationId,\n            String leaderId,\n            byte[] leaderAssignment,\n            String followerId,\n            byte[] followerAssignment\n        ) {\n            this.generationId = generationId;\n            this.leaderId = leaderId;\n            this.leaderAssignment = leaderAssignment;\n            this.followerId = followerId;\n            this.followerAssignment = followerAssignment;\n        }\n    }\n\n    public static class PendingMemberGroupResult {\n        String leaderId;\n        String followerId;\n        JoinGroupResponseData pendingMemberResponse;\n\n        public PendingMemberGroupResult(\n            String leaderId,\n            String followerId,\n            JoinGroupResponseData pendingMemberResponse\n        ) {\n            this.leaderId = leaderId;\n            this.followerId = followerId;\n            this.pendingMemberResponse = pendingMemberResponse;\n        }\n    }\n\n    public static class JoinResult {\n        CompletableFuture<JoinGroupResponseData> joinFuture;\n        List<CoordinatorRecord> records;\n        CompletableFuture<Void> appendFuture;\n\n        public JoinResult(\n            CompletableFuture<JoinGroupResponseData> joinFuture,\n            CoordinatorResult<Void, CoordinatorRecord> coordinatorResult\n        ) {\n            this.joinFuture = joinFuture;\n            this.records = coordinatorResult.records();\n            this.appendFuture = coordinatorResult.appendFuture();\n        }\n    }\n\n    public static class SyncResult {\n        CompletableFuture<SyncGroupResponseData> syncFuture;\n        List<CoordinatorRecord> records;\n        CompletableFuture<Void> appendFuture;\n\n        public SyncResult(\n            CompletableFuture<SyncGroupResponseData> syncFuture,\n            CoordinatorResult<Void, CoordinatorRecord> coordinatorResult\n        ) {\n            this.syncFuture = syncFuture;\n            this.records = coordinatorResult.records();\n            this.appendFuture = coordinatorResult.appendFuture();\n        }\n    }\n\n    public static class JoinGroupRequestBuilder {\n        String groupId = null;\n        String groupInstanceId = null;\n        String memberId = null;\n        String protocolType = \"consumer\";\n        JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols = new JoinGroupRequestData.JoinGroupRequestProtocolCollection(0);\n        int sessionTimeoutMs = 500;\n        int rebalanceTimeoutMs = 500;\n        String reason = null;\n\n        JoinGroupRequestBuilder withGroupId(String groupId) {\n            this.groupId = groupId;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withGroupInstanceId(String groupInstanceId) {\n            this.groupInstanceId = groupInstanceId;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withMemberId(String memberId) {\n            this.memberId = memberId;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withDefaultProtocolTypeAndProtocols() {\n            this.protocols = toProtocols(\"range\");\n            return this;\n        }\n\n        JoinGroupRequestBuilder withProtocolSuperset() {\n            this.protocols = toProtocols(\"range\", \"roundrobin\");\n            return this;\n        }\n\n        JoinGroupRequestBuilder withProtocolType(String protocolType) {\n            this.protocolType = protocolType;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withProtocols(JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols) {\n            this.protocols = protocols;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withRebalanceTimeoutMs(int rebalanceTimeoutMs) {\n            this.rebalanceTimeoutMs = rebalanceTimeoutMs;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withSessionTimeoutMs(int sessionTimeoutMs) {\n            this.sessionTimeoutMs = sessionTimeoutMs;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withReason(String reason) {\n            this.reason = reason;\n            return this;\n        }\n\n        JoinGroupRequestData build() {\n            return new JoinGroupRequestData()\n                .setGroupId(groupId)\n                .setGroupInstanceId(groupInstanceId)\n                .setMemberId(memberId)\n                .setProtocolType(protocolType)\n                .setProtocols(protocols)\n                .setRebalanceTimeoutMs(rebalanceTimeoutMs)\n                .setSessionTimeoutMs(sessionTimeoutMs)\n                .setReason(reason);\n        }\n    }\n\n    public static class SyncGroupRequestBuilder {\n        String groupId = null;\n        String groupInstanceId = null;\n        String memberId = null;\n        String protocolType = \"consumer\";\n        String protocolName = \"range\";\n        int generationId = 0;\n        List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment = Collections.emptyList();\n\n        SyncGroupRequestBuilder withGroupId(String groupId) {\n            this.groupId = groupId;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withGroupInstanceId(String groupInstanceId) {\n            this.groupInstanceId = groupInstanceId;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withMemberId(String memberId) {\n            this.memberId = memberId;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withGenerationId(int generationId) {\n            this.generationId = generationId;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withProtocolType(String protocolType) {\n            this.protocolType = protocolType;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withProtocolName(String protocolName) {\n            this.protocolName = protocolName;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withAssignment(List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment) {\n            this.assignment = assignment;\n            return this;\n        }\n\n\n        SyncGroupRequestData build() {\n            return new SyncGroupRequestData()\n                .setGroupId(groupId)\n                .setGroupInstanceId(groupInstanceId)\n                .setMemberId(memberId)\n                .setGenerationId(generationId)\n                .setProtocolType(protocolType)\n                .setProtocolName(protocolName)\n                .setAssignments(assignment);\n        }\n    }\n\n    public static class Builder {\n        private final MockTime time = new MockTime();\n        private final MockCoordinatorTimer<Void, CoordinatorRecord> timer = new MockCoordinatorTimer<>(time);\n        private final LogContext logContext = new LogContext();\n        private final SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        private MetadataImage metadataImage;\n        private List<ConsumerGroupPartitionAssignor> consumerGroupAssignors = Collections.singletonList(new MockPartitionAssignor(\"range\"));\n        private final List<ConsumerGroupBuilder> consumerGroupBuilders = new ArrayList<>();\n        private int consumerGroupMaxSize = Integer.MAX_VALUE;\n        private int consumerGroupMetadataRefreshIntervalMs = Integer.MAX_VALUE;\n        private int classicGroupMaxSize = Integer.MAX_VALUE;\n        private int classicGroupInitialRebalanceDelayMs = 3000;\n        private final int classicGroupNewMemberJoinTimeoutMs = 5 * 60 * 1000;\n        private int classicGroupMinSessionTimeoutMs = 10;\n        private int classicGroupMaxSessionTimeoutMs = 10 * 60 * 1000;\n        private final GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        private ConsumerGroupMigrationPolicy consumerGroupMigrationPolicy = ConsumerGroupMigrationPolicy.DISABLED;\n        // Share group configs\n        private ShareGroupPartitionAssignor shareGroupAssignor = new MockPartitionAssignor(\"share\");\n        private int shareGroupMaxSize = Integer.MAX_VALUE;\n\n        public Builder withMetadataImage(MetadataImage metadataImage) {\n            this.metadataImage = metadataImage;\n            return this;\n        }\n\n        public Builder withConsumerGroupAssignors(List<ConsumerGroupPartitionAssignor> assignors) {\n            this.consumerGroupAssignors = assignors;\n            return this;\n        }\n\n        public Builder withConsumerGroup(ConsumerGroupBuilder builder) {\n            this.consumerGroupBuilders.add(builder);\n            return this;\n        }\n\n        public Builder withConsumerGroupMaxSize(int consumerGroupMaxSize) {\n            this.consumerGroupMaxSize = consumerGroupMaxSize;\n            return this;\n        }\n\n        public Builder withConsumerGroupMetadataRefreshIntervalMs(int consumerGroupMetadataRefreshIntervalMs) {\n            this.consumerGroupMetadataRefreshIntervalMs = consumerGroupMetadataRefreshIntervalMs;\n            return this;\n        }\n\n        public Builder withClassicGroupMaxSize(int classicGroupMaxSize) {\n            this.classicGroupMaxSize = classicGroupMaxSize;\n            return this;\n        }\n\n        public Builder withClassicGroupInitialRebalanceDelayMs(int classicGroupInitialRebalanceDelayMs) {\n            this.classicGroupInitialRebalanceDelayMs = classicGroupInitialRebalanceDelayMs;\n            return this;\n        }\n\n        public Builder withClassicGroupMinSessionTimeoutMs(int classicGroupMinSessionTimeoutMs) {\n            this.classicGroupMinSessionTimeoutMs = classicGroupMinSessionTimeoutMs;\n            return this;\n        }\n\n        public Builder withClassicGroupMaxSessionTimeoutMs(int classicGroupMaxSessionTimeoutMs) {\n            this.classicGroupMaxSessionTimeoutMs = classicGroupMaxSessionTimeoutMs;\n            return this;\n        }\n\n        public Builder withConsumerGroupMigrationPolicy(ConsumerGroupMigrationPolicy consumerGroupMigrationPolicy) {\n            this.consumerGroupMigrationPolicy = consumerGroupMigrationPolicy;\n            return this;\n        }\n\n        public Builder withShareGroupAssignor(ShareGroupPartitionAssignor shareGroupAssignor) {\n            this.shareGroupAssignor = shareGroupAssignor;\n            return this;\n        }\n\n        public Builder withShareGroupMaxSize(int shareGroupMaxSize) {\n            this.shareGroupMaxSize = shareGroupMaxSize;\n            return this;\n        }\n\n        public GroupMetadataManagerTestContext build() {\n            if (metadataImage == null) metadataImage = MetadataImage.EMPTY;\n            if (consumerGroupAssignors == null) consumerGroupAssignors = Collections.emptyList();\n\n            GroupMetadataManagerTestContext context = new GroupMetadataManagerTestContext(\n                time,\n                timer,\n                snapshotRegistry,\n                metrics,\n                new GroupMetadataManager.Builder()\n                    .withSnapshotRegistry(snapshotRegistry)\n                    .withLogContext(logContext)\n                    .withTime(time)\n                    .withTimer(timer)\n                    .withMetadataImage(metadataImage)\n                    .withConsumerGroupHeartbeatInterval(5000)\n                    .withConsumerGroupSessionTimeout(45000)\n                    .withConsumerGroupMaxSize(consumerGroupMaxSize)\n                    .withConsumerGroupAssignors(consumerGroupAssignors)\n                    .withConsumerGroupMetadataRefreshIntervalMs(consumerGroupMetadataRefreshIntervalMs)\n                    .withClassicGroupMaxSize(classicGroupMaxSize)\n                    .withClassicGroupMinSessionTimeoutMs(classicGroupMinSessionTimeoutMs)\n                    .withClassicGroupMaxSessionTimeoutMs(classicGroupMaxSessionTimeoutMs)\n                    .withClassicGroupInitialRebalanceDelayMs(classicGroupInitialRebalanceDelayMs)\n                    .withClassicGroupNewMemberJoinTimeoutMs(classicGroupNewMemberJoinTimeoutMs)\n                    .withGroupCoordinatorMetricsShard(metrics)\n                    .withConsumerGroupMigrationPolicy(consumerGroupMigrationPolicy)\n                    .withShareGroupAssignor(shareGroupAssignor)\n                    .withShareGroupMaxSize(shareGroupMaxSize)\n                    .build(),\n                classicGroupInitialRebalanceDelayMs,\n                classicGroupNewMemberJoinTimeoutMs\n            );\n\n            consumerGroupBuilders.forEach(builder -> builder.build(metadataImage.topics()).forEach(context::replay));\n            consumerGroupBuilders.forEach(builder -> builder.build(metadataImage.topics()).forEach(context::replay));\n\n            context.commit();\n\n            return context;\n        }\n    }\n\n    final MockTime time;\n    final MockCoordinatorTimer<Void, CoordinatorRecord> timer;\n    final SnapshotRegistry snapshotRegistry;\n    final GroupCoordinatorMetricsShard metrics;\n    final GroupMetadataManager groupMetadataManager;\n    final int classicGroupInitialRebalanceDelayMs;\n    final int classicGroupNewMemberJoinTimeoutMs;\n\n    long lastCommittedOffset = 0L;\n    long lastWrittenOffset = 0L;\n\n    public GroupMetadataManagerTestContext(\n        MockTime time,\n        MockCoordinatorTimer<Void, CoordinatorRecord> timer,\n        SnapshotRegistry snapshotRegistry,\n        GroupCoordinatorMetricsShard metrics,\n        GroupMetadataManager groupMetadataManager,\n        int classicGroupInitialRebalanceDelayMs,\n        int classicGroupNewMemberJoinTimeoutMs\n    ) {\n        this.time = time;\n        this.timer = timer;\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.groupMetadataManager = groupMetadataManager;\n        this.classicGroupInitialRebalanceDelayMs = classicGroupInitialRebalanceDelayMs;\n        this.classicGroupNewMemberJoinTimeoutMs = classicGroupNewMemberJoinTimeoutMs;\n        snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n    }\n\n    public void commit() {\n        long lastCommittedOffset = this.lastCommittedOffset;\n        this.lastCommittedOffset = lastWrittenOffset;\n        snapshotRegistry.deleteSnapshotsUpTo(lastCommittedOffset);\n    }\n\n    public void rollback() {\n        lastWrittenOffset = lastCommittedOffset;\n        snapshotRegistry.revertToSnapshot(lastCommittedOffset);\n    }\n\n    public ConsumerGroup.ConsumerGroupState consumerGroupState(\n        String groupId\n    ) {\n        return groupMetadataManager\n            .consumerGroup(groupId)\n            .state();\n    }\n\n    public ShareGroup.ShareGroupState shareGroupState(\n        String groupId\n    ) {\n        return groupMetadataManager\n            .shareGroup(groupId)\n            .state();\n    }\n\n    public MemberState consumerGroupMemberState(\n        String groupId,\n        String memberId\n    ) {\n        return groupMetadataManager\n            .consumerGroup(groupId)\n            .getOrMaybeCreateMember(memberId, false)\n            .state();\n    }\n\n    public CoordinatorResult<ConsumerGroupHeartbeatResponseData, CoordinatorRecord> consumerGroupHeartbeat(\n        ConsumerGroupHeartbeatRequestData request\n    ) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.CONSUMER_GROUP_HEARTBEAT,\n                ApiKeys.CONSUMER_GROUP_HEARTBEAT.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        CoordinatorResult<ConsumerGroupHeartbeatResponseData, CoordinatorRecord> result = groupMetadataManager.consumerGroupHeartbeat(\n            context,\n            request\n        );\n\n        if (result.replayRecords()) {\n            result.records().forEach(this::replay);\n        }\n        return result;\n    }\n\n    public CoordinatorResult<ShareGroupHeartbeatResponseData, CoordinatorRecord> shareGroupHeartbeat(\n        ShareGroupHeartbeatRequestData request\n    ) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.SHARE_GROUP_HEARTBEAT,\n                ApiKeys.SHARE_GROUP_HEARTBEAT.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        CoordinatorResult<ShareGroupHeartbeatResponseData, CoordinatorRecord> result = groupMetadataManager.shareGroupHeartbeat(\n            context,\n            request\n        );\n\n        result.records().forEach(this::replay);\n        return result;\n    }\n\n    public List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> sleep(long ms) {\n        time.sleep(ms);\n        List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = timer.poll();\n        timeouts.forEach(timeout -> {\n            if (timeout.result.replayRecords()) {\n                timeout.result.records().forEach(this::replay);\n            }\n        });\n        return timeouts;\n    }\n\n    public void assertSessionTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(groupSessionTimeoutKey(groupId, memberId));\n        assertNotNull(timeout);\n        assertEquals(time.milliseconds() + delayMs, timeout.deadlineMs);\n    }\n\n    public void assertNoSessionTimeout(\n        String groupId,\n        String memberId\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(groupSessionTimeoutKey(groupId, memberId));\n        assertNull(timeout);\n    }\n\n    public MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> assertRebalanceTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupRebalanceTimeoutKey(groupId, memberId));\n        assertNotNull(timeout);\n        assertEquals(time.milliseconds() + delayMs, timeout.deadlineMs);\n        return timeout;\n    }\n\n    public void assertNoRebalanceTimeout(\n        String groupId,\n        String memberId\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupRebalanceTimeoutKey(groupId, memberId));\n        assertNull(timeout);\n    }\n\n    public MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> assertJoinTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupJoinKey(groupId, memberId));\n        assertNotNull(timeout);\n        assertEquals(time.milliseconds() + delayMs, timeout.deadlineMs);\n        return timeout;\n    }\n\n    public void assertNoJoinTimeout(\n        String groupId,\n        String memberId\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupJoinKey(groupId, memberId));\n        assertNull(timeout);\n    }\n\n    public MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> assertSyncTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupSyncKey(groupId, memberId));\n        assertNotNull(timeout);\n        assertEquals(time.milliseconds() + delayMs, timeout.deadlineMs);\n        return timeout;\n    }\n\n    public void assertNoSyncTimeout(\n        String groupId,\n        String memberId\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupSyncKey(groupId, memberId));\n        assertNull(timeout);\n    }\n\n    ClassicGroup createClassicGroup(String groupId) {\n        return groupMetadataManager.getOrMaybeCreateClassicGroup(groupId, true);\n    }\n\n    public JoinResult sendClassicGroupJoin(\n        JoinGroupRequestData request\n    ) {\n        return sendClassicGroupJoin(request, false);\n    }\n\n    public JoinResult sendClassicGroupJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId\n    ) {\n        return sendClassicGroupJoin(request, requireKnownMemberId, false);\n    }\n\n    public JoinResult sendClassicGroupJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    ) {\n        // requireKnownMemberId is true: version >= 4 (See JoinGroupRequest#requiresKnownMemberId())\n        // supportSkippingAssignment is true: version >= 9 (See JoinGroupRequest#supportsSkippingAssignment())\n        short joinGroupVersion = 3;\n\n        if (requireKnownMemberId) {\n            joinGroupVersion = 4;\n            if (supportSkippingAssignment) {\n                joinGroupVersion = ApiKeys.JOIN_GROUP.latestVersion();\n            }\n        }\n\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.JOIN_GROUP,\n                joinGroupVersion,\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        CompletableFuture<JoinGroupResponseData> responseFuture = new CompletableFuture<>();\n        CoordinatorResult<Void, CoordinatorRecord> coordinatorResult = groupMetadataManager.classicGroupJoin(\n            context,\n            request,\n            responseFuture\n        );\n\n        if (coordinatorResult.replayRecords()) {\n            coordinatorResult.records().forEach(this::replay);\n        }\n\n        return new JoinResult(responseFuture, coordinatorResult);\n    }\n\n    public JoinGroupResponseData joinClassicGroupAsDynamicMemberAndCompleteRebalance(\n        String groupId\n    ) throws Exception {\n        ClassicGroup group = createClassicGroup(groupId);\n\n        JoinGroupResponseData leaderJoinResponse =\n            joinClassicGroupAsDynamicMemberAndCompleteJoin(new JoinGroupRequestBuilder()\n                .withGroupId(groupId)\n                .withMemberId(UNKNOWN_MEMBER_ID)\n                .withDefaultProtocolTypeAndProtocols()\n                .withRebalanceTimeoutMs(10000)\n                .withSessionTimeoutMs(5000)\n                .build());\n\n        assertEquals(1, leaderJoinResponse.generationId());\n        assertTrue(group.isInState(COMPLETING_REBALANCE));\n\n        SyncResult syncResult = sendClassicGroupSync(new SyncGroupRequestBuilder()\n            .withGroupId(groupId)\n            .withMemberId(leaderJoinResponse.memberId())\n            .withGenerationId(leaderJoinResponse.generationId())\n            .build());\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newGroupMetadataRecord(group, group.groupAssignment(), MetadataVersion.latestTesting())),\n            syncResult.records\n        );\n        // Simulate a successful write to the log.\n        syncResult.appendFuture.complete(null);\n\n        assertTrue(syncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), syncResult.syncFuture.get().errorCode());\n        assertTrue(group.isInState(STABLE));\n\n        return leaderJoinResponse;\n    }\n\n    public JoinGroupResponseData joinClassicGroupAsDynamicMemberAndCompleteJoin(\n        JoinGroupRequestData request\n    ) throws ExecutionException, InterruptedException {\n        boolean requireKnownMemberId = true;\n        String newMemberId = request.memberId();\n\n        if (request.memberId().equals(UNKNOWN_MEMBER_ID)) {\n            // Since member id is required, we need another round to get the successful join group result.\n            JoinResult firstJoinResult = sendClassicGroupJoin(\n                request,\n                requireKnownMemberId\n            );\n            assertTrue(firstJoinResult.records.isEmpty());\n            assertTrue(firstJoinResult.joinFuture.isDone());\n            assertEquals(Errors.MEMBER_ID_REQUIRED.code(), firstJoinResult.joinFuture.get().errorCode());\n            newMemberId = firstJoinResult.joinFuture.get().memberId();\n        }\n\n        // Second round\n        JoinGroupRequestData secondRequest = new JoinGroupRequestData()\n            .setGroupId(request.groupId())\n            .setMemberId(newMemberId)\n            .setProtocolType(request.protocolType())\n            .setProtocols(request.protocols())\n            .setSessionTimeoutMs(request.sessionTimeoutMs())\n            .setRebalanceTimeoutMs(request.rebalanceTimeoutMs())\n            .setReason(request.reason());\n\n        JoinResult secondJoinResult = sendClassicGroupJoin(\n            secondRequest,\n            requireKnownMemberId\n        );\n\n        assertTrue(secondJoinResult.records.isEmpty());\n        List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = sleep(classicGroupInitialRebalanceDelayMs);\n        assertEquals(1, timeouts.size());\n        assertTrue(secondJoinResult.joinFuture.isDone());\n        assertEquals(Errors.NONE.code(), secondJoinResult.joinFuture.get().errorCode());\n\n        return secondJoinResult.joinFuture.get();\n    }\n\n    public JoinGroupResponseData joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    ) throws ExecutionException, InterruptedException {\n        return joinClassicGroupAndCompleteJoin(\n            request,\n            requireKnownMemberId,\n            supportSkippingAssignment,\n            classicGroupInitialRebalanceDelayMs\n        );\n    }\n\n    public JoinGroupResponseData joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment,\n        int advanceClockMs\n    ) throws ExecutionException, InterruptedException {\n        if (requireKnownMemberId && request.groupInstanceId().isEmpty()) {\n            return joinClassicGroupAsDynamicMemberAndCompleteJoin(request);\n        }\n\n        try {\n            JoinResult joinResult = sendClassicGroupJoin(\n                request,\n                requireKnownMemberId,\n                supportSkippingAssignment\n            );\n\n            sleep(advanceClockMs);\n            assertTrue(joinResult.joinFuture.isDone());\n            assertEquals(Errors.NONE.code(), joinResult.joinFuture.get().errorCode());\n            return joinResult.joinFuture.get();\n        } catch (Exception e) {\n            fail(\"Failed to due: \" + e.getMessage());\n        }\n        return null;\n    }\n\n    public SyncResult sendClassicGroupSync(SyncGroupRequestData request) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.SYNC_GROUP,\n                ApiKeys.SYNC_GROUP.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        CompletableFuture<SyncGroupResponseData> responseFuture = new CompletableFuture<>();\n\n        CoordinatorResult<Void, CoordinatorRecord> coordinatorResult = groupMetadataManager.classicGroupSync(\n            context,\n            request,\n            responseFuture\n        );\n\n        if (coordinatorResult.replayRecords()) {\n            coordinatorResult.records().forEach(this::replay);\n        }\n\n        return new SyncResult(responseFuture, coordinatorResult);\n    }\n\n    public RebalanceResult staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId\n    ) throws Exception {\n        return staticMembersJoinAndRebalance(\n            groupId,\n            leaderInstanceId,\n            followerInstanceId,\n            10000,\n            5000\n        );\n    }\n\n    public RebalanceResult staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId,\n        int rebalanceTimeoutMs,\n        int sessionTimeoutMs\n    ) throws Exception {\n        ClassicGroup group = createClassicGroup(groupId);\n\n        JoinGroupRequestData joinRequest = new JoinGroupRequestBuilder()\n            .withGroupId(groupId)\n            .withGroupInstanceId(leaderInstanceId)\n            .withMemberId(UNKNOWN_MEMBER_ID)\n            .withProtocolType(\"consumer\")\n            .withProtocolSuperset()\n            .withRebalanceTimeoutMs(rebalanceTimeoutMs)\n            .withSessionTimeoutMs(sessionTimeoutMs)\n            .build();\n\n        JoinResult leaderJoinResult = sendClassicGroupJoin(joinRequest);\n        JoinResult followerJoinResult = sendClassicGroupJoin(joinRequest.setGroupInstanceId(followerInstanceId));\n\n        assertTrue(leaderJoinResult.records.isEmpty());\n        assertTrue(followerJoinResult.records.isEmpty());\n        assertFalse(leaderJoinResult.joinFuture.isDone());\n        assertFalse(followerJoinResult.joinFuture.isDone());\n\n        // The goal for two timer advance is to let first group initial join complete and set newMemberAdded flag to false. Next advance is\n        // to trigger the rebalance as needed for follower delayed join. One large time advance won't help because we could only populate one\n        // delayed join from purgatory and the new delayed op is created at that time and never be triggered.\n        assertNoOrEmptyResult(sleep(classicGroupInitialRebalanceDelayMs));\n        assertNoOrEmptyResult(sleep(classicGroupInitialRebalanceDelayMs));\n\n        assertTrue(leaderJoinResult.joinFuture.isDone());\n        assertTrue(followerJoinResult.joinFuture.isDone());\n        assertEquals(Errors.NONE.code(), leaderJoinResult.joinFuture.get().errorCode());\n        assertEquals(Errors.NONE.code(), followerJoinResult.joinFuture.get().errorCode());\n        assertEquals(1, leaderJoinResult.joinFuture.get().generationId());\n        assertEquals(1, followerJoinResult.joinFuture.get().generationId());\n        assertEquals(2, group.numMembers());\n        assertEquals(1, group.generationId());\n        assertTrue(group.isInState(COMPLETING_REBALANCE));\n\n        String leaderId = leaderJoinResult.joinFuture.get().memberId();\n        String followerId = followerJoinResult.joinFuture.get().memberId();\n        List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment = new ArrayList<>();\n        assignment.add(new SyncGroupRequestData.SyncGroupRequestAssignment().setMemberId(leaderId)\n                                                                            .setAssignment(new byte[]{1}));\n        assignment.add(new SyncGroupRequestData.SyncGroupRequestAssignment().setMemberId(followerId)\n                                                                            .setAssignment(new byte[]{2}));\n\n        SyncGroupRequestData syncRequest = new SyncGroupRequestBuilder()\n            .withGroupId(groupId)\n            .withGroupInstanceId(leaderInstanceId)\n            .withMemberId(leaderId)\n            .withGenerationId(1)\n            .withAssignment(assignment)\n            .build();\n\n        SyncResult leaderSyncResult = sendClassicGroupSync(syncRequest);\n\n        // The generated record should contain the new assignment.\n        Map<String, byte[]> groupAssignment = assignment.stream().collect(Collectors.toMap(\n            SyncGroupRequestData.SyncGroupRequestAssignment::memberId, SyncGroupRequestData.SyncGroupRequestAssignment::assignment\n        ));\n        assertEquals(\n            Collections.singletonList(\n                CoordinatorRecordHelpers.newGroupMetadataRecord(group, groupAssignment, MetadataVersion.latestTesting())),\n            leaderSyncResult.records\n        );\n\n        // Simulate a successful write to the log.\n        leaderSyncResult.appendFuture.complete(null);\n\n        assertTrue(leaderSyncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), leaderSyncResult.syncFuture.get().errorCode());\n        assertTrue(group.isInState(STABLE));\n\n        SyncResult followerSyncResult = sendClassicGroupSync(\n            syncRequest.setGroupInstanceId(followerInstanceId)\n                       .setMemberId(followerId)\n                       .setAssignments(Collections.emptyList())\n        );\n\n        assertTrue(followerSyncResult.records.isEmpty());\n        assertTrue(followerSyncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), followerSyncResult.syncFuture.get().errorCode());\n        assertTrue(group.isInState(STABLE));\n\n        assertEquals(2, group.numMembers());\n        assertEquals(1, group.generationId());\n\n        return new RebalanceResult(\n            1,\n            leaderId,\n            leaderSyncResult.syncFuture.get().assignment(),\n            followerId,\n            followerSyncResult.syncFuture.get().assignment()\n        );\n    }\n\n    public PendingMemberGroupResult setupGroupWithPendingMember(ClassicGroup group) throws Exception {\n        // Add the first member\n        JoinGroupRequestData joinRequest = new JoinGroupRequestBuilder()\n            .withGroupId(group.groupId())\n            .withMemberId(UNKNOWN_MEMBER_ID)\n            .withDefaultProtocolTypeAndProtocols()\n            .withRebalanceTimeoutMs(10000)\n            .withSessionTimeoutMs(5000)\n            .build();\n\n        JoinGroupResponseData leaderJoinResponse =\n            joinClassicGroupAsDynamicMemberAndCompleteJoin(joinRequest);\n\n        List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment = new ArrayList<>();\n        assignment.add(new SyncGroupRequestData.SyncGroupRequestAssignment().setMemberId(leaderJoinResponse.memberId()));\n        SyncGroupRequestData syncRequest = new SyncGroupRequestBuilder()\n            .withGroupId(group.groupId())\n            .withMemberId(leaderJoinResponse.memberId())\n            .withGenerationId(leaderJoinResponse.generationId())\n            .withAssignment(assignment)\n            .build();\n\n        SyncResult syncResult = sendClassicGroupSync(syncRequest);\n\n        // Now the group is stable, with the one member that joined above\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newGroupMetadataRecord(group, group.groupAssignment(), MetadataVersion.latestTesting())),\n            syncResult.records\n        );\n        // Simulate a successful write to log.\n        syncResult.appendFuture.complete(null);\n\n        assertTrue(syncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), syncResult.syncFuture.get().errorCode());\n\n        // Start the join for the second member\n        JoinResult followerJoinResult = sendClassicGroupJoin(\n            joinRequest.setMemberId(UNKNOWN_MEMBER_ID)\n        );\n\n        assertTrue(followerJoinResult.records.isEmpty());\n        assertFalse(followerJoinResult.joinFuture.isDone());\n\n        JoinResult leaderJoinResult = sendClassicGroupJoin(\n            joinRequest.setMemberId(leaderJoinResponse.memberId())\n        );\n\n        assertTrue(leaderJoinResult.records.isEmpty());\n        assertTrue(group.isInState(COMPLETING_REBALANCE));\n        assertTrue(leaderJoinResult.joinFuture.isDone());\n        assertTrue(followerJoinResult.joinFuture.isDone());\n        assertEquals(Errors.NONE.code(), leaderJoinResult.joinFuture.get().errorCode());\n        assertEquals(Errors.NONE.code(), followerJoinResult.joinFuture.get().errorCode());\n        assertEquals(leaderJoinResult.joinFuture.get().generationId(), followerJoinResult.joinFuture.get().generationId());\n        assertEquals(leaderJoinResponse.memberId(), leaderJoinResult.joinFuture.get().leader());\n        assertEquals(leaderJoinResponse.memberId(), followerJoinResult.joinFuture.get().leader());\n\n        int nextGenerationId = leaderJoinResult.joinFuture.get().generationId();\n        String followerId = followerJoinResult.joinFuture.get().memberId();\n\n        // Stabilize the group\n        syncResult = sendClassicGroupSync(syncRequest.setGenerationId(nextGenerationId));\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newGroupMetadataRecord(group, group.groupAssignment(), MetadataVersion.latestTesting())),\n            syncResult.records\n        );\n        // Simulate a successful write to log.\n        syncResult.appendFuture.complete(null);\n\n        assertTrue(syncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), syncResult.syncFuture.get().errorCode());\n        assertTrue(group.isInState(STABLE));\n\n        // Re-join an existing member, to transition the group to PreparingRebalance state.\n        leaderJoinResult = sendClassicGroupJoin(\n            joinRequest.setMemberId(leaderJoinResponse.memberId()));\n\n        assertTrue(leaderJoinResult.records.isEmpty());\n        assertFalse(leaderJoinResult.joinFuture.isDone());\n        assertTrue(group.isInState(PREPARING_REBALANCE));\n\n        // Create a pending member in the group\n        JoinResult pendingMemberJoinResult = sendClassicGroupJoin(\n            joinRequest\n                .setMemberId(UNKNOWN_MEMBER_ID)\n                .setSessionTimeoutMs(2500),\n            true\n        );\n\n        assertTrue(pendingMemberJoinResult.records.isEmpty());\n        assertTrue(pendingMemberJoinResult.joinFuture.isDone());\n        assertEquals(Errors.MEMBER_ID_REQUIRED.code(), pendingMemberJoinResult.joinFuture.get().errorCode());\n        assertEquals(1, group.numPendingJoinMembers());\n\n        // Re-join the second existing member\n        followerJoinResult = sendClassicGroupJoin(\n            joinRequest.setMemberId(followerId).setSessionTimeoutMs(5000)\n        );\n\n        assertTrue(followerJoinResult.records.isEmpty());\n        assertFalse(followerJoinResult.joinFuture.isDone());\n        assertTrue(group.isInState(PREPARING_REBALANCE));\n        assertEquals(2, group.numMembers());\n        assertEquals(1, group.numPendingJoinMembers());\n\n        return new PendingMemberGroupResult(\n            leaderJoinResponse.memberId(),\n            followerId,\n            pendingMemberJoinResult.joinFuture.get()\n        );\n    }\n\n    public void verifySessionExpiration(ClassicGroup group, int timeoutMs) {\n        Set<String> expectedHeartbeatKeys = group.allMembers().stream()\n                                                 .map(member -> classicGroupHeartbeatKey(group.groupId(), member.memberId())).collect(Collectors.toSet());\n\n        // Member should be removed as session expires.\n        List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = sleep(timeoutMs);\n        List<CoordinatorRecord> expectedRecords = Collections.singletonList(newGroupMetadataRecord(\n            group.groupId(),\n            new GroupMetadataValue()\n                .setMembers(Collections.emptyList())\n                .setGeneration(group.generationId())\n                .setLeader(null)\n                .setProtocolType(\"consumer\")\n                .setProtocol(null)\n                .setCurrentStateTimestamp(time.milliseconds()),\n            MetadataVersion.latestTesting()\n        ));\n\n\n        Set<String> heartbeatKeys = timeouts.stream().map(timeout -> timeout.key).collect(Collectors.toSet());\n        assertEquals(expectedHeartbeatKeys, heartbeatKeys);\n\n        // Only the last member leaving the group should result in the empty group metadata record.\n        int timeoutsSize = timeouts.size();\n        assertEquals(expectedRecords, timeouts.get(timeoutsSize - 1).result.records());\n        assertNoOrEmptyResult(timeouts.subList(0, timeoutsSize - 1));\n        assertTrue(group.isInState(EMPTY));\n        assertEquals(0, group.numMembers());\n    }\n\n    public CoordinatorResult<HeartbeatResponseData, CoordinatorRecord> sendClassicGroupHeartbeat(\n        HeartbeatRequestData request\n    ) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.HEARTBEAT,\n                ApiKeys.HEARTBEAT.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        return groupMetadataManager.classicGroupHeartbeat(\n            context,\n            request\n        );\n    }\n\n    public List<ListGroupsResponseData.ListedGroup> sendListGroups(List<String> statesFilter, List<String> typesFilter) {\n        Set<String> statesFilterSet = new HashSet<>(statesFilter);\n        Set<String> typesFilterSet = new HashSet<>(typesFilter);\n        return groupMetadataManager.listGroups(statesFilterSet, typesFilterSet, lastCommittedOffset);\n    }\n\n    public List<ConsumerGroupDescribeResponseData.DescribedGroup> sendConsumerGroupDescribe(List<String> groupIds) {\n        return groupMetadataManager.consumerGroupDescribe(groupIds, lastCommittedOffset);\n    }\n\n    public List<DescribeGroupsResponseData.DescribedGroup> describeGroups(List<String> groupIds) {\n        return groupMetadataManager.describeGroups(groupIds, lastCommittedOffset);\n    }\n\n    public List<ShareGroupDescribeResponseData.DescribedGroup> sendShareGroupDescribe(List<String> groupIds) {\n        return groupMetadataManager.shareGroupDescribe(groupIds, lastCommittedOffset);\n    }\n\n    public void verifyHeartbeat(\n        String groupId,\n        JoinGroupResponseData joinResponse,\n        Errors expectedError\n    ) {\n        HeartbeatRequestData request = new HeartbeatRequestData()\n            .setGroupId(groupId)\n            .setMemberId(joinResponse.memberId())\n            .setGenerationId(joinResponse.generationId());\n\n        if (expectedError == Errors.UNKNOWN_MEMBER_ID) {\n            assertThrows(UnknownMemberIdException.class, () -> sendClassicGroupHeartbeat(request));\n        } else {\n            HeartbeatResponseData response = sendClassicGroupHeartbeat(request).response();\n            assertEquals(expectedError.code(), response.errorCode());\n        }\n    }\n\n    public List<JoinGroupResponseData> joinWithNMembers(\n        String groupId,\n        int numMembers,\n        int rebalanceTimeoutMs,\n        int sessionTimeoutMs\n    ) {\n        ClassicGroup group = createClassicGroup(groupId);\n        boolean requireKnownMemberId = true;\n\n        // First join requests\n        JoinGroupRequestData request = new JoinGroupRequestBuilder()\n            .withGroupId(groupId)\n            .withMemberId(UNKNOWN_MEMBER_ID)\n            .withDefaultProtocolTypeAndProtocols()\n            .withRebalanceTimeoutMs(rebalanceTimeoutMs)\n            .withSessionTimeoutMs(sessionTimeoutMs)\n            .build();\n\n        List<String> memberIds = IntStream.range(0, numMembers).mapToObj(i -> {\n            JoinResult joinResult = sendClassicGroupJoin(request, requireKnownMemberId);\n\n            assertTrue(joinResult.records.isEmpty());\n            assertTrue(joinResult.joinFuture.isDone());\n\n            try {\n                return joinResult.joinFuture.get().memberId();\n            } catch (Exception e) {\n                fail(\"Unexpected exception: \" + e.getMessage());\n            }\n            return null;\n        }).collect(Collectors.toList());\n\n        // Second join requests\n        List<CompletableFuture<JoinGroupResponseData>> secondJoinFutures = IntStream.range(0, numMembers).mapToObj(i -> {\n            JoinResult joinResult = sendClassicGroupJoin(request.setMemberId(memberIds.get(i)), requireKnownMemberId);\n\n            assertTrue(joinResult.records.isEmpty());\n            assertFalse(joinResult.joinFuture.isDone());\n\n            return joinResult.joinFuture;\n        }).collect(Collectors.toList());\n\n        // Advance clock by initial rebalance delay.\n        assertNoOrEmptyResult(sleep(classicGroupInitialRebalanceDelayMs));\n        secondJoinFutures.forEach(future -> assertFalse(future.isDone()));\n        // Advance clock by rebalance timeout to complete join phase.\n        assertNoOrEmptyResult(sleep(rebalanceTimeoutMs));\n\n        List<JoinGroupResponseData> joinResponses = secondJoinFutures.stream().map(future -> {\n            assertTrue(future.isDone());\n            try {\n                assertEquals(Errors.NONE.code(), future.get().errorCode());\n                return future.get();\n            } catch (Exception e) {\n                fail(\"Unexpected exception: \" + e.getMessage());\n            }\n            return null;\n        }).collect(Collectors.toList());\n\n        assertEquals(numMembers, group.numMembers());\n        assertTrue(group.isInState(COMPLETING_REBALANCE));\n\n        return joinResponses;\n    }\n\n    public CoordinatorResult<LeaveGroupResponseData, CoordinatorRecord> sendClassicGroupLeave(\n        LeaveGroupRequestData request\n    ) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.LEAVE_GROUP,\n                ApiKeys.LEAVE_GROUP.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        return groupMetadataManager.classicGroupLeave(context, request);\n    }\n\n    public void verifyDescribeGroupsReturnsDeadGroup(String groupId) {\n        List<DescribeGroupsResponseData.DescribedGroup> describedGroups =\n            describeGroups(Collections.singletonList(groupId));\n\n        assertEquals(\n            Collections.singletonList(new DescribeGroupsResponseData.DescribedGroup()\n                .setGroupId(groupId)\n                .setGroupState(DEAD.toString())\n            ),\n            describedGroups\n        );\n    }\n\n    public void verifyClassicGroupSyncToConsumerGroup(\n        String groupId,\n        String memberId,\n        int generationId,\n        String protocolName,\n        String protocolType,\n        List<TopicPartition> topicPartitionList,\n        short version\n    ) throws Exception {\n        GroupMetadataManagerTestContext.SyncResult syncResult = sendClassicGroupSync(\n            new GroupMetadataManagerTestContext.SyncGroupRequestBuilder()\n                .withGroupId(groupId)\n                .withMemberId(memberId)\n                .withGenerationId(generationId)\n                .withProtocolName(protocolName)\n                .withProtocolType(protocolType)\n                .build()\n        );\n        assertEquals(Collections.emptyList(), syncResult.records);\n        assertFalse(syncResult.syncFuture.isDone());\n\n        // Simulate a successful write to log.\n        syncResult.appendFuture.complete(null);\n        assertSyncGroupResponseEquals(\n            new SyncGroupResponseData()\n                .setProtocolType(protocolType)\n                .setProtocolName(protocolName)\n                .setAssignment(ConsumerProtocol.serializeAssignment(\n                    new ConsumerPartitionAssignor.Assignment(topicPartitionList),\n                    version\n                ).array()),\n            syncResult.syncFuture.get()\n        );\n        assertSessionTimeout(groupId, memberId, 5000);\n        assertNoSyncTimeout(groupId, memberId);\n    }\n\n    public void verifyClassicGroupSyncToConsumerGroup(\n        String groupId,\n        String memberId,\n        int generationId,\n        String protocolName,\n        String protocolType,\n        List<TopicPartition> topicPartitionList\n    ) throws Exception {\n        verifyClassicGroupSyncToConsumerGroup(\n            groupId,\n            memberId,\n            generationId,\n            protocolName,\n            protocolType,\n            topicPartitionList,\n            ConsumerProtocolAssignment.HIGHEST_SUPPORTED_VERSION\n        );\n    }\n\n    private ApiMessage messageOrNull(ApiMessageAndVersion apiMessageAndVersion) {\n        if (apiMessageAndVersion == null) {\n            return null;\n        } else {\n            return apiMessageAndVersion.message();\n        }\n    }\n\n    public void replay(\n        CoordinatorRecord record\n    ) {\n        replay(record, null);\n    }\n\n    public void replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    ) {\n        ApiMessageAndVersion key = record.key();\n        ApiMessageAndVersion value = record.value();\n\n        if (key == null) {\n            throw new IllegalStateException(\"Received a null key in \" + record);\n        }\n\n        switch (key.version()) {\n            case GroupMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (GroupMetadataKey) key.message(),\n                    (GroupMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            case ConsumerGroupMemberMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupMemberMetadataKey) key.message(),\n                    (ConsumerGroupMemberMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            case ConsumerGroupMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupMetadataKey) key.message(),\n                    (ConsumerGroupMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            case ConsumerGroupPartitionMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupPartitionMetadataKey) key.message(),\n                    (ConsumerGroupPartitionMetadataValue) messageOrNull(value),\n                    groupType != null ? groupType : GroupType.CONSUMER\n                );\n                break;\n\n            case ConsumerGroupTargetAssignmentMemberKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupTargetAssignmentMemberKey) key.message(),\n                    (ConsumerGroupTargetAssignmentMemberValue) messageOrNull(value),\n                    groupType != null ? groupType : GroupType.CONSUMER\n                );\n                break;\n\n            case ConsumerGroupTargetAssignmentMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupTargetAssignmentMetadataKey) key.message(),\n                    (ConsumerGroupTargetAssignmentMetadataValue) messageOrNull(value),\n                    groupType != null ? groupType : GroupType.CONSUMER\n                );\n                break;\n\n            case ConsumerGroupCurrentMemberAssignmentKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupCurrentMemberAssignmentKey) key.message(),\n                    (ConsumerGroupCurrentMemberAssignmentValue) messageOrNull(value),\n                    groupType != null ? groupType : GroupType.CONSUMER\n                );\n                break;\n\n            case ShareGroupMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ShareGroupMetadataKey) key.message(),\n                    (ShareGroupMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            case ShareGroupMemberMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ShareGroupMemberMetadataKey) key.message(),\n                    (ShareGroupMemberMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            default:\n                throw new IllegalStateException(\"Received an unknown record type \" + key.version()\n                    + \" in \" + record);\n        }\n\n        lastWrittenOffset++;\n        idempotentCreateSnapshot(lastWrittenOffset);\n    }\n\n    private void idempotentCreateSnapshot(long epoch) {\n        snapshotRegistry.getOrCreateSnapshot(epoch);\n    }\n\n    void onUnloaded() {\n        groupMetadataManager.onUnloaded();\n    }\n}",
                "methodCount": 87
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 12,
                "candidates": [
                    {
                        "lineStart": 121,
                        "lineEnd": 124,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method assertNoOrEmptyResult to class Assertions",
                        "description": "move method assertNoOrEmptyResult to PsiClass:Assertions\nRationale: The method assertNoOrEmptyResult() is heavily focused on validating conditions and results, which aligns with the existing responsibilities of the Assertions class. This class already contains several other assertion methods, so adding assertNoOrEmptyResult() here maintains consistency and centralizes all assertion-related functionality in one place. Additionally, the method uses assertTrue() and assertEquals(), which are typical assertion operations, further supporting its placement in the Assertions class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1517,
                        "lineEnd": 1519,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() directly calls the getOrCreateSnapshot() method of SnapshotRegistry and does not use any members specific to the existing class where it is currently implemented. Moving idempotentCreateSnapshot() to SnapshotRegistry would enhance cohesion, reduce coupling between classes, and place the method closer to the relevant data and functionality it operates on. SnapshotRegistry already handles snapshot creation and management, making it the most suitable place for this method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 726,
                        "lineEnd": 728,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method createClassicGroup to class GroupMetadataManager",
                        "description": "Move method createClassicGroup to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The method `createClassicGroup` is closely tied to the internal data structures and functionalities managed by the `GroupMetadataManager` class, including the creation and fetching processes of classic groups. Specifically, this method involves getting or possibly creating a `ClassicGroup`, which directly interacts with the `GroupMetadataManager`'s methods. Therefore, it is logical for this method to be relocated to the `GroupMetadataManager` class, given that it pertains to the core responsibilities and operations within this manager class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1521,
                        "lineEnd": 1523,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method onUnloaded to class GroupMetadataManager",
                        "description": "Move method onUnloaded to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The method onUnloaded() interacts directly with the groupMetadataManager instance, making it tightly coupled to the GroupMetadataManager class. By moving this method to GroupMetadataManager, we improve encapsulation and ensure that group state unloading logic remains within the class responsible for managing group metadata. This leads to better code structure and maintainability.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 167,
                        "lineEnd": 183,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method newGroupMetadataRecord to class CoordinatorRecordHelpers",
                        "description": "move method newGroupMetadataRecord to PsiClass:CoordinatorRecordHelpers\nRationale: The method newGroupMetadataRecord is a utility function that creates a CoordinatorRecord for group metadata. The CoordinatorRecordHelpers class already houses similar methods, making it a cohesive place to centralize this logic. This method generates records, aligning perfectly with the existing set of record-creation utilities in CoordinatorRecordHelpers. Therefore, moving the method here keeps related functionalities together and enhances code maintainability.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1346,
                        "lineEnd": 1357,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method verifyDescribeGroupsReturnsDeadGroup to class GroupCoordinatorMetricsShard",
                        "description": "Move method verifyDescribeGroupsReturnsDeadGroup to org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsShard\nRationale: The method `verifyDescribeGroupsReturnsDeadGroup` is directly related to group states and their descriptions, which aligns closely with the responsibilities of the `GroupCoordinatorMetricsShard` class. This class contains logic for handling consumer groups and their states (including DEAD state), making it the natural place for a method that verifies the state of described groups.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1229,
                        "lineEnd": 1231,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method sendConsumerGroupDescribe to class GroupMetadataManager",
                        "description": "Move method sendConsumerGroupDescribe to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The method `sendConsumerGroupDescribe` directly interacts with the `GroupMetadataManager` to invoke the `consumerGroupDescribe` method, making GroupMetadataManager the more appropriate class for this method instead of a separate class. This consolidation of functionality reduces complexity, allowing easier maintenance and better encapsulation.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1237,
                        "lineEnd": 1239,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method sendShareGroupDescribe to class GroupMetadataManager",
                        "description": "Move method sendShareGroupDescribe to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The method `sendShareGroupDescribe` heavily relies on the `shareGroupDescribe` method provided by `GroupMetadataManager` to fetch the described share groups. Since `GroupMetadataManager` already manages and describes groups, including share groups, it is more appropriate to move this method there. This avoids redundancy and ensures that all group metadata related operations are logically centralized within the `GroupMetadataManager` class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 1233,
                        "lineEnd": 1235,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method describeGroups to class GroupMetadataManager",
                        "description": "Move method describeGroups to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The method describeGroups() heavily relies on the functionality provided by GroupMetadataManager, specifically the groupMetadataManager.describeGroups(...) method. The GroupMetadataManager class handles various group-related operations such as replay, request handling, and managing group states, all of which align perfectly with the functionality of the describeGroups() method. Moving the method to GroupMetadataManager consolidates related operations in one class and adheres to the principle of cohesive class design.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 564,
                        "lineEnd": 572,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method consumerGroupMemberState to class GroupMetadataManager",
                        "description": "Move method consumerGroupMemberState to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The method `consumerGroupMemberState` retrieves the state of a consumer group member, directly interacting with `groupMetadataManager` to find or create the member. As such, it's a natural extension of the existing responsibilities of the `GroupMetadataManager` class. Keeping it within `GroupMetadataManager` centralizes group metadata operations, aligning with the class's purpose of managing group metadata, including the state of consumer group members.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 556,
                        "lineEnd": 562,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method shareGroupState to class GroupMetadataManager",
                        "description": "Move method shareGroupState to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The method shareGroupState() heavily relies on the groupMetadataManager, which is an instance of GroupMetadataManager. The method fetches a ShareGroup from GroupMetadataManager using groupMetadataManager.shareGroup(groupId) call and then retrieves the state of that ShareGroup. Given this dependency and the ownership of the relevant data and operations within GroupMetadataManager, it is logical and efficient to move the shareGroupState() method to the GroupMetadataManager class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 548,
                        "lineEnd": 554,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method consumerGroupState to class GroupMetadataManager",
                        "description": "Move method consumerGroupState to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: null",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "assertNoOrEmptyResult",
                            "method_signature": "public static void assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)",
                            "target_class": "MockCoordinatorTimer",
                            "rationale": "This method is specifically checking the results of timeouts, which are related to the MockCoordinatorTimer. It would be more logical for it to reside in MockCoordinatorTimer to keep related functionality together."
                        },
                        {
                            "method_name": "toProtocols",
                            "method_signature": "public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toProtocols(String... protocolNames)",
                            "target_class": "JoinGroupRequestData",
                            "rationale": "This method is constructing JoinGroupRequestProtocolCollection, which is specific to JoinGroupRequestData. It is more appropriate to be in JoinGroupRequestData to encapsulate the logic related to its own protocol construction."
                        },
                        {
                            "method_name": "toConsumerProtocol",
                            "method_signature": "public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toConsumerProtocol(List<String> topicNames, List<TopicPartition> ownedPartitions, short version)",
                            "target_class": "JoinGroupRequestData",
                            "rationale": "These methods are creating a collection of JoinGroupRequestProtocols related directly to JoinGroupRequestData. Housing this logic within JoinGroupRequestData improves cohesion by keeping functionality specific to handling its protocols in the same class."
                        },
                        {
                            "method_name": "newGroupMetadataRecord",
                            "method_signature": "public static CoordinatorRecord newGroupMetadataRecord(String groupId, GroupMetadataValue value, MetadataVersion metadataVersion)",
                            "target_class": "CoordinatorRecordHelpers",
                            "rationale": "The method's main purpose is to create a CoordinatorRecord by combining multiple entities, making it more appropriate to place it in a utility class like CoordinatorRecordHelpers to follow the Single Responsibility Principle and reduce complexity in the test context class."
                        }
                    ],
                    "llm_response_time": 6262
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "toProtocols",
                            "method_signature": "public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toProtocols(String... protocolNames)",
                            "target_class": "JoinGroupRequestData",
                            "rationale": "The toProtocols method is directly involved in creating JoinGroupRequestProtocolCollection which is part of JoinGroupRequestData. It makes sense for this method to be hosted by the class that is its primary subject."
                        },
                        {
                            "method_name": "toConsumerProtocol",
                            "method_signature": "public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toConsumerProtocol(List<String> topicNames, List<TopicPartition> ownedPartitions)",
                            "target_class": "JoinGroupRequestData",
                            "rationale": "The toConsumerProtocol method deals with creating JoinGroupRequestProtocolCollection, which is a responsibility closely related to JoinGroupRequestData. Moving it there would centralize all related functionalities."
                        },
                        {
                            "method_name": "toConsumerProtocol",
                            "method_signature": "public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toConsumerProtocol(List<String> topicNames, List<TopicPartition> ownedPartitions, short version)",
                            "target_class": "JoinGroupRequestData",
                            "rationale": "Similar to the above, this overloaded version of the method also focuses on creating JoinGroupRequestProtocolCollection and should thus reside in the JoinGroupRequestData class."
                        },
                        {
                            "method_name": "newGroupMetadataRecord",
                            "method_signature": "public static CoordinatorRecord newGroupMetadataRecord(String groupId, GroupMetadataValue value, MetadataVersion metadataVersion)",
                            "target_class": "CoordinatorRecord",
                            "rationale": "The newGroupMetadataRecord method is specifically about creating CoordinatorRecords. Hence, it should be moved to the CoordinatorRecord class as this enhances cohesion."
                        }
                    ],
                    "llm_response_time": 5598
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "assertNoOrEmptyResult",
                            "method_signature": "public static void assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)",
                            "target_class": "CoordinatorResultTestHelper",
                            "rationale": "The assertNoOrEmptyResult method appears to be solely concerned with asserting conditions on a List of MockCoordinatorTimer.ExpiredTimeout objects, making it more generic and better suited for a helper class focused on testing CoordinatorResult behavior."
                        },
                        {
                            "method_name": "toProtocols",
                            "method_signature": "public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toProtocols(String... protocolNames)",
                            "target_class": "JoinGroupRequestHelper",
                            "rationale": "The toProtocols method generates protocol collections for JoinGroupRequest, making it better suited for a helper class focused on building or managing JoinGroupRequests."
                        },
                        {
                            "method_name": "toConsumerProtocol",
                            "method_signature": "public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toConsumerProtocol(List<String> topicNames, List<TopicPartition> ownedPartitions)",
                            "target_class": "JoinGroupRequestHelper",
                            "rationale": "The toConsumerProtocol method similarly constructs JoinGroupRequest protocol collections, and should be part of a class focused on building or managing JoinGroupRequests."
                        },
                        {
                            "method_name": "toConsumerProtocol",
                            "method_signature": "public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toConsumerProtocol(List<String> topicNames, List<TopicPartition> ownedPartitions, short version)",
                            "target_class": "JoinGroupRequestHelper",
                            "rationale": "As with the other toConsumerProtocol method, this one should also reside in a class focused on constructing or managing JoinGroupRequests."
                        },
                        {
                            "method_name": "newGroupMetadataRecord",
                            "method_signature": "public static CoordinatorRecord newGroupMetadataRecord(String groupId, GroupMetadataValue value, MetadataVersion metadataVersion)",
                            "target_class": "CoordinatorRecordFactory",
                            "rationale": "The newGroupMetadataRecord method constructs CoordinatorRecord instances based on group metadata, making it more appropriate for a factory class dedicated to creating CoordinatorRecord objects."
                        }
                    ],
                    "llm_response_time": 5535
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "assertNoOrEmptyResult",
                            "method_signature": "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "newGroupMetadataRecord",
                            "method_signature": "public static newGroupMetadataRecord(\n        String groupId,\n        GroupMetadataValue value,\n        MetadataVersion metadataVersion\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rollback",
                            "method_signature": "public rollback()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "consumerGroupState",
                            "method_signature": "public consumerGroupState(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shareGroupState",
                            "method_signature": "public shareGroupState(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "consumerGroupMemberState",
                            "method_signature": "public consumerGroupMemberState(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "consumerGroupHeartbeat",
                            "method_signature": "public consumerGroupHeartbeat(\n        ConsumerGroupHeartbeatRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shareGroupHeartbeat",
                            "method_signature": "public shareGroupHeartbeat(\n        ShareGroupHeartbeatRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSessionTimeout",
                            "method_signature": "public assertSessionTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoSessionTimeout",
                            "method_signature": "public assertNoSessionTimeout(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertRebalanceTimeout",
                            "method_signature": "public assertRebalanceTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoRebalanceTimeout",
                            "method_signature": "public assertNoRebalanceTimeout(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertJoinTimeout",
                            "method_signature": "public assertJoinTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoJoinTimeout",
                            "method_signature": "public assertNoJoinTimeout(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSyncTimeout",
                            "method_signature": "public assertSyncTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoSyncTimeout",
                            "method_signature": "public assertNoSyncTimeout(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createClassicGroup",
                            "method_signature": " createClassicGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupJoin",
                            "method_signature": "public sendClassicGroupJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinClassicGroupAsDynamicMemberAndCompleteRebalance",
                            "method_signature": "public joinClassicGroupAsDynamicMemberAndCompleteRebalance(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinClassicGroupAsDynamicMemberAndCompleteJoin",
                            "method_signature": "public joinClassicGroupAsDynamicMemberAndCompleteJoin(\n        JoinGroupRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinClassicGroupAndCompleteJoin",
                            "method_signature": "public joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment,\n        int advanceClockMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupSync",
                            "method_signature": "public sendClassicGroupSync(SyncGroupRequestData request)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "staticMembersJoinAndRebalance",
                            "method_signature": "public staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId,\n        int rebalanceTimeoutMs,\n        int sessionTimeoutMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifySessionExpiration",
                            "method_signature": "public verifySessionExpiration(ClassicGroup group, int timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupHeartbeat",
                            "method_signature": "public sendClassicGroupHeartbeat(\n        HeartbeatRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendListGroups",
                            "method_signature": "public sendListGroups(List<String> statesFilter, List<String> typesFilter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendConsumerGroupDescribe",
                            "method_signature": "public sendConsumerGroupDescribe(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "describeGroups",
                            "method_signature": "public describeGroups(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendShareGroupDescribe",
                            "method_signature": "public sendShareGroupDescribe(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyHeartbeat",
                            "method_signature": "public verifyHeartbeat(\n        String groupId,\n        JoinGroupResponseData joinResponse,\n        Errors expectedError\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinWithNMembers",
                            "method_signature": "public joinWithNMembers(\n        String groupId,\n        int numMembers,\n        int rebalanceTimeoutMs,\n        int sessionTimeoutMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupLeave",
                            "method_signature": "public sendClassicGroupLeave(\n        LeaveGroupRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyDescribeGroupsReturnsDeadGroup",
                            "method_signature": "public verifyDescribeGroupsReturnsDeadGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUnloaded",
                            "method_signature": " onUnloaded()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoOrEmptyResult",
                            "method_signature": "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createClassicGroup",
                            "method_signature": " createClassicGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUnloaded",
                            "method_signature": " onUnloaded()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "newGroupMetadataRecord",
                            "method_signature": "public static newGroupMetadataRecord(\n        String groupId,\n        GroupMetadataValue value,\n        MetadataVersion metadataVersion\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyDescribeGroupsReturnsDeadGroup",
                            "method_signature": "public verifyDescribeGroupsReturnsDeadGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendConsumerGroupDescribe",
                            "method_signature": "public sendConsumerGroupDescribe(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendShareGroupDescribe",
                            "method_signature": "public sendShareGroupDescribe(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "describeGroups",
                            "method_signature": "public describeGroups(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifySessionExpiration",
                            "method_signature": "public verifySessionExpiration(ClassicGroup group, int timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "consumerGroupMemberState",
                            "method_signature": "public consumerGroupMemberState(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shareGroupState",
                            "method_signature": "public shareGroupState(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "consumerGroupState",
                            "method_signature": "public consumerGroupState(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "public replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    )": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.20233735874653516
                    },
                    "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)": {
                        "first": {
                            "method_name": "assertNoOrEmptyResult",
                            "method_signature": "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.29842338035812693
                    },
                    "private idempotentCreateSnapshot(long epoch)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3393231261521954
                    },
                    " createClassicGroup(String groupId)": {
                        "first": {
                            "method_name": "createClassicGroup",
                            "method_signature": " createClassicGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.36832426033396204
                    },
                    " onUnloaded()": {
                        "first": {
                            "method_name": "onUnloaded",
                            "method_signature": " onUnloaded()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.37305123419653774
                    },
                    "public static newGroupMetadataRecord(\n        String groupId,\n        GroupMetadataValue value,\n        MetadataVersion metadataVersion\n    )": {
                        "first": {
                            "method_name": "newGroupMetadataRecord",
                            "method_signature": "public static newGroupMetadataRecord(\n        String groupId,\n        GroupMetadataValue value,\n        MetadataVersion metadataVersion\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3770557066232308
                    },
                    "public build()": {
                        "first": {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4057646903399639
                    },
                    "public verifyDescribeGroupsReturnsDeadGroup(String groupId)": {
                        "first": {
                            "method_name": "verifyDescribeGroupsReturnsDeadGroup",
                            "method_signature": "public verifyDescribeGroupsReturnsDeadGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4155351154151203
                    },
                    "public sendConsumerGroupDescribe(List<String> groupIds)": {
                        "first": {
                            "method_name": "sendConsumerGroupDescribe",
                            "method_signature": "public sendConsumerGroupDescribe(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4243861739367998
                    },
                    "public sendShareGroupDescribe(List<String> groupIds)": {
                        "first": {
                            "method_name": "sendShareGroupDescribe",
                            "method_signature": "public sendShareGroupDescribe(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4243861739367998
                    },
                    "public describeGroups(List<String> groupIds)": {
                        "first": {
                            "method_name": "describeGroups",
                            "method_signature": "public describeGroups(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.42550479981242395
                    },
                    "public verifySessionExpiration(ClassicGroup group, int timeoutMs)": {
                        "first": {
                            "method_name": "verifySessionExpiration",
                            "method_signature": "public verifySessionExpiration(ClassicGroup group, int timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.43455985118824997
                    },
                    "public consumerGroupMemberState(\n        String groupId,\n        String memberId\n    )": {
                        "first": {
                            "method_name": "consumerGroupMemberState",
                            "method_signature": "public consumerGroupMemberState(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4419783503391342
                    },
                    "public shareGroupState(\n        String groupId\n    )": {
                        "first": {
                            "method_name": "shareGroupState",
                            "method_signature": "public shareGroupState(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.46646226883862885
                    },
                    "public consumerGroupState(\n        String groupId\n    )": {
                        "first": {
                            "method_name": "consumerGroupState",
                            "method_signature": "public consumerGroupState(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4674332609203864
                    }
                },
                "voyage": {
                    "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)": {
                        "first": {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.18947883152560974
                    },
                    "public rollback()": {
                        "first": {
                            "method_name": "rollback",
                            "method_signature": "public rollback()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3080694831365844
                    },
                    "public commit()": {
                        "first": {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.34733226922160326
                    },
                    " onUnloaded()": {
                        "first": {
                            "method_name": "onUnloaded",
                            "method_signature": " onUnloaded()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3574108287028482
                    },
                    "private idempotentCreateSnapshot(long epoch)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.37773333770696077
                    },
                    "public sendListGroups(List<String> statesFilter, List<String> typesFilter)": {
                        "first": {
                            "method_name": "sendListGroups",
                            "method_signature": "public sendListGroups(List<String> statesFilter, List<String> typesFilter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.43766084666859456
                    },
                    "public shareGroupState(\n        String groupId\n    )": {
                        "first": {
                            "method_name": "shareGroupState",
                            "method_signature": "public shareGroupState(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4467376621105366
                    },
                    "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)": {
                        "first": {
                            "method_name": "assertNoOrEmptyResult",
                            "method_signature": "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.45193355814609404
                    },
                    "public sendShareGroupDescribe(List<String> groupIds)": {
                        "first": {
                            "method_name": "sendShareGroupDescribe",
                            "method_signature": "public sendShareGroupDescribe(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.45536525315108217
                    },
                    "public sleep(long ms)": {
                        "first": {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4571983137680928
                    },
                    "public describeGroups(List<String> groupIds)": {
                        "first": {
                            "method_name": "describeGroups",
                            "method_signature": "public describeGroups(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4829902974817234
                    },
                    "public sendConsumerGroupDescribe(List<String> groupIds)": {
                        "first": {
                            "method_name": "sendConsumerGroupDescribe",
                            "method_signature": "public sendConsumerGroupDescribe(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.49439387537473556
                    },
                    "public consumerGroupState(\n        String groupId\n    )": {
                        "first": {
                            "method_name": "consumerGroupState",
                            "method_signature": "public consumerGroupState(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5088322097705229
                    },
                    "public consumerGroupMemberState(\n        String groupId,\n        String memberId\n    )": {
                        "first": {
                            "method_name": "consumerGroupMemberState",
                            "method_signature": "public consumerGroupMemberState(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.535854751099399
                    },
                    " createClassicGroup(String groupId)": {
                        "first": {
                            "method_name": "createClassicGroup",
                            "method_signature": " createClassicGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5549160331221453
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)",
                        "public build()",
                        "public sendConsumerGroupDescribe(List<String> groupIds)",
                        "public sendShareGroupDescribe(List<String> groupIds)",
                        "public describeGroups(List<String> groupIds)",
                        "public verifySessionExpiration(ClassicGroup group, int timeoutMs)",
                        "public verifyDescribeGroupsReturnsDeadGroup(String groupId)",
                        "private idempotentCreateSnapshot(long epoch)"
                    ],
                    "llm_response_time": 4356
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "public replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    )",
                        "private idempotentCreateSnapshot(long epoch)",
                        " onUnloaded()",
                        " createClassicGroup(String groupId)",
                        "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)"
                    ],
                    "llm_response_time": 5222
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "public replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    )",
                        "private idempotentCreateSnapshot(long epoch)",
                        "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)"
                    ],
                    "llm_response_time": 4596
                },
                "voyage": {
                    "priority_method_names": [
                        "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)",
                        " createClassicGroup(String groupId)",
                        "public sendListGroups(List<String> statesFilter, List<String> typesFilter)",
                        "public sendShareGroupDescribe(List<String> groupIds)",
                        "public sendConsumerGroupDescribe(List<String> groupIds)",
                        "public describeGroups(List<String> groupIds)",
                        "public rollback()",
                        "public commit()",
                        "public sleep(long ms)",
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "private idempotentCreateSnapshot(long epoch)",
                        " onUnloaded()"
                    ],
                    "llm_response_time": 2847
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(long epoch)",
                        " onUnloaded()",
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "public rollback()",
                        "public commit()"
                    ],
                    "llm_response_time": 4562
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "public rollback()",
                        "public commit()"
                    ],
                    "llm_response_time": 1861
                }
            },
            "targetClassMap": {
                "replay": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2973,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "assertNoOrEmptyResult": {
                    "target_classes": [
                        {
                            "class_name": "TestUtil",
                            "similarity_score": 0.2788866755113585
                        },
                        {
                            "class_name": "AssignmentTestUtil",
                            "similarity_score": 0.3100194575128992
                        },
                        {
                            "class_name": "Assertions",
                            "similarity_score": 0.3114222159379954
                        },
                        {
                            "class_name": "GroupCoordinatorConfigTest",
                            "similarity_score": 0.17491527034192925
                        },
                        {
                            "class_name": "CoordinatorRecordHelpersTest",
                            "similarity_score": 0.17066921534597945
                        },
                        {
                            "class_name": "OffsetMetadataManagerTest",
                            "similarity_score": 0.19312680589973524
                        },
                        {
                            "class_name": "GroupCoordinatorServiceTest",
                            "similarity_score": 0.16764108345552195
                        },
                        {
                            "class_name": "GroupMetadataManagerTest",
                            "similarity_score": 0.13325079983720237
                        },
                        {
                            "class_name": "ExpiredTimeout",
                            "similarity_score": 0.23000322710873394
                        },
                        {
                            "class_name": "GroupCoordinatorShardTest",
                            "similarity_score": 0.10458573537752495
                        },
                        {
                            "class_name": "CoordinatorRecordSerdeTest",
                            "similarity_score": 0.1718302273894527
                        },
                        {
                            "class_name": "CoordinatorRecordTest",
                            "similarity_score": 0.23424278964210216
                        },
                        {
                            "class_name": "ScheduledTimeout",
                            "similarity_score": 0.24096579867074966
                        },
                        {
                            "class_name": "SyncGroupRequestBuilder",
                            "similarity_score": 0.20672455764868075
                        },
                        {
                            "class_name": "SyncResult",
                            "similarity_score": 0.2636352520041483
                        },
                        {
                            "class_name": "RebalanceResult",
                            "similarity_score": 0.15118578920369088
                        },
                        {
                            "class_name": "OffsetExpirationConditionImplTest",
                            "similarity_score": 0.0870542864324558
                        },
                        {
                            "class_name": "JoinGroupRequestBuilder",
                            "similarity_score": 0.20455960047249264
                        },
                        {
                            "class_name": "JoinResult",
                            "similarity_score": 0.2636352520041483
                        },
                        {
                            "class_name": "OffsetMetadataManagerTestContext",
                            "similarity_score": 0.2893800111592325
                        },
                        {
                            "class_name": "NoOpPartitionAssignor",
                            "similarity_score": 0.3305898024536431
                        },
                        {
                            "class_name": "PendingMemberGroupResult",
                            "similarity_score": 0.22592402852876597
                        },
                        {
                            "class_name": "OffsetAndMetadataTest",
                            "similarity_score": 0.16787229821795305
                        },
                        {
                            "class_name": "MockCoordinatorTimer",
                            "similarity_score": 0.30489226938312647
                        },
                        {
                            "class_name": "MockPartitionAssignor",
                            "similarity_score": 0.354374653931171
                        },
                        {
                            "class_name": "MetadataImageBuilder",
                            "similarity_score": 0.22338875108890427
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Assertions",
                        "MockPartitionAssignor",
                        "NoOpPartitionAssignor"
                    ],
                    "llm_response_time": 3781,
                    "similarity_computation_time": 38,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.3043426172871949
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 2047,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "createClassicGroup": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.2217300042423741
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager"
                    ],
                    "llm_response_time": 7005,
                    "similarity_computation_time": 10,
                    "similarity_metric": "cosine"
                },
                "onUnloaded": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.239443772869678
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager"
                    ],
                    "llm_response_time": 7461,
                    "similarity_computation_time": 13,
                    "similarity_metric": "cosine"
                },
                "newGroupMetadataRecord": {
                    "target_classes": [
                        {
                            "class_name": "MetricsTestUtils",
                            "similarity_score": 0.322639714762719
                        },
                        {
                            "class_name": "TestUtil",
                            "similarity_score": 0.4364357804719847
                        },
                        {
                            "class_name": "AssignmentTestUtil",
                            "similarity_score": 0.35617560762191036
                        },
                        {
                            "class_name": "Utils",
                            "similarity_score": 0.24839136907156353
                        },
                        {
                            "class_name": "Assertions",
                            "similarity_score": 0.2988939244247672
                        },
                        {
                            "class_name": "CoordinatorRecordHelpers",
                            "similarity_score": 0.5541473663882635
                        },
                        {
                            "class_name": "GroupCoordinatorConfigTest",
                            "similarity_score": 0.14216231625954498
                        },
                        {
                            "class_name": "OffsetAndMetadata",
                            "similarity_score": 0.2070129067625894
                        },
                        {
                            "class_name": "GroupCoordinatorRuntimeMetricsTest",
                            "similarity_score": 0.3244428422615251
                        },
                        {
                            "class_name": "UniformHomogeneousAssignmentBuilder",
                            "similarity_score": 0.22346311495201185
                        },
                        {
                            "class_name": "Assignment",
                            "similarity_score": 0.30330238568918466
                        },
                        {
                            "class_name": "ShareGroupMember",
                            "similarity_score": 0.23320481945560997
                        },
                        {
                            "class_name": "UniformHeterogeneousAssignmentBuilder",
                            "similarity_score": 0.17828477610551882
                        },
                        {
                            "class_name": "ConsumerGroupMember",
                            "similarity_score": 0.22876432494278748
                        },
                        {
                            "class_name": "CoordinatorRecordHelpersTest",
                            "similarity_score": 0.6302068142117512
                        },
                        {
                            "class_name": "TopicMetadata",
                            "similarity_score": 0.24710515693807072
                        },
                        {
                            "class_name": "CoordinatorRuntimeTest",
                            "similarity_score": 0.27387995944133814
                        },
                        {
                            "class_name": "TargetAssignmentBuilder",
                            "similarity_score": 0.21585350537313489
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.2017088139858897
                        },
                        {
                            "class_name": "GroupCoordinatorMetrics",
                            "similarity_score": 0.2877470625675957
                        },
                        {
                            "class_name": "ConsumerGroup",
                            "similarity_score": 0.19627395539398926
                        },
                        {
                            "class_name": "OffsetMetadataManagerTest",
                            "similarity_score": 0.453962065527539
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.22839442817662306
                        },
                        {
                            "class_name": "GroupCoordinatorService",
                            "similarity_score": 0.34476597906843726
                        },
                        {
                            "class_name": "ClassicGroupMember",
                            "similarity_score": 0.18406541035473284
                        },
                        {
                            "class_name": "GroupCoordinatorServiceTest",
                            "similarity_score": 0.4303623230765838
                        },
                        {
                            "class_name": "GroupMetadataManagerTest",
                            "similarity_score": 0.3831555119084723
                        },
                        {
                            "class_name": "ClassicGroup",
                            "similarity_score": 0.18300630832965165
                        },
                        {
                            "class_name": "SimpleAssignor",
                            "similarity_score": 0.40473389258811004
                        },
                        {
                            "class_name": "SimpleAssignorTest",
                            "similarity_score": 0.4006214832176801
                        },
                        {
                            "class_name": "TargetAssignmentBuilderTest",
                            "similarity_score": 0.2539068034735112
                        },
                        {
                            "class_name": "TargetAssignmentBuilderTestContext",
                            "similarity_score": 0.3685841799289822
                        },
                        {
                            "class_name": "TargetAssignmentResult",
                            "similarity_score": 0.1927726389365997
                        },
                        {
                            "class_name": "MemberAssignmentImpl",
                            "similarity_score": 0.20344711469278987
                        },
                        {
                            "class_name": "MemberSubscriptionAndAssignmentImpl",
                            "similarity_score": 0.193075699678783
                        },
                        {
                            "class_name": "MockCoordinatorTimer",
                            "similarity_score": 0.26205584772474105
                        },
                        {
                            "class_name": "MockPartitionAssignor",
                            "similarity_score": 0.2372321010475645
                        },
                        {
                            "class_name": "MetadataImageBuilder",
                            "similarity_score": 0.2064741604835056
                        },
                        {
                            "class_name": "SnapshottableCoordinatorTest",
                            "similarity_score": 0.4541868715470695
                        },
                        {
                            "class_name": "NoOpPartitionAssignor",
                            "similarity_score": 0.3259286857530667
                        },
                        {
                            "class_name": "LoadSummary",
                            "similarity_score": 0.15691973428978243
                        },
                        {
                            "class_name": "MultiThreadedEventProcessor",
                            "similarity_score": 0.18045133184627038
                        },
                        {
                            "class_name": "MultiThreadedEventProcessorTest",
                            "similarity_score": 0.35588659020358354
                        },
                        {
                            "class_name": "CoordinatorRecord",
                            "similarity_score": 0.1590909090909091
                        },
                        {
                            "class_name": "CoordinatorRecordSerde",
                            "similarity_score": 0.4373259099663821
                        },
                        {
                            "class_name": "CoordinatorRecordSerdeTest",
                            "similarity_score": 0.5201108220135132
                        },
                        {
                            "class_name": "CoordinatorRecordTest",
                            "similarity_score": 0.41056019142373384
                        },
                        {
                            "class_name": "CoordinatorResult",
                            "similarity_score": 0.10815151826269974
                        },
                        {
                            "class_name": "CoordinatorResultTest",
                            "similarity_score": 0.32771521214916555
                        },
                        {
                            "class_name": "CoordinatorRuntime",
                            "similarity_score": 0.14834351428348694
                        },
                        {
                            "class_name": "ClassicGroupMemberTest",
                            "similarity_score": 0.3697445984124977
                        },
                        {
                            "class_name": "ClassicGroupTest",
                            "similarity_score": 0.3345495855196774
                        },
                        {
                            "class_name": "OffsetAndMetadataTest",
                            "similarity_score": 0.39682581316657833
                        },
                        {
                            "class_name": "OffsetConfig",
                            "similarity_score": 0.050598941337546215
                        },
                        {
                            "class_name": "OffsetExpirationConditionImpl",
                            "similarity_score": 0.09798501839458537
                        },
                        {
                            "class_name": "OffsetExpirationConditionImplTest",
                            "similarity_score": 0.11920399101642995
                        },
                        {
                            "class_name": "OffsetMetadataManagerTestContext",
                            "similarity_score": 0.4540083594357357
                        },
                        {
                            "class_name": "ScheduledTimeout",
                            "similarity_score": 0.19064124952775927
                        },
                        {
                            "class_name": "TopicIds",
                            "similarity_score": 0.3200268342843653
                        },
                        {
                            "class_name": "TopicIdsTest",
                            "similarity_score": 0.2284743161148241
                        },
                        {
                            "class_name": "TopicMetadataTest",
                            "similarity_score": 0.27961767918289415
                        },
                        {
                            "class_name": "CurrentAssignmentBuilder",
                            "similarity_score": 0.15658606347631715
                        },
                        {
                            "class_name": "CurrentAssignmentBuilderTest",
                            "similarity_score": 0.2824973642921633
                        },
                        {
                            "class_name": "OptimizedUniformAssignmentBuilderTest",
                            "similarity_score": 0.38584434331819495
                        },
                        {
                            "class_name": "RangeAssignor",
                            "similarity_score": 0.19389669152343153
                        },
                        {
                            "class_name": "RangeAssignorTest",
                            "similarity_score": 0.39893389446692334
                        },
                        {
                            "class_name": "RangeSetTest",
                            "similarity_score": 0.2752656235527004
                        },
                        {
                            "class_name": "DeadlineAndEpoch",
                            "similarity_score": 0.22630095274240716
                        },
                        {
                            "class_name": "EventAccumulator",
                            "similarity_score": 0.15895946841274297
                        },
                        {
                            "class_name": "EventAccumulatorTest",
                            "similarity_score": 0.3124630577674038
                        },
                        {
                            "class_name": "RebalanceResult",
                            "similarity_score": 0.165615734242165
                        },
                        {
                            "class_name": "SubscribedTopicDescriberImpl",
                            "similarity_score": 0.10800821250689281
                        },
                        {
                            "class_name": "SubscribedTopicMetadataTest",
                            "similarity_score": 0.25595031428049014
                        },
                        {
                            "class_name": "ExpiredTimeout",
                            "similarity_score": 0.20696378450628986
                        },
                        {
                            "class_name": "AssignmentTest",
                            "similarity_score": 0.29538238824785923
                        },
                        {
                            "class_name": "ConsumerGroupBuilder",
                            "similarity_score": 0.3068610452103943
                        },
                        {
                            "class_name": "ConsumerGroupMemberTest",
                            "similarity_score": 0.3275465466688927
                        },
                        {
                            "class_name": "ConsumerGroupTest",
                            "similarity_score": 0.31886131298411935
                        },
                        {
                            "class_name": "ShareGroup",
                            "similarity_score": 0.30683717914393227
                        },
                        {
                            "class_name": "ShareGroupAssignmentBuilder",
                            "similarity_score": 0.11037659867723072
                        },
                        {
                            "class_name": "ShareGroupMemberTest",
                            "similarity_score": 0.25863163491855584
                        },
                        {
                            "class_name": "ShareGroupTest",
                            "similarity_score": 0.361459418537259
                        },
                        {
                            "class_name": "SyncGroupRequestBuilder",
                            "similarity_score": 0.2094712513167502
                        },
                        {
                            "class_name": "PendingMemberGroupResult",
                            "similarity_score": 0.22980970388562794
                        },
                        {
                            "class_name": "UniformAssignor",
                            "similarity_score": 0.14865882924943327
                        },
                        {
                            "class_name": "UniformHeterogeneousAssignmentBuilderTest",
                            "similarity_score": 0.3856418022670195
                        },
                        {
                            "class_name": "SyncResult",
                            "similarity_score": 0.1856558243265828
                        },
                        {
                            "class_name": "JoinGroupRequestBuilder",
                            "similarity_score": 0.21546520679428585
                        },
                        {
                            "class_name": "JoinResult",
                            "similarity_score": 0.1856558243265828
                        },
                        {
                            "class_name": "GroupCoordinatorConfig",
                            "similarity_score": 0.18954141979041048
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.2507652625124375
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShardTest",
                            "similarity_score": 0.2819279143595284
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsTest",
                            "similarity_score": 0.29338600125431696
                        },
                        {
                            "class_name": "GroupCoordinatorRuntimeMetrics",
                            "similarity_score": 0.19518306998242588
                        },
                        {
                            "class_name": "GroupCoordinatorShard",
                            "similarity_score": 0.16767121993388498
                        },
                        {
                            "class_name": "GroupCoordinatorShardTest",
                            "similarity_score": 0.40305827002854594
                        },
                        {
                            "class_name": "GroupSpecImpl",
                            "similarity_score": 0.2131061249300771
                        },
                        {
                            "class_name": "GroupSpecImplTest",
                            "similarity_score": 0.33628404143185137
                        },
                        {
                            "class_name": "UnknownRecordTypeException",
                            "similarity_score": 0.1432003111516314
                        },
                        {
                            "class_name": "InMemoryPartitionWriter",
                            "similarity_score": 0.30329072524501627
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "CoordinatorRecordHelpers",
                        "CoordinatorRecordSerdeTest",
                        "CoordinatorRecordHelpersTest"
                    ],
                    "llm_response_time": 4634,
                    "similarity_computation_time": 82,
                    "similarity_metric": "cosine"
                },
                "build": {
                    "target_classes": [
                        {
                            "class_name": "ShareGroupPartitionAssignor",
                            "similarity_score": 0.07142857142857142
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ShareGroupPartitionAssignor"
                    ],
                    "llm_response_time": 1571,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "verifyDescribeGroupsReturnsDeadGroup": {
                    "target_classes": [
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.36817560849209524
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.24900742200520393
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.26621786583356427
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.1763284455893675
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupCoordinatorMetricsShard",
                        "SnapshotRegistry",
                        "MockTime"
                    ],
                    "llm_response_time": 4578,
                    "similarity_computation_time": 16,
                    "similarity_metric": "cosine"
                },
                "sendConsumerGroupDescribe": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.20699090459068448
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager"
                    ],
                    "llm_response_time": 7134,
                    "similarity_computation_time": 9,
                    "similarity_metric": "cosine"
                },
                "sendShareGroupDescribe": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.20699090459068448
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager"
                    ],
                    "llm_response_time": 10773,
                    "similarity_computation_time": 10,
                    "similarity_metric": "cosine"
                },
                "describeGroups": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.20699090459068448
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager"
                    ],
                    "llm_response_time": 7251,
                    "similarity_computation_time": 10,
                    "similarity_metric": "cosine"
                },
                "verifySessionExpiration": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2830,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "consumerGroupMemberState": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.21788377350774316
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager"
                    ],
                    "llm_response_time": 6721,
                    "similarity_computation_time": 9,
                    "similarity_metric": "cosine"
                },
                "shareGroupState": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.23005802754419868
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager"
                    ],
                    "llm_response_time": 11225,
                    "similarity_computation_time": 10,
                    "similarity_metric": "cosine"
                },
                "consumerGroupState": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.23005802754419868
                        }
                    ],
                    "llm_response_time": 7759,
                    "similarity_computation_time": 9,
                    "similarity_metric": "cosine",
                    "target_class_priority_explanation": "```json\n[\n    {\n        \"target_class\": \"GroupMetadataManager\",\n        \"rationale\": \"The method consumerGroupState(String groupId) interacts directly with the consumerGroup(String groupId) method and derives data corresponding to the group's state. This fits the responsibility of GroupMetadataManager, which manages the metadata and states of all groups, including consumer groups. Housing this method in GroupMetadataManager provides a cohesive design and enhances the encapsulation of group-related operations.\",\n    }\n]\n```"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from private replay(producerId long, record CoordinatorRecord) : void in class org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 480,
                    "endLine": 509,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private replay(producerId long, record CoordinatorRecord) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 484,
                    "endLine": 484,
                    "startColumn": 13,
                    "endColumn": 69,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 480,
                    "endLine": 509,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private replay(producerId long, record CoordinatorRecord) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 484,
                    "endLine": 484,
                    "startColumn": 13,
                    "endColumn": 73,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastWrittenOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 607,
        "extraction_results": {
            "success": true,
            "newCommitHash": "d356b9402b3973ba6e350870df2390fa05f0d01d",
            "newBranchName": "extract-idempotentCreateSnapshot-replay-130af38-2"
        },
        "telemetry": {
            "id": "05f7df39-b462-47ba-9d1e-015acff5279a",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 483,
                "lineStart": 97,
                "lineEnd": 579,
                "bodyLineStart": 97,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                "sourceCode": "static class OffsetMetadataManagerTestContext {\n        public static class Builder {\n            private final MockTime time = new MockTime();\n            private final MockCoordinatorTimer<Void, CoordinatorRecord> timer = new MockCoordinatorTimer<>(time);\n            private final LogContext logContext = new LogContext();\n            private final SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n            private GroupMetadataManager groupMetadataManager = null;\n            private MetadataImage metadataImage = null;\n            private GroupCoordinatorConfig config = null;\n            private GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n\n            Builder withOffsetMetadataMaxSize(int offsetMetadataMaxSize) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(offsetMetadataMaxSize, 60000L, 24 * 60);\n                return this;\n            }\n\n            Builder withOffsetsRetentionMinutes(int offsetsRetentionMinutes) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, offsetsRetentionMinutes);\n                return this;\n            }\n\n            Builder withGroupMetadataManager(GroupMetadataManager groupMetadataManager) {\n                this.groupMetadataManager = groupMetadataManager;\n                return this;\n            }\n\n            OffsetMetadataManagerTestContext build() {\n                if (metadataImage == null) metadataImage = MetadataImage.EMPTY;\n                if (config == null) {\n                    config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, 24);\n                }\n\n                if (groupMetadataManager == null) {\n                    groupMetadataManager = new GroupMetadataManager.Builder()\n                        .withTime(time)\n                        .withTimer(timer)\n                        .withSnapshotRegistry(snapshotRegistry)\n                        .withLogContext(logContext)\n                        .withMetadataImage(metadataImage)\n                        .withConsumerGroupAssignors(Collections.singletonList(new RangeAssignor()))\n                        .withGroupCoordinatorMetricsShard(metrics)\n                        .build();\n                }\n\n                OffsetMetadataManager offsetMetadataManager = new OffsetMetadataManager.Builder()\n                    .withTime(time)\n                    .withLogContext(logContext)\n                    .withSnapshotRegistry(snapshotRegistry)\n                    .withMetadataImage(metadataImage)\n                    .withGroupMetadataManager(groupMetadataManager)\n                    .withGroupCoordinatorConfig(config)\n                    .withGroupCoordinatorMetricsShard(metrics)\n                    .build();\n\n                return new OffsetMetadataManagerTestContext(\n                    time,\n                    timer,\n                    snapshotRegistry,\n                    metrics,\n                    groupMetadataManager,\n                    offsetMetadataManager\n                );\n            }\n        }\n\n        final MockTime time;\n        final MockCoordinatorTimer<Void, CoordinatorRecord> timer;\n        final SnapshotRegistry snapshotRegistry;\n        final GroupCoordinatorMetricsShard metrics;\n        final GroupMetadataManager groupMetadataManager;\n        final OffsetMetadataManager offsetMetadataManager;\n\n        long lastCommittedOffset = 0L;\n        long lastWrittenOffset = 0L;\n\n        OffsetMetadataManagerTestContext(\n            MockTime time,\n            MockCoordinatorTimer<Void, CoordinatorRecord> timer,\n            SnapshotRegistry snapshotRegistry,\n            GroupCoordinatorMetricsShard metrics,\n            GroupMetadataManager groupMetadataManager,\n            OffsetMetadataManager offsetMetadataManager\n        ) {\n            this.time = time;\n            this.timer = timer;\n            this.snapshotRegistry = snapshotRegistry;\n            this.metrics = metrics;\n            this.groupMetadataManager = groupMetadataManager;\n            this.offsetMetadataManager = offsetMetadataManager;\n        }\n\n        public Group getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        ) {\n            switch (groupType) {\n                case CLASSIC:\n                    return groupMetadataManager.getOrMaybeCreateClassicGroup(\n                        groupId,\n                        true\n                    );\n                case CONSUMER:\n                    return groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n                        groupId,\n                        true\n                    );\n                default:\n                    throw new IllegalArgumentException(\"Invalid group type: \" + groupType);\n            }\n        }\n\n        public void commit() {\n            long lastCommittedOffset = this.lastCommittedOffset;\n            this.lastCommittedOffset = lastWrittenOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastCommittedOffset);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(\n            OffsetCommitRequestData request\n        ) {\n            return commitOffset(ApiKeys.OFFSET_COMMIT.latestVersion(), request);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.OFFSET_COMMIT,\n                    version,\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = offsetMetadataManager.commitOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.TXN_OFFSET_COMMIT,\n                    ApiKeys.TXN_OFFSET_COMMIT.latestVersion(),\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> result = offsetMetadataManager.commitTransactionalOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(record -> replay(\n                request.producerId(),\n                record\n            ));\n\n            return result;\n        }\n\n        public List<CoordinatorRecord> deletePartitions(\n            List<TopicPartition> topicPartitions\n        ) {\n            List<CoordinatorRecord> records = offsetMetadataManager.onPartitionsDeleted(topicPartitions);\n            records.forEach(this::replay);\n            return records;\n        }\n\n        public CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> deleteOffsets(\n            OffsetDeleteRequestData request\n        ) {\n            CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> result = offsetMetadataManager.deleteOffsets(request);\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public int deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        ) {\n            List<CoordinatorRecord> addedRecords = new ArrayList<>();\n            int numDeletedOffsets = offsetMetadataManager.deleteAllOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return numDeletedOffsets;\n        }\n\n        public boolean cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records) {\n            List<CoordinatorRecord> addedRecords = new ArrayList<>();\n            boolean isOffsetsEmptyForGroup = offsetMetadataManager.cleanupExpiredOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return isOffsetsEmptyForGroup;\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            return fetchOffsets(\n                groupId,\n                null,\n                -1,\n                topics,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch)\n                    .setTopics(topics),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        ) {\n            return fetchAllOffsets(\n                groupId,\n                null,\n                -1,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchAllOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> sleep(long ms) {\n            time.sleep(ms);\n            List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = timer.poll();\n            timeouts.forEach(timeout -> {\n                if (timeout.result.replayRecords()) {\n                    timeout.result.records().forEach(this::replay);\n                }\n            });\n            return timeouts;\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        ) {\n            commitOffset(\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                time.milliseconds()\n            );\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            commitOffset(\n                RecordBatch.NO_PRODUCER_ID,\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                commitTimestamp\n            );\n        }\n\n        public void commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            replay(producerId, CoordinatorRecordHelpers.newOffsetCommitRecord(\n                groupId,\n                topic,\n                partition,\n                new OffsetAndMetadata(\n                    offset,\n                    OptionalInt.of(leaderEpoch),\n                    \"metadata\",\n                    commitTimestamp,\n                    OptionalLong.empty()\n                ),\n                MetadataVersion.latestTesting()\n            ));\n        }\n\n        public void deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            replay(CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\n                groupId,\n                topic,\n                partition\n            ));\n        }\n\n        private ApiMessage messageOrNull(ApiMessageAndVersion apiMessageAndVersion) {\n            if (apiMessageAndVersion == null) {\n                return null;\n            } else {\n                return apiMessageAndVersion.message();\n            }\n        }\n\n        private void replay(\n            CoordinatorRecord record\n        ) {\n            replay(\n                RecordBatch.NO_PRODUCER_ID,\n                record\n            );\n        }\n\n        private void replay(\n            long producerId,\n            CoordinatorRecord record\n        ) {\n            idempotentCreateSnapshot(lastWrittenOffset);\n\n            ApiMessageAndVersion key = record.key();\n            ApiMessageAndVersion value = record.value();\n\n            if (key == null) {\n                throw new IllegalStateException(\"Received a null key in \" + record);\n            }\n\n            switch (key.version()) {\n                case OffsetCommitKey.HIGHEST_SUPPORTED_VERSION:\n                    offsetMetadataManager.replay(\n                        lastWrittenOffset,\n                        producerId,\n                        (OffsetCommitKey) key.message(),\n                        (OffsetCommitValue) messageOrNull(value)\n                    );\n                    break;\n\n                default:\n                    throw new IllegalStateException(\"Received an unknown record type \" + key.version()\n                        + \" in \" + record);\n            }\n\n            lastWrittenOffset++;\n        }\n\n        private void idempotentCreateSnapshot(long lastWrittenOffset) {\n            snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n        }\n\n        private void replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        ) {\n            idempotentCreateSnapshot(lastWrittenOffset);\n            offsetMetadataManager.replayEndTransactionMarker(producerId, result);\n            lastWrittenOffset++;\n        }\n\n        public void testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        ) {\n            final OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection requestTopicCollection =\n                new OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection(Collections.singletonList(\n                    new OffsetDeleteRequestData.OffsetDeleteRequestTopic()\n                        .setName(topic)\n                        .setPartitions(Collections.singletonList(\n                            new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(partition)\n                        ))\n                ).iterator());\n\n            final OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection expectedResponsePartitionCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection();\n            expectedResponsePartitionCollection.add(\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartition()\n                    .setPartitionIndex(partition)\n                    .setErrorCode(expectedError.code())\n            );\n\n            final OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection expectedResponseTopicCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection(Collections.singletonList(\n                    new OffsetDeleteResponseData.OffsetDeleteResponseTopic()\n                        .setName(topic)\n                        .setPartitions(expectedResponsePartitionCollection)\n                ).iterator());\n\n            List<CoordinatorRecord> expectedRecords = Collections.emptyList();\n            if (hasOffset(groupId, topic, partition) && expectedError == Errors.NONE) {\n                expectedRecords = Collections.singletonList(\n                    CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(groupId, topic, partition)\n                );\n            }\n\n            final CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> coordinatorResult = deleteOffsets(\n                new OffsetDeleteRequestData()\n                    .setGroupId(groupId)\n                    .setTopics(requestTopicCollection)\n            );\n\n            assertEquals(new OffsetDeleteResponseData().setTopics(expectedResponseTopicCollection), coordinatorResult.response());\n            assertEquals(expectedRecords, coordinatorResult.records());\n        }\n\n        public boolean hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            return offsetMetadataManager.hasCommittedOffset(groupId, topic, partition) ||\n                offsetMetadataManager.hasPendingTransactionalOffsets(groupId, topic, partition);\n        }\n    }",
                "methodCount": 30
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 6,
                "candidates": [
                    {
                        "lineStart": 510,
                        "lineEnd": 512,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() directly interfaces with the SnapshotRegistry class by calling the getOrCreateSnapshot() method, which is a crucial part of SnapshotRegistry's responsibilities. Moving the idempotentCreateSnapshot() method to SnapshotRegistry promotes encapsulation by keeping all snapshot-related logic within the SnapshotRegistry class, eliminating redundant accessor methods and reducing coupling which can improve maintainability and readability of the code.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 390,
                        "lineEnd": 405,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class MockTime",
                        "description": "Move method commitOffset to org.apache.kafka.common.utils.MockTime\nRationale: The method 'commitOffset' utilizes the current time obtained from the 'time.milliseconds()'. The 'MockTime' class provides the current time through the 'milliseconds()' method, indicating that 'commitOffset' could rely on 'MockTime' to obtain this timing information. Therefore, moving 'commitOffset' to 'MockTime' can streamline the usage of time-dependent functionalities and ensure that time-related operations are centrally managed within the 'MockTime' class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 213,
                        "lineEnd": 217,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class GroupCoordinatorMetricsShard",
                        "description": "Move method commitOffset to org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsShard\nRationale: The commitOffset() method is highly relevant to the functionality of the GroupCoordinatorMetricsShard class, which deals with managing offsets and groups in a Kafka coordinator context. Moving the method to GroupCoordinatorMetricsShard aligns the method's responsibilities closely with the metrics and offset management capabilities already in the class. This ensures a higher cohesion within the class and keeps related functionalities together.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 570,
                        "lineEnd": 577,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method hasOffset to class OffsetMetadataManager",
                        "description": "Move method hasOffset to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The `hasOffset` method relies directly on methods from the `OffsetMetadataManager` class (`hasCommittedOffset` and `hasPendingTransactionalOffsets`). This indicates that the `OffsetMetadataManager` is already handling most of the related functionality. Keeping this method within `OffsetMetadataManager` will modularize the offset management responsibilities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 187,
                        "lineEnd": 205,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method getOrMaybeCreateGroup to class GroupMetadataManager",
                        "description": "Move method getOrMaybeCreateGroup to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The method 'getOrMaybeCreateGroup' is a dispatcher that calls other methods from GroupMetadataManager depending on the GroupType. Hence, it should be a part of GroupMetadataManager to align with the Single Responsibility Principle and keep related functionalities cohesive.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 462,
                        "lineEnd": 468,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method messageOrNull to class ApiMessageAndVersion",
                        "description": "Move method messageOrNull to org.apache.kafka.server.common.ApiMessageAndVersion\nRationale: The 'messageOrNull' method directly deals with the 'ApiMessageAndVersion' class, accessing its 'message' method. By placing this method within 'ApiMessageAndVersion', it improves cohesion and encapsulation, making the utility method directly accessible in the context of its primary use-case. This eliminates an unnecessary dependency on external classes for such a basic utility function.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": "Builder withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "GroupCoordinatorConfig",
                            "rationale": "This method is specific to configuring GroupCoordinatorConfig and thus should be moved there."
                        },
                        {
                            "method_name": "withOffsetsRetentionMinutes",
                            "method_signature": "Builder withOffsetsRetentionMinutes(int offsetsRetentionMinutes)",
                            "target_class": "GroupCoordinatorConfig",
                            "rationale": "This method is specific to configuring the retention minutes of GroupCoordinatorConfig and should be part of that class."
                        },
                        {
                            "method_name": "withGroupMetadataManager",
                            "method_signature": "Builder withGroupMetadataManager(GroupMetadataManager groupMetadataManager)",
                            "target_class": "GroupMetadataManager",
                            "rationale": "This method is related to GroupMetadataManager configuration and will be more cohesive within the GroupMetadataManager class."
                        },
                        {
                            "method_name": "testOffsetDeleteWith",
                            "method_signature": "public void testOffsetDeleteWith(String groupId, String topic, int partition, Errors expectedError)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "This method tests the deletion of offsets which is more related to OffsetMetadataManager functionality and should be moved there."
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public boolean hasOffset(String groupId, String topic, int partition)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "This method checks the existence of offsets, which is a core responsibility of OffsetMetadataManager."
                        }
                    ],
                    "llm_response_time": 4717
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "commit",
                            "method_signature": "public void commit()",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The method deals with snapshots and deleting snapshots up to a specific offset, which is closely related to the snapshot management responsibilities."
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(OffsetCommitRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "The method handles offset commit operations, which are directly related to the responsibilities of the OffsetMetadataManager class."
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(short version, OffsetCommitRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Same as above, this overloaded method also handles offset operations closely related to OffsetMetadataManager."
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> commitTransactionalOffset(TxnOffsetCommitRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Similar to the other commit methods, this one deals with transactional offset commits, making it a task for OffsetMetadataManager."
                        },
                        {
                            "method_name": "deletePartitions",
                            "method_signature": "public List<CoordinatorRecord> deletePartitions(List<TopicPartition> topicPartitions)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "The method handles deletion of partitions, which aligns with the responsibilities of OffsetMetadataManager as it manages offset metadata."
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> deleteOffsets(OffsetDeleteRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Deleting offsets is closely related to offset metadata management and fits within the responsibilities of OffsetMetadataManager."
                        },
                        {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public int deleteAllOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Deleting all offsets for a group is a task that belongs to OffsetMetadataManager, given its role in managing offset metadata."
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public boolean cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Cleaning up expired offsets is in line with the OffsetMetadataManager\u2019s responsibilities related to managing the lifecycle of offsets."
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(String groupId, List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Fetching offsets, which are part of the offset metadata, should be handled by the OffsetMetadataManager."
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(String groupId, String memberId, int memberEpoch, List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Similar to the other fetchOffsets method, this overloaded method is responsible for retrieving offset metadata."
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(String groupId, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Fetching all offsets is aligned with OffsetMetadataManager\u2019s role in managing offset metadata."
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(String groupId, String memberId, int memberEpoch, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "This overloaded method should also be part of OffsetMetadataManager due to its responsibility in fetching offset metadata."
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public void commitOffset(String groupId, String topic, int partition, long offset, int leaderEpoch)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Committing offsets fits within the scope of offset metadata management, making OffsetMetadataManager the appropriate class."
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public void commitOffset(String groupId, String topic, int partition, long offset, int leaderEpoch, long commitTimestamp)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "This overloaded method also handles committing offsets and should be moved to OffsetMetadataManager."
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public void commitOffset(long producerId, String groupId, String topic, int partition, long offset, int leaderEpoch, long commitTimestamp)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Like the other commit offset methods, this one should also belong to OffsetMetadataManager due to its offset committing functionality."
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public void deleteOffset(String groupId, String topic, int partition)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Deleting a specific offset is a sub-task of offset metadata management, meaning it should be handled by OffsetMetadataManager."
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public boolean hasOffset(String groupId, String topic, int partition)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Checking the existence of an offset belongs to OffsetMetadataManager as it\u2019s part of managing offset metadata."
                        }
                    ],
                    "llm_response_time": 11471
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": "Builder withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "GroupCoordinatorConfigTest",
                            "rationale": "The method `withOffsetMetadataMaxSize` creates a `GroupCoordinatorConfig` using `GroupCoordinatorConfigTest.createGroupCoordinatorConfig`, indicating that it primarily involves `GroupCoordinatorConfig` and should be moved to `GroupCoordinatorConfigTest` for better encapsulation."
                        },
                        {
                            "method_name": "withOffsetsRetentionMinutes",
                            "method_signature": "Builder withOffsetsRetentionMinutes(int offsetsRetentionMinutes)",
                            "target_class": "GroupCoordinatorConfigTest",
                            "rationale": "Similar to `withOffsetMetadataMaxSize`, the method `withOffsetsRetentionMinutes` revolves around the creation of a `GroupCoordinatorConfig` object using `GroupCoordinatorConfigTest.createGroupCoordinatorConfig`, making it a better fit within `GroupCoordinatorConfigTest`."
                        },
                        {
                            "method_name": "withGroupMetadataManager",
                            "method_signature": "Builder withGroupMetadataManager(GroupMetadataManager groupMetadataManager)",
                            "target_class": "GroupMetadataManager",
                            "rationale": "The method `withGroupMetadataManager` primarily deals with setting a `GroupMetadataManager` instance. Moving it to `GroupMetadataManager` can help in centralizing the logic related to assigning or configuring group metadata managers."
                        }
                    ],
                    "llm_response_time": 3823
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deletePartitions",
                            "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long lastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testOffsetDeleteWith",
                            "method_signature": "public testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long lastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deletePartitions",
                            "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(long lastWrittenOffset)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long lastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3661704578137439
                    },
                    "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )": {
                        "first": {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.36704609496973545
                    },
                    "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.39479973254262324
                    },
                    "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4229545887356226
                    },
                    "public commitOffset(\n            OffsetCommitRequestData request\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.42531130879089224
                    },
                    "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                        "first": {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4497067973233583
                    },
                    "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )": {
                        "first": {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.45693269895919714
                    },
                    "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )": {
                        "first": {
                            "method_name": "deletePartitions",
                            "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4579600021991615
                    },
                    "public sleep(long ms)": {
                        "first": {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.46272510370226666
                    },
                    "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )": {
                        "first": {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4756129905283165
                    },
                    "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.48272274818617783
                    },
                    "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )": {
                        "first": {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4862087309316981
                    },
                    "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)": {
                        "first": {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.49450578763221575
                    },
                    "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)": {
                        "first": {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5040037740243367
                    },
                    "public commit()": {
                        "first": {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5278603726439697
                    }
                },
                "voyage": {
                    "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)": {
                        "first": {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.19582079698382762
                    },
                    "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )": {
                        "first": {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3322513630830509
                    },
                    "public sleep(long ms)": {
                        "first": {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.42762762703626733
                    },
                    "public commit()": {
                        "first": {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.48091098567940815
                    },
                    "private idempotentCreateSnapshot(long lastWrittenOffset)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long lastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.49046309206094246
                    },
                    "public commitOffset(\n            OffsetCommitRequestData request\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5414659382273233
                    },
                    "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )": {
                        "first": {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5615134162668379
                    },
                    "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )": {
                        "first": {
                            "method_name": "deletePartitions",
                            "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5720064840962604
                    },
                    "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5834906865949685
                    },
                    "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)": {
                        "first": {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5842143382172316
                    },
                    "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                        "first": {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5956090936253587
                    },
                    "public deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        )": {
                        "first": {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6122514122856123
                    },
                    "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )": {
                        "first": {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6204644364099181
                    },
                    "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6236884591907732
                    },
                    "public testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        )": {
                        "first": {
                            "method_name": "testOffsetDeleteWith",
                            "method_signature": "public testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6328551316595161
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(long lastWrittenOffset)",
                        "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                        "public sleep(long ms)",
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "public commit()"
                    ],
                    "llm_response_time": 2851
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(long lastWrittenOffset)"
                    ],
                    "llm_response_time": 2025
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                        "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                        "private idempotentCreateSnapshot(long lastWrittenOffset)"
                    ],
                    "llm_response_time": 4012
                },
                "voyage": {
                    "priority_method_names": [
                        "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                        "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                        "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                        "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                        "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                        "public commit()",
                        "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                        "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                        "public deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        )",
                        "public testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        )",
                        "public sleep(long ms)",
                        "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                        "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                        "private idempotentCreateSnapshot(long lastWrittenOffset)",
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)"
                    ],
                    "llm_response_time": 3577
                },
                "voyage-5": {
                    "priority_method_names": [
                        "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                        "public commit()",
                        "public sleep(long ms)",
                        "private idempotentCreateSnapshot(long lastWrittenOffset)",
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)"
                    ],
                    "llm_response_time": 4096
                },
                "voyage-3": {
                    "priority_method_names": [
                        "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                        "public sleep(long ms)",
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)"
                    ],
                    "llm_response_time": 3363
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 1861,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "replayEndTransactionMarker": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 5708,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "commitOffset": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 3228,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "replay": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2797,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "hasOffset": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.18169682579561122
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.265533546076446
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManager",
                        "GroupMetadataManager"
                    ],
                    "llm_response_time": 9697,
                    "similarity_computation_time": 11,
                    "similarity_metric": "cosine"
                },
                "deleteOffsets": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2840,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "deletePartitions": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1678,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "sleep": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 1005,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "commitTransactionalOffset": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2619,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "getOrMaybeCreateGroup": {
                    "target_classes": [
                        {
                            "class_name": "GroupType",
                            "similarity_score": 0.32015738118488507
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.3341613256230055
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager",
                        "GroupType"
                    ],
                    "llm_response_time": 7583,
                    "similarity_computation_time": 11,
                    "similarity_metric": "cosine"
                },
                "cleanupExpiredOffsets": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 2649,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "messageOrNull": {
                    "target_classes": [
                        {
                            "class_name": "ApiMessageAndVersion",
                            "similarity_score": 0.6106401198187058
                        },
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.5914470146353319
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.4220009252215372
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.4615361902486966
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.35945481197585616
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.48967032494126206
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ApiMessageAndVersion",
                        "OffsetMetadataManager",
                        "MockTime"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 19,
                    "similarity_metric": "cosine"
                },
                "commit": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from private replayEndTransactionMarker(producerId long, result TransactionResult) : void in class org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 511,
                    "endLine": 518,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private replayEndTransactionMarker(producerId long, result TransactionResult) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 515,
                    "endLine": 515,
                    "startColumn": 13,
                    "endColumn": 69,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 511,
                    "endLine": 518,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private replayEndTransactionMarker(producerId long, result TransactionResult) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 515,
                    "endLine": 515,
                    "startColumn": 13,
                    "endColumn": 73,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastWrittenOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 608,
        "extraction_results": {
            "success": true,
            "newCommitHash": "729270178ac2a09f43888f33602453e319a798e1",
            "newBranchName": "extract-idempotentCreateSnapshot-replayEndTransactionMarker-130af38"
        },
        "telemetry": {
            "id": "f3c0473d-2812-4cef-8387-898e0a693309",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 483,
                "lineStart": 97,
                "lineEnd": 579,
                "bodyLineStart": 97,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                "sourceCode": "static class OffsetMetadataManagerTestContext {\n        public static class Builder {\n            private final MockTime time = new MockTime();\n            private final MockCoordinatorTimer<Void, CoordinatorRecord> timer = new MockCoordinatorTimer<>(time);\n            private final LogContext logContext = new LogContext();\n            private final SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n            private GroupMetadataManager groupMetadataManager = null;\n            private MetadataImage metadataImage = null;\n            private GroupCoordinatorConfig config = null;\n            private GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n\n            Builder withOffsetMetadataMaxSize(int offsetMetadataMaxSize) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(offsetMetadataMaxSize, 60000L, 24 * 60);\n                return this;\n            }\n\n            Builder withOffsetsRetentionMinutes(int offsetsRetentionMinutes) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, offsetsRetentionMinutes);\n                return this;\n            }\n\n            Builder withGroupMetadataManager(GroupMetadataManager groupMetadataManager) {\n                this.groupMetadataManager = groupMetadataManager;\n                return this;\n            }\n\n            OffsetMetadataManagerTestContext build() {\n                if (metadataImage == null) metadataImage = MetadataImage.EMPTY;\n                if (config == null) {\n                    config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, 24);\n                }\n\n                if (groupMetadataManager == null) {\n                    groupMetadataManager = new GroupMetadataManager.Builder()\n                        .withTime(time)\n                        .withTimer(timer)\n                        .withSnapshotRegistry(snapshotRegistry)\n                        .withLogContext(logContext)\n                        .withMetadataImage(metadataImage)\n                        .withConsumerGroupAssignors(Collections.singletonList(new RangeAssignor()))\n                        .withGroupCoordinatorMetricsShard(metrics)\n                        .build();\n                }\n\n                OffsetMetadataManager offsetMetadataManager = new OffsetMetadataManager.Builder()\n                    .withTime(time)\n                    .withLogContext(logContext)\n                    .withSnapshotRegistry(snapshotRegistry)\n                    .withMetadataImage(metadataImage)\n                    .withGroupMetadataManager(groupMetadataManager)\n                    .withGroupCoordinatorConfig(config)\n                    .withGroupCoordinatorMetricsShard(metrics)\n                    .build();\n\n                return new OffsetMetadataManagerTestContext(\n                    time,\n                    timer,\n                    snapshotRegistry,\n                    metrics,\n                    groupMetadataManager,\n                    offsetMetadataManager\n                );\n            }\n        }\n\n        final MockTime time;\n        final MockCoordinatorTimer<Void, CoordinatorRecord> timer;\n        final SnapshotRegistry snapshotRegistry;\n        final GroupCoordinatorMetricsShard metrics;\n        final GroupMetadataManager groupMetadataManager;\n        final OffsetMetadataManager offsetMetadataManager;\n\n        long lastCommittedOffset = 0L;\n        long lastWrittenOffset = 0L;\n\n        OffsetMetadataManagerTestContext(\n            MockTime time,\n            MockCoordinatorTimer<Void, CoordinatorRecord> timer,\n            SnapshotRegistry snapshotRegistry,\n            GroupCoordinatorMetricsShard metrics,\n            GroupMetadataManager groupMetadataManager,\n            OffsetMetadataManager offsetMetadataManager\n        ) {\n            this.time = time;\n            this.timer = timer;\n            this.snapshotRegistry = snapshotRegistry;\n            this.metrics = metrics;\n            this.groupMetadataManager = groupMetadataManager;\n            this.offsetMetadataManager = offsetMetadataManager;\n        }\n\n        public Group getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        ) {\n            switch (groupType) {\n                case CLASSIC:\n                    return groupMetadataManager.getOrMaybeCreateClassicGroup(\n                        groupId,\n                        true\n                    );\n                case CONSUMER:\n                    return groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n                        groupId,\n                        true\n                    );\n                default:\n                    throw new IllegalArgumentException(\"Invalid group type: \" + groupType);\n            }\n        }\n\n        public void commit() {\n            long lastCommittedOffset = this.lastCommittedOffset;\n            this.lastCommittedOffset = lastWrittenOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastCommittedOffset);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(\n            OffsetCommitRequestData request\n        ) {\n            return commitOffset(ApiKeys.OFFSET_COMMIT.latestVersion(), request);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.OFFSET_COMMIT,\n                    version,\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = offsetMetadataManager.commitOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.TXN_OFFSET_COMMIT,\n                    ApiKeys.TXN_OFFSET_COMMIT.latestVersion(),\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> result = offsetMetadataManager.commitTransactionalOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(record -> replay(\n                request.producerId(),\n                record\n            ));\n\n            return result;\n        }\n\n        public List<CoordinatorRecord> deletePartitions(\n            List<TopicPartition> topicPartitions\n        ) {\n            List<CoordinatorRecord> records = offsetMetadataManager.onPartitionsDeleted(topicPartitions);\n            records.forEach(this::replay);\n            return records;\n        }\n\n        public CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> deleteOffsets(\n            OffsetDeleteRequestData request\n        ) {\n            CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> result = offsetMetadataManager.deleteOffsets(request);\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public int deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        ) {\n            List<CoordinatorRecord> addedRecords = new ArrayList<>();\n            int numDeletedOffsets = offsetMetadataManager.deleteAllOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return numDeletedOffsets;\n        }\n\n        public boolean cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records) {\n            List<CoordinatorRecord> addedRecords = new ArrayList<>();\n            boolean isOffsetsEmptyForGroup = offsetMetadataManager.cleanupExpiredOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return isOffsetsEmptyForGroup;\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            return fetchOffsets(\n                groupId,\n                null,\n                -1,\n                topics,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch)\n                    .setTopics(topics),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        ) {\n            return fetchAllOffsets(\n                groupId,\n                null,\n                -1,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchAllOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> sleep(long ms) {\n            time.sleep(ms);\n            List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = timer.poll();\n            timeouts.forEach(timeout -> {\n                if (timeout.result.replayRecords()) {\n                    timeout.result.records().forEach(this::replay);\n                }\n            });\n            return timeouts;\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        ) {\n            commitOffset(\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                time.milliseconds()\n            );\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            commitOffset(\n                RecordBatch.NO_PRODUCER_ID,\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                commitTimestamp\n            );\n        }\n\n        public void commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            replay(producerId, CoordinatorRecordHelpers.newOffsetCommitRecord(\n                groupId,\n                topic,\n                partition,\n                new OffsetAndMetadata(\n                    offset,\n                    OptionalInt.of(leaderEpoch),\n                    \"metadata\",\n                    commitTimestamp,\n                    OptionalLong.empty()\n                ),\n                MetadataVersion.latestTesting()\n            ));\n        }\n\n        public void deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            replay(CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\n                groupId,\n                topic,\n                partition\n            ));\n        }\n\n        private ApiMessage messageOrNull(ApiMessageAndVersion apiMessageAndVersion) {\n            if (apiMessageAndVersion == null) {\n                return null;\n            } else {\n                return apiMessageAndVersion.message();\n            }\n        }\n\n        private void replay(\n            CoordinatorRecord record\n        ) {\n            replay(\n                RecordBatch.NO_PRODUCER_ID,\n                record\n            );\n        }\n\n        private void replay(\n            long producerId,\n            CoordinatorRecord record\n        ) {\n            idempotentCreateSnapshot(lastWrittenOffset);\n\n            ApiMessageAndVersion key = record.key();\n            ApiMessageAndVersion value = record.value();\n\n            if (key == null) {\n                throw new IllegalStateException(\"Received a null key in \" + record);\n            }\n\n            switch (key.version()) {\n                case OffsetCommitKey.HIGHEST_SUPPORTED_VERSION:\n                    offsetMetadataManager.replay(\n                        lastWrittenOffset,\n                        producerId,\n                        (OffsetCommitKey) key.message(),\n                        (OffsetCommitValue) messageOrNull(value)\n                    );\n                    break;\n\n                default:\n                    throw new IllegalStateException(\"Received an unknown record type \" + key.version()\n                        + \" in \" + record);\n            }\n\n            lastWrittenOffset++;\n        }\n\n        private void replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        ) {\n            idempotentCreateSnapshot(lastWrittenOffset);\n            offsetMetadataManager.replayEndTransactionMarker(producerId, result);\n            lastWrittenOffset++;\n        }\n\n        private void idempotentCreateSnapshot(long lastWrittenOffset) {\n            snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n        }\n\n        public void testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        ) {\n            final OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection requestTopicCollection =\n                new OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection(Collections.singletonList(\n                    new OffsetDeleteRequestData.OffsetDeleteRequestTopic()\n                        .setName(topic)\n                        .setPartitions(Collections.singletonList(\n                            new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(partition)\n                        ))\n                ).iterator());\n\n            final OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection expectedResponsePartitionCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection();\n            expectedResponsePartitionCollection.add(\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartition()\n                    .setPartitionIndex(partition)\n                    .setErrorCode(expectedError.code())\n            );\n\n            final OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection expectedResponseTopicCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection(Collections.singletonList(\n                    new OffsetDeleteResponseData.OffsetDeleteResponseTopic()\n                        .setName(topic)\n                        .setPartitions(expectedResponsePartitionCollection)\n                ).iterator());\n\n            List<CoordinatorRecord> expectedRecords = Collections.emptyList();\n            if (hasOffset(groupId, topic, partition) && expectedError == Errors.NONE) {\n                expectedRecords = Collections.singletonList(\n                    CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(groupId, topic, partition)\n                );\n            }\n\n            final CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> coordinatorResult = deleteOffsets(\n                new OffsetDeleteRequestData()\n                    .setGroupId(groupId)\n                    .setTopics(requestTopicCollection)\n            );\n\n            assertEquals(new OffsetDeleteResponseData().setTopics(expectedResponseTopicCollection), coordinatorResult.response());\n            assertEquals(expectedRecords, coordinatorResult.records());\n        }\n\n        public boolean hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            return offsetMetadataManager.hasCommittedOffset(groupId, topic, partition) ||\n                offsetMetadataManager.hasPendingTransactionalOffsets(groupId, topic, partition);\n        }\n    }",
                "methodCount": 30
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 6,
                "candidates": [
                    {
                        "lineStart": 519,
                        "lineEnd": 521,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() directly interfaces with the SnapshotRegistry class by calling the getOrCreateSnapshot() method, which is a crucial part of SnapshotRegistry's responsibilities. Moving the idempotentCreateSnapshot() method to SnapshotRegistry promotes encapsulation by keeping all snapshot-related logic within the SnapshotRegistry class, eliminating redundant accessor methods and reducing coupling which can improve maintainability and readability of the code.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 390,
                        "lineEnd": 405,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class MockTime",
                        "description": "Move method commitOffset to org.apache.kafka.common.utils.MockTime\nRationale: The method 'commitOffset' utilizes the current time obtained from the 'time.milliseconds()'. The 'MockTime' class provides the current time through the 'milliseconds()' method, indicating that 'commitOffset' could rely on 'MockTime' to obtain this timing information. Therefore, moving 'commitOffset' to 'MockTime' can streamline the usage of time-dependent functionalities and ensure that time-related operations are centrally managed within the 'MockTime' class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 213,
                        "lineEnd": 217,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method commitOffset to class GroupCoordinatorMetricsShard",
                        "description": "Move method commitOffset to org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsShard\nRationale: The commitOffset() method is highly relevant to the functionality of the GroupCoordinatorMetricsShard class, which deals with managing offsets and groups in a Kafka coordinator context. Moving the method to GroupCoordinatorMetricsShard aligns the method's responsibilities closely with the metrics and offset management capabilities already in the class. This ensures a higher cohesion within the class and keeps related functionalities together.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 570,
                        "lineEnd": 577,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method hasOffset to class OffsetMetadataManager",
                        "description": "Move method hasOffset to org.apache.kafka.coordinator.group.OffsetMetadataManager\nRationale: The `hasOffset` method relies directly on methods from the `OffsetMetadataManager` class (`hasCommittedOffset` and `hasPendingTransactionalOffsets`). This indicates that the `OffsetMetadataManager` is already handling most of the related functionality. Keeping this method within `OffsetMetadataManager` will modularize the offset management responsibilities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 187,
                        "lineEnd": 205,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method getOrMaybeCreateGroup to class GroupMetadataManager",
                        "description": "Move method getOrMaybeCreateGroup to org.apache.kafka.coordinator.group.GroupMetadataManager\nRationale: The method 'getOrMaybeCreateGroup' is a dispatcher that calls other methods from GroupMetadataManager depending on the GroupType. Hence, it should be a part of GroupMetadataManager to align with the Single Responsibility Principle and keep related functionalities cohesive.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 462,
                        "lineEnd": 468,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method messageOrNull to class ApiMessageAndVersion",
                        "description": "Move method messageOrNull to org.apache.kafka.server.common.ApiMessageAndVersion\nRationale: The 'messageOrNull' method directly deals with the 'ApiMessageAndVersion' class, accessing its 'message' method. By placing this method within 'ApiMessageAndVersion', it improves cohesion and encapsulation, making the utility method directly accessible in the context of its primary use-case. This eliminates an unnecessary dependency on external classes for such a basic utility function.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "commit",
                            "method_signature": "public void commit()",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The commit method is primarily concerned with deleting snapshots up to a certain offset, which is a responsibility more closely related to the SnapshotRegistry class."
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(OffsetCommitRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "This method's primary action is committing offsets, which is the core functionality of the OffsetMetadataManager class."
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(short version, OffsetCommitRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Similar rationale as the other commitOffset method, this version involves committing offsets, which falls under the responsibilities of the OffsetMetadataManager."
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> commitTransactionalOffset(TxnOffsetCommitRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "This method commits transactional offsets, which are more suitably managed by the OffsetMetadataManager class."
                        },
                        {
                            "method_name": "deletePartitions",
                            "method_signature": "public List<CoordinatorRecord> deletePartitions(List<TopicPartition> topicPartitions)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Deleting partitions and associated offsets should be handled by the OffsetMetadataManager."
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> deleteOffsets(OffsetDeleteRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Deleting offsets is the responsibility of OffsetMetadataManager and should be managed there."
                        },
                        {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public int deleteAllOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Deleting all offsets for a specified group is related to offset management, which is the responsibility of the OffsetMetadataManager."
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public boolean cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Cleaning up expired offsets is directly related to offset management and should be handled by the OffsetMetadataManager."
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(String groupId, List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Fetching offsets is a specific task related to offset management, which should be done by OffsetMetadataManager."
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(String groupId, String memberId, int memberEpoch, List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Fetching offsets for specific members and epochs is relevant to the responsibilities of the OffsetMetadataManager."
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(String groupId, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Fetching all offsets for a given group should be handled by the OffsetMetadataManager."
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(String groupId, String memberId, int memberEpoch, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Fetching all offsets for specific members and epochs falls under the responsibilities of the OffsetMetadataManager."
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public void commitOffset(String groupId, String topic, int partition, long offset, int leaderEpoch)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Committing offsets for specific group, topic, and partition is better handled by the OffsetMetadataManager."
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public void commitOffset(String groupId, String topic, int partition, long offset, int leaderEpoch, long commitTimestamp)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Committing offsets with additional commit timestamp is a task suited for the OffsetMetadataManager."
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public void commitOffset(long producerId, String groupId, String topic, int partition, long offset, int leaderEpoch, long commitTimestamp)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Committing offsets with additional producer information should be done by the OffsetMetadataManager."
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public void deleteOffset(String groupId, String topic, int partition)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Deleting specific offsets related to group, topic, and partition belongs in the OffsetMetadataManager class."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private void replay(CoordinatorRecord record)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Replaying CoordinatorRecords is a core functionality of the OffsetMetadataManager."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private void replay(long producerId, CoordinatorRecord record)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Replaying CoordinatorRecords with producer information should be managed by the OffsetMetadataManager."
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private void replayEndTransactionMarker(long producerId, TransactionResult result)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Replaying end transaction markers is directly linked to the responsibilities of the OffsetMetadataManager."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(long lastWrittenOffset)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "Creating snapshots based on offsets is the primary responsibility of the SnapshotRegistry class."
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public boolean hasOffset(String groupId, String topic, int partition)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Checking if offsets are committed or pending for specific groups, topics, and partitions is the responsibility of the OffsetMetadataManager."
                        }
                    ],
                    "llm_response_time": 15285
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(OffsetCommitRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "This method directly interacts with the OffsetMetadataManager for committing offsets, making it more cohesive to place it within OffsetMetadataManager."
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(short version, OffsetCommitRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Similar to the previous method, this one also interacts directly with OffsetMetadataManager for committing offsets with a specified version, making it more appropriate within OffsetMetadataManager."
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> commitTransactionalOffset(TxnOffsetCommitRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "This method handles the transactional offset commit which is a core functionality of OffsetMetadataManager, hence should be moved there."
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> deleteOffsets(OffsetDeleteRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Deleting offsets is a central operation of OffsetMetadataManager, and this method should logically reside there."
                        },
                        {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public int deleteAllOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "This method deals with deleting all offsets for a given group which is a direct responsibility of OffsetMetadataManager."
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public boolean cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Cleaning up expired offsets is a specialized operation that fits more naturally within OffsetMetadataManager."
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(String groupId, List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Fetching offsets is a core functionality for managing offsets within OffsetMetadataManager."
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(String groupId, String memberId, int memberEpoch, List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "This method also fetches offsets but with extended parameters, and it belongs in OffsetMetadataManager just like the simpler fetchOffsets method."
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(String groupId, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Fetching all offsets for a group is a comprehensive offset management operation, thus should be moved to OffsetMetadataManager."
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(String groupId, String memberId, int memberEpoch, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Similar to the previous method but with additional parameters, this one should also be part of OffsetMetadataManager."
                        }
                    ],
                    "llm_response_time": 8757
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(short version, OffsetCommitRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "The method commitOffset interacts directly with and manipulates OffsetMetadataManager and CoordinatorResult classes. This behavior suggests a more natural fit within OffsetMetadataManager."
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> commitTransactionalOffset(TxnOffsetCommitRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "The method commitTransactionalOffset is built around OffsetMetadataManager and coordinates transaction-related offsets. It is logical to place this within the OffsetMetadataManager class."
                        },
                        {
                            "method_name": "deletePartitions",
                            "method_signature": "public List<CoordinatorRecord> deletePartitions(List<TopicPartition> topicPartitions)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "The deletePartitions method primarily operates by calling the OffsetMetadataManager.onPartitionsDeleted method and manages its records. It should reside within OffsetMetadataManager."
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> deleteOffsets(OffsetDeleteRequestData request)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "The deleteOffsets method delegates most of its work to the OffsetMetadataManager.deleteOffsets method and handles records. It naturally fits within OffsetMetadataManager."
                        },
                        {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public int deleteAllOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "The primary responsibility of deleteAllOffsets is to interact with and manage offsets via OffsetMetadataManager. It is more suitably placed within the OffsetMetadataManager class."
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public boolean cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "This method is focused on managing offsets and their expiration, utilizing the facilities provided by OffsetMetadataManager, and thus belongs in that class."
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(String groupId, String memberId, int memberEpoch, List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "fetchOffsets relies heavily on methods and data from OffsetMetadataManager, making it a more cohesive and logical fit within that class."
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(String groupId, String memberId, int memberEpoch, long committedOffset)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "Similar to fetchOffsets, fetchAllOffsets primarily utilizes OffsetMetadataManager for its operations and belongs in that class."
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public boolean hasOffset(String groupId, String topic, int partition)",
                            "target_class": "OffsetMetadataManager",
                            "rationale": "The hasOffset method directly checks the state managed by OffsetMetadataManager and would be more appropriately placed within it."
                        }
                    ],
                    "llm_response_time": 7448
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deletePartitions",
                            "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long lastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testOffsetDeleteWith",
                            "method_signature": "public testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long lastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deletePartitions",
                            "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(long lastWrittenOffset)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long lastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3661704578137439
                    },
                    "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )": {
                        "first": {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.36704609496973545
                    },
                    "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.39479973254262324
                    },
                    "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4229545887356226
                    },
                    "public commitOffset(\n            OffsetCommitRequestData request\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.42531130879089224
                    },
                    "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                        "first": {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4497067973233583
                    },
                    "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )": {
                        "first": {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.45693269895919714
                    },
                    "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )": {
                        "first": {
                            "method_name": "deletePartitions",
                            "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4579600021991615
                    },
                    "public sleep(long ms)": {
                        "first": {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.46272510370226666
                    },
                    "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )": {
                        "first": {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4756129905283165
                    },
                    "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.48272274818617783
                    },
                    "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )": {
                        "first": {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4862087309316981
                    },
                    "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)": {
                        "first": {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.49450578763221575
                    },
                    "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)": {
                        "first": {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5040037740243367
                    },
                    "public commit()": {
                        "first": {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5278603726439697
                    }
                },
                "voyage": {
                    "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)": {
                        "first": {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.194728082586309
                    },
                    "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )": {
                        "first": {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.32974010225032624
                    },
                    "public sleep(long ms)": {
                        "first": {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.42845566185252165
                    },
                    "public commit()": {
                        "first": {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.480905886027607
                    },
                    "private idempotentCreateSnapshot(long lastWrittenOffset)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long lastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.49140030998584455
                    },
                    "public commitOffset(\n            OffsetCommitRequestData request\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5414472040604922
                    },
                    "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )": {
                        "first": {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5617325996162863
                    },
                    "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )": {
                        "first": {
                            "method_name": "deletePartitions",
                            "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5699262083561158
                    },
                    "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)": {
                        "first": {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5852671870379208
                    },
                    "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5855649652634872
                    },
                    "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                        "first": {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5951912476656116
                    },
                    "public deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        )": {
                        "first": {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6110565118759469
                    },
                    "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )": {
                        "first": {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6191873011910383
                    },
                    "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )": {
                        "first": {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6236813108929176
                    },
                    "public testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        )": {
                        "first": {
                            "method_name": "testOffsetDeleteWith",
                            "method_signature": "public testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6331196480973885
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(long lastWrittenOffset)",
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "public sleep(long ms)",
                        "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                        "public commit()"
                    ],
                    "llm_response_time": 3129
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(long lastWrittenOffset)"
                    ],
                    "llm_response_time": 4038
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(long lastWrittenOffset)"
                    ],
                    "llm_response_time": 2919
                },
                "voyage": {
                    "priority_method_names": [
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                        "public commit()",
                        "private idempotentCreateSnapshot(long lastWrittenOffset)",
                        "public sleep(long ms)"
                    ],
                    "llm_response_time": 2661
                },
                "voyage-5": {
                    "priority_method_names": [
                        "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                        "public sleep(long ms)",
                        "public commit()",
                        "private idempotentCreateSnapshot(long lastWrittenOffset)",
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)"
                    ],
                    "llm_response_time": 4666
                },
                "voyage-3": {
                    "priority_method_names": [
                        "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                        "public sleep(long ms)",
                        "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)"
                    ],
                    "llm_response_time": 6421
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "replayEndTransactionMarker": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "commitOffset": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "replay": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "hasOffset": {
                    "target_classes": [
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.18169682579561122
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.265533546076446
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "OffsetMetadataManager",
                        "GroupMetadataManager"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 10,
                    "similarity_metric": "cosine"
                },
                "deleteOffsets": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "deletePartitions": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "sleep": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "commitTransactionalOffset": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "getOrMaybeCreateGroup": {
                    "target_classes": [
                        {
                            "class_name": "GroupType",
                            "similarity_score": 0.32015738118488507
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.3341613256230055
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "GroupMetadataManager",
                        "GroupType"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 8,
                    "similarity_metric": "cosine"
                },
                "cleanupExpiredOffsets": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "messageOrNull": {
                    "target_classes": [
                        {
                            "class_name": "ApiMessageAndVersion",
                            "similarity_score": 0.6106401198187058
                        },
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.5914470146353319
                        },
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.4220009252215372
                        },
                        {
                            "class_name": "GroupCoordinatorMetricsShard",
                            "similarity_score": 0.4615361902486966
                        },
                        {
                            "class_name": "GroupMetadataManager",
                            "similarity_score": 0.35945481197585616
                        },
                        {
                            "class_name": "OffsetMetadataManager",
                            "similarity_score": 0.48967032494126206
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ApiMessageAndVersion",
                        "OffsetMetadataManager",
                        "MockTime"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 10,
                    "similarity_metric": "cosine"
                },
                "commit": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testTimelineGaugeCounters() : void in class org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsShardTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 48,
                    "endLine": 99,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testTimelineGaugeCounters() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 64,
                    "endLine": 64,
                    "startColumn": 9,
                    "endColumn": 52,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 48,
                    "endLine": 99,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testTimelineGaugeCounters() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 64,
                    "endLine": 64,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 90,
                    "endLine": 90,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(2000)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 609,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a90c2d6088f885dc16d184ed13771ee9623b805d",
            "newBranchName": "extract-idempotentCreateSnapshot-testTimelineGaugeCounters-130af38"
        },
        "telemetry": {
            "id": "3614dccd-571c-42ca-8ced-ad0dcfaae5a4",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 201,
                "lineStart": 46,
                "lineEnd": 246,
                "bodyLineStart": 46,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                "sourceCode": "public class GroupCoordinatorMetricsShardTest {\n\n    @Test\n    public void testTimelineGaugeCounters() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(snapshotRegistry, tp);\n\n        shard.incrementNumOffsets();\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD);\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        // The value should not be updated until the offset has been committed.\n        assertEquals(0, shard.numOffsets());\n        assertEquals(0, shard.numConsumerGroups());\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n\n        shard.commitUpTo(1000);\n        assertEquals(1, shard.numOffsets());\n        assertEquals(5, shard.numConsumerGroups());\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n\n        shard.decrementNumOffsets();\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD);\n\n        snapshotRegistry.getOrCreateSnapshot(2000);\n        shard.commitUpTo(2000);\n        assertEquals(0, shard.numOffsets());\n        assertEquals(0, shard.numConsumerGroups());\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(1000);\n    }\n\n    @Test\n    public void testGenericGroupStateTransitionMetrics() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(new SnapshotRegistry(new LogContext()), tp);\n        coordinatorMetrics.activateMetricsShard(shard);\n\n        LogContext logContext = new LogContext();\n        ClassicGroup group0 = new ClassicGroup(logContext, \"groupId0\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group1 = new ClassicGroup(logContext, \"groupId1\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group2 = new ClassicGroup(logContext, \"groupId2\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group3 = new ClassicGroup(logContext, \"groupId3\", EMPTY, Time.SYSTEM, shard);\n\n        IntStream.range(0, 4).forEach(__ -> shard.incrementNumClassicGroups(EMPTY));\n\n        assertEquals(4, shard.numClassicGroups());\n\n        group0.transitionTo(PREPARING_REBALANCE);\n        group0.transitionTo(COMPLETING_REBALANCE);\n        group1.transitionTo(PREPARING_REBALANCE);\n        group2.transitionTo(DEAD);\n\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.EMPTY));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.DEAD));\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.STABLE));\n\n        group0.transitionTo(STABLE);\n        group1.transitionTo(COMPLETING_REBALANCE);\n        group3.transitionTo(DEAD);\n\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.EMPTY));\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        assertEquals(2, shard.numClassicGroups(ClassicGroupState.DEAD));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.STABLE));\n\n        assertGaugeValue(\n            metrics,\n            metrics.metricName(\"group-count\", \"group-coordinator-metrics\", Collections.singletonMap(\"protocol\", \"classic\")),\n            4\n        );\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroups\"), 4);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsEmpty\"), 0);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsPreparingRebalance\"), 0);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsCompletingRebalance\"), 1);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsDead\"), 2);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsStable\"), 1);\n    }\n\n    @Test\n    public void testConsumerGroupStateTransitionMetrics() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(snapshotRegistry, tp);\n        coordinatorMetrics.activateMetricsShard(shard);\n\n        ConsumerGroup group0 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-0\",\n            shard\n        );\n        ConsumerGroup group1 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-1\",\n            shard\n        );\n        ConsumerGroup group2 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-2\",\n            shard\n        );\n        ConsumerGroup group3 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-3\",\n            shard\n        );\n\n        IntStream.range(0, 4).forEach(__ -> shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        shard.commitUpTo(1000);\n        assertEquals(4, shard.numConsumerGroups());\n        assertEquals(4, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n\n        ConsumerGroupMember member0 = group0.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member1 = group1.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member2 = group2.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member3 = group3.getOrMaybeCreateMember(\"member-id\", true);\n        group0.updateMember(member0);\n        group1.updateMember(member1);\n        group2.updateMember(member2);\n        group3.updateMember(member3);\n\n        snapshotRegistry.getOrCreateSnapshot(2000);\n        shard.commitUpTo(2000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(4, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        group2.setGroupEpoch(1);\n        group3.setGroupEpoch(1);\n\n        snapshotRegistry.getOrCreateSnapshot(3000);\n        shard.commitUpTo(3000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        group2.setTargetAssignmentEpoch(1);\n\n        // Set member2 to ASSIGNING state.\n        new ConsumerGroupMember.Builder(member2)\n            .setPartitionsPendingRevocation(Collections.singletonMap(Uuid.ZERO_UUID, Collections.singleton(0)))\n            .build();\n\n        snapshotRegistry.getOrCreateSnapshot(4000);\n        shard.commitUpTo(4000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        assertGaugeValue(metrics, metrics.metricName(\"group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"protocol\", \"consumer\")), 4);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.EMPTY.toString())), 0);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.ASSIGNING.toString())), 1);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.RECONCILING.toString())), 1);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.STABLE.toString())), 2);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.DEAD.toString())), 0);\n    }\n}",
                "methodCount": 4
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 100,
                        "lineEnd": 102,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method `idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)` is directly involved with the creation and management of snapshots, and it calls the `getOrCreateSnapshot` method which already exists within the `SnapshotRegistry` class. Since the method is tightly coupled with the functionality provided by `SnapshotRegistry`, moving it into the `SnapshotRegistry` class ensures cohesion and encapsulates all snapshot-related operations within a single, logical entity.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The method idempotentCreateSnapshot operates on an instance of SnapshotRegistry and creates a snapshot. It makes more sense for it to be a method within the SnapshotRegistry class."
                        }
                    ],
                    "llm_response_time": 2204
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The method idempotentCreateSnapshot operates on a SnapshotRegistry instance and is closely related to its functionality. Moving it to the SnapshotRegistry class makes the code more cohesive and encapsulated."
                        }
                    ],
                    "llm_response_time": 1806
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The method idempotentCreateSnapshot primarily deals with the SnapshotRegistry instance and its snapshot creation. Hence, it logically belongs to the SnapshotRegistry class."
                        }
                    ],
                    "llm_response_time": 2810
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.07334800440146717
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.44523321255057197
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1312
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "voyage": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 2296,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testConsumerGroupStateTransitionMetrics() : void in class org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsShardTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 154,
                    "endLine": 241,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testConsumerGroupStateTransitionMetrics() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 187,
                    "endLine": 187,
                    "startColumn": 9,
                    "endColumn": 52,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 154,
                    "endLine": 241,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testConsumerGroupStateTransitionMetrics() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 187,
                    "endLine": 187,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 201,
                    "endLine": 201,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(2000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 209,
                    "endLine": 209,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(3000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 222,
                    "endLine": 222,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(4000)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 610,
        "extraction_results": {
            "success": true,
            "newCommitHash": "e118294a728c0c6eae034ce5820762d2b6315112",
            "newBranchName": "extract-idempotentCreateSnapshot-testConsumerGroupStateTransitionMetrics-130af38"
        },
        "telemetry": {
            "id": "4b336b7a-5499-42f3-aa07-167546bc0264",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 201,
                "lineStart": 46,
                "lineEnd": 246,
                "bodyLineStart": 46,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                "sourceCode": "public class GroupCoordinatorMetricsShardTest {\n\n    @Test\n    public void testTimelineGaugeCounters() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(snapshotRegistry, tp);\n\n        shard.incrementNumOffsets();\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD);\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        // The value should not be updated until the offset has been committed.\n        assertEquals(0, shard.numOffsets());\n        assertEquals(0, shard.numConsumerGroups());\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n\n        shard.commitUpTo(1000);\n        assertEquals(1, shard.numOffsets());\n        assertEquals(5, shard.numConsumerGroups());\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n\n        shard.decrementNumOffsets();\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD);\n\n        snapshotRegistry.getOrCreateSnapshot(2000);\n        shard.commitUpTo(2000);\n        assertEquals(0, shard.numOffsets());\n        assertEquals(0, shard.numConsumerGroups());\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n    }\n\n    @Test\n    public void testGenericGroupStateTransitionMetrics() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(new SnapshotRegistry(new LogContext()), tp);\n        coordinatorMetrics.activateMetricsShard(shard);\n\n        LogContext logContext = new LogContext();\n        ClassicGroup group0 = new ClassicGroup(logContext, \"groupId0\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group1 = new ClassicGroup(logContext, \"groupId1\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group2 = new ClassicGroup(logContext, \"groupId2\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group3 = new ClassicGroup(logContext, \"groupId3\", EMPTY, Time.SYSTEM, shard);\n\n        IntStream.range(0, 4).forEach(__ -> shard.incrementNumClassicGroups(EMPTY));\n\n        assertEquals(4, shard.numClassicGroups());\n\n        group0.transitionTo(PREPARING_REBALANCE);\n        group0.transitionTo(COMPLETING_REBALANCE);\n        group1.transitionTo(PREPARING_REBALANCE);\n        group2.transitionTo(DEAD);\n\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.EMPTY));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.DEAD));\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.STABLE));\n\n        group0.transitionTo(STABLE);\n        group1.transitionTo(COMPLETING_REBALANCE);\n        group3.transitionTo(DEAD);\n\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.EMPTY));\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        assertEquals(2, shard.numClassicGroups(ClassicGroupState.DEAD));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.STABLE));\n\n        assertGaugeValue(\n            metrics,\n            metrics.metricName(\"group-count\", \"group-coordinator-metrics\", Collections.singletonMap(\"protocol\", \"classic\")),\n            4\n        );\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroups\"), 4);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsEmpty\"), 0);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsPreparingRebalance\"), 0);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsCompletingRebalance\"), 1);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsDead\"), 2);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsStable\"), 1);\n    }\n\n    @Test\n    public void testConsumerGroupStateTransitionMetrics() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(snapshotRegistry, tp);\n        coordinatorMetrics.activateMetricsShard(shard);\n\n        ConsumerGroup group0 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-0\",\n            shard\n        );\n        ConsumerGroup group1 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-1\",\n            shard\n        );\n        ConsumerGroup group2 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-2\",\n            shard\n        );\n        ConsumerGroup group3 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-3\",\n            shard\n        );\n\n        IntStream.range(0, 4).forEach(__ -> shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        shard.commitUpTo(1000);\n        assertEquals(4, shard.numConsumerGroups());\n        assertEquals(4, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n\n        ConsumerGroupMember member0 = group0.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member1 = group1.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member2 = group2.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member3 = group3.getOrMaybeCreateMember(\"member-id\", true);\n        group0.updateMember(member0);\n        group1.updateMember(member1);\n        group2.updateMember(member2);\n        group3.updateMember(member3);\n\n        snapshotRegistry.getOrCreateSnapshot(2000);\n        shard.commitUpTo(2000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(4, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        group2.setGroupEpoch(1);\n        group3.setGroupEpoch(1);\n\n        snapshotRegistry.getOrCreateSnapshot(3000);\n        shard.commitUpTo(3000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        group2.setTargetAssignmentEpoch(1);\n\n        // Set member2 to ASSIGNING state.\n        new ConsumerGroupMember.Builder(member2)\n            .setPartitionsPendingRevocation(Collections.singletonMap(Uuid.ZERO_UUID, Collections.singleton(0)))\n            .build();\n\n        snapshotRegistry.getOrCreateSnapshot(4000);\n        shard.commitUpTo(4000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        assertGaugeValue(metrics, metrics.metricName(\"group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"protocol\", \"consumer\")), 4);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.EMPTY.toString())), 0);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.ASSIGNING.toString())), 1);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.RECONCILING.toString())), 1);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.STABLE.toString())), 2);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.DEAD.toString())), 0);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(1000);\n    }\n}",
                "methodCount": 4
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 242,
                        "lineEnd": 244,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method `idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)` is directly involved with the creation and management of snapshots, and it calls the `getOrCreateSnapshot` method which already exists within the `SnapshotRegistry` class. Since the method is tightly coupled with the functionality provided by `SnapshotRegistry`, moving it into the `SnapshotRegistry` class ensures cohesion and encapsulates all snapshot-related operations within a single, logical entity.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The idempotentCreateSnapshot method directly interacts with SnapshotRegistry to perform an operation that is conceptually more aligned with its responsibilities. Moving it there would centralize snapshot-related logic and maintain separation of concerns."
                        }
                    ],
                    "llm_response_time": 2078
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The method idempotentCreateSnapshot is tightly coupled with the SnapshotRegistry. By moving this method to SnapshotRegistry, we enhance the cohesion of the class."
                        }
                    ],
                    "llm_response_time": 1522
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The method idempotentCreateSnapshot is directly manipulating the state of the SnapshotRegistry class and thus would be more appropriately situated within the SnapshotRegistry class."
                        }
                    ],
                    "llm_response_time": 2043
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.07334800440146717
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.44302328300117577
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1286
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "voyage": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public aggregateShards() : void in class org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 120,
                    "endLine": 172,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public aggregateShards() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 157,
                    "endLine": 157,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 120,
                    "endLine": 172,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public aggregateShards() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 157,
                    "endLine": 157,
                    "startColumn": 9,
                    "endColumn": 57,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry0.idempotentCreateSnapshot(1000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 158,
                    "endLine": 158,
                    "startColumn": 9,
                    "endColumn": 57,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry1.idempotentCreateSnapshot(1500)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 611,
        "extraction_results": {
            "success": true,
            "newCommitHash": "f76c771f07e85852f1fca4bd5e7c2e881fe59b87",
            "newBranchName": "extract-idempotentCreateSnapshot-aggregateShards-130af38"
        },
        "telemetry": {
            "id": "dac18f95-ecca-4d84-9eb0-246b61ab1062",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 159,
                "lineStart": 50,
                "lineEnd": 208,
                "bodyLineStart": 50,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                "sourceCode": "public class GroupCoordinatorMetricsTest {\n\n    @Test\n    public void testMetricNames() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n\n        HashSet<org.apache.kafka.common.MetricName> expectedMetrics = new HashSet<>(Arrays.asList(\n            metrics.metricName(\"offset-commit-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-commit-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-expiration-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-expiration-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-deletion-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-deletion-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"group-completed-rebalance-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"group-completed-rebalance-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"consumer-group-rebalance-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"consumer-group-rebalance-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\n                \"group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"protocol\", \"classic\")),\n            metrics.metricName(\n                \"group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"protocol\", \"consumer\")),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.EMPTY.toString())),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.ASSIGNING.toString())),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.RECONCILING.toString())),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.STABLE.toString())),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.DEAD.toString()))\n        ));\n\n        try {\n            try (GroupCoordinatorMetrics ignored = new GroupCoordinatorMetrics(registry, metrics)) {\n                HashSet<String> expectedRegistry = new HashSet<>(Arrays.asList(\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumOffsets\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroups\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsPreparingRebalance\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsCompletingRebalance\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsStable\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsDead\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsEmpty\"\n                ));\n\n                assertMetricsForTypeEqual(registry, \"kafka.coordinator.group\", expectedRegistry);\n                expectedMetrics.forEach(metricName -> assertTrue(metrics.metrics().containsKey(metricName)));\n            }\n            assertMetricsForTypeEqual(registry, \"kafka.coordinator.group\", Collections.emptySet());\n            expectedMetrics.forEach(metricName -> assertFalse(metrics.metrics().containsKey(metricName)));\n        } finally {\n            registry.shutdown();\n        }\n    }\n\n    @Test\n    public void aggregateShards() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        SnapshotRegistry snapshotRegistry0 = new SnapshotRegistry(new LogContext());\n        SnapshotRegistry snapshotRegistry1 = new SnapshotRegistry(new LogContext());\n        TopicPartition tp0 = new TopicPartition(\"__consumer_offsets\", 0);\n        TopicPartition tp1 = new TopicPartition(\"__consumer_offsets\", 1);\n        GroupCoordinatorMetricsShard shard0 = coordinatorMetrics.newMetricsShard(snapshotRegistry0, tp0);\n        GroupCoordinatorMetricsShard shard1 = coordinatorMetrics.newMetricsShard(snapshotRegistry1, tp1);\n        coordinatorMetrics.activateMetricsShard(shard0);\n        coordinatorMetrics.activateMetricsShard(shard1);\n\n        IntStream.range(0, 5).forEach(__ -> shard0.incrementNumClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        IntStream.range(0, 1).forEach(__ -> shard0.decrementNumClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        IntStream.range(0, 5).forEach(__ -> shard1.incrementNumClassicGroups(ClassicGroupState.STABLE));\n        IntStream.range(0, 4).forEach(__ -> shard1.incrementNumClassicGroups(ClassicGroupState.DEAD));\n        IntStream.range(0, 4).forEach(__ -> shard1.decrementNumClassicGroups(ClassicGroupState.EMPTY));\n\n        IntStream.range(0, 5).forEach(__ -> shard0.incrementNumConsumerGroups(ConsumerGroupState.ASSIGNING));\n        IntStream.range(0, 5).forEach(__ -> shard1.incrementNumConsumerGroups(ConsumerGroupState.RECONCILING));\n        IntStream.range(0, 3).forEach(__ -> shard1.decrementNumConsumerGroups(ConsumerGroupState.DEAD));\n\n        IntStream.range(0, 6).forEach(__ -> shard0.incrementNumOffsets());\n        IntStream.range(0, 2).forEach(__ -> shard1.incrementNumOffsets());\n        IntStream.range(0, 1).forEach(__ -> shard1.decrementNumOffsets());\n\n        assertEquals(4, shard0.numClassicGroups());\n        assertEquals(5, shard1.numClassicGroups());\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroups\"), 9);\n        assertGaugeValue(\n            metrics,\n            metrics.metricName(\"group-count\", METRICS_GROUP, Collections.singletonMap(\"protocol\", \"classic\")),\n            9\n        );\n\n        idempotentCreateSnapshot(snapshotRegistry0);\n        snapshotRegistry1.getOrCreateSnapshot(1500);\n        shard0.commitUpTo(1000);\n        shard1.commitUpTo(1500);\n\n        assertEquals(5, shard0.numConsumerGroups());\n        assertEquals(2, shard1.numConsumerGroups());\n        assertEquals(6, shard0.numOffsets());\n        assertEquals(1, shard1.numOffsets());\n        assertGaugeValue(\n            metrics,\n            metrics.metricName(\"group-count\", METRICS_GROUP, Collections.singletonMap(\"protocol\", \"consumer\")),\n            7\n        );\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumOffsets\"), 7);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0) {\n        snapshotRegistry0.getOrCreateSnapshot(1000);\n    }\n\n    @Test\n    public void testGlobalSensors() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Time time = new MockTime();\n        Metrics metrics = new Metrics(time);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(\n            new SnapshotRegistry(new LogContext()), new TopicPartition(\"__consumer_offsets\", 0)\n        );\n\n        shard.record(CLASSIC_GROUP_COMPLETED_REBALANCES_SENSOR_NAME, 10);\n        assertMetricValue(metrics, metrics.metricName(\"group-completed-rebalance-rate\", GroupCoordinatorMetrics.METRICS_GROUP), 1.0 / 3.0);\n        assertMetricValue(metrics, metrics.metricName(\"group-completed-rebalance-count\", GroupCoordinatorMetrics.METRICS_GROUP), 10);\n\n        shard.record(OFFSET_COMMITS_SENSOR_NAME, 20);\n        assertMetricValue(metrics, metrics.metricName(\"offset-commit-rate\", GroupCoordinatorMetrics.METRICS_GROUP), 2.0 / 3.0);\n        assertMetricValue(metrics, metrics.metricName(\"offset-commit-count\", GroupCoordinatorMetrics.METRICS_GROUP), 20);\n\n        shard.record(OFFSET_EXPIRED_SENSOR_NAME, 30);\n        assertMetricValue(metrics, metrics.metricName(\"offset-expiration-rate\", GroupCoordinatorMetrics.METRICS_GROUP), 1.0);\n        assertMetricValue(metrics, metrics.metricName(\"offset-expiration-count\", GroupCoordinatorMetrics.METRICS_GROUP), 30);\n\n        shard.record(CONSUMER_GROUP_REBALANCES_SENSOR_NAME, 50);\n        assertMetricValue(metrics, metrics.metricName(\"consumer-group-rebalance-rate\", GroupCoordinatorMetrics.METRICS_GROUP), 5.0 / 3.0);\n        assertMetricValue(metrics, metrics.metricName(\"consumer-group-rebalance-count\", GroupCoordinatorMetrics.METRICS_GROUP), 50);\n    }\n\n    private void assertMetricValue(Metrics metrics, MetricName metricName, double val) {\n        assertEquals(val, metrics.metric(metricName).metricValue());\n    }\n}",
                "methodCount": 5
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 2,
                "candidates": [
                    {
                        "lineStart": 204,
                        "lineEnd": 206,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method assertMetricValue to class Metrics",
                        "description": "Move method assertMetricValue to org.apache.kafka.common.metrics.Metrics\nRationale: The `assertMetricValue()` method interacts closely with the `Metrics` class by calling its `metric()` method and verifying the metric value. Since the method's primary functionality is built around the `Metrics` class, it naturally fits within this class. Keeping it in `Metrics` ensures better encapsulation and direct access to the necessary data and methods related to assertions on metrics values.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 173,
                        "lineEnd": 175,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot directly interacts with the SnapshotRegistry by utilizing its getOrCreateSnapshot method. Placing this method inside SnapshotRegistry would encapsulate the functionality within the class that it predominantly operates on. Additionally, since the method's primary responsibility is to ensure the creation of a snapshot in the registry, it logically belongs to the SnapshotRegistry class for better cohesion and to follow the Single Responsibility Principle.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The idempotentCreateSnapshot method deals specifically with the creation of snapshots in the SnapshotRegistry class. Moving it to the SnapshotRegistry class centralizes the snapshot creation logic there, improving cohesion and making the functionality more discoverable."
                        },
                        {
                            "method_name": "assertMetricValue",
                            "method_signature": "private void assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                            "target_class": "MetricsAssertionUtil",
                            "rationale": "The assertMetricValue method is a utility function for assertions, which is not specific to the GroupCoordinatorMetricsTest class. Moving it to a dedicated utility class like MetricsAssertionUtil would make it reusable in other test classes, promoting code reusability and separation of concerns."
                        }
                    ],
                    "llm_response_time": 4296
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry): void",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The method idempotentCreateSnapshot does not directly pertain to GroupCoordinatorMetricsTest and instead directly manipulates the SnapshotRegistry. Since it's a utility operation specific to SnapshotRegistry, it should be moved to that class."
                        }
                    ],
                    "llm_response_time": 2454
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "assertMetricsForTypeEqual",
                            "method_signature": "private void assertMetricsForTypeEqual(MetricsRegistry registry, String type, Set<String> expectedMetrics)",
                            "target_class": "MetricsAssertionHelper",
                            "rationale": "The method `assertMetricsForTypeEqual` performs metric assertion, which is a general functionality and can be reused in other test classes. A dedicated helper class for metrics assertions would be more appropriate."
                        },
                        {
                            "method_name": "assertGaugeValue",
                            "method_signature": "private void assertGaugeValue(MetricsRegistry registry, MetricName metricName, double expectedValue)",
                            "target_class": "MetricsAssertionHelper",
                            "rationale": "Similar to `assertMetricsForTypeEqual`, the `assertGaugeValue` method deals with asserting gauge values on metrics, which is a reusable functionality that belongs in a specialized helper class."
                        }
                    ],
                    "llm_response_time": 4168
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertMetricValue",
                            "method_signature": "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "assertMetricValue",
                            "method_signature": "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private assertMetricValue(Metrics metrics, MetricName metricName, double val)": {
                        "first": {
                            "method_name": "assertMetricValue",
                            "method_signature": "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.14530212501928982
                    },
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.17332893138462377
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4108297646013861
                    },
                    "private assertMetricValue(Metrics metrics, MetricName metricName, double val)": {
                        "first": {
                            "method_name": "assertMetricValue",
                            "method_signature": "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5190482215389689
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)"
                    ],
                    "llm_response_time": 3349
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)"
                    ],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)"
                    ],
                    "llm_response_time": 0
                },
                "voyage": {
                    "priority_method_names": [
                        "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)"
                    ],
                    "llm_response_time": 3484
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)"
                    ],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "assertMetricValue": {
                    "target_classes": [
                        {
                            "class_name": "Metrics",
                            "similarity_score": 0.1657536369617933
                        },
                        {
                            "class_name": "MetricName",
                            "similarity_score": 0.13141220939221307
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Metrics",
                        "MetricName"
                    ],
                    "llm_response_time": 2954,
                    "similarity_computation_time": 8,
                    "similarity_metric": "cosine"
                },
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 2070,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAsListedGroup() : void in class org.apache.kafka.coordinator.group.modern.consumer.ConsumerGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1104,
                    "endLine": 1121,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAsListedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1113,
                    "endLine": 1113,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1104,
                    "endLine": 1121,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAsListedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1113,
                    "endLine": 1113,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1118,
                    "endLine": 1118,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 612,
        "extraction_results": {
            "success": true,
            "newCommitHash": "8703298f378f5767ab7c5b8e0740c9b6b94fce07",
            "newBranchName": "extract-idempotentCreateSnapshot-testAsListedGroup-130af38-1"
        },
        "telemetry": {
            "id": "77e12708-f8c0-4952-8227-7506cb2aa91f",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1370,
                "lineStart": 74,
                "lineEnd": 1443,
                "bodyLineStart": 74,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                "sourceCode": "public class ConsumerGroupTest {\n\n    private ConsumerGroup createConsumerGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ConsumerGroup(\n            snapshotRegistry,\n            groupId,\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        // Create a member.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        consumerGroup.updateMember(member);\n\n        // Get that member back.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            consumerGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testNoStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Create a new member which is not static\n        consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        assertNull(consumerGroup.staticMember(\"instance-id\"));\n    }\n\n    @Test\n    public void testGetStaticMemberByInstanceId() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.staticMember(\"instance\"));\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n        assertEquals(member.memberId(), consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testRemoveStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n        assertNull(consumerGroup.staticMember(\"instance\"));\n        assertNull(consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsReassignedBeforeBeingRevoked() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(Collections.emptyMap())\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .setPartitionsPendingRevocation(Collections.emptyMap())\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsNotReleased() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        ConsumerGroupMember m2 = new ConsumerGroupMember.Builder(\"m2\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        // m2 should not be able to acquire foo-1 because the partition is\n        // still owned by another member.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.updateMember(m2));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        consumerGroup.removeMember(member.memberId());\n\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testWaitingOnUnreleasedPartition() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n        String memberId1 = Uuid.randomUuid().toString();\n        String memberId2 = Uuid.randomUuid().toString();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        consumerGroup.updateTargetAssignment(memberId1, new Assignment(mkAssignment(\n            mkTopicAssignment(fooTopicId, 1, 2, 3),\n            mkTopicAssignment(zarTopicId, 7, 8, 9)\n        )));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(memberId1)\n            .setMemberEpoch(10)\n            .setState(MemberState.UNRELEASED_PARTITIONS)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertFalse(consumerGroup.waitingOnUnreleasedPartition(member1));\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(memberId2)\n            .setMemberEpoch(10)\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 7)))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertTrue(consumerGroup.waitingOnUnreleasedPartition(member1));\n    }\n\n    @Test\n    public void testGroupState() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member2);\n        consumerGroup.setGroupEpoch(2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        consumerGroup.setTargetAssignmentEpoch(2);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        // Member 2 is not stable so the group stays in reconciling state.\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.UNREVOKED_PARTITIONS)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.UNREVOKED_PARTITIONS, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n\n        consumerGroup.removeMember(\"member1\");\n        consumerGroup.removeMember(\"member2\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n\n        assertEquals(Group.GroupType.parse(\"classic\"), Group.GroupType.CLASSIC);\n\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Consumer\"), Group.GroupType.CONSUMER);\n\n        // Test with invalid group type.\n        assertEquals(Group.GroupType.parse(\"Invalid\"), Group.GroupType.UNKNOWN);\n    }\n\n    @Test\n    public void testPreferredServerAssignor() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(\"uniform\")\n            .build();\n\n        // The group is empty so the preferred assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member1)\n        );\n\n        // Update the group with member 1.\n        consumerGroup.updateMember(member1);\n\n        // Member 1 is in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has been removed but this is not reflected in the group yet so\n        // we pass the removed member. The assignor should be range.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member1, null)\n        );\n\n        // Member 2 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member2)\n        );\n\n        // Update the group with member 2.\n        consumerGroup.updateMember(member2);\n\n        // Member 1 and 2 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Update the group with member 3.\n        consumerGroup.updateMember(member3);\n\n        // Member 1, 2 and 3 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Members without assignors\n        ConsumerGroupMember updatedMember1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(null)\n            .build();\n\n        // Member 1 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        Optional<String> assignor = consumerGroup.computePreferredServerAssignor(member1, updatedMember1);\n        assertTrue(assignor.equals(Optional.of(\"range\")) || assignor.equals(Optional.of(\"uniform\")));\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember1);\n\n        // Member 2 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.computePreferredServerAssignor(member2, updatedMember2)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember2);\n\n        // Only member 3 is left in the group so the assignor should be uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 3 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member3, updatedMember3)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember3);\n\n        // The group is empty so the assignor should be empty as well.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        consumerGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        consumerGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        consumerGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        ConsumerGroupMember member4 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        consumerGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(snapshotRegistry, \"test-group\", metricsShard);\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        consumerGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, group.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch() + 1);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch() + 1, group.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        group.setGroupEpoch(group.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        group.requestMetadataRefresh();\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateTransactionalOffsetCommit(short version) {\n        boolean isTransactional = true;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create a member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetCommit(\"member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        group.validateOffsetCommit(\"member-id\", \"\", 0, isTransactional, version);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        boolean isTransactional = false;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create members.\n        group.updateMember(\n            new ConsumerGroupMember\n                .Builder(\"new-protocol-member-id\").build()\n        );\n        group.updateMember(\n            new ConsumerGroupMember.Builder(\"old-protocol-member-id\")\n                .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n                .build()\n        );\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        if (version >= 9) {\n            assertThrows(StaleMemberEpochException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        }\n        assertThrows(IllegalGenerationException.class, () ->\n            group.validateOffsetCommit(\"old-protocol-member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        if (version >= 9) {\n            group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version);\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version));\n        }\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE.toString(), group.stateAsString(1));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-foo\",\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n\n        // Simulate a call from the admin client without member id and member epoch.\n        group.validateOffsetFetch(null, -1, Long.MAX_VALUE);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE));\n\n        // Create a member.\n        idempotentCreateSnapshot(snapshotRegistry);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // The member does not exist at last committed offset 0.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, 0));\n\n        // The member exists but the epoch is stale when the last committed offset is not considered.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 10, Long.MAX_VALUE));\n\n        // This should succeed.\n        group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        assertDoesNotThrow(consumerGroup::validateDeleteGroup);\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        long currentTimestamp = 30000L;\n        long commitTimestamp = 20000L;\n        long offsetsRetentionMs = 10000L;\n        OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(15000L, OptionalInt.empty(), \"\", commitTimestamp, OptionalLong.empty());\n        ConsumerGroup group = new ConsumerGroup(new SnapshotRegistry(new LogContext()), \"group-id\", mock(GroupCoordinatorMetricsShard.class));\n\n        Optional<OffsetExpirationCondition> offsetExpirationCondition = group.offsetExpirationCondition();\n        assertTrue(offsetExpirationCondition.isPresent());\n\n        OffsetExpirationConditionImpl condition = (OffsetExpirationConditionImpl) offsetExpirationCondition.get();\n        assertEquals(commitTimestamp, condition.baseTimestamp().apply(offsetAndMetadata));\n        assertTrue(condition.isOffsetExpired(offsetAndMetadata, currentTimestamp, offsetsRetentionMs));\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(consumerGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(consumerGroup.isSubscribedToTopic(\"bar\"));\n\n        consumerGroup.removeMember(\"member1\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"foo\"));\n\n        consumerGroup.removeMember(\"member2\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-id-1\", mock(GroupCoordinatorMetricsShard.class));\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .setServerAssignorName(\"assignorName\")\n                .build());\n        group.updateMember(new ConsumerGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ConsumerGroupDescribeResponseData.DescribedGroup expected = new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ConsumerGroup.ConsumerGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ConsumerGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                    .setSubscribedTopicRegex(\"\"),\n                new ConsumerGroupDescribeResponseData.Member().setMemberId(\"member2\")\n                    .setSubscribedTopicRegex(\"\")\n            ));\n        ConsumerGroupDescribeResponseData.DescribedGroup actual = group.asDescribedGroup(1, \"\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testStateTransitionMetrics() {\n        // Confirm metrics is not updated when a new ConsumerGroup is created but only when the group transitions\n        // its state.\n        GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(\n            new SnapshotRegistry(new LogContext()),\n            \"group-id\",\n            metrics\n        );\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(0)).onConsumerGroupStateTransition(null, ConsumerGroup.ConsumerGroupState.EMPTY);\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.EMPTY, ConsumerGroup.ConsumerGroupState.RECONCILING);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.RECONCILING, ConsumerGroup.ConsumerGroupState.ASSIGNING);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.ASSIGNING, ConsumerGroup.ConsumerGroupState.STABLE);\n\n        consumerGroup.removeMember(\"member\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.STABLE, ConsumerGroup.ConsumerGroupState.EMPTY);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(group.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(group.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(group.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    @Test\n    public void testClassicMembersSupportedProtocols() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> rangeProtocol = new ArrayList<>();\n        rangeProtocol.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> roundRobinAndRangeProtocols = new ArrayList<>();\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"roundrobin\")\n            .setMetadata(new byte[0]));\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(1, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"range\", \"sticky\"))));\n        assertFalse(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertFalse(consumerGroup.classicMembersSupportedProtocols().containsKey(\"roundrobin\"));\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n    }\n\n    @Test\n    public void testNumClassicProtocolMembers() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> protocols = new ArrayList<>();\n        protocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        // The group has member 1 (using the classic protocol).\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n\n        // The group has member 1 (using the classic protocol) and member 2 (using the consumer protocol).\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-1\"));\n        assertTrue(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the consumer protocol) and member 3 (using the consumer protocol).\n        consumerGroup.removeMember(member1.memberId());\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member-3\")\n            .build();\n        consumerGroup.updateMember(member3);\n        assertEquals(0, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the classic protocol).\n        consumerGroup.removeMember(member2.memberId());\n        member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n    }\n}",
                "methodCount": 34
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 1122,
                        "lineEnd": 1124,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) is interacting exclusively with the SnapshotRegistry class by calling its getOrCreateSnapshot method. This method appears to logically belong with the SnapshotRegistry because it operates on the SnapshotRegistry object directly, which indicates that the method fits well with the responsibilities of the SnapshotRegistry class. Moving it to this class will encapsulate functionality related to creating snapshots within the SnapshotRegistry, following the principle of cohesive class design.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private ConsumerGroup createConsumerGroup(String groupId)",
                            "target_class": "ConsumerGroupFactory",
                            "rationale": "The method `createConsumerGroup` is responsible for creating instances of `ConsumerGroup`, which indicates that its functionality fits better in a factory class responsible for creating `ConsumerGroup` objects. This keeps the test class more focused on testing and less on object creation."
                        }
                    ],
                    "llm_response_time": 3432
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private ConsumerGroup createConsumerGroup(String groupId)",
                            "target_class": "Factory",
                            "rationale": "This method creates an instance of ConsumerGroup and does not operate directly on the state of ConsumerGroupTest. It should be moved to a factory class to better adhere to the Single Responsibility Principle and streamline object creation."
                        }
                    ],
                    "llm_response_time": 2349
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private ConsumerGroup createConsumerGroup(String groupId)",
                            "target_class": "ConsumerGroupFactory",
                            "rationale": "The createConsumerGroup method is responsible for creating instances of ConsumerGroup. This responsibility is centered around object creation and can be moved to a dedicated factory class, such as ConsumerGroupFactory, to adhere to the Single Responsibility Principle and keep the ConsumerGroupTest class focused on testing."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The idempotentCreateSnapshot method directly deals with operations on the SnapshotRegistry object. Encapsulating snapshot creation logic within the SnapshotRegistry class would make the code more cohesive and easier to maintain."
                        }
                    ],
                    "llm_response_time": 3787
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.13520961404876064
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3480612588264244
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [],
                    "llm_response_time": 1989
                },
                "tf-idf-5": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "voyage": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "voyage-5": {
                    "priority_method_names": [],
                    "llm_response_time": 1
                },
                "voyage-3": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 2326,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testValidateOffsetFetch() : void in class org.apache.kafka.coordinator.group.modern.consumer.ConsumerGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1123,
                    "endLine": 1153,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testValidateOffsetFetch() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1140,
                    "endLine": 1140,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1123,
                    "endLine": 1153,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testValidateOffsetFetch() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1140,
                    "endLine": 1140,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 613,
        "extraction_results": {
            "success": true,
            "newCommitHash": "34bebf3af2ff8ca4a4ac8a279765fc681c4b95ea",
            "newBranchName": "extract-idempotentCreateSnapshot-testValidateOffsetFetch-130af38"
        },
        "telemetry": {
            "id": "357aa626-d1fb-4911-b65a-6d140a378bee",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1370,
                "lineStart": 74,
                "lineEnd": 1443,
                "bodyLineStart": 74,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                "sourceCode": "public class ConsumerGroupTest {\n\n    private ConsumerGroup createConsumerGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ConsumerGroup(\n            snapshotRegistry,\n            groupId,\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        // Create a member.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        consumerGroup.updateMember(member);\n\n        // Get that member back.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            consumerGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testNoStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Create a new member which is not static\n        consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        assertNull(consumerGroup.staticMember(\"instance-id\"));\n    }\n\n    @Test\n    public void testGetStaticMemberByInstanceId() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.staticMember(\"instance\"));\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n        assertEquals(member.memberId(), consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testRemoveStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n        assertNull(consumerGroup.staticMember(\"instance\"));\n        assertNull(consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsReassignedBeforeBeingRevoked() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(Collections.emptyMap())\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .setPartitionsPendingRevocation(Collections.emptyMap())\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsNotReleased() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        ConsumerGroupMember m2 = new ConsumerGroupMember.Builder(\"m2\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        // m2 should not be able to acquire foo-1 because the partition is\n        // still owned by another member.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.updateMember(m2));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        consumerGroup.removeMember(member.memberId());\n\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testWaitingOnUnreleasedPartition() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n        String memberId1 = Uuid.randomUuid().toString();\n        String memberId2 = Uuid.randomUuid().toString();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        consumerGroup.updateTargetAssignment(memberId1, new Assignment(mkAssignment(\n            mkTopicAssignment(fooTopicId, 1, 2, 3),\n            mkTopicAssignment(zarTopicId, 7, 8, 9)\n        )));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(memberId1)\n            .setMemberEpoch(10)\n            .setState(MemberState.UNRELEASED_PARTITIONS)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertFalse(consumerGroup.waitingOnUnreleasedPartition(member1));\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(memberId2)\n            .setMemberEpoch(10)\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 7)))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertTrue(consumerGroup.waitingOnUnreleasedPartition(member1));\n    }\n\n    @Test\n    public void testGroupState() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member2);\n        consumerGroup.setGroupEpoch(2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        consumerGroup.setTargetAssignmentEpoch(2);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        // Member 2 is not stable so the group stays in reconciling state.\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.UNREVOKED_PARTITIONS)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.UNREVOKED_PARTITIONS, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n\n        consumerGroup.removeMember(\"member1\");\n        consumerGroup.removeMember(\"member2\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n\n        assertEquals(Group.GroupType.parse(\"classic\"), Group.GroupType.CLASSIC);\n\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Consumer\"), Group.GroupType.CONSUMER);\n\n        // Test with invalid group type.\n        assertEquals(Group.GroupType.parse(\"Invalid\"), Group.GroupType.UNKNOWN);\n    }\n\n    @Test\n    public void testPreferredServerAssignor() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(\"uniform\")\n            .build();\n\n        // The group is empty so the preferred assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member1)\n        );\n\n        // Update the group with member 1.\n        consumerGroup.updateMember(member1);\n\n        // Member 1 is in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has been removed but this is not reflected in the group yet so\n        // we pass the removed member. The assignor should be range.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member1, null)\n        );\n\n        // Member 2 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member2)\n        );\n\n        // Update the group with member 2.\n        consumerGroup.updateMember(member2);\n\n        // Member 1 and 2 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Update the group with member 3.\n        consumerGroup.updateMember(member3);\n\n        // Member 1, 2 and 3 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Members without assignors\n        ConsumerGroupMember updatedMember1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(null)\n            .build();\n\n        // Member 1 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        Optional<String> assignor = consumerGroup.computePreferredServerAssignor(member1, updatedMember1);\n        assertTrue(assignor.equals(Optional.of(\"range\")) || assignor.equals(Optional.of(\"uniform\")));\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember1);\n\n        // Member 2 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.computePreferredServerAssignor(member2, updatedMember2)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember2);\n\n        // Only member 3 is left in the group so the assignor should be uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 3 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member3, updatedMember3)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember3);\n\n        // The group is empty so the assignor should be empty as well.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        consumerGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        consumerGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        consumerGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        ConsumerGroupMember member4 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        consumerGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(snapshotRegistry, \"test-group\", metricsShard);\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        consumerGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, group.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch() + 1);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch() + 1, group.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        group.setGroupEpoch(group.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        group.requestMetadataRefresh();\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateTransactionalOffsetCommit(short version) {\n        boolean isTransactional = true;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create a member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetCommit(\"member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        group.validateOffsetCommit(\"member-id\", \"\", 0, isTransactional, version);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        boolean isTransactional = false;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create members.\n        group.updateMember(\n            new ConsumerGroupMember\n                .Builder(\"new-protocol-member-id\").build()\n        );\n        group.updateMember(\n            new ConsumerGroupMember.Builder(\"old-protocol-member-id\")\n                .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n                .build()\n        );\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        if (version >= 9) {\n            assertThrows(StaleMemberEpochException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        }\n        assertThrows(IllegalGenerationException.class, () ->\n            group.validateOffsetCommit(\"old-protocol-member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        if (version >= 9) {\n            group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version);\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version));\n        }\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE.toString(), group.stateAsString(1));\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-foo\",\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n\n        // Simulate a call from the admin client without member id and member epoch.\n        group.validateOffsetFetch(null, -1, Long.MAX_VALUE);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE));\n\n        // Create a member.\n        idempotentCreateSnapshot(snapshotRegistry);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // The member does not exist at last committed offset 0.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, 0));\n\n        // The member exists but the epoch is stale when the last committed offset is not considered.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 10, Long.MAX_VALUE));\n\n        // This should succeed.\n        group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        assertDoesNotThrow(consumerGroup::validateDeleteGroup);\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        long currentTimestamp = 30000L;\n        long commitTimestamp = 20000L;\n        long offsetsRetentionMs = 10000L;\n        OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(15000L, OptionalInt.empty(), \"\", commitTimestamp, OptionalLong.empty());\n        ConsumerGroup group = new ConsumerGroup(new SnapshotRegistry(new LogContext()), \"group-id\", mock(GroupCoordinatorMetricsShard.class));\n\n        Optional<OffsetExpirationCondition> offsetExpirationCondition = group.offsetExpirationCondition();\n        assertTrue(offsetExpirationCondition.isPresent());\n\n        OffsetExpirationConditionImpl condition = (OffsetExpirationConditionImpl) offsetExpirationCondition.get();\n        assertEquals(commitTimestamp, condition.baseTimestamp().apply(offsetAndMetadata));\n        assertTrue(condition.isOffsetExpired(offsetAndMetadata, currentTimestamp, offsetsRetentionMs));\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(consumerGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(consumerGroup.isSubscribedToTopic(\"bar\"));\n\n        consumerGroup.removeMember(\"member1\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"foo\"));\n\n        consumerGroup.removeMember(\"member2\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-id-1\", mock(GroupCoordinatorMetricsShard.class));\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .setServerAssignorName(\"assignorName\")\n                .build());\n        group.updateMember(new ConsumerGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ConsumerGroupDescribeResponseData.DescribedGroup expected = new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ConsumerGroup.ConsumerGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ConsumerGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                    .setSubscribedTopicRegex(\"\"),\n                new ConsumerGroupDescribeResponseData.Member().setMemberId(\"member2\")\n                    .setSubscribedTopicRegex(\"\")\n            ));\n        ConsumerGroupDescribeResponseData.DescribedGroup actual = group.asDescribedGroup(1, \"\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testStateTransitionMetrics() {\n        // Confirm metrics is not updated when a new ConsumerGroup is created but only when the group transitions\n        // its state.\n        GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(\n            new SnapshotRegistry(new LogContext()),\n            \"group-id\",\n            metrics\n        );\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(0)).onConsumerGroupStateTransition(null, ConsumerGroup.ConsumerGroupState.EMPTY);\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.EMPTY, ConsumerGroup.ConsumerGroupState.RECONCILING);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.RECONCILING, ConsumerGroup.ConsumerGroupState.ASSIGNING);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.ASSIGNING, ConsumerGroup.ConsumerGroupState.STABLE);\n\n        consumerGroup.removeMember(\"member\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.STABLE, ConsumerGroup.ConsumerGroupState.EMPTY);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(group.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(group.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(group.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    @Test\n    public void testClassicMembersSupportedProtocols() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> rangeProtocol = new ArrayList<>();\n        rangeProtocol.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> roundRobinAndRangeProtocols = new ArrayList<>();\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"roundrobin\")\n            .setMetadata(new byte[0]));\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(1, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"range\", \"sticky\"))));\n        assertFalse(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertFalse(consumerGroup.classicMembersSupportedProtocols().containsKey(\"roundrobin\"));\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n    }\n\n    @Test\n    public void testNumClassicProtocolMembers() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> protocols = new ArrayList<>();\n        protocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        // The group has member 1 (using the classic protocol).\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n\n        // The group has member 1 (using the classic protocol) and member 2 (using the consumer protocol).\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-1\"));\n        assertTrue(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the consumer protocol) and member 3 (using the consumer protocol).\n        consumerGroup.removeMember(member1.memberId());\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member-3\")\n            .build();\n        consumerGroup.updateMember(member3);\n        assertEquals(0, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the classic protocol).\n        consumerGroup.removeMember(member2.memberId());\n        member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n    }\n}",
                "methodCount": 34
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 1154,
                        "lineEnd": 1156,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) is interacting exclusively with the SnapshotRegistry class by calling its getOrCreateSnapshot method. This method appears to logically belong with the SnapshotRegistry because it operates on the SnapshotRegistry object directly, which indicates that the method fits well with the responsibilities of the SnapshotRegistry class. Moving it to this class will encapsulate functionality related to creating snapshots within the SnapshotRegistry, following the principle of cohesive class design.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private ConsumerGroup createConsumerGroup(String groupId)",
                            "target_class": "ConsumerGroupFactory",
                            "rationale": "The createConsumerGroup method is responsible for creating instances of the ConsumerGroup class. This is functionality that is more appropriate for a factory class dedicated to creating ConsumerGroup instances, thereby adhering to the Single Responsibility Principle."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The idempotentCreateSnapshot method operates on the SnapshotRegistry and is more closely related to the responsibilities of the SnapshotRegistry class. Moving this method there keeps related functionality together and adheres to the Single Responsibility Principle."
                        }
                    ],
                    "llm_response_time": 3742
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private ConsumerGroup createConsumerGroup(String groupId)",
                            "target_class": "ConsumerGroupFactory",
                            "rationale": "The createConsumerGroup method is responsible for constructing a ConsumerGroup instance, which aligns it more closely with factory or builder pattern responsibilities rather than the test class itself."
                        }
                    ],
                    "llm_response_time": 2618
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private ConsumerGroup createConsumerGroup(String groupId)",
                            "target_class": "ConsumerGroupFactory",
                            "rationale": "The createConsumerGroup() method is responsible for creating ConsumerGroup instances and can be moved to a factory class like ConsumerGroupFactory to follow the Single Responsibility Principle. This will make the ConsumerGroupTest class focused only on testing and not on object creation."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistryHelper",
                            "rationale": "The idempotentCreateSnapshot method operates on SnapshotRegistry and can be better placed in a utility or helper class like SnapshotRegistryHelper. This will make the test class cleaner and the SnapshotRegistry-related methods more modular."
                        }
                    ],
                    "llm_response_time": 3763
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.13520961404876064
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.34690363445902683
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1728
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "voyage": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAsDescribedGroup() : void in class org.apache.kafka.coordinator.group.modern.consumer.ConsumerGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1243,
                    "endLine": 1276,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAsDescribedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1247,
                    "endLine": 1247,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1243,
                    "endLine": 1276,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAsDescribedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1247,
                    "endLine": 1247,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1256,
                    "endLine": 1256,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 614,
        "extraction_results": {
            "success": true,
            "newCommitHash": "7226b58c84508faac4518b8bf061266c17e4bdf7",
            "newBranchName": "extract-idempotentCreateSnapshot-testAsDescribedGroup-130af38"
        },
        "telemetry": {
            "id": "0cc38517-aa6e-468d-9a73-e0273c4f70db",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1370,
                "lineStart": 74,
                "lineEnd": 1443,
                "bodyLineStart": 74,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                "sourceCode": "public class ConsumerGroupTest {\n\n    private ConsumerGroup createConsumerGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ConsumerGroup(\n            snapshotRegistry,\n            groupId,\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        // Create a member.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        consumerGroup.updateMember(member);\n\n        // Get that member back.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            consumerGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testNoStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Create a new member which is not static\n        consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        assertNull(consumerGroup.staticMember(\"instance-id\"));\n    }\n\n    @Test\n    public void testGetStaticMemberByInstanceId() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.staticMember(\"instance\"));\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n        assertEquals(member.memberId(), consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testRemoveStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n        assertNull(consumerGroup.staticMember(\"instance\"));\n        assertNull(consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsReassignedBeforeBeingRevoked() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(Collections.emptyMap())\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .setPartitionsPendingRevocation(Collections.emptyMap())\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsNotReleased() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        ConsumerGroupMember m2 = new ConsumerGroupMember.Builder(\"m2\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        // m2 should not be able to acquire foo-1 because the partition is\n        // still owned by another member.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.updateMember(m2));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        consumerGroup.removeMember(member.memberId());\n\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testWaitingOnUnreleasedPartition() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n        String memberId1 = Uuid.randomUuid().toString();\n        String memberId2 = Uuid.randomUuid().toString();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        consumerGroup.updateTargetAssignment(memberId1, new Assignment(mkAssignment(\n            mkTopicAssignment(fooTopicId, 1, 2, 3),\n            mkTopicAssignment(zarTopicId, 7, 8, 9)\n        )));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(memberId1)\n            .setMemberEpoch(10)\n            .setState(MemberState.UNRELEASED_PARTITIONS)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertFalse(consumerGroup.waitingOnUnreleasedPartition(member1));\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(memberId2)\n            .setMemberEpoch(10)\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 7)))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertTrue(consumerGroup.waitingOnUnreleasedPartition(member1));\n    }\n\n    @Test\n    public void testGroupState() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member2);\n        consumerGroup.setGroupEpoch(2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        consumerGroup.setTargetAssignmentEpoch(2);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        // Member 2 is not stable so the group stays in reconciling state.\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.UNREVOKED_PARTITIONS)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.UNREVOKED_PARTITIONS, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n\n        consumerGroup.removeMember(\"member1\");\n        consumerGroup.removeMember(\"member2\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n\n        assertEquals(Group.GroupType.parse(\"classic\"), Group.GroupType.CLASSIC);\n\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Consumer\"), Group.GroupType.CONSUMER);\n\n        // Test with invalid group type.\n        assertEquals(Group.GroupType.parse(\"Invalid\"), Group.GroupType.UNKNOWN);\n    }\n\n    @Test\n    public void testPreferredServerAssignor() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(\"uniform\")\n            .build();\n\n        // The group is empty so the preferred assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member1)\n        );\n\n        // Update the group with member 1.\n        consumerGroup.updateMember(member1);\n\n        // Member 1 is in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has been removed but this is not reflected in the group yet so\n        // we pass the removed member. The assignor should be range.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member1, null)\n        );\n\n        // Member 2 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member2)\n        );\n\n        // Update the group with member 2.\n        consumerGroup.updateMember(member2);\n\n        // Member 1 and 2 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Update the group with member 3.\n        consumerGroup.updateMember(member3);\n\n        // Member 1, 2 and 3 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Members without assignors\n        ConsumerGroupMember updatedMember1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(null)\n            .build();\n\n        // Member 1 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        Optional<String> assignor = consumerGroup.computePreferredServerAssignor(member1, updatedMember1);\n        assertTrue(assignor.equals(Optional.of(\"range\")) || assignor.equals(Optional.of(\"uniform\")));\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember1);\n\n        // Member 2 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.computePreferredServerAssignor(member2, updatedMember2)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember2);\n\n        // Only member 3 is left in the group so the assignor should be uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 3 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member3, updatedMember3)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember3);\n\n        // The group is empty so the assignor should be empty as well.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        consumerGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        consumerGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        consumerGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        ConsumerGroupMember member4 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        consumerGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(snapshotRegistry, \"test-group\", metricsShard);\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        consumerGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, group.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch() + 1);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch() + 1, group.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        group.setGroupEpoch(group.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        group.requestMetadataRefresh();\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateTransactionalOffsetCommit(short version) {\n        boolean isTransactional = true;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create a member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetCommit(\"member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        group.validateOffsetCommit(\"member-id\", \"\", 0, isTransactional, version);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        boolean isTransactional = false;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create members.\n        group.updateMember(\n            new ConsumerGroupMember\n                .Builder(\"new-protocol-member-id\").build()\n        );\n        group.updateMember(\n            new ConsumerGroupMember.Builder(\"old-protocol-member-id\")\n                .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n                .build()\n        );\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        if (version >= 9) {\n            assertThrows(StaleMemberEpochException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        }\n        assertThrows(IllegalGenerationException.class, () ->\n            group.validateOffsetCommit(\"old-protocol-member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        if (version >= 9) {\n            group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version);\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version));\n        }\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE.toString(), group.stateAsString(1));\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-foo\",\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n\n        // Simulate a call from the admin client without member id and member epoch.\n        group.validateOffsetFetch(null, -1, Long.MAX_VALUE);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE));\n\n        // Create a member.\n        idempotentCreateSnapshot(snapshotRegistry);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // The member does not exist at last committed offset 0.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, 0));\n\n        // The member exists but the epoch is stale when the last committed offset is not considered.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 10, Long.MAX_VALUE));\n\n        // This should succeed.\n        group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        assertDoesNotThrow(consumerGroup::validateDeleteGroup);\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        long currentTimestamp = 30000L;\n        long commitTimestamp = 20000L;\n        long offsetsRetentionMs = 10000L;\n        OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(15000L, OptionalInt.empty(), \"\", commitTimestamp, OptionalLong.empty());\n        ConsumerGroup group = new ConsumerGroup(new SnapshotRegistry(new LogContext()), \"group-id\", mock(GroupCoordinatorMetricsShard.class));\n\n        Optional<OffsetExpirationCondition> offsetExpirationCondition = group.offsetExpirationCondition();\n        assertTrue(offsetExpirationCondition.isPresent());\n\n        OffsetExpirationConditionImpl condition = (OffsetExpirationConditionImpl) offsetExpirationCondition.get();\n        assertEquals(commitTimestamp, condition.baseTimestamp().apply(offsetAndMetadata));\n        assertTrue(condition.isOffsetExpired(offsetAndMetadata, currentTimestamp, offsetsRetentionMs));\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(consumerGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(consumerGroup.isSubscribedToTopic(\"bar\"));\n\n        consumerGroup.removeMember(\"member1\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"foo\"));\n\n        consumerGroup.removeMember(\"member2\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-id-1\", mock(GroupCoordinatorMetricsShard.class));\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .setServerAssignorName(\"assignorName\")\n                .build());\n        group.updateMember(new ConsumerGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ConsumerGroupDescribeResponseData.DescribedGroup expected = new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ConsumerGroup.ConsumerGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ConsumerGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                    .setSubscribedTopicRegex(\"\"),\n                new ConsumerGroupDescribeResponseData.Member().setMemberId(\"member2\")\n                    .setSubscribedTopicRegex(\"\")\n            ));\n        ConsumerGroupDescribeResponseData.DescribedGroup actual = group.asDescribedGroup(1, \"\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testStateTransitionMetrics() {\n        // Confirm metrics is not updated when a new ConsumerGroup is created but only when the group transitions\n        // its state.\n        GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(\n            new SnapshotRegistry(new LogContext()),\n            \"group-id\",\n            metrics\n        );\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(0)).onConsumerGroupStateTransition(null, ConsumerGroup.ConsumerGroupState.EMPTY);\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.EMPTY, ConsumerGroup.ConsumerGroupState.RECONCILING);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.RECONCILING, ConsumerGroup.ConsumerGroupState.ASSIGNING);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.ASSIGNING, ConsumerGroup.ConsumerGroupState.STABLE);\n\n        consumerGroup.removeMember(\"member\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.STABLE, ConsumerGroup.ConsumerGroupState.EMPTY);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(group.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(group.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(group.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    @Test\n    public void testClassicMembersSupportedProtocols() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> rangeProtocol = new ArrayList<>();\n        rangeProtocol.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> roundRobinAndRangeProtocols = new ArrayList<>();\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"roundrobin\")\n            .setMetadata(new byte[0]));\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(1, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"range\", \"sticky\"))));\n        assertFalse(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertFalse(consumerGroup.classicMembersSupportedProtocols().containsKey(\"roundrobin\"));\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n    }\n\n    @Test\n    public void testNumClassicProtocolMembers() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> protocols = new ArrayList<>();\n        protocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        // The group has member 1 (using the classic protocol).\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n\n        // The group has member 1 (using the classic protocol) and member 2 (using the consumer protocol).\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-1\"));\n        assertTrue(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the consumer protocol) and member 3 (using the consumer protocol).\n        consumerGroup.removeMember(member1.memberId());\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member-3\")\n            .build();\n        consumerGroup.updateMember(member3);\n        assertEquals(0, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the classic protocol).\n        consumerGroup.removeMember(member2.memberId());\n        member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n    }\n}",
                "methodCount": 34
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 1277,
                        "lineEnd": 1279,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) is interacting exclusively with the SnapshotRegistry class by calling its getOrCreateSnapshot method. This method appears to logically belong with the SnapshotRegistry because it operates on the SnapshotRegistry object directly, which indicates that the method fits well with the responsibilities of the SnapshotRegistry class. Moving it to this class will encapsulate functionality related to creating snapshots within the SnapshotRegistry, following the principle of cohesive class design.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private ConsumerGroup createConsumerGroup(String groupId)",
                            "target_class": "ConsumerGroupFactory",
                            "rationale": "The method `createConsumerGroup` is a factory method that creates instances of `ConsumerGroup`. Factory methods are often better suited in factory classes to streamline object creation logic."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistryHelper",
                            "rationale": "The method `idempotentCreateSnapshot` operates on `SnapshotRegistry` and serves to create snapshots in an idempotent manner. Methods that operate on a specific object type typically belong in helper or utility classes related to that type."
                        }
                    ],
                    "llm_response_time": 3793
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private ConsumerGroup createConsumerGroup(String groupId)",
                            "target_class": "ConsumerGroupFactory",
                            "rationale": "The createConsumerGroup method serves the purpose of instantiating a new ConsumerGroup with the necessary dependencies, which aligns more closely with the responsibilities of a factory class. Moving it to a new ConsumerGroupFactory class will adhere to the Single Responsibility Principle and make the ConsumerGroupTest class cleaner."
                        }
                    ],
                    "llm_response_time": 2896
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private ConsumerGroup createConsumerGroup(String groupId)",
                            "target_class": "ConsumerGroupFactory",
                            "rationale": "The createConsumerGroup method is responsible for creating ConsumerGroup instances, which indicates it would be more appropriate in a factory or utility class designed for the instantiation of ConsumerGroup objects. Moving this method helps keep the test class focused on testing functionalities rather than object creation."
                        }
                    ],
                    "llm_response_time": 2636
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.13520961404876064
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3485749109925722
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 2022
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 2
                },
                "voyage": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testIsInStatesCaseInsensitive() : void in class org.apache.kafka.coordinator.group.modern.consumer.ConsumerGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1318,
                    "endLine": 1338,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testIsInStatesCaseInsensitive() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1327,
                    "endLine": 1327,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1318,
                    "endLine": 1338,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testIsInStatesCaseInsensitive() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1327,
                    "endLine": 1327,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1334,
                    "endLine": 1334,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 615,
        "extraction_results": {
            "success": true,
            "newCommitHash": "2afeb4dfa4330fb206a36754f2ee15817ba410df",
            "newBranchName": "extract-idempotentCreateSnapshot-testIsInStatesCaseInsensitive-130af38"
        },
        "telemetry": {
            "id": "098d1c7c-4f3e-4363-9445-8c18cd1bfb89",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1370,
                "lineStart": 74,
                "lineEnd": 1443,
                "bodyLineStart": 74,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                "sourceCode": "public class ConsumerGroupTest {\n\n    private ConsumerGroup createConsumerGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ConsumerGroup(\n            snapshotRegistry,\n            groupId,\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        // Create a member.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        consumerGroup.updateMember(member);\n\n        // Get that member back.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            consumerGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testNoStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Create a new member which is not static\n        consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        assertNull(consumerGroup.staticMember(\"instance-id\"));\n    }\n\n    @Test\n    public void testGetStaticMemberByInstanceId() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.staticMember(\"instance\"));\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n        assertEquals(member.memberId(), consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testRemoveStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n        assertNull(consumerGroup.staticMember(\"instance\"));\n        assertNull(consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsReassignedBeforeBeingRevoked() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(Collections.emptyMap())\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .setPartitionsPendingRevocation(Collections.emptyMap())\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsNotReleased() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        ConsumerGroupMember m2 = new ConsumerGroupMember.Builder(\"m2\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        // m2 should not be able to acquire foo-1 because the partition is\n        // still owned by another member.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.updateMember(m2));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        consumerGroup.removeMember(member.memberId());\n\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testWaitingOnUnreleasedPartition() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n        String memberId1 = Uuid.randomUuid().toString();\n        String memberId2 = Uuid.randomUuid().toString();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        consumerGroup.updateTargetAssignment(memberId1, new Assignment(mkAssignment(\n            mkTopicAssignment(fooTopicId, 1, 2, 3),\n            mkTopicAssignment(zarTopicId, 7, 8, 9)\n        )));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(memberId1)\n            .setMemberEpoch(10)\n            .setState(MemberState.UNRELEASED_PARTITIONS)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertFalse(consumerGroup.waitingOnUnreleasedPartition(member1));\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(memberId2)\n            .setMemberEpoch(10)\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 7)))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertTrue(consumerGroup.waitingOnUnreleasedPartition(member1));\n    }\n\n    @Test\n    public void testGroupState() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member2);\n        consumerGroup.setGroupEpoch(2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        consumerGroup.setTargetAssignmentEpoch(2);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        // Member 2 is not stable so the group stays in reconciling state.\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.UNREVOKED_PARTITIONS)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.UNREVOKED_PARTITIONS, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n\n        consumerGroup.removeMember(\"member1\");\n        consumerGroup.removeMember(\"member2\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n\n        assertEquals(Group.GroupType.parse(\"classic\"), Group.GroupType.CLASSIC);\n\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Consumer\"), Group.GroupType.CONSUMER);\n\n        // Test with invalid group type.\n        assertEquals(Group.GroupType.parse(\"Invalid\"), Group.GroupType.UNKNOWN);\n    }\n\n    @Test\n    public void testPreferredServerAssignor() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(\"uniform\")\n            .build();\n\n        // The group is empty so the preferred assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member1)\n        );\n\n        // Update the group with member 1.\n        consumerGroup.updateMember(member1);\n\n        // Member 1 is in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has been removed but this is not reflected in the group yet so\n        // we pass the removed member. The assignor should be range.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member1, null)\n        );\n\n        // Member 2 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member2)\n        );\n\n        // Update the group with member 2.\n        consumerGroup.updateMember(member2);\n\n        // Member 1 and 2 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Update the group with member 3.\n        consumerGroup.updateMember(member3);\n\n        // Member 1, 2 and 3 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Members without assignors\n        ConsumerGroupMember updatedMember1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(null)\n            .build();\n\n        // Member 1 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        Optional<String> assignor = consumerGroup.computePreferredServerAssignor(member1, updatedMember1);\n        assertTrue(assignor.equals(Optional.of(\"range\")) || assignor.equals(Optional.of(\"uniform\")));\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember1);\n\n        // Member 2 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.computePreferredServerAssignor(member2, updatedMember2)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember2);\n\n        // Only member 3 is left in the group so the assignor should be uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 3 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member3, updatedMember3)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember3);\n\n        // The group is empty so the assignor should be empty as well.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        consumerGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        consumerGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        consumerGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        ConsumerGroupMember member4 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        consumerGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(snapshotRegistry, \"test-group\", metricsShard);\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        consumerGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, group.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch() + 1);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch() + 1, group.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        group.setGroupEpoch(group.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        group.requestMetadataRefresh();\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateTransactionalOffsetCommit(short version) {\n        boolean isTransactional = true;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create a member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetCommit(\"member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        group.validateOffsetCommit(\"member-id\", \"\", 0, isTransactional, version);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        boolean isTransactional = false;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create members.\n        group.updateMember(\n            new ConsumerGroupMember\n                .Builder(\"new-protocol-member-id\").build()\n        );\n        group.updateMember(\n            new ConsumerGroupMember.Builder(\"old-protocol-member-id\")\n                .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n                .build()\n        );\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        if (version >= 9) {\n            assertThrows(StaleMemberEpochException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        }\n        assertThrows(IllegalGenerationException.class, () ->\n            group.validateOffsetCommit(\"old-protocol-member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        if (version >= 9) {\n            group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version);\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version));\n        }\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE.toString(), group.stateAsString(1));\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-foo\",\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n\n        // Simulate a call from the admin client without member id and member epoch.\n        group.validateOffsetFetch(null, -1, Long.MAX_VALUE);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE));\n\n        // Create a member.\n        idempotentCreateSnapshot(snapshotRegistry);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // The member does not exist at last committed offset 0.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, 0));\n\n        // The member exists but the epoch is stale when the last committed offset is not considered.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 10, Long.MAX_VALUE));\n\n        // This should succeed.\n        group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        assertDoesNotThrow(consumerGroup::validateDeleteGroup);\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        long currentTimestamp = 30000L;\n        long commitTimestamp = 20000L;\n        long offsetsRetentionMs = 10000L;\n        OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(15000L, OptionalInt.empty(), \"\", commitTimestamp, OptionalLong.empty());\n        ConsumerGroup group = new ConsumerGroup(new SnapshotRegistry(new LogContext()), \"group-id\", mock(GroupCoordinatorMetricsShard.class));\n\n        Optional<OffsetExpirationCondition> offsetExpirationCondition = group.offsetExpirationCondition();\n        assertTrue(offsetExpirationCondition.isPresent());\n\n        OffsetExpirationConditionImpl condition = (OffsetExpirationConditionImpl) offsetExpirationCondition.get();\n        assertEquals(commitTimestamp, condition.baseTimestamp().apply(offsetAndMetadata));\n        assertTrue(condition.isOffsetExpired(offsetAndMetadata, currentTimestamp, offsetsRetentionMs));\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(consumerGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(consumerGroup.isSubscribedToTopic(\"bar\"));\n\n        consumerGroup.removeMember(\"member1\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"foo\"));\n\n        consumerGroup.removeMember(\"member2\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-id-1\", mock(GroupCoordinatorMetricsShard.class));\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .setServerAssignorName(\"assignorName\")\n                .build());\n        group.updateMember(new ConsumerGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ConsumerGroupDescribeResponseData.DescribedGroup expected = new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ConsumerGroup.ConsumerGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ConsumerGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                    .setSubscribedTopicRegex(\"\"),\n                new ConsumerGroupDescribeResponseData.Member().setMemberId(\"member2\")\n                    .setSubscribedTopicRegex(\"\")\n            ));\n        ConsumerGroupDescribeResponseData.DescribedGroup actual = group.asDescribedGroup(1, \"\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testStateTransitionMetrics() {\n        // Confirm metrics is not updated when a new ConsumerGroup is created but only when the group transitions\n        // its state.\n        GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(\n            new SnapshotRegistry(new LogContext()),\n            \"group-id\",\n            metrics\n        );\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(0)).onConsumerGroupStateTransition(null, ConsumerGroup.ConsumerGroupState.EMPTY);\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.EMPTY, ConsumerGroup.ConsumerGroupState.RECONCILING);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.RECONCILING, ConsumerGroup.ConsumerGroupState.ASSIGNING);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.ASSIGNING, ConsumerGroup.ConsumerGroupState.STABLE);\n\n        consumerGroup.removeMember(\"member\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.STABLE, ConsumerGroup.ConsumerGroupState.EMPTY);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(group.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(group.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(group.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testClassicMembersSupportedProtocols() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> rangeProtocol = new ArrayList<>();\n        rangeProtocol.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> roundRobinAndRangeProtocols = new ArrayList<>();\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"roundrobin\")\n            .setMetadata(new byte[0]));\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(1, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"range\", \"sticky\"))));\n        assertFalse(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertFalse(consumerGroup.classicMembersSupportedProtocols().containsKey(\"roundrobin\"));\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n    }\n\n    @Test\n    public void testNumClassicProtocolMembers() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> protocols = new ArrayList<>();\n        protocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        // The group has member 1 (using the classic protocol).\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n\n        // The group has member 1 (using the classic protocol) and member 2 (using the consumer protocol).\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-1\"));\n        assertTrue(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the consumer protocol) and member 3 (using the consumer protocol).\n        consumerGroup.removeMember(member1.memberId());\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member-3\")\n            .build();\n        consumerGroup.updateMember(member3);\n        assertEquals(0, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the classic protocol).\n        consumerGroup.removeMember(member2.memberId());\n        member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n    }\n}",
                "methodCount": 34
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 1339,
                        "lineEnd": 1341,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) is interacting exclusively with the SnapshotRegistry class by calling its getOrCreateSnapshot method. This method appears to logically belong with the SnapshotRegistry because it operates on the SnapshotRegistry object directly, which indicates that the method fits well with the responsibilities of the SnapshotRegistry class. Moving it to this class will encapsulate functionality related to creating snapshots within the SnapshotRegistry, following the principle of cohesive class design.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private ConsumerGroup createConsumerGroup(String groupId)",
                            "target_class": "ConsumerGroupFactory",
                            "rationale": "The method createConsumerGroup is responsible for instantiating ConsumerGroup objects with necessary dependencies. This behavior is more aligned with a factory responsibility rather than a test class responsibility. Moving this method to a ConsumerGroupFactory class adheres to the Single Responsibility Principle and makes it reusable outside the test context."
                        }
                    ],
                    "llm_response_time": 15253
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private ConsumerGroup createConsumerGroup(String groupId)",
                            "target_class": "ConsumerGroupFactory",
                            "rationale": "The method `createConsumerGroup` is responsible for creating instances of `ConsumerGroup`. This responsibility can be better managed by a factory class, `ConsumerGroupFactory`, which will encapsulate the creation logic for `ConsumerGroup` instances, promoting single responsibility principle."
                        }
                    ],
                    "llm_response_time": 2733
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private ConsumerGroup createConsumerGroup(String groupId)",
                            "target_class": "ConsumerGroupFactory",
                            "rationale": "The method createConsumerGroup() is a utility method to instantiate ConsumerGroup objects. It does not rely on any instance variables from ConsumerGroupTest and can be moved to a factory class like ConsumerGroupFactory."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistryHelper",
                            "rationale": "The method idempotentCreateSnapshot() deals with the creation of snapshots in a snapshot registry, which is a general utility. This functionality can be separated into a helper class like SnapshotRegistryHelper to keep the test class focused on test logic."
                        }
                    ],
                    "llm_response_time": 3301
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.13520961404876064
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3465356000703552
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [],
                    "llm_response_time": 6072
                },
                "tf-idf-5": {
                    "priority_method_names": [],
                    "llm_response_time": 2
                },
                "tf-df-3": {
                    "priority_method_names": [],
                    "llm_response_time": 2
                },
                "voyage": {
                    "priority_method_names": [],
                    "llm_response_time": 1
                },
                "voyage-5": {
                    "priority_method_names": [],
                    "llm_response_time": 4
                },
                "voyage-3": {
                    "priority_method_names": [],
                    "llm_response_time": 1
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAsListedGroup() : void in class org.apache.kafka.coordinator.group.modern.share.ShareGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 677,
                    "endLine": 692,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAsListedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 681,
                    "endLine": 681,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 677,
                    "endLine": 692,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAsListedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 681,
                    "endLine": 681,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 687,
                    "endLine": 687,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 616,
        "extraction_results": {
            "success": true,
            "newCommitHash": "615b67dc17c192240f1a481c9167bb8eb032c9f8",
            "newBranchName": "extract-idempotentCreateSnapshot-testAsListedGroup-130af38"
        },
        "telemetry": {
            "id": "99691c4d-8cce-475f-91fb-92462b6d28b8",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 790,
                "lineStart": 56,
                "lineEnd": 845,
                "bodyLineStart": 56,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                "sourceCode": "public class ShareGroupTest {\n\n    @Test\n    public void testType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(Group.GroupType.SHARE, shareGroup.type());\n    }\n\n    @Test\n    public void testProtocolType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(\"share\", shareGroup.protocolType());\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        // Create a member.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        shareGroup.updateMember(member);\n\n        // Get that member back.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            shareGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ShareGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(member, shareGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        ShareGroupMember member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n        shareGroup.updateMember(member);\n        assertTrue(shareGroup.hasMember(\"member\"));\n\n        shareGroup.removeMember(\"member\");\n        assertFalse(shareGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 6));\n\n        member = new ShareGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ShareGroupMember m1 = new ShareGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        shareGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n\n        shareGroup.removeMember(member.memberId());\n\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testGroupState() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(ShareGroup.ShareGroupState.EMPTY, shareGroup.state());\n        assertEquals(\"Empty\", shareGroup.stateAsString());\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        shareGroup.updateMember(member1);\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertEquals(\"Stable\", shareGroup.stateAsString());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n        assertEquals(Group.GroupType.parse(\"share\"), Group.GroupType.SHARE);\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Share\"), Group.GroupType.SHARE);\n        assertEquals(Group.GroupType.parse(\"SHare\"), Group.GroupType.SHARE);\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        shareGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        shareGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        shareGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        ShareGroupMember member4 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        shareGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"test-group\");\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        shareGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, shareGroup.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch() + 1);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch() + 1, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        shareGroup.setGroupEpoch(shareGroup.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        shareGroup.requestMetadataRefresh();\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetCommit(null, null, -1, false, version));\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        assertEquals(ShareGroupState.STABLE, shareGroup.state(1));\n        assertEquals(\"Stable\", shareGroup.stateAsString(1));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::offsetExpirationCondition);\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetFetch(null, -1, -1));\n    }\n\n    @Test\n    public void testValidateOffsetDelete() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::validateOffsetDelete);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state());\n        assertDoesNotThrow(shareGroup::validateDeleteGroup);\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        shareGroup.updateMember(member1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        shareGroup.updateMember(member1);\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(shareGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(shareGroup.isSubscribedToTopic(\"bar\"));\n\n        shareGroup.removeMember(\"member1\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"foo\"));\n\n        shareGroup.removeMember(\"member2\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-id-1\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY.toString(), shareGroup.stateAsString(0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .build());\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ShareGroupDescribeResponseData.DescribedGroup expected = new ShareGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ShareGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ShareGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\")),\n                new ShareGroupDescribeResponseData.Member().setMemberId(\"member2\")\n            ));\n        ShareGroupDescribeResponseData.DescribedGroup actual = shareGroup.asDescribedGroup(1, \"assignorName\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    private ShareGroup createShareGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ShareGroup(\n            snapshotRegistry,\n            groupId\n        );\n    }\n}",
                "methodCount": 26
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 693,
                        "lineEnd": 695,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) is interacting exclusively with the SnapshotRegistry class by calling its getOrCreateSnapshot method. This method appears to logically belong with the SnapshotRegistry because it operates on the SnapshotRegistry object directly, which indicates that the method fits well with the responsibilities of the SnapshotRegistry class. Moving it to this class will encapsulate functionality related to creating snapshots within the SnapshotRegistry, following the principle of cohesive class design.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The method creates a snapshot in the SnapshotRegistry and does not utilize or modify any members of the ShareGroupTest class. It is more relevant to the SnapshotRegistry class where it can be called whenever a snapshot is needed."
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private ShareGroup createShareGroup(String groupId)",
                            "target_class": "ShareGroup",
                            "rationale": "The method creates an instance of ShareGroup and initializes it with a SnapshotRegistry. Placing such a factory-like method in the ShareGroup class would centralize ShareGroup initialization and reduce redundancy."
                        }
                    ],
                    "llm_response_time": 3325
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "testGroupTypeFromString",
                            "method_signature": "public void testGroupTypeFromString()",
                            "target_class": "GroupTypeTest",
                            "rationale": "This method is specifically testing the GroupType enumeration from the Group class. It does not involve any functionality specific to the ShareGroup class and should be moved to a new test class that focuses on GroupType."
                        },
                        {
                            "method_name": "testIsInStatesCaseInsensitive",
                            "method_signature": "public void testIsInStatesCaseInsensitive()",
                            "target_class": "ShareGroupStateTest",
                            "rationale": "This test method is focused on testing the case insensitivity of the ShareGroup's state. This functionality is generic and should be tested in a separate class specifically for ShareGroupState."
                        }
                    ],
                    "llm_response_time": 3095
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "testAddingMemberUpdatesPartitionEpoch",
                            "method_signature": "public void testAddingMemberUpdatesPartitionEpoch()",
                            "target_class": "ShareGroupMemberTest",
                            "rationale": "The method `testAddingMemberUpdatesPartitionEpoch` involves operations that are closely integrated with the `ShareGroupMember` class, suggesting that this test might logically belong there to keep related functionalities bundled together."
                        },
                        {
                            "method_name": "testUpdatingMember",
                            "method_signature": "public void testUpdatingMember()",
                            "target_class": "ShareGroupMemberTest",
                            "rationale": "The method `testUpdatingMember` is testing functionality related to the `ShareGroupMember`, hence it would make more sense to have it within the `ShareGroupMemberTest` class."
                        },
                        {
                            "method_name": "testRemoveMember",
                            "method_signature": "public void testRemoveMember()",
                            "target_class": "ShareGroupMemberTest",
                            "rationale": "The `testRemoveMember` functionality is specifically tied to the `ShareGroupMember`, so moving it to the `ShareGroupMemberTest` ensures cohesion and encapsulates member-specific tests together."
                        },
                        {
                            "method_name": "testComputeSubscriptionMetadata",
                            "method_signature": "public void testComputeSubscriptionMetadata()",
                            "target_class": "MetadataImageTest",
                            "rationale": "The method `testComputeSubscriptionMetadata` interacts closely with the `MetadataImage` and its related components, suggesting that it is better placed in `MetadataImageTest` for appropriate context and test grouping."
                        },
                        {
                            "method_name": "testMetadataRefreshDeadline",
                            "method_signature": "public void testMetadataRefreshDeadline()",
                            "target_class": "MetadataImageTest",
                            "rationale": "The `testMetadataRefreshDeadline` method is validating functionality that's focused on metadata handling, making `MetadataImageTest` a more suitable home for such tests."
                        }
                    ],
                    "llm_response_time": 16675
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.17298872485207484
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.36472014915730905
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1753
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "voyage": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 5
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAsDescribedGroup() : void in class org.apache.kafka.coordinator.group.modern.share.ShareGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 785,
                    "endLine": 815,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAsDescribedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 789,
                    "endLine": 789,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 785,
                    "endLine": 815,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAsDescribedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 789,
                    "endLine": 789,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 797,
                    "endLine": 797,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 617,
        "extraction_results": {
            "success": true,
            "newCommitHash": "6763e67eb5b4151e73ba5a9ed1ee0dd90c71759f",
            "newBranchName": "extract-idempotentCreateSnapshot-testAsDescribedGroup-130af38"
        },
        "telemetry": {
            "id": "52bb2ec3-7402-436c-91f9-189e138f5727",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 790,
                "lineStart": 56,
                "lineEnd": 845,
                "bodyLineStart": 56,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                "sourceCode": "public class ShareGroupTest {\n\n    @Test\n    public void testType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(Group.GroupType.SHARE, shareGroup.type());\n    }\n\n    @Test\n    public void testProtocolType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(\"share\", shareGroup.protocolType());\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        // Create a member.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        shareGroup.updateMember(member);\n\n        // Get that member back.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            shareGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ShareGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(member, shareGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        ShareGroupMember member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n        shareGroup.updateMember(member);\n        assertTrue(shareGroup.hasMember(\"member\"));\n\n        shareGroup.removeMember(\"member\");\n        assertFalse(shareGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 6));\n\n        member = new ShareGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ShareGroupMember m1 = new ShareGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        shareGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n\n        shareGroup.removeMember(member.memberId());\n\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testGroupState() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(ShareGroup.ShareGroupState.EMPTY, shareGroup.state());\n        assertEquals(\"Empty\", shareGroup.stateAsString());\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        shareGroup.updateMember(member1);\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertEquals(\"Stable\", shareGroup.stateAsString());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n        assertEquals(Group.GroupType.parse(\"share\"), Group.GroupType.SHARE);\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Share\"), Group.GroupType.SHARE);\n        assertEquals(Group.GroupType.parse(\"SHare\"), Group.GroupType.SHARE);\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        shareGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        shareGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        shareGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        ShareGroupMember member4 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        shareGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"test-group\");\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        shareGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, shareGroup.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch() + 1);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch() + 1, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        shareGroup.setGroupEpoch(shareGroup.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        shareGroup.requestMetadataRefresh();\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetCommit(null, null, -1, false, version));\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        assertEquals(ShareGroupState.STABLE, shareGroup.state(1));\n        assertEquals(\"Stable\", shareGroup.stateAsString(1));\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::offsetExpirationCondition);\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetFetch(null, -1, -1));\n    }\n\n    @Test\n    public void testValidateOffsetDelete() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::validateOffsetDelete);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state());\n        assertDoesNotThrow(shareGroup::validateDeleteGroup);\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        shareGroup.updateMember(member1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        shareGroup.updateMember(member1);\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(shareGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(shareGroup.isSubscribedToTopic(\"bar\"));\n\n        shareGroup.removeMember(\"member1\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"foo\"));\n\n        shareGroup.removeMember(\"member2\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-id-1\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY.toString(), shareGroup.stateAsString(0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .build());\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ShareGroupDescribeResponseData.DescribedGroup expected = new ShareGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ShareGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ShareGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\")),\n                new ShareGroupDescribeResponseData.Member().setMemberId(\"member2\")\n            ));\n        ShareGroupDescribeResponseData.DescribedGroup actual = shareGroup.asDescribedGroup(1, \"assignorName\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    private ShareGroup createShareGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ShareGroup(\n            snapshotRegistry,\n            groupId\n        );\n    }\n}",
                "methodCount": 26
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 816,
                        "lineEnd": 818,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) is interacting exclusively with the SnapshotRegistry class by calling its getOrCreateSnapshot method. This method appears to logically belong with the SnapshotRegistry because it operates on the SnapshotRegistry object directly, which indicates that the method fits well with the responsibilities of the SnapshotRegistry class. Moving it to this class will encapsulate functionality related to creating snapshots within the SnapshotRegistry, following the principle of cohesive class design.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "testType",
                            "method_signature": "public void testType()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testType() is specifically testing functionality of the ShareGroup class. It is more appropriate to separate these test functionalities into a dedicated ShareGroupTests class."
                        },
                        {
                            "method_name": "testProtocolType",
                            "method_signature": "public void testProtocolType()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testProtocolType() is specifically testing functionality of the ShareGroup class. It ensures better organization and separation of concerns if moved to a dedicated ShareGroupTests class."
                        },
                        {
                            "method_name": "testGetOrCreateMember",
                            "method_signature": "public void testGetOrCreateMember()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testGetOrCreateMember() is testing the getOrMaybeCreateMember() method of ShareGroup. Moving this method to ShareGroupTests helps in better organizing the test cases."
                        },
                        {
                            "method_name": "testUpdateMember",
                            "method_signature": "public void testUpdateMember()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testUpdateMember() checks the updateMember() method's functionality, which belongs to the ShareGroup class. Therefore, it\u2019s better placed in ShareGroupTests for cleaner and focused testing."
                        },
                        {
                            "method_name": "testRemoveMember",
                            "method_signature": "public void testRemoveMember()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testRemoveMember() tests functionality related to removing members from ShareGroup and should be in a dedicated ShareGroupTests class."
                        },
                        {
                            "method_name": "testUpdatingMemberUpdatesPartitionEpoch",
                            "method_signature": "public void testUpdatingMemberUpdatesPartitionEpoch()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testUpdatingMemberUpdatesPartitionEpoch() is testing a specific internal detail of the ShareGroup class and fits better in ShareGroupTests."
                        },
                        {
                            "method_name": "testRemovePartitionEpochs",
                            "method_signature": "public void testRemovePartitionEpochs()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testRemovePartitionEpochs() checks the removePartitionEpochs() method under various conditions. It's part of the ShareGroup functionality and thus is better suited in ShareGroupTests."
                        },
                        {
                            "method_name": "testAddPartitionEpochs",
                            "method_signature": "public void testAddPartitionEpochs()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testAddPartitionEpochs() tests the addPartitionEpochs() method in the ShareGroup class. Moving this into ShareGroupTests makes the test cases more organized."
                        },
                        {
                            "method_name": "testDeletingMemberRemovesPartitionEpoch",
                            "method_signature": "public void testDeletingMemberRemovesPartitionEpoch()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testDeletingMemberRemovesPartitionEpoch() validates the deletion of members and its effect on partition epochs. It should reside in ShareGroupTests for consistency."
                        },
                        {
                            "method_name": "testGroupState",
                            "method_signature": "public void testGroupState()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testGroupState() is concerned with validating the state transitions in ShareGroup, hence it fits better in a separate ShareGroupTests class."
                        },
                        {
                            "method_name": "testGroupTypeFromString",
                            "method_signature": "public void testGroupTypeFromString()",
                            "target_class": "GroupTests",
                            "rationale": "testGroupTypeFromString() tests functionality related to parsing group types. It logically fits within GroupTests rather than ShareGroupTest."
                        },
                        {
                            "method_name": "testUpdateSubscriptionMetadata",
                            "method_signature": "public void testUpdateSubscriptionMetadata()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testUpdateSubscriptionMetadata() deals with updating subscription metadata under ShareGroup. Placing it in ShareGroupTests maintains a cleaner test structure."
                        },
                        {
                            "method_name": "testUpdateSubscribedTopicNamesAndSubscriptionType",
                            "method_signature": "public void testUpdateSubscribedTopicNamesAndSubscriptionType()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testUpdateSubscribedTopicNamesAndSubscriptionType() checks topic name updates in ShareGroup, and should be in ShareGroupTests."
                        },
                        {
                            "method_name": "testUpdateInvertedAssignment",
                            "method_signature": "public void testUpdateInvertedAssignment()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testUpdateInvertedAssignment() focuses on inverted assignment updates within ShareGroup. Moving this to ShareGroupTests can provide a focused testing environment."
                        },
                        {
                            "method_name": "testMetadataRefreshDeadline",
                            "method_signature": "public void testMetadataRefreshDeadline()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testMetadataRefreshDeadline() evaluates the metadata refresh logic in ShareGroup, making it better suited for ShareGroupTests."
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "public void testValidateOffsetCommit(short version)",
                            "target_class": "ShareGroupTests",
                            "rationale": "testValidateOffsetCommit() checks offset commit validation logic within ShareGroup and should be placed in ShareGroupTests."
                        },
                        {
                            "method_name": "testAsListedGroup",
                            "method_signature": "public void testAsListedGroup()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testAsListedGroup() transforms ShareGroup into a listed format. It's better placed in ShareGroupTests for better organization."
                        },
                        {
                            "method_name": "testOffsetExpirationCondition",
                            "method_signature": "public void testOffsetExpirationCondition()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testOffsetExpirationCondition() checks the offset expiration condition logic in ShareGroup. It's more appropriate in ShareGroupTests class."
                        },
                        {
                            "method_name": "testValidateOffsetFetch",
                            "method_signature": "public void testValidateOffsetFetch()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testValidateOffsetFetch() checks offset fetch validation logic in ShareGroup and fits better in ShareGroupTests."
                        },
                        {
                            "method_name": "testValidateOffsetDelete",
                            "method_signature": "public void testValidateOffsetDelete()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testValidateOffsetDelete() evaluates offset deletion logic in ShareGroup, hence belongs in ShareGroupTests."
                        },
                        {
                            "method_name": "testValidateDeleteGroup",
                            "method_signature": "public void testValidateDeleteGroup()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testValidateDeleteGroup() validates the delete group logic in ShareGroup and should be part of ShareGroupTests."
                        },
                        {
                            "method_name": "testIsSubscribedToTopic",
                            "method_signature": "public void testIsSubscribedToTopic()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testIsSubscribedToTopic() checks topic subscription within ShareGroup, making it more appropriate for ShareGroupTests."
                        },
                        {
                            "method_name": "testAsDescribedGroup",
                            "method_signature": "public void testAsDescribedGroup()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testAsDescribedGroup() validates ShareGroup's ability to be described, placing it within ShareGroupTests maintains the cleaner structure."
                        },
                        {
                            "method_name": "testIsInStatesCaseInsensitive",
                            "method_signature": "public void testIsInStatesCaseInsensitive()",
                            "target_class": "ShareGroupTests",
                            "rationale": "testIsInStatesCaseInsensitive() evaluates the states functionality ensuring that it is case-insensitive in ShareGroup. It\u2019s best placed in ShareGroupTests."
                        }
                    ],
                    "llm_response_time": 16881
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private ShareGroup createShareGroup(String groupId)",
                            "target_class": "ShareGroupFactory",
                            "rationale": "The method creates an instance of ShareGroup and is used repeatedly for creating ShareGroup objects in tests. This method is better placed in a factory class or utility class to adhere to the Single Responsibility Principle."
                        }
                    ],
                    "llm_response_time": 2280
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private ShareGroup createShareGroup(String groupId)",
                            "target_class": "ShareGroupUtils",
                            "rationale": "The createShareGroup method creates a ShareGroup object and can be reused by other test classes. Moving it to a utility class like ShareGroupUtils improves code reusability and separates object creation from test implementation."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotUtils",
                            "rationale": "The idempotentCreateSnapshot method creates or retrieves a snapshot in the SnapshotRegistry and can be reused by other test classes. Moving it to a utility class like SnapshotUtils improves code reusability and separates snapshot actions from test implementation."
                        }
                    ],
                    "llm_response_time": 6681
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.17298872485207484
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3640488639576087
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 2239
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "voyage": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testIsInStatesCaseInsensitive() : void in class org.apache.kafka.coordinator.group.modern.share.ShareGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 817,
                    "endLine": 832,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testIsInStatesCaseInsensitive() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 821,
                    "endLine": 821,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 817,
                    "endLine": 832,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testIsInStatesCaseInsensitive() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 821,
                    "endLine": 821,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 828,
                    "endLine": 828,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 618,
        "extraction_results": {
            "success": true,
            "newCommitHash": "7e477e8eadf30c6142f19bbb76cff3b1bf7d8587",
            "newBranchName": "extract-idempotentCreateSnapshot-testIsInStatesCaseInsensitive-130af38"
        },
        "telemetry": {
            "id": "3f8a2c8d-6aeb-4c02-a49c-e9f74dd2caaa",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 790,
                "lineStart": 56,
                "lineEnd": 845,
                "bodyLineStart": 56,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                "sourceCode": "public class ShareGroupTest {\n\n    @Test\n    public void testType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(Group.GroupType.SHARE, shareGroup.type());\n    }\n\n    @Test\n    public void testProtocolType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(\"share\", shareGroup.protocolType());\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        // Create a member.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        shareGroup.updateMember(member);\n\n        // Get that member back.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            shareGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ShareGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(member, shareGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        ShareGroupMember member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n        shareGroup.updateMember(member);\n        assertTrue(shareGroup.hasMember(\"member\"));\n\n        shareGroup.removeMember(\"member\");\n        assertFalse(shareGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 6));\n\n        member = new ShareGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ShareGroupMember m1 = new ShareGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        shareGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n\n        shareGroup.removeMember(member.memberId());\n\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testGroupState() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(ShareGroup.ShareGroupState.EMPTY, shareGroup.state());\n        assertEquals(\"Empty\", shareGroup.stateAsString());\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        shareGroup.updateMember(member1);\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertEquals(\"Stable\", shareGroup.stateAsString());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n        assertEquals(Group.GroupType.parse(\"share\"), Group.GroupType.SHARE);\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Share\"), Group.GroupType.SHARE);\n        assertEquals(Group.GroupType.parse(\"SHare\"), Group.GroupType.SHARE);\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        shareGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        shareGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        shareGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        ShareGroupMember member4 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        shareGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"test-group\");\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        shareGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, shareGroup.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch() + 1);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch() + 1, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        shareGroup.setGroupEpoch(shareGroup.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        shareGroup.requestMetadataRefresh();\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetCommit(null, null, -1, false, version));\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        assertEquals(ShareGroupState.STABLE, shareGroup.state(1));\n        assertEquals(\"Stable\", shareGroup.stateAsString(1));\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::offsetExpirationCondition);\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetFetch(null, -1, -1));\n    }\n\n    @Test\n    public void testValidateOffsetDelete() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::validateOffsetDelete);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state());\n        assertDoesNotThrow(shareGroup::validateDeleteGroup);\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        shareGroup.updateMember(member1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        shareGroup.updateMember(member1);\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(shareGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(shareGroup.isSubscribedToTopic(\"bar\"));\n\n        shareGroup.removeMember(\"member1\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"foo\"));\n\n        shareGroup.removeMember(\"member2\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-id-1\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY.toString(), shareGroup.stateAsString(0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .build());\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ShareGroupDescribeResponseData.DescribedGroup expected = new ShareGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ShareGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ShareGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\")),\n                new ShareGroupDescribeResponseData.Member().setMemberId(\"member2\")\n            ));\n        ShareGroupDescribeResponseData.DescribedGroup actual = shareGroup.asDescribedGroup(1, \"assignorName\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    private ShareGroup createShareGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ShareGroup(\n            snapshotRegistry,\n            groupId\n        );\n    }\n}",
                "methodCount": 26
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 833,
                        "lineEnd": 835,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) is interacting exclusively with the SnapshotRegistry class by calling its getOrCreateSnapshot method. This method appears to logically belong with the SnapshotRegistry because it operates on the SnapshotRegistry object directly, which indicates that the method fits well with the responsibilities of the SnapshotRegistry class. Moving it to this class will encapsulate functionality related to creating snapshots within the SnapshotRegistry, following the principle of cohesive class design.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "This helper method is related to snapshot creation, which is a responsibility of SnapshotRegistry."
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private ShareGroup createShareGroup(String groupId)",
                            "target_class": "ShareGroup",
                            "rationale": "This method initializes a ShareGroup, which is more appropriate to be within the ShareGroup class itself."
                        },
                        {
                            "method_name": "mkAssignment",
                            "method_signature": "private static Map<Uuid, Set<Integer>> mkAssignment(Uuid topic, Integer... partitions)",
                            "target_class": "AssignmentUtils",
                            "rationale": "This method likely helps in creating assignments, which might be better located in a utility class specifically for assignments."
                        },
                        {
                            "method_name": "mkTopicAssignment",
                            "method_signature": "private static Map.Entry<Uuid, Set<Integer>> mkTopicAssignment(Uuid topic, Integer... partitions)",
                            "target_class": "AssignmentUtils",
                            "rationale": "Similar to mkAssignment, it deals with topic assignments and should be placed in a utility class for assignment operations."
                        },
                        {
                            "method_name": "mkMap",
                            "method_signature": "private static <K, V> Map<K, V> mkMap(Map.Entry<K, V>... entries)",
                            "target_class": "CollectionUtils",
                            "rationale": "This method is a general utility for creating maps and belongs in a utility class specialized for collections."
                        },
                        {
                            "method_name": "mkEntry",
                            "method_signature": "private static <K, V> Map.Entry<K, V> mkEntry(K key, V value)",
                            "target_class": "CollectionUtils",
                            "rationale": "Similar to mkMap, this method helps create map entries and should be in a collections utility class."
                        },
                        {
                            "method_name": "mkMapOfPartitionRacks",
                            "method_signature": "private static Map<Uuid, Map<Integer, String>> mkMapOfPartitionRacks(Integer... partitions)",
                            "target_class": "AssignmentUtils",
                            "rationale": "This method is related specifically to partition racks, which would fit better within a utility class for assignments."
                        }
                    ],
                    "llm_response_time": 6503
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The idempotentCreateSnapshot method directly interacts with the SnapshotRegistry class and should be moved there to encapsulate the responsibility of creating or getting a snapshot."
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private ShareGroup createShareGroup(String groupId)",
                            "target_class": "ShareGroup",
                            "rationale": "The createShareGroup method instantiates and initializes ShareGroup objects, so it logically belongs to the ShareGroup class to centralize the creation logic."
                        }
                    ],
                    "llm_response_time": 3216
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The method idempotentCreateSnapshot() directly manages instances of SnapshotRegistry, making it more cohesive to be part of the SnapshotRegistry class itself."
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private ShareGroup createShareGroup(String groupId)",
                            "target_class": "ShareGroup",
                            "rationale": "The method createShareGroup() constructs an instance of the ShareGroup class, which indicates that it logically belongs to the ShareGroup class."
                        }
                    ],
                    "llm_response_time": 2923
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.17298872485207484
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.36655271729213784
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 2292
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "voyage": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAddEntriesWithSnapshots() : Map<Integer,String> in class org.apache.kafka.jmh.timeline.TimelineHashMapBenchmark & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                    "startLine": 70,
                    "endLine": 90,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAddEntriesWithSnapshots() : Map<Integer,String>"
                },
                {
                    "filePath": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                    "startLine": 81,
                    "endLine": 81,
                    "startColumn": 17,
                    "endColumn": 61,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                    "startLine": 70,
                    "endLine": 90,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAddEntriesWithSnapshots() : Map<Integer,String>"
                },
                {
                    "filePath": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                    "startLine": 81,
                    "endLine": 81,
                    "startColumn": 17,
                    "endColumn": 65,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(epoch)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 619,
        "extraction_results": {
            "success": true,
            "newCommitHash": "ebceb5d1219775a9e83c01195bca606578cff305",
            "newBranchName": "extract-idempotentCreateSnapshot-testAddEntriesWithSnapshots-130af38"
        },
        "telemetry": {
            "id": "44b82755-0c42-4293-8bb1-7956e36987d9",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 58,
                "lineStart": 38,
                "lineEnd": 95,
                "bodyLineStart": 38,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                "sourceCode": "@State(Scope.Benchmark)\n@Fork(value = 1)\n@Warmup(iterations = 3)\n@Measurement(iterations = 10)\n@BenchmarkMode(Mode.AverageTime)\n@OutputTimeUnit(TimeUnit.MILLISECONDS)\n\npublic class TimelineHashMapBenchmark {\n    private static final int NUM_ENTRIES = 1_000_000;\n\n    @Benchmark\n    public Map<Integer, String> testAddEntriesInHashMap() {\n        HashMap<Integer, String> map = new HashMap<>(NUM_ENTRIES);\n        for (int i = 0; i < NUM_ENTRIES; i++) {\n            int key = (int) (0xffffffff & ((i * 2862933555777941757L) + 3037000493L));\n            map.put(key, String.valueOf(key));\n        }\n        return map;\n    }\n\n    @Benchmark\n    public Map<Integer, String> testAddEntriesInTimelineMap() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TimelineHashMap<Integer, String> map =\n            new TimelineHashMap<>(snapshotRegistry, NUM_ENTRIES);\n        for (int i = 0; i < NUM_ENTRIES; i++) {\n            int key = (int) (0xffffffff & ((i * 2862933555777941757L) + 3037000493L));\n            map.put(key, String.valueOf(key));\n        }\n        return map;\n    }\n\n    @Benchmark\n    public Map<Integer, String> testAddEntriesWithSnapshots() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TimelineHashMap<Integer, String> map =\n            new TimelineHashMap<>(snapshotRegistry, NUM_ENTRIES);\n        long epoch = 0;\n        int j = 0;\n        for (int i = 0; i < NUM_ENTRIES; i++) {\n            int key = (int) (0xffffffff & ((i * 2862933555777941757L) + 3037000493L));\n            if (j > 10 && key % 3 == 0) {\n                snapshotRegistry.deleteSnapshotsUpTo(epoch - 1000);\n                idempotentCreateSnapshot(snapshotRegistry, epoch);\n                j = 0;\n            } else {\n                j++;\n            }\n            map.put(key, String.valueOf(key));\n            epoch++;\n        }\n        return map;\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch) {\n        snapshotRegistry.getOrCreateSnapshot(epoch);\n    }\n}",
                "methodCount": 4
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 91,
                        "lineEnd": 93,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method `idempotentCreateSnapshot` interacts closely with the SnapshotRegistry class by invoking its method `getOrCreateSnapshot`. This indicates that its primary responsibility aligns with the operations and maintenance of snapshots, which are central to the SnapshotRegistry class. By moving this method into SnapshotRegistry, it organizes related functionality in one place, enhancing modularity and maintainability. Additionally, having this method within SnapshotRegistry simplifies method access and testing as it reduces inter-class dependencies.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The idempotentCreateSnapshot method deals specifically with operations on the SnapshotRegistry, and it's better suited to be encapsulated within the SnapshotRegistry class. This will improve cohesion by keeping snapshot-related logic within the SnapshotRegistry class, making it more maintainable."
                        }
                    ],
                    "llm_response_time": 1933
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The method idempotentCreateSnapshot is specific to the SnapshotRegistry class as it directly operates on SnapshotRegistry objects. Moving it to SnapshotRegistry will make the code cleaner and encapsulate related functionality within the appropriate class."
                        }
                    ],
                    "llm_response_time": 3746
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The idempotentCreateSnapshot method directly manipulates the SnapshotRegistry and should belong to that class to minimize dependencies and improve encapsulation."
                        }
                    ],
                    "llm_response_time": 1634
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.21790681682020446
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.512615868483097
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)"
                    ],
                    "llm_response_time": 1714
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)"
                    ],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)"
                    ],
                    "llm_response_time": 1
                },
                "voyage": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)"
                    ],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.27411570338737873
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 5016,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from private OffsetControlManager(logContext LogContext, snapshotRegistry SnapshotRegistry, metrics QuorumControllerMetrics, time Time) in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 142,
                    "endLine": 164,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private OffsetControlManager(logContext LogContext, snapshotRegistry SnapshotRegistry, metrics QuorumControllerMetrics, time Time)"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 159,
                    "endLine": 159,
                    "startColumn": 9,
                    "endColumn": 51,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 142,
                    "endLine": 164,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private OffsetControlManager(logContext LogContext, snapshotRegistry SnapshotRegistry, metrics QuorumControllerMetrics, time Time)"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 159,
                    "endLine": 159,
                    "startColumn": 9,
                    "endColumn": 55,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(-1L)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 620,
        "extraction_results": {
            "success": false
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package activate(newNextWriteOffset long) : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 236,
                    "endLine": 255,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package activate(newNextWriteOffset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 252,
                    "endLine": 252,
                    "startColumn": 9,
                    "endColumn": 64,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 236,
                    "endLine": 255,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package activate(newNextWriteOffset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 252,
                    "endLine": 252,
                    "startColumn": 9,
                    "endColumn": 68,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastStableOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 621,
        "extraction_results": {
            "success": true,
            "newCommitHash": "7a2505c28c1143648be2a4758aca3af514748fde",
            "newBranchName": "extract-idempotentCreateSnapshot-activate-130af38"
        },
        "telemetry": {
            "id": "e0efb303-8f6d-4523-9212-5d16fe248e7c",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        idempotentCreateSnapshot(lastStableOffset);\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    private void idempotentCreateSnapshot(long epoch) {\n        snapshotRegistry.getOrCreateSnapshot(epoch);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                idempotentCreateSnapshot(lastStableOffset);\n            }\n        }\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        snapshotRegistry.getOrCreateSnapshot(offset - 1);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 3,
                "candidates": [
                    {
                        "lineStart": 256,
                        "lineEnd": 258,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() directly calls the getOrCreateSnapshot() method of SnapshotRegistry and does not use any members specific to the existing class where it is currently implemented. Moving idempotentCreateSnapshot() to SnapshotRegistry would enhance cohesion, reduce coupling between classes, and place the method closer to the relevant data and functionality it operates on. SnapshotRegistry already handles snapshot creation and management, making it the most suitable place for this method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 295,
                        "lineEnd": 313,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method handleScheduleAtomicAppend to class Time",
                        "description": "Move method handleScheduleAtomicAppend to org.apache.kafka.common.utils.Time\nRationale: The method `handleScheduleAtomicAppend` does not seem to fit into the `Time` class for multiple reasons: it deals with scheduling writes, managing offsets, interacting with snapshots, and updating metrics, which are not responsibilities of a time abstraction class. The `Time` class is fundamentally focused on providing time-related utilities, such as fetching the current time in various units and waiting for conditions, and it is not designed to handle operations related to record scheduling and offsets. Hence, the method does not belong in the `Time` class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 260,
                        "lineEnd": 275,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method deactivate to class Time",
                        "description": "Move method deactivate to org.apache.kafka.common.utils.Time\nRationale: The deactivate() method doesn't logically fit within the responsibilities of the Time class. The Time class is focused on managing and abstracting time-related functionality, such as keeping track of current time in various resolutions, sleeping for a certain period, and managing timed conditions. The deactivate() method, on the other hand, is concerned with managing the state and metrics of a QuorumController, which should be part of a class that deals with managing those entities directly.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "replay(BeginTransactionRecord message, long offset)",
                            "method_signature": "public void replay(BeginTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The replay of transaction-related records is a specialized function that is better handled by a class designed for transaction management, such as a TransactionManager class."
                        },
                        {
                            "method_name": "replay(EndTransactionRecord message, long offset)",
                            "method_signature": "public void replay(EndTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "This method deals specifically with ending transactions and is more logically grouped with other transaction management functionalities."
                        },
                        {
                            "method_name": "replay(AbortTransactionRecord message, long offset)",
                            "method_signature": "public void replay(AbortTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "Aborting transactions is also a specialized task which fits better within a TransactionManager class."
                        }
                    ],
                    "llm_response_time": 3101
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(BeginTransactionRecord message, long offset)",
                            "target_class": "TransactionRecordHandler",
                            "rationale": "The method is handling the replay of transaction records, which seems more appropriate for a class specifically designed for handling transaction records."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(EndTransactionRecord message, long offset)",
                            "target_class": "TransactionRecordHandler",
                            "rationale": "Similar to the previous replay method, this method is focused on handling the replay of transaction end records and should reside in a class managing transaction records."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(AbortTransactionRecord message, long offset)",
                            "target_class": "TransactionRecordHandler",
                            "rationale": "This method handles replay for abort transactions and should be within a class that comprehensively handles all kinds of transaction-related records."
                        }
                    ],
                    "llm_response_time": 3195
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "setLogContext",
                            "method_signature": "Builder setLogContext(LogContext logContext)",
                            "target_class": "Builder",
                            "rationale": "This method is used solely to set the 'logContext' property of the Builder class."
                        },
                        {
                            "method_name": "setSnapshotRegistry",
                            "method_signature": "Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry)",
                            "target_class": "Builder",
                            "rationale": "This method is used solely to set the 'snapshotRegistry' property of the Builder class."
                        },
                        {
                            "method_name": "setMetrics",
                            "method_signature": "Builder setMetrics(QuorumControllerMetrics metrics)",
                            "target_class": "Builder",
                            "rationale": "This method is used solely to set the 'metrics' property of the Builder class."
                        },
                        {
                            "method_name": "setTime",
                            "method_signature": "Builder setTime(Time time)",
                            "target_class": "Builder",
                            "rationale": "This method is used solely to set the 'time' property of the Builder class."
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public OffsetControlManager build()",
                            "target_class": "Builder",
                            "rationale": "This method is responsible for creating an instance of OffsetControlManager and therefore belongs in the Builder class."
                        }
                    ],
                    "llm_response_time": 3446
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(long epoch)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.326162140590694
                    },
                    "public replay(BeginTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4020483828726073
                    },
                    "public replay(AbortTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.45319752739537034
                    },
                    " handleScheduleAtomicAppend(long endOffset)": {
                        "first": {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.48983622212732597
                    },
                    " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                        "first": {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5739781820469859
                    },
                    " maybeAdvanceLastStableOffset()": {
                        "first": {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.623374484966326
                    },
                    " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                        "first": {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6236593019434067
                    },
                    " deactivate()": {
                        "first": {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6247940469492365
                    },
                    " endLoadSnapshot(long timestamp)": {
                        "first": {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6448743385292846
                    },
                    " activate(long newNextWriteOffset)": {
                        "first": {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6504312161414489
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(long epoch)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.49160704019400675
                    },
                    "public replay(AbortTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5071662704639679
                    },
                    " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                        "first": {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5133710238873651
                    },
                    "public replay(BeginTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5223344267882046
                    },
                    " maybeAdvanceLastStableOffset()": {
                        "first": {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6084566517211638
                    },
                    " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                        "first": {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.645371184580444
                    },
                    " endLoadSnapshot(long timestamp)": {
                        "first": {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.646124975239574
                    },
                    " handleScheduleAtomicAppend(long endOffset)": {
                        "first": {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6572761534292785
                    },
                    " deactivate()": {
                        "first": {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6705628699865729
                    },
                    " activate(long newNextWriteOffset)": {
                        "first": {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.7238642203430652
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "public replay(BeginTransactionRecord message, long offset)",
                        "public replay(AbortTransactionRecord message, long offset)",
                        " activate(long newNextWriteOffset)",
                        " deactivate()",
                        " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                        "private idempotentCreateSnapshot(long epoch)"
                    ],
                    "llm_response_time": 2361
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(long epoch)",
                        "public replay(AbortTransactionRecord message, long offset)",
                        "public replay(BeginTransactionRecord message, long offset)"
                    ],
                    "llm_response_time": 4864
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(long epoch)",
                        "public replay(BeginTransactionRecord message, long offset)",
                        "public replay(AbortTransactionRecord message, long offset)"
                    ],
                    "llm_response_time": 5076
                },
                "voyage": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(long epoch)",
                        "public replay(AbortTransactionRecord message, long offset)",
                        "public replay(BeginTransactionRecord message, long offset)"
                    ],
                    "llm_response_time": 2190
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(long epoch)",
                        " maybeAdvanceLastStableOffset()",
                        " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                        "public replay(BeginTransactionRecord message, long offset)",
                        "public replay(AbortTransactionRecord message, long offset)"
                    ],
                    "llm_response_time": 4621
                },
                "voyage-3": {
                    "priority_method_names": [
                        "public replay(AbortTransactionRecord message, long offset)",
                        "private idempotentCreateSnapshot(long epoch)"
                    ],
                    "llm_response_time": 4440
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.3043426172871949
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1306631486813383
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry",
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "replay": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.2081809070085429
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 1936,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "handleScheduleAtomicAppend": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.6338711620940399
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 2175,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "beginLoadSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.318117793495802
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 1744,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "maybeAdvanceLastStableOffset": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.3314072644103849
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 1723,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "handleCommitBatch": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.572161601845791
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 1901,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "deactivate": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.3608583306041376
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 1775,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "endLoadSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.41920438262537035
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 1636,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "activate": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.5065693392912584
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 1195,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package handleScheduleAtomicAppend(endOffset long) : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 292,
                    "endLine": 310,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package handleScheduleAtomicAppend(endOffset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 301,
                    "endLine": 301,
                    "startColumn": 9,
                    "endColumn": 57,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 292,
                    "endLine": 310,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package handleScheduleAtomicAppend(endOffset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 301,
                    "endLine": 301,
                    "startColumn": 9,
                    "endColumn": 61,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(endOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 622,
        "extraction_results": {
            "success": true,
            "newCommitHash": "2e8ab6ec5591a47e6a5217eac0b790aacf3695fb",
            "newBranchName": "extract-idempotentCreateSnapshot-handleScheduleAtomicAppend-130af38"
        },
        "telemetry": {
            "id": "9ec55b8f-d436-43b1-bf32-0e640c69c04b",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        idempotentCreateSnapshot(lastStableOffset);\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        idempotentCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    private void idempotentCreateSnapshot(long endOffset) {\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                idempotentCreateSnapshot(lastStableOffset);\n            }\n        }\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        idempotentCreateSnapshot(offset - 1);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 3,
                "candidates": [
                    {
                        "lineStart": 311,
                        "lineEnd": 313,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The idempotentCreateSnapshot method relies heavily on the snapshotRegistry instance, utilizing its getOrCreateSnapshot method to create snapshots based on the provided endOffset. Since this functionality closely aligns with the core responsibilities of the SnapshotRegistry class, it would be most appropriate to move this method there.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 291,
                        "lineEnd": 309,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method handleScheduleAtomicAppend to class Time",
                        "description": "Move method handleScheduleAtomicAppend to org.apache.kafka.common.utils.Time\nRationale: The handleScheduleAtomicAppend method relies on time-related functionalities to set metrics for timestamps. Moving it to the Time class consolidates all time-related operations within a single class, maintaining a separation of concerns. Though it seems like this method is related to scheduling and metrics rather than time itself, it heavily utilizes the time.milliseconds() method which indicates that the Time class could be a fitting target. Moreover, the Time class is designed to be thread-safe, which might be beneficial for operations like scheduleAtomicAppend, improving overall application robustness.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 256,
                        "lineEnd": 271,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method deactivate to class Time",
                        "description": "Move method deactivate to org.apache.kafka.common.utils.Time\nRationale: The deactivate() method doesn't logically fit within the responsibilities of the Time class. The Time class is focused on managing and abstracting time-related functionality, such as keeping track of current time in various resolutions, sleeping for a certain period, and managing timed conditions. The deactivate() method, on the other hand, is concerned with managing the state and metrics of a QuorumController, which should be part of a class that deals with managing those entities directly.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(BeginTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The replay method for BeginTransactionRecord is directly related to managing transaction states, which logically falls under TransactionManager."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(EndTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The replay method for EndTransactionRecord is related to managing transactions, which is more appropriate for the TransactionManager class."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(AbortTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The replay method for AbortTransactionRecord deals with handling transaction abort scenarios, making it suitable for the TransactionManager class."
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": "void handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "CommitManager",
                            "rationale": "The handleCommitBatch method processes commit batches, which is a separate concern from offset control and should be handled by a CommitManager class."
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": "void handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "AppendManager",
                            "rationale": "Managing scheduled appends is distinct from offset control and better suited for a class focused on appends, such as AppendManager."
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": "void beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "SnapshotManager",
                            "rationale": "Loading snapshots is a specific responsibility around managing snapshots, which makes it better suited for a SnapshotManager class."
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": "void endLoadSnapshot(long timestamp)",
                            "target_class": "SnapshotManager",
                            "rationale": "Ending the loading of snapshots is related to snapshot management, which is more appropriate for the SnapshotManager class."
                        }
                    ],
                    "llm_response_time": 6195
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(BeginTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The replay method that handles BeginTransactionRecord is transaction-specific and primarily concerns transaction management instead of offset control."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(EndTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "Similar to the replay method for BeginTransactionRecord, handling EndTransactionRecord is more relevant to transaction management."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(AbortTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "Handling AbortTransactionRecord is also transaction-specific, making it more suitable for a TransactionManager class."
                        }
                    ],
                    "llm_response_time": 3331
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": "void handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "Batch",
                            "rationale": "The handleCommitBatch method interacts directly with the Batch object and updates its state. Moving this method to the Batch class would make it more cohesive and maintainable."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "void replay(BeginTransactionRecord message, long offset)",
                            "target_class": "BeginTransactionRecord",
                            "rationale": "The replay method is closely related to BeginTransactionRecord and deals with replaying the transaction. Therefore, it fits better within the BeginTransactionRecord class."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "void replay(EndTransactionRecord message, long offset)",
                            "target_class": "EndTransactionRecord",
                            "rationale": "The replay method is closely associated with EndTransactionRecord and pertains to replaying end transactions. Hence, moving this method to the EndTransactionRecord class would be appropriate."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "void replay(AbortTransactionRecord message, long offset)",
                            "target_class": "AbortTransactionRecord",
                            "rationale": "The replay method handles the replaying of abort transactions and is associated with the AbortTransactionRecord. Moving this method to the AbortTransactionRecord class would improve cohesion."
                        },
                        {
                            "method_name": "metrics",
                            "method_signature": "QuorumControllerMetrics metrics()",
                            "target_class": "QuorumControllerMetrics",
                            "rationale": "The metrics method is used to retrieve the metrics associated with the state of the quorum controller. Moving it to the QuorumControllerMetrics class would centralize the responsibility for managing metrics."
                        }
                    ],
                    "llm_response_time": 5138
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(long endOffset)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3288576954716088
                    },
                    "public replay(AbortTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.45316879763988827
                    },
                    " handleScheduleAtomicAppend(long endOffset)": {
                        "first": {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.49031739719817796
                    },
                    " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                        "first": {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.573945552037463
                    },
                    " maybeAdvanceLastStableOffset()": {
                        "first": {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6233399231341831
                    },
                    " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                        "first": {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6236247721810713
                    },
                    " deactivate()": {
                        "first": {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6247600158455892
                    },
                    " endLoadSnapshot(long timestamp)": {
                        "first": {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.644837953258935
                    },
                    " activate(long newNextWriteOffset)": {
                        "first": {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6503939239809882
                    }
                },
                "voyage": {
                    "public replay(AbortTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5049842858766387
                    },
                    " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                        "first": {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5126655198753425
                    },
                    "private idempotentCreateSnapshot(long endOffset)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5510502060696468
                    },
                    " maybeAdvanceLastStableOffset()": {
                        "first": {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6088439669570727
                    },
                    " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                        "first": {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.639581494334759
                    },
                    " endLoadSnapshot(long timestamp)": {
                        "first": {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6418961294892257
                    },
                    " handleScheduleAtomicAppend(long endOffset)": {
                        "first": {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6485778588136819
                    },
                    " deactivate()": {
                        "first": {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6697900339745598
                    },
                    " activate(long newNextWriteOffset)": {
                        "first": {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.7226699456326655
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [],
                    "llm_response_time": 5501
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(long endOffset)",
                        "public replay(AbortTransactionRecord message, long offset)"
                    ],
                    "llm_response_time": 4329
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "public replay(AbortTransactionRecord message, long offset)",
                        "private idempotentCreateSnapshot(long endOffset)"
                    ],
                    "llm_response_time": 4197
                },
                "voyage": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(long endOffset)",
                        " maybeAdvanceLastStableOffset()",
                        " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                        " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                        " endLoadSnapshot(long timestamp)",
                        " handleScheduleAtomicAppend(long endOffset)",
                        " activate(long newNextWriteOffset)"
                    ],
                    "llm_response_time": 5658
                },
                "voyage-5": {
                    "priority_method_names": [
                        "public replay(AbortTransactionRecord message, long offset)",
                        "private idempotentCreateSnapshot(long endOffset)"
                    ],
                    "llm_response_time": 5036
                },
                "voyage-3": {
                    "priority_method_names": [],
                    "llm_response_time": 2538
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.1306631486813383
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry",
                        "Time"
                    ],
                    "llm_response_time": 2581,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "replay": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.2081809070085429
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "handleScheduleAtomicAppend": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.6338711620940399
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 2109,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "beginLoadSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.318117793495802
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "maybeAdvanceLastStableOffset": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.3314072644103849
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "handleCommitBatch": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.572161601845791
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "deactivate": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.3608583306041376
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "endLoadSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.41920438262537035
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "activate": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.5065693392912584
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package maybeAdvanceLastStableOffset() : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 312,
                    "endLine": 329,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package maybeAdvanceLastStableOffset() : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 326,
                    "endLine": 326,
                    "startColumn": 17,
                    "endColumn": 72,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 312,
                    "endLine": 329,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package maybeAdvanceLastStableOffset() : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 326,
                    "endLine": 326,
                    "startColumn": 17,
                    "endColumn": 76,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastStableOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 623,
        "extraction_results": {
            "success": true,
            "newCommitHash": "f02c9361287ce0b3d5874fb1da2a212cf97ae04f",
            "newBranchName": "extract-idempotentCreateSnapshot-maybeAdvanceLastStableOffset-130af38"
        },
        "telemetry": {
            "id": "8b99d0fa-afe0-4a2c-b1e9-377e97413d00",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        idempotentCreateSnapshot();\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                idempotentCreateSnapshot();\n            }\n        }\n    }\n\n    private void idempotentCreateSnapshot() {\n        snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        snapshotRegistry.getOrCreateSnapshot(offset - 1);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 4,
                "candidates": [
                    {
                        "lineStart": 330,
                        "lineEnd": 332,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class Time",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.common.utils.Time\nRationale: The method idempotentCreateSnapshot() appears to deal with snapshot management, primarily interacting with snapshotRegistry and using lastStableOffset. 'Time' interface is related to time management functions and is not logically coherent with the snapshot-related operation. Moving the method to 'Time' would violate single responsibility principle and introduce unnecessary cohesion between unrelated functionalities of time measurements and snapshot management. Therefore, 'Time' is not an appropriate target class for this method.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 291,
                        "lineEnd": 309,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method handleScheduleAtomicAppend to class Time",
                        "description": "Move method handleScheduleAtomicAppend to org.apache.kafka.common.utils.Time\nRationale: The method `handleScheduleAtomicAppend` does not seem to fit into the `Time` class for multiple reasons: it deals with scheduling writes, managing offsets, interacting with snapshots, and updating metrics, which are not responsibilities of a time abstraction class. The `Time` class is fundamentally focused on providing time-related utilities, such as fetching the current time in various units and waiting for conditions, and it is not designed to handle operations related to record scheduling and offsets. Hence, the method does not belong in the `Time` class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 256,
                        "lineEnd": 271,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method deactivate to class Time",
                        "description": "Move method deactivate to org.apache.kafka.common.utils.Time\nRationale: The deactivate() method doesn't logically fit within the responsibilities of the Time class. The Time class is focused on managing and abstracting time-related functionality, such as keeping track of current time in various resolutions, sleeping for a certain period, and managing timed conditions. The deactivate() method, on the other hand, is concerned with managing the state and metrics of a QuorumController, which should be part of a class that deals with managing those entities directly.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 235,
                        "lineEnd": 254,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method activate to class Time",
                        "description": "Move method activate to org.apache.kafka.common.utils.Time\nRationale: The 'activate' method is tightly coupled with the state management and activation logic of an 'OffsetControlManager' related to 'QuorumController', which is beyond the scope and purpose of the 'Time' class. The 'Time' class focuses on providing a standardized interface for time-related operations, such as getting the current time, converting time units, and handling timeouts. As these functionalities are unrelated to the state management and activation logic of an 'OffsetControlManager', moving the 'activate' method to the 'Time' class does not align with the class's responsibilities.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": "void handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "QuorumController",
                            "rationale": "The handleCommitBatch method is dealing with batch operations and typically such operations are more appropriate to be managed by a controller class that deals with batch management, such as the QuorumController. This will also help in better separation of concerns."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "void replay(BeginTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The replay method for BeginTransactionRecord is dealing with transaction records which suggests it belongs to a class focused on managing transactions, moving it to TransactionManager ensures that transaction-related logic is encapsulated within an appropriate class."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "void replay(EndTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "Similar to replay for BeginTransactionRecord, this method's primary concern is transaction handling. Hence, it fits better within the TransactionManager which should be responsible for all transaction lifecycle events."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "void replay(AbortTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "Aborting transactions is a specific aspect of transaction management. Placing this method under TransactionManager enhances cohesion by centralizing transaction logic in one class."
                        }
                    ],
                    "llm_response_time": 4367
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(BeginTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The method handles transactions, a concern related to a transaction management responsibility, making it appropriate to move to a TransactionManager class."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(EndTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The method deals with ending transactions, which falls under transaction management, thus it should be moved to the TransactionManager class."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(AbortTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "This method handles aborting transactions, which is a transaction management operation, and it would be more appropriate to transfer it to the TransactionManager class."
                        }
                    ],
                    "llm_response_time": 3535
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(BeginTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The method handles transaction records, which would be more appropriate in a class specifically dealing with transactions, such as 'TransactionManager'."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(EndTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The method handles transaction records, which would be more appropriate in a class specifically dealing with transactions, such as 'TransactionManager'."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(AbortTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The method handles transaction records, which would be more appropriate in a class specifically dealing with transactions, such as 'TransactionManager'."
                        }
                    ],
                    "llm_response_time": 2996
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot()": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3522953764676562
                    },
                    "public replay(BeginTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.40206112830788043
                    },
                    "public replay(AbortTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.453211894322331
                    },
                    " handleScheduleAtomicAppend(long endOffset)": {
                        "first": {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4898501156284456
                    },
                    " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                        "first": {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5739944991388558
                    },
                    " maybeAdvanceLastStableOffset()": {
                        "first": {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6233917680383996
                    },
                    " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                        "first": {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6236765689755942
                    },
                    " deactivate()": {
                        "first": {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6248110645866022
                    },
                    " endLoadSnapshot(long timestamp)": {
                        "first": {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6448925334742986
                    },
                    " activate(long newNextWriteOffset)": {
                        "first": {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6504498646273721
                    }
                },
                "voyage": {
                    "public replay(AbortTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5051708538963839
                    },
                    " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                        "first": {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5110187249003356
                    },
                    "public replay(BeginTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5202516787186635
                    },
                    "private idempotentCreateSnapshot()": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5511860143244525
                    },
                    " maybeAdvanceLastStableOffset()": {
                        "first": {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6100213828697582
                    },
                    " endLoadSnapshot(long timestamp)": {
                        "first": {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6433963438025715
                    },
                    " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                        "first": {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6450579794514504
                    },
                    " handleScheduleAtomicAppend(long endOffset)": {
                        "first": {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6570626689508942
                    },
                    " deactivate()": {
                        "first": {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6692273909957315
                    },
                    " activate(long newNextWriteOffset)": {
                        "first": {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.7181196687939895
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "public replay(BeginTransactionRecord message, long offset)",
                        "public replay(AbortTransactionRecord message, long offset)",
                        "private idempotentCreateSnapshot()"
                    ],
                    "llm_response_time": 6621
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "public replay(AbortTransactionRecord message, long offset)",
                        "public replay(BeginTransactionRecord message, long offset)",
                        "private idempotentCreateSnapshot()"
                    ],
                    "llm_response_time": 4273
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "public replay(BeginTransactionRecord message, long offset)",
                        "public replay(AbortTransactionRecord message, long offset)",
                        "private idempotentCreateSnapshot()"
                    ],
                    "llm_response_time": 5383
                },
                "voyage": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot()",
                        "public replay(BeginTransactionRecord message, long offset)",
                        "public replay(AbortTransactionRecord message, long offset)"
                    ],
                    "llm_response_time": 2723
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot()",
                        " maybeAdvanceLastStableOffset()",
                        " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                        "public replay(AbortTransactionRecord message, long offset)",
                        "public replay(BeginTransactionRecord message, long offset)"
                    ],
                    "llm_response_time": 5074
                },
                "voyage-3": {
                    "priority_method_names": [
                        "public replay(AbortTransactionRecord message, long offset)",
                        "public replay(BeginTransactionRecord message, long offset)"
                    ],
                    "llm_response_time": 3252
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.14113233090687777
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 1989,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "replay": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.2081809070085429
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "handleScheduleAtomicAppend": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.6338711620940399
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "beginLoadSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.318117793495802
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "maybeAdvanceLastStableOffset": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.3314072644103849
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 1865,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "handleCommitBatch": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.572161601845791
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "deactivate": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.3608583306041376
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "endLoadSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.41920438262537035
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "activate": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.5065693392912584
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 2540,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package endLoadSnapshot(timestamp long) : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 354,
                    "endLine": 376,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package endLoadSnapshot(timestamp long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 365,
                    "endLine": 365,
                    "startColumn": 9,
                    "endColumn": 79,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 354,
                    "endLine": 376,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package endLoadSnapshot(timestamp long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 365,
                    "endLine": 365,
                    "startColumn": 9,
                    "endColumn": 83,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "this.snapshotRegistry.idempotentCreateSnapshot(currentSnapshotId.offset())"
                }
            ],
            "isStatic": false
        },
        "ref_id": 624,
        "extraction_results": {
            "success": true,
            "newCommitHash": "e4bf466a438e0f393135b84ed4b5611c7608ac3e",
            "newBranchName": "extract-idempotentCreateSnapshot-endLoadSnapshot-130af38"
        },
        "telemetry": {
            "id": "f4f52051-c606-4ce9-84b0-407347eebc9e",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n            }\n        }\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        idempotentCreateSnapshot();\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    private void idempotentCreateSnapshot() {\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        snapshotRegistry.getOrCreateSnapshot(offset - 1);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 3,
                "candidates": [
                    {
                        "lineStart": 377,
                        "lineEnd": 379,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class Time",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.common.utils.Time\nRationale: The method `idempotentCreateSnapshot` interacts primarily with the `snapshotRegistry` and `currentSnapshotId` fields, suggesting it deals with snapshot management rather than time or scheduling-related functionality. None of the provided classes, including `Time`, seem to be directly related to snapshot management. Unless `snapshotRegistry` and `currentSnapshotId` belong to `Time` or are closely related to time-based functionality, moving the method to `Time` may not be appropriate. Instead, a new class specifically dealing with snapshot management might be more suitable if such a class can be identified. Further understanding of the broader context of the existing class holding `idempotentCreateSnapshot` would be beneficial.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 291,
                        "lineEnd": 309,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method handleScheduleAtomicAppend to class Time",
                        "description": "Move method handleScheduleAtomicAppend to org.apache.kafka.common.utils.Time\nRationale: The method `handleScheduleAtomicAppend` does not seem to fit into the `Time` class for multiple reasons: it deals with scheduling writes, managing offsets, interacting with snapshots, and updating metrics, which are not responsibilities of a time abstraction class. The `Time` class is fundamentally focused on providing time-related utilities, such as fetching the current time in various units and waiting for conditions, and it is not designed to handle operations related to record scheduling and offsets. Hence, the method does not belong in the `Time` class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 256,
                        "lineEnd": 271,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method deactivate to class Time",
                        "description": "Move method deactivate to org.apache.kafka.common.utils.Time\nRationale: The deactivate() method doesn't logically fit within the responsibilities of the Time class. The Time class is focused on managing and abstracting time-related functionality, such as keeping track of current time in various resolutions, sleeping for a certain period, and managing timed conditions. The deactivate() method, on the other hand, is concerned with managing the state and metrics of a QuorumController, which should be part of a class that deals with managing those entities directly.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "replay(BeginTransactionRecord message, long offset)",
                            "method_signature": "public void replay(BeginTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The replay(BeginTransactionRecord message, long offset) method deals specifically with managing and replaying transaction records, which logically belongs to a class managing transactions."
                        },
                        {
                            "method_name": "replay(EndTransactionRecord message, long offset)",
                            "method_signature": "public void replay(EndTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The replay(EndTransactionRecord message, long offset) method deals specifically with ending transaction records, which logically belongs to a class managing transactions."
                        },
                        {
                            "method_name": "replay(AbortTransactionRecord message, long offset)",
                            "method_signature": "public void replay(AbortTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The replay(AbortTransactionRecord message, long offset) method deals specifically with aborting transaction records, which logically belongs to a class managing transactions."
                        }
                    ],
                    "llm_response_time": 3405
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "setLogContext",
                            "method_signature": "public Builder setLogContext(LogContext logContext)",
                            "target_class": "Builder",
                            "rationale": "This method sets a builder property and should be part of the Builder class to follow the builder design pattern properly."
                        },
                        {
                            "method_name": "setSnapshotRegistry",
                            "method_signature": "public Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry)",
                            "target_class": "Builder",
                            "rationale": "This method sets a builder property and should be part of the Builder class to follow the builder design pattern properly."
                        },
                        {
                            "method_name": "setMetrics",
                            "method_signature": "public Builder setMetrics(QuorumControllerMetrics metrics)",
                            "target_class": "Builder",
                            "rationale": "This method sets a builder property and should be part of the Builder class to follow the builder design pattern properly."
                        },
                        {
                            "method_name": "setTime",
                            "method_signature": "public Builder setTime(Time time)",
                            "target_class": "Builder",
                            "rationale": "This method sets a builder property and should be part of the Builder class to follow the builder design pattern properly."
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public OffsetControlManager build()",
                            "target_class": "Builder",
                            "rationale": "This method finalizes the building process and should be part of the Builder class to follow the builder design pattern properly."
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": "void maybeAdvanceLastStableOffset()",
                            "target_class": "OffsetControlManager",
                            "rationale": "This method checks and advances the stable offset if needed and fits well within the current class as it deals directly with the internal state of the `OffsetControlManager`."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot()",
                            "target_class": "OffsetControlManager",
                            "rationale": "This method creates a snapshot in idempotent fashion which is a core functionality related to snapshot management, hence it should remain within this class."
                        }
                    ],
                    "llm_response_time": 5918
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "replayBeginTransactionRecord",
                            "method_signature": "public void replayBeginTransactionRecord(BeginTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "Replaying a BeginTransactionRecord is closely associated with transaction management. It should be part of the TransactionManager, which specializes in handling and managing transactions."
                        },
                        {
                            "method_name": "replayEndTransactionRecord",
                            "method_signature": "public void replayEndTransactionRecord(EndTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "Replaying an EndTransactionRecord is specifically about transactions and should be in the TransactionManager class where it can be handled appropriately within the context of managing transactions."
                        },
                        {
                            "method_name": "replayAbortTransactionRecord",
                            "method_signature": "public void replayAbortTransactionRecord(AbortTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "Replaying an AbortTransactionRecord involves transaction logic and should be moved to the TransactionManager class to encapsulate all transaction-related operations in one place."
                        }
                    ],
                    "llm_response_time": 3736
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot()": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3522953764676562
                    },
                    "public replay(BeginTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.40206112830788043
                    },
                    "public replay(AbortTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.453211894322331
                    },
                    " handleScheduleAtomicAppend(long endOffset)": {
                        "first": {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4898501156284456
                    },
                    " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                        "first": {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5739944991388558
                    },
                    " maybeAdvanceLastStableOffset()": {
                        "first": {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6233917680383996
                    },
                    " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                        "first": {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6236765689755942
                    },
                    " deactivate()": {
                        "first": {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6248110645866022
                    },
                    " endLoadSnapshot(long timestamp)": {
                        "first": {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6448925334742986
                    },
                    " activate(long newNextWriteOffset)": {
                        "first": {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6504498646273721
                    }
                },
                "voyage": {
                    "public replay(AbortTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5096583443675277
                    },
                    " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                        "first": {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5141522299155448
                    },
                    "public replay(BeginTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5240874208150137
                    },
                    "private idempotentCreateSnapshot()": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5631970598511161
                    },
                    " maybeAdvanceLastStableOffset()": {
                        "first": {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6103719568638888
                    },
                    " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                        "first": {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6397624047209258
                    },
                    " endLoadSnapshot(long timestamp)": {
                        "first": {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.642863072780253
                    },
                    " handleScheduleAtomicAppend(long endOffset)": {
                        "first": {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6592053510199872
                    },
                    " deactivate()": {
                        "first": {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6712922957250084
                    },
                    " activate(long newNextWriteOffset)": {
                        "first": {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.719501830770968
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [],
                    "llm_response_time": 4995
                },
                "tf-idf-5": {
                    "priority_method_names": [],
                    "llm_response_time": 3729
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "public replay(BeginTransactionRecord message, long offset)",
                        "public replay(AbortTransactionRecord message, long offset)",
                        "private idempotentCreateSnapshot()"
                    ],
                    "llm_response_time": 2872
                },
                "voyage": {
                    "priority_method_names": [],
                    "llm_response_time": 5082
                },
                "voyage-5": {
                    "priority_method_names": [
                        "public replay(BeginTransactionRecord message, long offset)",
                        "public replay(AbortTransactionRecord message, long offset)",
                        "private idempotentCreateSnapshot()"
                    ],
                    "llm_response_time": 4483
                },
                "voyage-3": {
                    "priority_method_names": [
                        "public replay(AbortTransactionRecord message, long offset)",
                        "public replay(BeginTransactionRecord message, long offset)"
                    ],
                    "llm_response_time": 4122
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.14113233090687777
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 2748,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "replay": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.2081809070085429
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "handleScheduleAtomicAppend": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.6338711620940399
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "beginLoadSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.318117793495802
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "maybeAdvanceLastStableOffset": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.3314072644103849
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 2017,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "handleCommitBatch": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.572161601845791
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "deactivate": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.3608583306041376
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "endLoadSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.41920438262537035
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 2376,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "activate": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.5065693392912584
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 1775,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public replay(message BeginTransactionRecord, offset long) : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 378,
                    "endLine": 389,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public replay(message BeginTransactionRecord, offset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 386,
                    "endLine": 386,
                    "startColumn": 9,
                    "endColumn": 58,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 378,
                    "endLine": 389,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public replay(message BeginTransactionRecord, offset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 386,
                    "endLine": 386,
                    "startColumn": 9,
                    "endColumn": 62,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(offset - 1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 625,
        "extraction_results": {
            "success": true,
            "newCommitHash": "d0d1ef2e01ed6532f34cdbb087e241a4f6880007",
            "newBranchName": "extract-idempotentCreateSnapshot-replay-130af38"
        },
        "telemetry": {
            "id": "947551bb-6810-4119-930e-f6a5f589c9c1",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n            }\n        }\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        idempotentCreateSnapshot(offset);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    private void idempotentCreateSnapshot(long offset) {\n        snapshotRegistry.getOrCreateSnapshot(offset - 1);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 3,
                "candidates": [
                    {
                        "lineStart": 390,
                        "lineEnd": 392,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method 'idempotentCreateSnapshot' specifically deals with creating snapshots, which is central to the SnapshotRegistry class's responsibilities. The method uses 'snapshotRegistry.getOrCreateSnapshot' to handle snapshots, making it more logically coherent to place it within the SnapshotRegistry class. Doing so ensures that all snapshot-related functionalities are encapsulated in one class, following the Single Responsibility Principle.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 291,
                        "lineEnd": 309,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method handleScheduleAtomicAppend to class Time",
                        "description": "Move method handleScheduleAtomicAppend to org.apache.kafka.common.utils.Time\nRationale: The method `handleScheduleAtomicAppend` does not seem to fit into the `Time` class for multiple reasons: it deals with scheduling writes, managing offsets, interacting with snapshots, and updating metrics, which are not responsibilities of a time abstraction class. The `Time` class is fundamentally focused on providing time-related utilities, such as fetching the current time in various units and waiting for conditions, and it is not designed to handle operations related to record scheduling and offsets. Hence, the method does not belong in the `Time` class.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    },
                    {
                        "lineStart": 256,
                        "lineEnd": 271,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method deactivate to class Time",
                        "description": "Move method deactivate to org.apache.kafka.common.utils.Time\nRationale: The deactivate() method doesn't logically fit within the responsibilities of the Time class. The Time class is focused on managing and abstracting time-related functionality, such as keeping track of current time in various resolutions, sleeping for a certain period, and managing timed conditions. The deactivate() method, on the other hand, is concerned with managing the state and metrics of a QuorumController, which should be part of a class that deals with managing those entities directly.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": "void handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "Batch",
                            "rationale": "The handleCommitBatch method deals with actions specific to batch processing, and its primary function is to apply committed batches. Moving it to the Batch class allows the Batch class to manage its own state and transitions better."
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": "void beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "beginLoadSnapshot deals with the initialization and resetting of snapshots, which is closely tied to the SnapshotRegistry responsibilities. Moving it to SnapshotRegistry encapsulates snapshot handling within the relevant class."
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": "void endLoadSnapshot(long timestamp)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "endLoadSnapshot finalizes the loading of snapshots and modifies the state accordingly. It belongs with other snapshot-related methods in SnapshotRegistry to form a cohesive API."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "void replay(BeginTransactionRecord message, long offset)",
                            "target_class": "TransactionRecordHandler",
                            "rationale": "The replay method is specific to handling transactions within a batch (BeginTransactionRecord). Creating a TransactionRecordHandler class to manage transaction states ensures that transaction logic is encapsulated and can be maintained easily."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "void replay(EndTransactionRecord message, long offset)",
                            "target_class": "TransactionRecordHandler",
                            "rationale": "Similar to the previous replay method, this handles EndTransactionRecord within transactions. It logically belongs to a handler or manager dedicated to transaction lifecycle management."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "void replay(AbortTransactionRecord message, long offset)",
                            "target_class": "TransactionRecordHandler",
                            "rationale": "This method handles aborting transactions within a replay process. Placing it within a specialized TransactionRecordHandler class ensures that transaction operations are organized in one place."
                        }
                    ],
                    "llm_response_time": 5734
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(BeginTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The method specifically handles the beginning of a transaction. Transaction-specific methods typically belong to a class responsible for managing transactions such as TransactionManager."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(EndTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The method specifically handles the end of a transaction. Transaction-specific methods typically belong to a class responsible for managing transactions such as TransactionManager."
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public void replay(AbortTransactionRecord message, long offset)",
                            "target_class": "TransactionManager",
                            "rationale": "The method specifically handles the aborting of a transaction. Transaction-specific methods typically belong to a class responsible for managing transactions such as TransactionManager."
                        }
                    ],
                    "llm_response_time": 3924
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "metrics",
                            "method_signature": "public QuorumControllerMetrics metrics()",
                            "target_class": "QuorumControllerMetrics",
                            "rationale": "The method returns QuorumControllerMetrics and it's more appropriate for it to reside directly within the QuorumControllerMetrics class for better cohesion."
                        },
                        {
                            "method_name": "snapshotRegistry",
                            "method_signature": "public SnapshotRegistry snapshotRegistry()",
                            "target_class": "SnapshotRegistry",
                            "rationale": "Accessor methods tightly coupled with the internal state might be better placed directly in the class that encapsulates that state, in this case, SnapshotRegistry."
                        }
                    ],
                    "llm_response_time": 2428
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(long offset)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.29957996542755694
                    },
                    "public replay(AbortTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.45468662649890557
                    },
                    " handleScheduleAtomicAppend(long endOffset)": {
                        "first": {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.4897528859389902
                    },
                    " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                        "first": {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.57388030870975
                    },
                    " maybeAdvanceLastStableOffset()": {
                        "first": {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6232708167123373
                    },
                    " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                        "first": {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6235557298590018
                    },
                    " deactivate()": {
                        "first": {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6246919703173314
                    },
                    " endLoadSnapshot(long timestamp)": {
                        "first": {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.644765201190868
                    },
                    " activate(long newNextWriteOffset)": {
                        "first": {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6503193588991747
                    }
                },
                "voyage": {
                    "public replay(AbortTransactionRecord message, long offset)": {
                        "first": {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.509419711485846
                    },
                    " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                        "first": {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5116387671050442
                    },
                    "private idempotentCreateSnapshot(long offset)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.5422637127365022
                    },
                    " maybeAdvanceLastStableOffset()": {
                        "first": {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6132881796658732
                    },
                    " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                        "first": {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6390118378197097
                    },
                    " endLoadSnapshot(long timestamp)": {
                        "first": {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6456418867502444
                    },
                    " handleScheduleAtomicAppend(long endOffset)": {
                        "first": {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6595121955404046
                    },
                    " deactivate()": {
                        "first": {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.6694947113408708
                    },
                    " activate(long newNextWriteOffset)": {
                        "first": {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.7221218062025735
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [],
                    "llm_response_time": 6480
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        " maybeAdvanceLastStableOffset()",
                        " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                        " handleScheduleAtomicAppend(long endOffset)"
                    ],
                    "llm_response_time": 1482
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(long offset)",
                        "public replay(AbortTransactionRecord message, long offset)"
                    ],
                    "llm_response_time": 3776
                },
                "voyage": {
                    "priority_method_names": [],
                    "llm_response_time": 4313
                },
                "voyage-5": {
                    "priority_method_names": [],
                    "llm_response_time": 4189
                },
                "voyage-3": {
                    "priority_method_names": [
                        "public replay(AbortTransactionRecord message, long offset)",
                        "private idempotentCreateSnapshot(long offset)"
                    ],
                    "llm_response_time": 3324
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.25127272810509715
                        },
                        {
                            "class_name": "Time",
                            "similarity_score": 0.12099576892601743
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry",
                        "Time"
                    ],
                    "llm_response_time": 3340,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "replay": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.2081809070085429
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "handleScheduleAtomicAppend": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.6338711620940399
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "beginLoadSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.318117793495802
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                },
                "maybeAdvanceLastStableOffset": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.3314072644103849
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "handleCommitBatch": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.572161601845791
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "deactivate": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.3608583306041376
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "endLoadSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.41920438262537035
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "activate": {
                    "target_classes": [
                        {
                            "class_name": "Time",
                            "similarity_score": 0.5065693392912584
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "Time"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testLoadSnapshot() : void in class org.apache.kafka.controller.AclControlManagerTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                    "startLine": 203,
                    "endLine": 238,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testLoadSnapshot() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                    "startLine": 206,
                    "endLine": 206,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                    "startLine": 203,
                    "endLine": 238,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testLoadSnapshot() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                    "startLine": 206,
                    "endLine": 206,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 626,
        "extraction_results": {
            "success": true,
            "newCommitHash": "41521201e2c0ea4f6a97d434a4a1b58e6b71b232",
            "newBranchName": "extract-idempotentCreateSnapshot-testLoadSnapshot-130af38"
        },
        "telemetry": {
            "id": "d840cbf3-6e62-4105-a7dc-575b6e5cc2b4",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 265,
                "lineStart": 81,
                "lineEnd": 345,
                "bodyLineStart": 81,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                "sourceCode": "@Timeout(value = 40)\npublic class AclControlManagerTest {\n    /**\n     * Verify that validateNewAcl catches invalid ACLs.\n     */\n    @Test\n    public void testValidateNewAcl() {\n        AclControlManager.validateNewAcl(new AclBinding(\n            new ResourcePattern(TOPIC, \"*\", LITERAL),\n            new AccessControlEntry(\"User:*\", \"*\", ALTER, ALLOW)));\n        assertEquals(\"Invalid patternType UNKNOWN\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateNewAcl(new AclBinding(\n                    new ResourcePattern(TOPIC, \"*\", PatternType.UNKNOWN),\n                    new AccessControlEntry(\"User:*\", \"*\", ALTER, ALLOW)))).\n                getMessage());\n        assertEquals(\"Invalid resourceType UNKNOWN\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateNewAcl(new AclBinding(\n                    new ResourcePattern(ResourceType.UNKNOWN, \"*\", LITERAL),\n                    new AccessControlEntry(\"User:*\", \"*\", ALTER, ALLOW)))).\n                getMessage());\n        assertEquals(\"Invalid operation UNKNOWN\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateNewAcl(new AclBinding(\n                    new ResourcePattern(TOPIC, \"*\", LITERAL),\n                    new AccessControlEntry(\"User:*\", \"*\", AclOperation.UNKNOWN, ALLOW)))).\n                getMessage());\n        assertEquals(\"Invalid permissionType UNKNOWN\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateNewAcl(new AclBinding(\n                    new ResourcePattern(TOPIC, \"*\", LITERAL),\n                    new AccessControlEntry(\"User:*\", \"*\", ALTER, AclPermissionType.UNKNOWN)))).\n                getMessage());\n    }\n\n    /**\n     * Verify that validateFilter catches invalid filters.\n     */\n    @Test\n    public void testValidateFilter() {\n        AclControlManager.validateFilter(new AclBindingFilter(\n            new ResourcePatternFilter(ResourceType.ANY, \"*\", LITERAL),\n            new AccessControlEntryFilter(\"User:*\", \"*\", AclOperation.ANY, AclPermissionType.ANY)));\n        assertEquals(\"Unknown patternFilter.\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateFilter(new AclBindingFilter(\n                    new ResourcePatternFilter(ResourceType.ANY, \"*\", PatternType.UNKNOWN),\n                    new AccessControlEntryFilter(\"User:*\", \"*\", AclOperation.ANY, AclPermissionType.ANY)))).\n                getMessage());\n        assertEquals(\"Unknown entryFilter.\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateFilter(new AclBindingFilter(\n                    new ResourcePatternFilter(ResourceType.ANY, \"*\", MATCH),\n                    new AccessControlEntryFilter(\"User:*\", \"*\", AclOperation.ANY, AclPermissionType.UNKNOWN)))).\n                getMessage());\n    }\n\n    static class MockClusterMetadataAuthorizer implements ClusterMetadataAuthorizer {\n        Map<Uuid, StandardAcl> acls = Collections.emptyMap();\n\n        @Override\n        public void setAclMutator(AclMutator aclMutator) {\n            // do nothing\n        }\n\n        @Override\n        public AclMutator aclMutatorOrException() {\n            throw new NotControllerException(\"The current node is not the active controller.\");\n        }\n\n        @Override\n        public void completeInitialLoad() {\n            // do nothing\n        }\n\n        @Override\n        public void completeInitialLoad(Exception e) {\n            // do nothing\n        }\n\n        @Override\n        public void loadSnapshot(Map<Uuid, StandardAcl> acls) {\n            this.acls = new HashMap<>(acls);\n        }\n\n        @Override\n        public void addAcl(Uuid id, StandardAcl acl) {\n            // do nothing\n        }\n\n        @Override\n        public void removeAcl(Uuid id) {\n            // do nothing\n        }\n\n        @Override\n        public Map<Endpoint, ? extends CompletionStage<Void>> start(AuthorizerServerInfo serverInfo) {\n            return null; // do nothing\n        }\n\n        @Override\n        public List<AuthorizationResult> authorize(AuthorizableRequestContext requestContext, List<Action> actions) {\n            return null; // do nothing\n        }\n\n        @Override\n        public Iterable<AclBinding> acls(AclBindingFilter filter) {\n            return null; // do nothing\n        }\n\n        @Override\n        public void close() throws IOException {\n            // do nothing\n        }\n\n        @Override\n        public void configure(Map<String, ?> configs) {\n            // do nothing\n        }\n    }\n\n    @Test\n    public void testLoadSnapshot() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        idempotentCreateSnapshot(snapshotRegistry);\n        AclControlManager manager = new AclControlManager.Builder().\n            setSnapshotRegistry(snapshotRegistry).\n            build();\n\n        // Load TEST_ACLS into the AclControlManager.\n        Set<ApiMessageAndVersion> loadedAcls = new HashSet<>();\n        for (StandardAclWithId acl : TEST_ACLS) {\n            AccessControlEntryRecord record = acl.toRecord();\n            assertTrue(loadedAcls.add(new ApiMessageAndVersion(record, (short) 0)));\n            manager.replay(acl.toRecord());\n        }\n\n        // Verify that the ACLs stored in the AclControlManager match the ones we expect.\n        Set<ApiMessageAndVersion> foundAcls = new HashSet<>();\n        for (Map.Entry<Uuid, StandardAcl> entry : manager.idToAcl().entrySet()) {\n            foundAcls.add(new ApiMessageAndVersion(\n                    new StandardAclWithId(entry.getKey(), entry.getValue()).toRecord(), (short) 0));\n        }\n        assertEquals(loadedAcls, foundAcls);\n\n        // Once we complete the snapshot load, the ACLs should be reflected in the authorizer.\n        MockClusterMetadataAuthorizer authorizer = new MockClusterMetadataAuthorizer();\n        authorizer.loadSnapshot(manager.idToAcl());\n        assertEquals(new HashSet<>(StandardAclTest.TEST_ACLS), new HashSet<>(authorizer.acls.values()));\n\n        // Test reverting to an empty state and then completing the snapshot load without\n        // setting an authorizer. This simulates the case where the user didn't configure\n        // a cluster metadata authorizer.\n        snapshotRegistry.revertToSnapshot(0);\n        authorizer.loadSnapshot(manager.idToAcl());\n        assertTrue(manager.idToAcl().isEmpty());\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testAddAndDelete() {\n        AclControlManager manager = new AclControlManager.Builder().build();\n        MockClusterMetadataAuthorizer authorizer = new MockClusterMetadataAuthorizer();\n        authorizer.loadSnapshot(manager.idToAcl());\n        manager.replay(StandardAclWithIdTest.TEST_ACLS.get(0).toRecord());\n        manager.replay(new RemoveAccessControlEntryRecord().\n            setId(TEST_ACLS.get(0).id()));\n        assertTrue(manager.idToAcl().isEmpty());\n    }\n\n    @Test\n    public void testCreateAclDeleteAcl() {\n        AclControlManager manager = new AclControlManager.Builder().build();\n        MockClusterMetadataAuthorizer authorizer = new MockClusterMetadataAuthorizer();\n        authorizer.loadSnapshot(manager.idToAcl());\n\n        List<AclBinding> toCreate = new ArrayList<>();\n        for (int i = 0; i < 3; i++) {\n            toCreate.add(TEST_ACLS.get(i).toBinding());\n        }\n        toCreate.add(new AclBinding(\n            new ResourcePattern(TOPIC, \"*\", PatternType.UNKNOWN),\n            new AccessControlEntry(\"User:*\", \"*\", ALTER, ALLOW)));\n\n        ControllerResult<List<AclCreateResult>> createResult = manager.createAcls(toCreate);\n\n        List<AclCreateResult> expectedResults = new ArrayList<>();\n        for (int i = 0; i < 3; i++) {\n            expectedResults.add(AclCreateResult.SUCCESS);\n        }\n        expectedResults.add(new AclCreateResult(\n            new InvalidRequestException(\"Invalid patternType UNKNOWN\")));\n\n        for (int i = 0; i < expectedResults.size(); i++) {\n            AclCreateResult expectedResult = expectedResults.get(i);\n            if (expectedResult.exception().isPresent()) {\n                assertEquals(expectedResult.exception().get().getMessage(),\n                    createResult.response().get(i).exception().get().getMessage());\n            } else {\n                assertFalse(createResult.response().get(i).exception().isPresent());\n            }\n        }\n        RecordTestUtils.replayAll(manager, createResult.records());\n        assertFalse(manager.idToAcl().isEmpty());\n\n        ControllerResult<List<AclDeleteResult>> deleteResult =\n            manager.deleteAcls(Arrays.asList(\n                new AclBindingFilter(\n                    new ResourcePatternFilter(ResourceType.ANY, null, LITERAL),\n                        AccessControlEntryFilter.ANY),\n                new AclBindingFilter(\n                    new ResourcePatternFilter(ResourceType.UNKNOWN, null, LITERAL),\n                        AccessControlEntryFilter.ANY)));\n        assertEquals(2, deleteResult.response().size());\n        Set<AclBinding> deleted = new HashSet<>();\n        for (AclDeleteResult.AclBindingDeleteResult result :\n                deleteResult.response().get(0).aclBindingDeleteResults()) {\n            assertEquals(Optional.empty(), result.exception());\n            deleted.add(result.aclBinding());\n        }\n        assertEquals(new HashSet<>(Arrays.asList(\n            TEST_ACLS.get(0).toBinding(),\n                TEST_ACLS.get(2).toBinding())), deleted);\n        assertEquals(InvalidRequestException.class,\n            deleteResult.response().get(1).exception().get().getClass());\n        RecordTestUtils.replayAll(manager, deleteResult.records());\n\n        Iterator<Map.Entry<Uuid, StandardAcl>> iterator = manager.idToAcl().entrySet().iterator();\n        assertEquals(TEST_ACLS.get(1).acl(), iterator.next().getValue());\n        assertFalse(iterator.hasNext());\n    }\n\n    @Test\n    public void testDeleteDedupe() {\n        AclControlManager manager = new AclControlManager.Builder().build();\n        MockClusterMetadataAuthorizer authorizer = new MockClusterMetadataAuthorizer();\n        authorizer.loadSnapshot(manager.idToAcl());\n\n        AclBinding aclBinding = new AclBinding(new ResourcePattern(TOPIC, \"topic-1\", LITERAL),\n                new AccessControlEntry(\"User:user\", \"10.0.0.1\", AclOperation.ALL, ALLOW));\n\n        ControllerResult<List<AclCreateResult>> createResult = manager.createAcls(Collections.singletonList(aclBinding));\n        Uuid id = ((AccessControlEntryRecord) createResult.records().get(0).message()).id();\n        assertEquals(1, createResult.records().size());\n\n        ControllerResult<List<AclDeleteResult>> deleteAclResultsAnyFilter = manager.deleteAcls(Collections.singletonList(AclBindingFilter.ANY));\n        assertEquals(1, deleteAclResultsAnyFilter.records().size());\n        assertEquals(id, ((RemoveAccessControlEntryRecord) deleteAclResultsAnyFilter.records().get(0).message()).id());\n        assertEquals(1, deleteAclResultsAnyFilter.response().size());\n\n        ControllerResult<List<AclDeleteResult>> deleteAclResultsSpecificFilter = manager.deleteAcls(Collections.singletonList(aclBinding.toFilter()));\n        assertEquals(1, deleteAclResultsSpecificFilter.records().size());\n        assertEquals(id, ((RemoveAccessControlEntryRecord) deleteAclResultsSpecificFilter.records().get(0).message()).id());\n        assertEquals(1, deleteAclResultsSpecificFilter.response().size());\n\n        ControllerResult<List<AclDeleteResult>> deleteAclResultsBothFilters = manager.deleteAcls(Arrays.asList(AclBindingFilter.ANY, aclBinding.toFilter()));\n        assertEquals(1, deleteAclResultsBothFilters.records().size());\n        assertEquals(id, ((RemoveAccessControlEntryRecord) deleteAclResultsBothFilters.records().get(0).message()).id());\n        assertEquals(2, deleteAclResultsBothFilters.response().size());\n    }\n}",
                "methodCount": 19
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 239,
                        "lineEnd": 241,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) is interacting exclusively with the SnapshotRegistry class by calling its getOrCreateSnapshot method. This method appears to logically belong with the SnapshotRegistry because it operates on the SnapshotRegistry object directly, which indicates that the method fits well with the responsibilities of the SnapshotRegistry class. Moving it to this class will encapsulate functionality related to creating snapshots within the SnapshotRegistry, following the principle of cohesive class design.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "validateNewAcl",
                            "method_signature": "public static void validateNewAcl(AclBinding aclBinding)",
                            "target_class": "AclBinding",
                            "rationale": "The validateNewAcl method primarily operates on the AclBinding object and doesn't require any state from the AclControlManager class. It's better aligned with the AclBinding class where the entities it validates belong."
                        },
                        {
                            "method_name": "validateFilter",
                            "method_signature": "public static void validateFilter(AclBindingFilter aclBindingFilter)",
                            "target_class": "AclBindingFilter",
                            "rationale": "The validateFilter method works with the AclBindingFilter object and doesn't need access to the state of the AclControlManager. This makes it more suitable for inclusion in the AclBindingFilter class."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The idempotentCreateSnapshot method is focused on creating snapshots in the SnapshotRegistry and does not depend on the state or behavior of the AclControlManager test class. Therefore, it should be moved to the SnapshotRegistry class."
                        }
                    ],
                    "llm_response_time": 4621
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "testValidateNewAcl",
                            "method_signature": "public void testValidateNewAcl()",
                            "target_class": "AclControlManagerTestSuite",
                            "rationale": "The method testValidateNewAcl is specifically testing the validateNewAcl method of AclControlManager. It is better grouped with other test methods related to AclControlManager in a test suite class dedicated to this purpose."
                        },
                        {
                            "method_name": "testValidateFilter",
                            "method_signature": "public void testValidateFilter()",
                            "target_class": "AclControlManagerTestSuite",
                            "rationale": "The method testValidateFilter is specifically testing the validateFilter method of AclControlManager. It should be grouped with other related tests in a dedicated test suite class."
                        },
                        {
                            "method_name": "testLoadSnapshot",
                            "method_signature": "public void testLoadSnapshot()",
                            "target_class": "AclControlManagerTestSuite",
                            "rationale": "The method testLoadSnapshot is specifically testing the loadSnapshot functionality of AclControlManager and should be part of a test suite focused on testing AclControlManager."
                        },
                        {
                            "method_name": "testAddAndDelete",
                            "method_signature": "public void testAddAndDelete()",
                            "target_class": "AclControlManagerTestSuite",
                            "rationale": "The method testAddAndDelete is directly testing the add and delete functionality of AclControlManager and should be part of a dedicated test suite class focused on AclControlManager."
                        },
                        {
                            "method_name": "testCreateAclDeleteAcl",
                            "method_signature": "public void testCreateAclDeleteAcl()",
                            "target_class": "AclControlManagerTestSuite",
                            "rationale": "The method testCreateAclDeleteAcl is specifically testing the creation and deletion of ACLs within AclControlManager. It would be better placed in a test suite dedicated to these operations."
                        },
                        {
                            "method_name": "testDeleteDedupe",
                            "method_signature": "public void testDeleteDedupe()",
                            "target_class": "AclControlManagerTestSuite",
                            "rationale": "The method testDeleteDedupe tests the deduplication behavior of deletion operations within AclControlManager. It should be part of a dedicated test suite for AclControlManager."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotManager",
                            "rationale": "The method idempotentCreateSnapshot handles snapshot creation, which is more appropriate for a class managing snapshots. This would improve cohesion by placing snapshot-related functionalities together."
                        }
                    ],
                    "llm_response_time": 5934
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The idempotentCreateSnapshot method is specifically handling snapshot creation, which is directly related to the SnapshotRegistry class. Moving it there would improve cohesion and make it more reusable for other classes that might use SnapshotRegistry."
                        }
                    ],
                    "llm_response_time": 3344
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.31128640318234513
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3954550031395101
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [],
                    "llm_response_time": 1662
                },
                "tf-idf-5": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "voyage": {
                    "priority_method_names": [],
                    "llm_response_time": 1
                },
                "voyage-5": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "voyage-3": {
                    "priority_method_names": [],
                    "llm_response_time": 1
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 1,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testUpdateFeatures() : void in class org.apache.kafka.controller.FeatureControlManagerTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 95,
                    "endLine": 125,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testUpdateFeatures() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 103,
                    "endLine": 103,
                    "startColumn": 9,
                    "endColumn": 50,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 94,
                    "endLine": 124,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testUpdateFeatures() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 102,
                    "endLine": 102,
                    "startColumn": 9,
                    "endColumn": 54,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(-1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 627,
        "extraction_results": {
            "success": true,
            "newCommitHash": "37798d9b1c134ab3b36c3c31004ba933de6687bd",
            "newBranchName": "extract-idempotentCreateSnapshot-testUpdateFeatures-130af38"
        },
        "telemetry": {
            "id": "3fa69ecb-92a7-4d29-88cf-15163544831e",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 405,
                "lineStart": 54,
                "lineEnd": 458,
                "bodyLineStart": 54,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                "sourceCode": "@Timeout(value = 40)\npublic class FeatureControlManagerTest {\n\n    @SuppressWarnings(\"unchecked\")\n    private static Map<String, VersionRange> rangeMap(Object... args) {\n        Map<String, VersionRange> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 3) {\n            String feature = (String) args[i];\n            Number low = (Number) args[i + 1];\n            Number high = (Number) args[i + 2];\n            result.put(feature, VersionRange.of(low.shortValue(), high.shortValue()));\n        }\n        return result;\n    }\n\n    private static Map<String, Short> versionMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    public static QuorumFeatures features(Object... args) {\n        Map<String, VersionRange> features = QuorumFeatures.defaultFeatureMap(true);\n        features.putAll(rangeMap(args));\n        return new QuorumFeatures(0, features, emptyList());\n    }\n\n    private static Map<String, Short> updateMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    @Test\n    public void testUpdateFeatures() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(\"foo\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(new FinalizedControllerFeatures(Collections.singletonMap(\"metadata.version\", (short) 4), -1),\n            manager.finalizedFeatures(-1));\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 3 for feature foo. Local controller 0 only supports versions 1-2\"))),\n            manager.updateFeatures(updateMap(\"foo\", 3),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false));\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n                updateMap(\"foo\", 2, \"bar\", 1), Collections.emptyMap(),\n                false);\n        Map<String, ApiError> expectedMap = new HashMap<>();\n        expectedMap.put(\"foo\", ApiError.NONE);\n        expectedMap.put(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 1 for feature bar. Local controller 0 does not support this feature.\"));\n        assertEquals(expectedMap, result.response());\n        List<ApiMessageAndVersion> expectedMessages = new ArrayList<>();\n        expectedMessages.add(new ApiMessageAndVersion(new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2),\n            (short) 0));\n        assertEquals(expectedMessages, result.records());\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(-1);\n    }\n\n    @Test\n    public void testReplay() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureLevelRecord record = new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2);\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setLogContext(logContext).\n                setQuorumFeatures(features(\"foo\", 1, 2)).\n                setSnapshotRegistry(snapshotRegistry).\n                setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n                build();\n        manager.replay(record);\n        snapshotRegistry.getOrCreateSnapshot(123);\n        assertEquals(new FinalizedControllerFeatures(versionMap(\"metadata.version\", 4, \"foo\", 2), 123),\n            manager.finalizedFeatures(123));\n    }\n\n    static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(\n        List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges,\n        List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges\n    ) {\n        return new ClusterFeatureSupportDescriber() {\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> brokerSupported() {\n                return brokerRanges.iterator();\n            }\n\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> controllerSupported() {\n                return controllerRanges.iterator();\n            }\n        };\n    }\n\n    @Test\n    public void testUpdateFeaturesErrorCases() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 0, 3)).\n            setSnapshotRegistry(snapshotRegistry).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(5, singletonMap(\"bar\", VersionRange.of(0, 3)))),\n                emptyList())).\n            build();\n\n        assertEquals(ControllerResult.atomicOf(emptyList(),\n            Collections.singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature foo. Broker 5 does not support this feature.\"))),\n                    manager.updateFeatures(updateMap(\"foo\", 3),\n                        Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        false));\n\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            updateMap(\"bar\", 3), Collections.emptyMap(), false);\n        assertEquals(Collections.singletonMap(\"bar\", ApiError.NONE), result.response());\n        manager.replay((FeatureLevelRecord) result.records().get(0).message());\n        snapshotRegistry.getOrCreateSnapshot(3);\n\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 2 for feature bar. Can't downgrade the version of this feature \" +\n                    \"without setting the upgrade type to either safe or unsafe downgrade.\"))),\n            manager.updateFeatures(updateMap(\"bar\", 2), Collections.emptyMap(), false));\n\n        assertEquals(\n            ControllerResult.atomicOf(\n                Collections.singletonList(\n                    new ApiMessageAndVersion(\n                        new FeatureLevelRecord()\n                            .setName(\"bar\")\n                            .setFeatureLevel((short) 2),\n                        (short) 0\n                    )\n                ),\n                Collections.singletonMap(\"bar\", ApiError.NONE)\n            ),\n            manager.updateFeatures(\n                updateMap(\"bar\", 2),\n                Collections.singletonMap(\"bar\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false)\n        );\n    }\n\n    @Test\n    public void testReplayRecords() throws Exception {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        ControllerResult<Map<String, ApiError>> result = manager.\n            updateFeatures(updateMap(\"foo\", 5, \"bar\", 1), Collections.emptyMap(), false);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_3_IV0, manager.metadataVersion());\n        assertEquals(Optional.of((short) 5), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"bar\"));\n        assertEquals(new HashSet<>(Arrays.asList(\n            MetadataVersion.FEATURE_NAME, \"foo\", \"bar\")),\n                manager.finalizedFeatures(Long.MAX_VALUE).featureNames());\n    }\n\n    private static final FeatureControlManager.Builder TEST_MANAGER_BUILDER1 =\n        new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_3_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV2);\n\n    @Test\n    public void testApplyMetadataVersionChangeRecord() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        manager.replay(new FeatureLevelRecord().\n            setName(MetadataVersion.FEATURE_NAME).\n            setFeatureLevel(MetadataVersion.IBP_3_3_IV3.featureLevel()));\n        assertEquals(MetadataVersion.IBP_3_3_IV3, manager.metadataVersion());\n    }\n\n    @Test\n    public void testCannotDowngradeToVersionBeforeMinimumSupportedKraftVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature metadata.version. Local controller 0 only \" +\n                \"supports versions 4-7\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUnsafeDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUpgradeToLowerVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 4 for feature metadata.version. Can't downgrade the \" +\n                \"version of this feature without setting the upgrade type to either safe or \" +\n                \"unsafe downgrade.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCanUpgradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUseSafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Refusing to perform the requested downgrade because \" +\n                \"it might delete metadata information.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testUnsafeDowngradeIsTemporarilyDisabled() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                                \"Invalid metadata.version 4. Unsafe metadata downgrade is not supported in this version.\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Disabled\n    @Test\n    public void testCanUseUnsafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCanUseSafeDowngradeIfMetadataDidNotChange() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                        MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV1.featureLevel())).\n                setMetadataVersion(MetadataVersion.IBP_3_1_IV0).\n                setMinimumBootstrapVersion(MetadataVersion.IBP_3_0_IV0).\n                build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_0_IV1.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCannotDowngradeBefore3_3_IV0() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                        \"Invalid metadata.version 3. Unable to set a metadata.version less than 3.3-IV0\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCreateFeatureLevelRecords() {\n        Map<String, VersionRange> localSupportedFeatures = new HashMap<>();\n        localSupportedFeatures.put(MetadataVersion.FEATURE_NAME, VersionRange.of(\n            MetadataVersion.IBP_3_0_IV1.featureLevel(), MetadataVersion.latestTesting().featureLevel()));\n        localSupportedFeatures.put(\"foo\", VersionRange.of(0, 2));\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(new QuorumFeatures(0, localSupportedFeatures, emptyList())).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(1, singletonMap(\"foo\", VersionRange.of(0, 3)))),\n                emptyList())).\n                build();\n        ControllerResult<Map<String, ApiError>> result  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 1),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UPGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 1), (short) 0)),\n                        Collections.singletonMap(\"foo\", ApiError.NONE)), result);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n\n        ControllerResult<Map<String, ApiError>> result2  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 0),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                        new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 0), (short) 0)),\n                Collections.singletonMap(\"foo\", ApiError.NONE)), result2);\n        RecordTestUtils.replayAll(manager, result2.records());\n        assertEquals(Optional.empty(), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n    }\n\n    @Test\n    public void testNoMetadataVersionChangeDuringMigration() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                    MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_5_IV1.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_4_IV0).\n            build();\n        BootstrapMetadata bootstrapMetadata = BootstrapMetadata.fromVersion(MetadataVersion.IBP_3_4_IV0, \"FeatureControlManagerTest\");\n        RecordTestUtils.replayAll(manager, bootstrapMetadata.records());\n        RecordTestUtils.replayOne(manager, ZkMigrationState.PRE_MIGRATION.toRecord());\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 10. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n\n        // Complete the migration\n        RecordTestUtils.replayOne(manager, ZkMigrationState.POST_MIGRATION.toRecord());\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n            singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n            false);\n        assertEquals(Errors.NONE, result.response().get(MetadataVersion.FEATURE_NAME).error());\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_5_IV1, manager.metadataVersion());\n    }\n}",
                "methodCount": 25
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 126,
                        "lineEnd": 128,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() interacts directly with the SnapshotRegistry class by calling its getOrCreateSnapshot() method. Keeping this method within SnapshotRegistry keeps the code cohesive and logically grouped, making it straightforward to understand and maintain.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "private static Map<String, VersionRange> rangeMap(Object... args)",
                            "target_class": "VersionRange",
                            "rationale": "The rangeMap method is primarily concerned with creating a map of VersionRange objects. It should be moved to the VersionRange class to better encapsulate the behavior and reduce coupling."
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static Map<String, Short> versionMap(Object... args)",
                            "target_class": "VersionRange",
                            "rationale": "The versionMap method is responsible for creating a map where the values are derived from version information, closely related to VersionRange. Moving it to the VersionRange class would make the code more cohesive."
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static Map<String, Short> updateMap(Object... args)",
                            "target_class": "VersionRange",
                            "rationale": "The updateMap method also deals with version information and creating a map of version-based values, which is conceptually related to VersionRange. It should be moved to the VersionRange class for better encapsulation."
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static QuorumFeatures features(Object... args)",
                            "target_class": "QuorumFeatures",
                            "rationale": "The features method constructs and returns an instance of QuorumFeatures, making it more appropriate for the logic to reside within the QuorumFeatures class."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The idempotentCreateSnapshot method is tightly coupled with SnapshotRegistry. Moving it to the SnapshotRegistry class would improve encapsulation and readability."
                        },
                        {
                            "method_name": "createFakeClusterFeatureSupportDescriber",
                            "method_signature": "static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(List<Entry<Integer, Map<String, VersionRange>>> brokerRanges, List<Entry<Integer, Map<String, VersionRange>>> controllerRanges)",
                            "target_class": "ClusterFeatureSupportDescriber",
                            "rationale": "createFakeClusterFeatureSupportDescriber creates instances of ClusterFeatureSupportDescriber. It should be a part of the ClusterFeatureSupportDescriber class to improve the logical grouping and usability."
                        }
                    ],
                    "llm_response_time": 8574
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "private static Map<String, VersionRange> rangeMap(Object... args)",
                            "target_class": "Utility",
                            "rationale": "The rangeMap method is a utility method that creates and returns a map. It does not access any instance variables of the FeatureControlManagerTest class or depend on its state. It can be moved to a utility class to promote reuse."
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static Map<String, Short> versionMap(Object... args)",
                            "target_class": "Utility",
                            "rationale": "The versionMap method is a utility method that creates and returns a map. Similar to rangeMap, it does not access any instance variables of the FeatureControlManagerTest class or depend on its state. It can be moved to a utility class for better reuse and separation of concerns."
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static Map<String, Short> updateMap(Object... args)",
                            "target_class": "Utility",
                            "rationale": "The updateMap method is a utility method that creates and returns a map. It does not access any instance variables of the FeatureControlManagerTest class or depend on its state. Moving this method to a utility class will improve reusability and maintainability."
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static QuorumFeatures features(Object... args)",
                            "target_class": "FeatureControlManagerBuilderHelper",
                            "rationale": "The features method facilitates the creation and initialization of QuorumFeatures. It doesn\u2019t depend on the state of FeatureControlManagerTest class. Moving it to a helper class related to building FeatureControlManager objects would provide better organization and clarity."
                        },
                        {
                            "method_name": "createFakeClusterFeatureSupportDescriber",
                            "method_signature": "static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges, List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges)",
                            "target_class": "FeatureControlManagerBuilderHelper",
                            "rationale": "The createFakeClusterFeatureSupportDescriber method creates a ClusterFeatureSupportDescriber instance. This method is not specifically related to FeatureControlManagerTest class itself. Moving it to a helper class concerned with building or configuring FeatureControlManager objects would improve code organization."
                        }
                    ],
                    "llm_response_time": 7019
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "private static Map<String, VersionRange> rangeMap(Object... args)",
                            "target_class": "VersionRangeUtil",
                            "rationale": "The method `rangeMap` is a utility function that manipulates VersionRange objects and doesn't rely on any instance variables of `FeatureControlManagerTest`. It is more appropriate to place it in a utility class like `VersionRangeUtil`."
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static Map<String, Short> versionMap(Object... args)",
                            "target_class": "VersionRangeUtil",
                            "rationale": "The method `versionMap` is a utility function that converts arguments into a map of short values and doesn't depend on the instance state of `FeatureControlManagerTest`. Such methods are better placed in a utility class like `VersionRangeUtil`."
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static QuorumFeatures features(Object... args)",
                            "target_class": "FeatureSetupUtil",
                            "rationale": "The method `features` sets up QuorumFeatures and does not use any of the instance variables of `FeatureControlManagerTest`. It should be moved to a class like `FeatureSetupUtil` which is responsible for setting up and manipulating feature-related configurations."
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static Map<String, Short> updateMap(Object... args)",
                            "target_class": "VersionRangeUtil",
                            "rationale": "The method `updateMap` is a utility that manipulates version mapping and does not rely on the internal state of `FeatureControlManagerTest`. It is better suited in a class like `VersionRangeUtil`."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotUtil",
                            "rationale": "The method `idempotentCreateSnapshot` creates or reuses snapshots and is relevant to snapshot management, which can be better managed in a dedicated snapshot utility class like `SnapshotUtil`."
                        },
                        {
                            "method_name": "createFakeClusterFeatureSupportDescriber",
                            "method_signature": "static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges, List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges)",
                            "target_class": "ClusterFeatureSupportDescriberUtil",
                            "rationale": "The method `createFakeClusterFeatureSupportDescriber` is a factory method for creating mock describers and does not depend on the state of `FeatureControlManagerTest`. It should be moved to a dedicated utility class like `ClusterFeatureSupportDescriberUtil`."
                        }
                    ],
                    "llm_response_time": 7255
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.25734306657093914
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.3709228592777801
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [],
                    "llm_response_time": 2212
                },
                "tf-idf-5": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "voyage": {
                    "priority_method_names": [],
                    "llm_response_time": 1
                },
                "voyage-5": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "voyage-3": {
                    "priority_method_names": [],
                    "llm_response_time": 1
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        },
                        {
                            "class_name": "Builder",
                            "similarity_score": 0.3710212605396185
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry",
                        "Builder"
                    ],
                    "llm_response_time": 2365,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testReplay() : void in class org.apache.kafka.controller.FeatureControlManagerTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 127,
                    "endLine": 145,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testReplay() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 134,
                    "endLine": 134,
                    "startColumn": 9,
                    "endColumn": 50,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 126,
                    "endLine": 144,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testReplay() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 133,
                    "endLine": 133,
                    "startColumn": 9,
                    "endColumn": 54,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(-1)"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 141,
                    "endLine": 141,
                    "startColumn": 9,
                    "endColumn": 55,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(123)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 628,
        "extraction_results": {
            "success": true,
            "newCommitHash": "8f45b80fb602932dd6473cbc2a55946fd956e4b5",
            "newBranchName": "extract-idempotentCreateSnapshot-testReplay-130af38"
        },
        "telemetry": {
            "id": "8e65df46-68d8-43c3-93f5-f57505466b27",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 405,
                "lineStart": 54,
                "lineEnd": 458,
                "bodyLineStart": 54,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                "sourceCode": "@Timeout(value = 40)\npublic class FeatureControlManagerTest {\n\n    @SuppressWarnings(\"unchecked\")\n    private static Map<String, VersionRange> rangeMap(Object... args) {\n        Map<String, VersionRange> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 3) {\n            String feature = (String) args[i];\n            Number low = (Number) args[i + 1];\n            Number high = (Number) args[i + 2];\n            result.put(feature, VersionRange.of(low.shortValue(), high.shortValue()));\n        }\n        return result;\n    }\n\n    private static Map<String, Short> versionMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    public static QuorumFeatures features(Object... args) {\n        Map<String, VersionRange> features = QuorumFeatures.defaultFeatureMap(true);\n        features.putAll(rangeMap(args));\n        return new QuorumFeatures(0, features, emptyList());\n    }\n\n    private static Map<String, Short> updateMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    @Test\n    public void testUpdateFeatures() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(\"foo\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(new FinalizedControllerFeatures(Collections.singletonMap(\"metadata.version\", (short) 4), -1),\n            manager.finalizedFeatures(-1));\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 3 for feature foo. Local controller 0 only supports versions 1-2\"))),\n            manager.updateFeatures(updateMap(\"foo\", 3),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false));\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n                updateMap(\"foo\", 2, \"bar\", 1), Collections.emptyMap(),\n                false);\n        Map<String, ApiError> expectedMap = new HashMap<>();\n        expectedMap.put(\"foo\", ApiError.NONE);\n        expectedMap.put(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 1 for feature bar. Local controller 0 does not support this feature.\"));\n        assertEquals(expectedMap, result.response());\n        List<ApiMessageAndVersion> expectedMessages = new ArrayList<>();\n        expectedMessages.add(new ApiMessageAndVersion(new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2),\n            (short) 0));\n        assertEquals(expectedMessages, result.records());\n    }\n\n    @Test\n    public void testReplay() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureLevelRecord record = new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2);\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setLogContext(logContext).\n                setQuorumFeatures(features(\"foo\", 1, 2)).\n                setSnapshotRegistry(snapshotRegistry).\n                setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n                build();\n        manager.replay(record);\n        snapshotRegistry.getOrCreateSnapshot(123);\n        assertEquals(new FinalizedControllerFeatures(versionMap(\"metadata.version\", 4, \"foo\", 2), 123),\n            manager.finalizedFeatures(123));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(-1);\n    }\n\n    static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(\n        List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges,\n        List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges\n    ) {\n        return new ClusterFeatureSupportDescriber() {\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> brokerSupported() {\n                return brokerRanges.iterator();\n            }\n\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> controllerSupported() {\n                return controllerRanges.iterator();\n            }\n        };\n    }\n\n    @Test\n    public void testUpdateFeaturesErrorCases() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 0, 3)).\n            setSnapshotRegistry(snapshotRegistry).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(5, singletonMap(\"bar\", VersionRange.of(0, 3)))),\n                emptyList())).\n            build();\n\n        assertEquals(ControllerResult.atomicOf(emptyList(),\n            Collections.singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature foo. Broker 5 does not support this feature.\"))),\n                    manager.updateFeatures(updateMap(\"foo\", 3),\n                        Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        false));\n\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            updateMap(\"bar\", 3), Collections.emptyMap(), false);\n        assertEquals(Collections.singletonMap(\"bar\", ApiError.NONE), result.response());\n        manager.replay((FeatureLevelRecord) result.records().get(0).message());\n        snapshotRegistry.getOrCreateSnapshot(3);\n\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 2 for feature bar. Can't downgrade the version of this feature \" +\n                    \"without setting the upgrade type to either safe or unsafe downgrade.\"))),\n            manager.updateFeatures(updateMap(\"bar\", 2), Collections.emptyMap(), false));\n\n        assertEquals(\n            ControllerResult.atomicOf(\n                Collections.singletonList(\n                    new ApiMessageAndVersion(\n                        new FeatureLevelRecord()\n                            .setName(\"bar\")\n                            .setFeatureLevel((short) 2),\n                        (short) 0\n                    )\n                ),\n                Collections.singletonMap(\"bar\", ApiError.NONE)\n            ),\n            manager.updateFeatures(\n                updateMap(\"bar\", 2),\n                Collections.singletonMap(\"bar\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false)\n        );\n    }\n\n    @Test\n    public void testReplayRecords() throws Exception {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        ControllerResult<Map<String, ApiError>> result = manager.\n            updateFeatures(updateMap(\"foo\", 5, \"bar\", 1), Collections.emptyMap(), false);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_3_IV0, manager.metadataVersion());\n        assertEquals(Optional.of((short) 5), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"bar\"));\n        assertEquals(new HashSet<>(Arrays.asList(\n            MetadataVersion.FEATURE_NAME, \"foo\", \"bar\")),\n                manager.finalizedFeatures(Long.MAX_VALUE).featureNames());\n    }\n\n    private static final FeatureControlManager.Builder TEST_MANAGER_BUILDER1 =\n        new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_3_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV2);\n\n    @Test\n    public void testApplyMetadataVersionChangeRecord() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        manager.replay(new FeatureLevelRecord().\n            setName(MetadataVersion.FEATURE_NAME).\n            setFeatureLevel(MetadataVersion.IBP_3_3_IV3.featureLevel()));\n        assertEquals(MetadataVersion.IBP_3_3_IV3, manager.metadataVersion());\n    }\n\n    @Test\n    public void testCannotDowngradeToVersionBeforeMinimumSupportedKraftVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature metadata.version. Local controller 0 only \" +\n                \"supports versions 4-7\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUnsafeDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUpgradeToLowerVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 4 for feature metadata.version. Can't downgrade the \" +\n                \"version of this feature without setting the upgrade type to either safe or \" +\n                \"unsafe downgrade.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCanUpgradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUseSafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Refusing to perform the requested downgrade because \" +\n                \"it might delete metadata information.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testUnsafeDowngradeIsTemporarilyDisabled() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                                \"Invalid metadata.version 4. Unsafe metadata downgrade is not supported in this version.\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Disabled\n    @Test\n    public void testCanUseUnsafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCanUseSafeDowngradeIfMetadataDidNotChange() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                        MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV1.featureLevel())).\n                setMetadataVersion(MetadataVersion.IBP_3_1_IV0).\n                setMinimumBootstrapVersion(MetadataVersion.IBP_3_0_IV0).\n                build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_0_IV1.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCannotDowngradeBefore3_3_IV0() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                        \"Invalid metadata.version 3. Unable to set a metadata.version less than 3.3-IV0\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCreateFeatureLevelRecords() {\n        Map<String, VersionRange> localSupportedFeatures = new HashMap<>();\n        localSupportedFeatures.put(MetadataVersion.FEATURE_NAME, VersionRange.of(\n            MetadataVersion.IBP_3_0_IV1.featureLevel(), MetadataVersion.latestTesting().featureLevel()));\n        localSupportedFeatures.put(\"foo\", VersionRange.of(0, 2));\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(new QuorumFeatures(0, localSupportedFeatures, emptyList())).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(1, singletonMap(\"foo\", VersionRange.of(0, 3)))),\n                emptyList())).\n                build();\n        ControllerResult<Map<String, ApiError>> result  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 1),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UPGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 1), (short) 0)),\n                        Collections.singletonMap(\"foo\", ApiError.NONE)), result);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n\n        ControllerResult<Map<String, ApiError>> result2  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 0),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                        new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 0), (short) 0)),\n                Collections.singletonMap(\"foo\", ApiError.NONE)), result2);\n        RecordTestUtils.replayAll(manager, result2.records());\n        assertEquals(Optional.empty(), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n    }\n\n    @Test\n    public void testNoMetadataVersionChangeDuringMigration() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                    MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_5_IV1.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_4_IV0).\n            build();\n        BootstrapMetadata bootstrapMetadata = BootstrapMetadata.fromVersion(MetadataVersion.IBP_3_4_IV0, \"FeatureControlManagerTest\");\n        RecordTestUtils.replayAll(manager, bootstrapMetadata.records());\n        RecordTestUtils.replayOne(manager, ZkMigrationState.PRE_MIGRATION.toRecord());\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 10. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n\n        // Complete the migration\n        RecordTestUtils.replayOne(manager, ZkMigrationState.POST_MIGRATION.toRecord());\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n            singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n            false);\n        assertEquals(Errors.NONE, result.response().get(MetadataVersion.FEATURE_NAME).error());\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_5_IV1, manager.metadataVersion());\n    }\n}",
                "methodCount": 25
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 146,
                        "lineEnd": 148,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() interacts directly with the SnapshotRegistry class by calling its getOrCreateSnapshot() method. Keeping this method within SnapshotRegistry keeps the code cohesive and logically grouped, making it straightforward to understand and maintain.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "private static Map<String, VersionRange> rangeMap(Object... args)",
                            "target_class": "VersionRange",
                            "rationale": "The rangeMap method creates a map of VersionRanges which are relevant to the VersionRange class. This makes it clearer and more cohesive from a design perspective."
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static Map<String, Short> versionMap(Object... args)",
                            "target_class": "VersionRange",
                            "rationale": "The versionMap method deals with mapping version ranges, which makes it appropriate to be relocated to the VersionRange class to keep related functionalities together."
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static QuorumFeatures features(Object... args)",
                            "target_class": "QuorumFeatures",
                            "rationale": "The features method constructs a QuorumFeatures object and populates it with default features, making it more aligned with the responsibilities of the QuorumFeatures class."
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static Map<String, Short> updateMap(Object... args)",
                            "target_class": "VersionRange",
                            "rationale": "The updateMap method builds a map of version ranges, a functionality that falls under the domain of the VersionRange class."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistry",
                            "rationale": "The idempotentCreateSnapshot method interacts directly with SnapshotRegistry objects. Moving it to the SnapshotRegistry class improves cohesion and keeps related functionalities together."
                        },
                        {
                            "method_name": "createFakeClusterFeatureSupportDescriber",
                            "method_signature": "static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges, List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges)",
                            "target_class": "ClusterFeatureSupportDescriber",
                            "rationale": "The createFakeClusterFeatureSupportDescriber method returns an instance of ClusterFeatureSupportDescriber. It is more appropriate to have factory methods or static creators within the class they instantiate."
                        }
                    ],
                    "llm_response_time": 6674
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "private static Map<String, VersionRange> rangeMap(Object... args)",
                            "target_class": "VersionRangeMapper",
                            "rationale": "The rangeMap method is a utility method for creating a map of VersionRanges, which is not specific to the FeatureControlManagerTest class. Moving it to a dedicated utility or mapper class following the Single Responsibility Principle (SRP) would make the code more modular and reusable."
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static Map<String, Short> versionMap(Object... args)",
                            "target_class": "VersionRangeMapper",
                            "rationale": "Similar to rangeMap, the versionMap method is also a utility method which creates a version map from given arguments. It should be moved to a utility class such as VersionRangeMapper to keep the FeatureControlManagerTest class focused on test logic."
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static Map<String, Short> updateMap(Object... args)",
                            "target_class": "VersionRangeMapper",
                            "rationale": "The updateMap method performs mapping tasks that are unrelated to the testing logic of FeatureControlManagerTest. Moving it to a utility class like VersionRangeMapper will make the code cleaner and promote better separation of concerns."
                        },
                        {
                            "method_name": "createFakeClusterFeatureSupportDescriber",
                            "method_signature": "static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges, List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges)",
                            "target_class": "ClusterFeatureSupportDescriberFactory",
                            "rationale": "The createFakeClusterFeatureSupportDescriber method constructs a fake ClusterFeatureSupportDescriber. This is more of a factory function and should be moved to a factory class like ClusterFeatureSupportDescriberFactory to keep the test class focused on testing logic."
                        }
                    ],
                    "llm_response_time": 5561
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "private static Map<String, VersionRange> rangeMap(Object... args)",
                            "target_class": "QuorumFeatures",
                            "rationale": "The rangeMap method deals with converting arbitrary arguments into a map that is associated with feature ranges. This logic is more relevant to the QuorumFeatures class, which handles feature-related operations."
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static Map<String, Short> versionMap(Object... args)",
                            "target_class": "QuorumFeatures",
                            "rationale": "The versionMap method functions similarly to rangeMap and deals with creating a map of feature versions. This makes it more pertinent to the QuorumFeatures class."
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static QuorumFeatures features(Object... args)",
                            "target_class": "QuorumFeatures",
                            "rationale": "The features method is directly related to the creation and manipulation of QuorumFeatures. Housing this method within the QuorumFeatures class aligns with its responsibility and ensures better encapsulation."
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static Map<String, Short> updateMap(Object... args)",
                            "target_class": "FeatureControlManager",
                            "rationale": "The updateMap method creates a map relevant to feature updates, which is highly relevant to the operations handled by the FeatureControlManager class."
                        },
                        {
                            "method_name": "createFakeClusterFeatureSupportDescriber",
                            "method_signature": "static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges, List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges)",
                            "target_class": "ClusterFeatureSupportDescriber",
                            "rationale": "This method generates a mock instance of ClusterFeatureSupportDescriber, making it more logical to house it within the same class or a related utility class."
                        }
                    ],
                    "llm_response_time": 5099
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.25734306657093914
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.37054625644332484
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1627
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "voyage": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 1
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)"
                    ],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        },
                        {
                            "class_name": "Builder",
                            "similarity_score": 0.3710212605396185
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry",
                        "Builder"
                    ],
                    "llm_response_time": 0,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testUpdateFeaturesErrorCases() : void in class org.apache.kafka.controller.FeatureControlManagerTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 164,
                    "endLine": 213,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testUpdateFeaturesErrorCases() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 188,
                    "endLine": 188,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 163,
                    "endLine": 212,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testUpdateFeaturesErrorCases() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 187,
                    "endLine": 187,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(3)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 629,
        "extraction_results": {
            "success": true,
            "newCommitHash": "bed236d73c831ae8aa19c7bdaed32680482e4f73",
            "newBranchName": "extract-idempotentCreateSnapshot-testUpdateFeaturesErrorCases-130af38"
        },
        "telemetry": {
            "id": "b9ab15d3-3e41-4e2b-bcde-ae08f22362ce",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 405,
                "lineStart": 54,
                "lineEnd": 458,
                "bodyLineStart": 54,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                "sourceCode": "@Timeout(value = 40)\npublic class FeatureControlManagerTest {\n\n    @SuppressWarnings(\"unchecked\")\n    private static Map<String, VersionRange> rangeMap(Object... args) {\n        Map<String, VersionRange> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 3) {\n            String feature = (String) args[i];\n            Number low = (Number) args[i + 1];\n            Number high = (Number) args[i + 2];\n            result.put(feature, VersionRange.of(low.shortValue(), high.shortValue()));\n        }\n        return result;\n    }\n\n    private static Map<String, Short> versionMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    public static QuorumFeatures features(Object... args) {\n        Map<String, VersionRange> features = QuorumFeatures.defaultFeatureMap(true);\n        features.putAll(rangeMap(args));\n        return new QuorumFeatures(0, features, emptyList());\n    }\n\n    private static Map<String, Short> updateMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    @Test\n    public void testUpdateFeatures() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(\"foo\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        snapshotRegistry.getOrCreateSnapshot(-1);\n        assertEquals(new FinalizedControllerFeatures(Collections.singletonMap(\"metadata.version\", (short) 4), -1),\n            manager.finalizedFeatures(-1));\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 3 for feature foo. Local controller 0 only supports versions 1-2\"))),\n            manager.updateFeatures(updateMap(\"foo\", 3),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false));\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n                updateMap(\"foo\", 2, \"bar\", 1), Collections.emptyMap(),\n                false);\n        Map<String, ApiError> expectedMap = new HashMap<>();\n        expectedMap.put(\"foo\", ApiError.NONE);\n        expectedMap.put(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 1 for feature bar. Local controller 0 does not support this feature.\"));\n        assertEquals(expectedMap, result.response());\n        List<ApiMessageAndVersion> expectedMessages = new ArrayList<>();\n        expectedMessages.add(new ApiMessageAndVersion(new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2),\n            (short) 0));\n        assertEquals(expectedMessages, result.records());\n    }\n\n    @Test\n    public void testReplay() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureLevelRecord record = new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2);\n\n        snapshotRegistry.getOrCreateSnapshot(-1);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setLogContext(logContext).\n                setQuorumFeatures(features(\"foo\", 1, 2)).\n                setSnapshotRegistry(snapshotRegistry).\n                setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n                build();\n        manager.replay(record);\n        snapshotRegistry.getOrCreateSnapshot(123);\n        assertEquals(new FinalizedControllerFeatures(versionMap(\"metadata.version\", 4, \"foo\", 2), 123),\n            manager.finalizedFeatures(123));\n    }\n\n    static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(\n        List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges,\n        List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges\n    ) {\n        return new ClusterFeatureSupportDescriber() {\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> brokerSupported() {\n                return brokerRanges.iterator();\n            }\n\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> controllerSupported() {\n                return controllerRanges.iterator();\n            }\n        };\n    }\n\n    @Test\n    public void testUpdateFeaturesErrorCases() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 0, 3)).\n            setSnapshotRegistry(snapshotRegistry).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(5, singletonMap(\"bar\", VersionRange.of(0, 3)))),\n                emptyList())).\n            build();\n\n        assertEquals(ControllerResult.atomicOf(emptyList(),\n            Collections.singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature foo. Broker 5 does not support this feature.\"))),\n                    manager.updateFeatures(updateMap(\"foo\", 3),\n                        Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        false));\n\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            updateMap(\"bar\", 3), Collections.emptyMap(), false);\n        assertEquals(Collections.singletonMap(\"bar\", ApiError.NONE), result.response());\n        manager.replay((FeatureLevelRecord) result.records().get(0).message());\n        idempotentCreateSnapshot(snapshotRegistry);\n\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 2 for feature bar. Can't downgrade the version of this feature \" +\n                    \"without setting the upgrade type to either safe or unsafe downgrade.\"))),\n            manager.updateFeatures(updateMap(\"bar\", 2), Collections.emptyMap(), false));\n\n        assertEquals(\n            ControllerResult.atomicOf(\n                Collections.singletonList(\n                    new ApiMessageAndVersion(\n                        new FeatureLevelRecord()\n                            .setName(\"bar\")\n                            .setFeatureLevel((short) 2),\n                        (short) 0\n                    )\n                ),\n                Collections.singletonMap(\"bar\", ApiError.NONE)\n            ),\n            manager.updateFeatures(\n                updateMap(\"bar\", 2),\n                Collections.singletonMap(\"bar\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false)\n        );\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(3);\n    }\n\n    @Test\n    public void testReplayRecords() throws Exception {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        ControllerResult<Map<String, ApiError>> result = manager.\n            updateFeatures(updateMap(\"foo\", 5, \"bar\", 1), Collections.emptyMap(), false);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_3_IV0, manager.metadataVersion());\n        assertEquals(Optional.of((short) 5), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"bar\"));\n        assertEquals(new HashSet<>(Arrays.asList(\n            MetadataVersion.FEATURE_NAME, \"foo\", \"bar\")),\n                manager.finalizedFeatures(Long.MAX_VALUE).featureNames());\n    }\n\n    private static final FeatureControlManager.Builder TEST_MANAGER_BUILDER1 =\n        new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_3_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV2);\n\n    @Test\n    public void testApplyMetadataVersionChangeRecord() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        manager.replay(new FeatureLevelRecord().\n            setName(MetadataVersion.FEATURE_NAME).\n            setFeatureLevel(MetadataVersion.IBP_3_3_IV3.featureLevel()));\n        assertEquals(MetadataVersion.IBP_3_3_IV3, manager.metadataVersion());\n    }\n\n    @Test\n    public void testCannotDowngradeToVersionBeforeMinimumSupportedKraftVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature metadata.version. Local controller 0 only \" +\n                \"supports versions 4-7\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUnsafeDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUpgradeToLowerVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 4 for feature metadata.version. Can't downgrade the \" +\n                \"version of this feature without setting the upgrade type to either safe or \" +\n                \"unsafe downgrade.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCanUpgradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUseSafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Refusing to perform the requested downgrade because \" +\n                \"it might delete metadata information.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testUnsafeDowngradeIsTemporarilyDisabled() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                                \"Invalid metadata.version 4. Unsafe metadata downgrade is not supported in this version.\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Disabled\n    @Test\n    public void testCanUseUnsafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCanUseSafeDowngradeIfMetadataDidNotChange() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                        MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV1.featureLevel())).\n                setMetadataVersion(MetadataVersion.IBP_3_1_IV0).\n                setMinimumBootstrapVersion(MetadataVersion.IBP_3_0_IV0).\n                build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_0_IV1.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCannotDowngradeBefore3_3_IV0() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                        \"Invalid metadata.version 3. Unable to set a metadata.version less than 3.3-IV0\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCreateFeatureLevelRecords() {\n        Map<String, VersionRange> localSupportedFeatures = new HashMap<>();\n        localSupportedFeatures.put(MetadataVersion.FEATURE_NAME, VersionRange.of(\n            MetadataVersion.IBP_3_0_IV1.featureLevel(), MetadataVersion.latestTesting().featureLevel()));\n        localSupportedFeatures.put(\"foo\", VersionRange.of(0, 2));\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(new QuorumFeatures(0, localSupportedFeatures, emptyList())).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(1, singletonMap(\"foo\", VersionRange.of(0, 3)))),\n                emptyList())).\n                build();\n        ControllerResult<Map<String, ApiError>> result  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 1),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UPGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 1), (short) 0)),\n                        Collections.singletonMap(\"foo\", ApiError.NONE)), result);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n\n        ControllerResult<Map<String, ApiError>> result2  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 0),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                        new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 0), (short) 0)),\n                Collections.singletonMap(\"foo\", ApiError.NONE)), result2);\n        RecordTestUtils.replayAll(manager, result2.records());\n        assertEquals(Optional.empty(), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n    }\n\n    @Test\n    public void testNoMetadataVersionChangeDuringMigration() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                    MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_5_IV1.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_4_IV0).\n            build();\n        BootstrapMetadata bootstrapMetadata = BootstrapMetadata.fromVersion(MetadataVersion.IBP_3_4_IV0, \"FeatureControlManagerTest\");\n        RecordTestUtils.replayAll(manager, bootstrapMetadata.records());\n        RecordTestUtils.replayOne(manager, ZkMigrationState.PRE_MIGRATION.toRecord());\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 10. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n\n        // Complete the migration\n        RecordTestUtils.replayOne(manager, ZkMigrationState.POST_MIGRATION.toRecord());\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n            singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n            false);\n        assertEquals(Errors.NONE, result.response().get(MetadataVersion.FEATURE_NAME).error());\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_5_IV1, manager.metadataVersion());\n    }\n}",
                "methodCount": 25
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 214,
                        "lineEnd": 216,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method idempotentCreateSnapshot to class SnapshotRegistry",
                        "description": "Move method idempotentCreateSnapshot to org.apache.kafka.timeline.SnapshotRegistry\nRationale: The method idempotentCreateSnapshot() interacts directly with the SnapshotRegistry class by calling its getOrCreateSnapshot method. Since this functionality is specific to snapshots and their management, consolidating it within the SnapshotRegistry class will enhance encapsulation and maintain cohesion of snapshot-related operations.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "private static Map<String, VersionRange> rangeMap(Object... args)",
                            "target_class": "VersionRangeUtil",
                            "rationale": "The method rangeMap deals with converting a variadic list of parameters into a map of version ranges, which is a utility operation and does not directly depend on any instance-specific data in FeatureControlManagerTest. Thus, it belongs to a utility class dedicated to version range operations."
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static Map<String, Short> versionMap(Object... args)",
                            "target_class": "VersionRangeUtil",
                            "rationale": "The versionMap method transforms a variadic list of parameters into a map of version numbers, which is a utility function not specific to the FeatureControlManagerTest class. It should be placed in a utility class that handles version-related operations, similar to rangeMap."
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static Map<String, Short> updateMap(Object... args)",
                            "target_class": "VersionRangeUtil",
                            "rationale": "The method updateMap converts a variadic list of parameters into a map of updated versions, which is a general utility operation. Moving it to a utility class like VersionRangeUtil would enhance cohesion and reduce coupling within the FeatureControlManagerTest class."
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static QuorumFeatures features(Object... args)",
                            "target_class": "FeatureBuilder",
                            "rationale": "The features method constructs QuorumFeatures, which is more relevant to the domain of feature configuration or setup logic. Moving this to a FeatureBuilder class would make the FeatureControlManagerTest class more focused on testing and less on set-up logic."
                        },
                        {
                            "method_name": "createFakeClusterFeatureSupportDescriber",
                            "method_signature": "static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges, List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges)",
                            "target_class": "ClusterFeatureSupportDescriberFactory",
                            "rationale": "The method createFakeClusterFeatureSupportDescriber is a factory method used for creating a support describer for cluster features. It does not belong to the FeatureControlManagerTest class because it pertains to object instantiation rather than test logic. A dedicated factory class would be a more appropriate place for it."
                        }
                    ],
                    "llm_response_time": 5932
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "private static Map<String, VersionRange> rangeMap(Object... args)",
                            "target_class": "VersionUtils",
                            "rationale": "The rangeMap method is a utility method that constructs a Map from input arguments. It would be more appropriate in a utility class like VersionUtils to promote reusability and separation of concerns."
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static Map<String, Short> versionMap(Object... args)",
                            "target_class": "VersionUtils",
                            "rationale": "The versionMap method is a utility method that transforms input arguments into a map of versions. Similar to rangeMap, it should reside in a utility class like VersionUtils for better modularity."
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static Map<String, Short> updateMap(Object... args)",
                            "target_class": "VersionUtils",
                            "rationale": "The updateMap method constructs a map from input arguments, which is a utility operation. It would be more appropriate in a dedicated utility class like VersionUtils to enhance code clarity and reusability."
                        },
                        {
                            "method_name": "createFakeClusterFeatureSupportDescriber",
                            "method_signature": "static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges, List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges)",
                            "target_class": "ClusterFeatureSupportUtils",
                            "rationale": "The createFakeClusterFeatureSupportDescriber method creates a mock implementation of ClusterFeatureSupportDescriber. It would be better placed in a utility class like ClusterFeatureSupportUtils to keep test infrastructure separate from test cases."
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static QuorumFeatures features(Object... args)",
                            "target_class": "FeatureUtils",
                            "rationale": "The features method constructs a QuorumFeatures object from input arguments, which is a general utility operation. Moving it to a utility class like FeatureUtils will enhance the modularity and readability of the code."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotUtils",
                            "rationale": "The idempotentCreateSnapshot method manages SnapshotRegistry operations, which are utility operations. It would improve separation of concerns to place this method in a class dedicated to snapshot operations like SnapshotUtils."
                        }
                    ],
                    "llm_response_time": 6077
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "private static Map<String, VersionRange> rangeMap(Object... args)",
                            "target_class": "VersionRangeUtils",
                            "rationale": "The method `rangeMap` deals with transforming a list of arguments into `VersionRange` objects and doesn't directly relate to the functionality of `FeatureControlManagerTest`. It would be more appropriate in a utility class dedicated to handling version ranges."
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static Map<String, Short> versionMap(Object... args)",
                            "target_class": "VersionRangeUtils",
                            "rationale": "Similar to `rangeMap`, the method `versionMap` handles manipulation and mapping that are not specific to `FeatureControlManagerTest`. It belongs in a utility class that handles version range-related operations."
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static Map<String, Short> updateMap(Object... args)",
                            "target_class": "VersionRangeUtils",
                            "rationale": "The `updateMap` method is another utility for dealing with feature version mappings and does not directly contribute to the test logic within `FeatureControlManagerTest`. It should be moved to the `VersionRangeUtils` class."
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static QuorumFeatures features(Object... args)",
                            "target_class": "QuorumFeaturesUtils",
                            "rationale": "The method `features` constructs a `QuorumFeatures` object from arguments and is a utility method that doesn't belong in the test class. Moving it to a utility class like `QuorumFeaturesUtils` would isolate this functionality better."
                        },
                        {
                            "method_name": "createFakeClusterFeatureSupportDescriber",
                            "method_signature": "static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges, List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges)",
                            "target_class": "ClusterFeatureSupportUtils",
                            "rationale": "The method `createFakeClusterFeatureSupportDescriber` is used to create a test implementation for `ClusterFeatureSupportDescriber`. It is related to creating cluster feature support descriptions which could be part of a utility class like `ClusterFeatureSupportUtils`."
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "SnapshotRegistryUtils",
                            "rationale": "The method `idempotentCreateSnapshot` handles creating a snapshot and is not specific to the `FeatureControlManagerTest`. It would be better placed in a utility class focused on snapshot operations, like `SnapshotRegistryUtils`."
                        }
                    ],
                    "llm_response_time": 6191
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.25734306657093914
                    }
                },
                "voyage": {
                    "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                        "first": {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.38740872259346915
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [],
                    "llm_response_time": 3217
                },
                "tf-idf-5": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [],
                    "llm_response_time": 1
                },
                "voyage": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "voyage-5": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                },
                "voyage-3": {
                    "priority_method_names": [],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "idempotentCreateSnapshot": {
                    "target_classes": [
                        {
                            "class_name": "SnapshotRegistry",
                            "similarity_score": 0.28491649277950165
                        },
                        {
                            "class_name": "Builder",
                            "similarity_score": 0.3710212605396185
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "SnapshotRegistry",
                        "Builder"
                    ],
                    "llm_response_time": 2242,
                    "similarity_computation_time": 5,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "b4d5f163a9c5a480efe0a1de3bebf12e35be2a7f",
        "url": "https://github.com/apache/kafka/commit/b4d5f163a9c5a480efe0a1de3bebf12e35be2a7f",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public votedKey() : Optional<ReplicaKey> extracted from public testElectionTimeout() : void in class org.apache.kafka.raft.VotedStateTest & moved to class org.apache.kafka.raft.UnattachedState",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 57,
                    "endLine": 78,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testElectionTimeout() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 63,
                    "endLine": 63,
                    "startColumn": 9,
                    "endColumn": 50,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/UnattachedState.java",
                    "startLine": 99,
                    "endLine": 101,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public votedKey() : Optional<ReplicaKey>"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/UnattachedState.java",
                    "startLine": 100,
                    "endLine": 100,
                    "startColumn": 9,
                    "endColumn": 25,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/UnattachedStateWithVoteTest.java",
                    "startLine": 59,
                    "endLine": 80,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testElectionTimeout() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/UnattachedStateWithVoteTest.java",
                    "startLine": 65,
                    "endLine": 65,
                    "startColumn": 32,
                    "endColumn": 48,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "state.votedKey()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 630,
        "extraction_results": {
            "success": true,
            "newCommitHash": "0545041c57c66aa4d42a43367f40b0139da63df4",
            "newBranchName": "extract-votedKey-testElectionTimeout-e1b2ade"
        },
        "telemetry": {
            "id": "b5b45ad9-92dd-4c91-83ef-e849d58ea5bb",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 93,
                "lineStart": 35,
                "lineEnd": 127,
                "bodyLineStart": 35,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                "sourceCode": "class VotedStateTest {\n\n    private final MockTime time = new MockTime();\n    private final LogContext logContext = new LogContext();\n    private final int epoch = 5;\n    private final int votedId = 1;\n    private final int electionTimeoutMs = 10000;\n\n    private VotedState newVotedState(\n        Uuid votedDirectoryId\n    ) {\n        return new VotedState(\n            time,\n            epoch,\n            ReplicaKey.of(votedId, votedDirectoryId),\n            Collections.emptySet(),\n            Optional.empty(),\n            electionTimeoutMs,\n            logContext\n        );\n    }\n\n    @Test\n    public void testElectionTimeout() {\n        VotedState state = newVotedState(ReplicaKey.NO_DIRECTORY_ID);\n        ReplicaKey votedKey  = ReplicaKey.of(votedId, ReplicaKey.NO_DIRECTORY_ID);\n\n        assertEquals(epoch, state.epoch());\n        votedKey(state, votedKey);\n        assertEquals(\n            ElectionState.withVotedCandidate(epoch, votedKey, Collections.emptySet()),\n            state.election()\n        );\n        assertEquals(electionTimeoutMs, state.remainingElectionTimeMs(time.milliseconds()));\n        assertFalse(state.hasElectionTimeoutExpired(time.milliseconds()));\n\n        time.sleep(5000);\n        assertEquals(electionTimeoutMs - 5000, state.remainingElectionTimeMs(time.milliseconds()));\n        assertFalse(state.hasElectionTimeoutExpired(time.milliseconds()));\n\n        time.sleep(5000);\n        assertEquals(0, state.remainingElectionTimeMs(time.milliseconds()));\n        assertTrue(state.hasElectionTimeoutExpired(time.milliseconds()));\n    }\n\n    private void votedKey(VotedState state, ReplicaKey votedKey) {\n        assertEquals(votedKey, state.votedKey());\n    }\n\n    @ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public void testCanGrantVoteWithoutDirectoryId(boolean isLogUpToDate) {\n        VotedState state = newVotedState(ReplicaKey.NO_DIRECTORY_ID);\n\n        assertTrue(\n            state.canGrantVote(ReplicaKey.of(votedId, ReplicaKey.NO_DIRECTORY_ID), isLogUpToDate)\n        );\n        assertTrue(\n            state.canGrantVote(\n                ReplicaKey.of(votedId, Uuid.randomUuid()),\n                isLogUpToDate\n            )\n        );\n\n        assertFalse(\n            state.canGrantVote(ReplicaKey.of(votedId + 1, ReplicaKey.NO_DIRECTORY_ID), isLogUpToDate)\n        );\n    }\n\n    @Test\n    void testCanGrantVoteWithDirectoryId() {\n        Uuid votedDirectoryId = Uuid.randomUuid();\n        VotedState state = newVotedState(votedDirectoryId);\n\n        assertTrue(state.canGrantVote(ReplicaKey.of(votedId, votedDirectoryId), false));\n\n        assertFalse(\n            state.canGrantVote(ReplicaKey.of(votedId, Uuid.randomUuid()), false)\n        );\n        assertFalse(state.canGrantVote(ReplicaKey.of(votedId, ReplicaKey.NO_DIRECTORY_ID), false));\n\n        assertFalse(state.canGrantVote(ReplicaKey.of(votedId + 1, votedDirectoryId), false));\n        assertFalse(state.canGrantVote(ReplicaKey.of(votedId + 1, ReplicaKey.NO_DIRECTORY_ID), false));\n    }\n\n    @Test\n    void testLeaderEndpoints() {\n        Uuid votedDirectoryId = Uuid.randomUuid();\n        VotedState state = newVotedState(votedDirectoryId);\n\n        assertEquals(Endpoints.empty(), state.leaderEndpoints());\n    }\n}",
                "methodCount": 6
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 1,
                "candidates": [
                    {
                        "lineStart": 79,
                        "lineEnd": 81,
                        "refactoringType": "MyMoveInstanceMethodRefactoring",
                        "refactoringInfo": "Move method votedKey to class ReplicaKey",
                        "description": "Move method votedKey to org.apache.kafka.raft.internals.ReplicaKey\nRationale: The method votedKey relies heavily on the ReplicaKey class, suggesting a close dependency on its functionality. The method's purpose is to assert the equality of a votedKey with the key from a VotedState object, making it appear to be a part of validation or behavior checking related to ReplicaKey instances. Placing it in the ReplicaKey class maintains the cohesion of related functionality and keeps the method where it is most likely to be relevant and useful.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "startedRefactoringFlow": false,
                        "undone": false
                    }
                ]
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "newVotedState",
                            "method_signature": "private VotedState newVotedState(Uuid votedDirectoryId)",
                            "target_class": "VotedState",
                            "rationale": "The method newVotedState is responsible for creating instances of VotedState using all the necessary parameters. This method can be moved to the VotedState class to encapsulate this logic and provide a specialized factory method for creating VotedState objects."
                        },
                        {
                            "method_name": "votedKey",
                            "method_signature": "private void votedKey(VotedState state, ReplicaKey votedKey)",
                            "target_class": "VotedState",
                            "rationale": "The method votedKey validates the key of the VotedState object. This is more related to the state properties and behavior of the VotedState class. It can be moved there to centralize the validation logic."
                        }
                    ],
                    "llm_response_time": 3178
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "newVotedState",
                            "method_signature": "private VotedState newVotedState(Uuid votedDirectoryId)",
                            "target_class": "VotedStateFactory",
                            "rationale": "The newVotedState method is responsible for creating an instance of VotedState and is more appropriate for a factory class responsible for object creation. Moving it would help in adhering to the Single Responsibility Principle."
                        },
                        {
                            "method_name": "votedKey",
                            "method_signature": "private void votedKey(VotedState state, ReplicaKey votedKey)",
                            "target_class": "VotedState",
                            "rationale": "The votedKey method checks the votedKey of the state, which is inherently part of the VotedState's responsibilities. It makes more sense for this method to be within the VotedState class to encapsulate the behavior and data more cohesively."
                        }
                    ],
                    "llm_response_time": 2427
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "newVotedState",
                            "method_signature": "private VotedState newVotedState(Uuid votedDirectoryId)",
                            "target_class": "VotedState",
                            "rationale": "The method 'newVotedState' is creating an instance of VotedState with various parameters that are specific to VotedState. Having this logic within the VotedState class itself or in a factory class would improve encapsulation and make the VotedStateTest class less cluttered."
                        },
                        {
                            "method_name": "votedKey",
                            "method_signature": "private void votedKey(VotedState state, ReplicaKey votedKey)",
                            "target_class": "VotedState",
                            "rationale": "The method 'votedKey' is performing an operation that strictly pertains to the state of VotedState. It makes sense to move this method to the VotedState class for better cohesion and encapsulation."
                        }
                    ],
                    "llm_response_time": 2294
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "votedKey",
                            "method_signature": "private votedKey(VotedState state, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "votedKey",
                            "method_signature": "private votedKey(VotedState state, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "tf-idf": {
                    "private votedKey(VotedState state, ReplicaKey votedKey)": {
                        "first": {
                            "method_name": "votedKey",
                            "method_signature": "private votedKey(VotedState state, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.28817620634373814
                    }
                },
                "voyage": {
                    "private votedKey(VotedState state, ReplicaKey votedKey)": {
                        "first": {
                            "method_name": "votedKey",
                            "method_signature": "private votedKey(VotedState state, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        "second": 0.7425211199944738
                    }
                }
            },
            "llmMethodPriority": {
                "tf-idf": {
                    "priority_method_names": [
                        "private votedKey(VotedState state, ReplicaKey votedKey)"
                    ],
                    "llm_response_time": 1080
                },
                "tf-idf-5": {
                    "priority_method_names": [
                        "private votedKey(VotedState state, ReplicaKey votedKey)"
                    ],
                    "llm_response_time": 0
                },
                "tf-df-3": {
                    "priority_method_names": [
                        "private votedKey(VotedState state, ReplicaKey votedKey)"
                    ],
                    "llm_response_time": 0
                },
                "voyage": {
                    "priority_method_names": [
                        "private votedKey(VotedState state, ReplicaKey votedKey)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-5": {
                    "priority_method_names": [
                        "private votedKey(VotedState state, ReplicaKey votedKey)"
                    ],
                    "llm_response_time": 0
                },
                "voyage-3": {
                    "priority_method_names": [
                        "private votedKey(VotedState state, ReplicaKey votedKey)"
                    ],
                    "llm_response_time": 0
                }
            },
            "targetClassMap": {
                "votedKey": {
                    "target_classes": [
                        {
                            "class_name": "VotedState",
                            "similarity_score": 0.3334822304866716
                        },
                        {
                            "class_name": "ReplicaKey",
                            "similarity_score": 0.33634998607300864
                        },
                        {
                            "class_name": "MockTime",
                            "similarity_score": 0.429766743189736
                        },
                        {
                            "class_name": "LogContext",
                            "similarity_score": 0.3995056298320393
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "ReplicaKey",
                        "MockTime",
                        "LogContext"
                    ],
                    "llm_response_time": 4615,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                }
            }
        }
    }
]