setup: context limit of 128k tokens
model: gpt-4o
temp: 0.5
approach: give llm all the methods in the class and ask it to sort.