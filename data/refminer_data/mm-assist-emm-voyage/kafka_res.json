[
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "5bc3aa428067dff1f2b9075ff5d1351fb05d4b10",
        "url": "https://github.com/apache/kafka/commit/5bc3aa428067dff1f2b9075ff5d1351fb05d4b10",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public flush(accessor DBAccessor) : void extracted from public flush() : void in class org.apache.kafka.streams.state.internals.RocksDBStore & moved to class org.apache.kafka.streams.state.internals.RocksDBStore.SingleColumnFamilyAccessor",
            "leftSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
                    "startLine": 643,
                    "endLine": 653,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public flush() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
                    "startLine": 649,
                    "endLine": 649,
                    "startColumn": 13,
                    "endColumn": 32,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
                    "startLine": 961,
                    "endLine": 964,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public flush(accessor DBAccessor) : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
                    "startLine": 963,
                    "endLine": 963,
                    "startColumn": 13,
                    "endColumn": 42,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
                    "startLine": 645,
                    "endLine": 655,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public flush() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
                    "startLine": 651,
                    "endLine": 651,
                    "startColumn": 13,
                    "endColumn": 41,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "cfAccessor.flush(dbAccessor)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 566,
        "extraction_results": {
            "success": true,
            "newCommitHash": "5d9207c097766bead70cfb2f4160323d9a750cfc",
            "newBranchName": "extract-flush-flush-c078e51"
        },
        "telemetry": {
            "id": "b434d28d-e429-4515-b669-5ea49025d887",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 856,
                "lineStart": 89,
                "lineEnd": 944,
                "bodyLineStart": 89,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
                "sourceCode": "/**\n * A persistent key-value store based on RocksDB.\n */\npublic class RocksDBStore implements KeyValueStore<Bytes, byte[]>, BatchWritingStore {\n    private static final Logger log = LoggerFactory.getLogger(RocksDBStore.class);\n\n    private static final CompressionType COMPRESSION_TYPE = CompressionType.NO_COMPRESSION;\n    private static final CompactionStyle COMPACTION_STYLE = CompactionStyle.UNIVERSAL;\n    private static final long WRITE_BUFFER_SIZE = 16 * 1024 * 1024L;\n    private static final long BLOCK_CACHE_SIZE = 50 * 1024 * 1024L;\n    private static final long BLOCK_SIZE = 4096L;\n    private static final int MAX_WRITE_BUFFERS = 3;\n    static final String DB_FILE_DIR = \"rocksdb\";\n\n    final String name;\n    private final String parentDir;\n    final Set<KeyValueIterator<Bytes, byte[]>> openIterators = Collections.synchronizedSet(new HashSet<>());\n    private boolean consistencyEnabled = false;\n\n    // VisibleForTesting\n    protected File dbDir;\n    RocksDB db;\n    RocksDBAccessor dbAccessor;\n\n    // the following option objects will be created in openDB and closed in the close() method\n    private RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter userSpecifiedOptions;\n    WriteOptions wOptions;\n    FlushOptions fOptions;\n    private Cache cache;\n    private BloomFilter filter;\n    private Statistics statistics;\n\n    private RocksDBConfigSetter configSetter;\n    private boolean userSpecifiedStatistics = false;\n\n    private final RocksDBMetricsRecorder metricsRecorder;\n    // if true, then open iterators (for range, prefix scan, and other operations) will be\n    // managed automatically (by this store instance). if false, then these iterators must be\n    // managed elsewhere (by the caller of those methods).\n    private final boolean autoManagedIterators;\n\n    protected volatile boolean open = false;\n    protected StateStoreContext context;\n    protected Position position;\n    private OffsetCheckpoint positionCheckpoint;\n\n    public RocksDBStore(final String name,\n                        final String metricsScope) {\n        this(name, DB_FILE_DIR, new RocksDBMetricsRecorder(metricsScope, name));\n    }\n\n    RocksDBStore(final String name,\n                 final String parentDir,\n                 final RocksDBMetricsRecorder metricsRecorder) {\n        this(name, parentDir, metricsRecorder, true);\n    }\n\n    RocksDBStore(final String name,\n                 final String parentDir,\n                 final RocksDBMetricsRecorder metricsRecorder,\n                 final boolean autoManagedIterators) {\n        this.name = name;\n        this.parentDir = parentDir;\n        this.metricsRecorder = metricsRecorder;\n        this.autoManagedIterators = autoManagedIterators;\n    }\n\n    @Deprecated\n    @Override\n    public void init(final ProcessorContext context,\n                     final StateStore root) {\n        if (context instanceof StateStoreContext) {\n            init((StateStoreContext) context, root);\n        } else {\n            throw new UnsupportedOperationException(\n                \"Use RocksDBStore#init(StateStoreContext, StateStore) instead.\"\n            );\n        }\n    }\n\n    @Override\n    public void init(final StateStoreContext context,\n                     final StateStore root) {\n        // open the DB dir\n        metricsRecorder.init(getMetricsImpl(context), context.taskId());\n        openDB(context.appConfigs(), context.stateDir());\n\n        final File positionCheckpointFile = new File(context.stateDir(), name() + \".position\");\n        this.positionCheckpoint = new OffsetCheckpoint(positionCheckpointFile);\n        this.position = StoreQueryUtils.readPositionFromCheckpoint(positionCheckpoint);\n\n        // value getter should always read directly from rocksDB\n        // since it is only for values that are already flushed\n        this.context = context;\n        context.register(\n            root,\n            (RecordBatchingStateRestoreCallback) this::restoreBatch,\n            () -> StoreQueryUtils.checkpointPosition(positionCheckpoint, position)\n        );\n        consistencyEnabled = StreamsConfig.InternalConfig.getBoolean(\n            context.appConfigs(),\n            IQ_CONSISTENCY_OFFSET_VECTOR_ENABLED,\n            false);\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    void openDB(final Map<String, Object> configs, final File stateDir) {\n        // initialize the default rocksdb options\n\n        final DBOptions dbOptions = new DBOptions();\n        final ColumnFamilyOptions columnFamilyOptions = new ColumnFamilyOptions();\n        userSpecifiedOptions = new RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter(dbOptions, columnFamilyOptions);\n\n        final BlockBasedTableConfigWithAccessibleCache tableConfig = new BlockBasedTableConfigWithAccessibleCache();\n        cache = new LRUCache(BLOCK_CACHE_SIZE);\n        tableConfig.setBlockCache(cache);\n        tableConfig.setBlockSize(BLOCK_SIZE);\n\n        filter = new BloomFilter();\n        tableConfig.setFilterPolicy(filter);\n\n        userSpecifiedOptions.optimizeFiltersForHits();\n        userSpecifiedOptions.setTableFormatConfig(tableConfig);\n        userSpecifiedOptions.setWriteBufferSize(WRITE_BUFFER_SIZE);\n        userSpecifiedOptions.setCompressionType(COMPRESSION_TYPE);\n        userSpecifiedOptions.setCompactionStyle(COMPACTION_STYLE);\n        userSpecifiedOptions.setMaxWriteBufferNumber(MAX_WRITE_BUFFERS);\n        userSpecifiedOptions.setCreateIfMissing(true);\n        userSpecifiedOptions.setErrorIfExists(false);\n        userSpecifiedOptions.setInfoLogLevel(InfoLogLevel.ERROR_LEVEL);\n        // this is the recommended way to increase parallelism in RocksDb\n        // note that the current implementation of setIncreaseParallelism affects the number\n        // of compaction threads but not flush threads (the latter remains one). Also,\n        // the parallelism value needs to be at least two because of the code in\n        // https://github.com/facebook/rocksdb/blob/62ad0a9b19f0be4cefa70b6b32876e764b7f3c11/util/options.cc#L580\n        // subtracts one from the value passed to determine the number of compaction threads\n        // (this could be a bug in the RocksDB code and their devs have been contacted).\n        userSpecifiedOptions.setIncreaseParallelism(Math.max(Runtime.getRuntime().availableProcessors(), 2));\n\n        wOptions = new WriteOptions();\n        wOptions.setDisableWAL(true);\n\n        fOptions = new FlushOptions();\n        fOptions.setWaitForFlush(true);\n\n        final Class<RocksDBConfigSetter> configSetterClass =\n                (Class<RocksDBConfigSetter>) configs.get(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG);\n\n        if (configSetterClass != null) {\n            configSetter = Utils.newInstance(configSetterClass);\n            configSetter.setConfig(name, userSpecifiedOptions, configs);\n        }\n\n        dbDir = new File(new File(stateDir, parentDir), name);\n        try {\n            Files.createDirectories(dbDir.getParentFile().toPath());\n            Files.createDirectories(dbDir.getAbsoluteFile().toPath());\n        } catch (final IOException fatal) {\n            throw new ProcessorStateException(fatal);\n        }\n\n        // Setup statistics before the database is opened, otherwise the statistics are not updated\n        // with the measurements from Rocks DB\n        setupStatistics(configs, dbOptions);\n        openRocksDB(dbOptions, columnFamilyOptions);\n        open = true;\n\n        addValueProvidersToMetricsRecorder();\n    }\n\n    private void setupStatistics(final Map<String, Object> configs, final DBOptions dbOptions) {\n        statistics = userSpecifiedOptions.statistics();\n        if (statistics == null) {\n            if (RecordingLevel.forName((String) configs.get(METRICS_RECORDING_LEVEL_CONFIG)) == RecordingLevel.DEBUG) {\n                statistics = new Statistics();\n                dbOptions.setStatistics(statistics);\n            }\n            userSpecifiedStatistics = false;\n        } else {\n            userSpecifiedStatistics = true;\n        }\n    }\n\n    private void addValueProvidersToMetricsRecorder() {\n        final TableFormatConfig tableFormatConfig = userSpecifiedOptions.tableFormatConfig();\n        if (tableFormatConfig instanceof BlockBasedTableConfigWithAccessibleCache) {\n            final Cache cache = ((BlockBasedTableConfigWithAccessibleCache) tableFormatConfig).blockCache();\n            metricsRecorder.addValueProviders(name, db, cache, userSpecifiedStatistics ? null : statistics);\n        } else if (tableFormatConfig instanceof BlockBasedTableConfig) {\n            throw new ProcessorStateException(\"The used block-based table format configuration does not expose the \" +\n                    \"block cache. Use the BlockBasedTableConfig instance provided by Options#tableFormatConfig() to configure \" +\n                    \"the block-based table format of RocksDB. Do not provide a new instance of BlockBasedTableConfig to \" +\n                    \"the RocksDB options.\");\n        } else {\n            metricsRecorder.addValueProviders(name, db, null, userSpecifiedStatistics ? null : statistics);\n        }\n    }\n\n    void openRocksDB(final DBOptions dbOptions,\n                     final ColumnFamilyOptions columnFamilyOptions) {\n        final List<ColumnFamilyHandle> columnFamilies = openRocksDB(\n                dbOptions,\n                new ColumnFamilyDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY, columnFamilyOptions)\n        );\n\n        dbAccessor = new SingleColumnFamilyAccessor(columnFamilies.get(0));\n    }\n\n    /**\n     * Open RocksDB while automatically creating any requested column families that don't yet exist.\n     */\n    protected List<ColumnFamilyHandle> openRocksDB(final DBOptions dbOptions,\n                                                   final ColumnFamilyDescriptor defaultColumnFamilyDescriptor,\n                                                   final ColumnFamilyDescriptor... columnFamilyDescriptors) {\n        final String absolutePath = dbDir.getAbsolutePath();\n        final List<ColumnFamilyDescriptor> extraDescriptors = Arrays.asList(columnFamilyDescriptors);\n        final List<ColumnFamilyDescriptor> allDescriptors = new ArrayList<>(1 + columnFamilyDescriptors.length);\n        allDescriptors.add(defaultColumnFamilyDescriptor);\n        allDescriptors.addAll(extraDescriptors);\n\n        try {\n            final Options options = new Options(dbOptions, defaultColumnFamilyDescriptor.getOptions());\n            final List<byte[]> allExisting = RocksDB.listColumnFamilies(options, absolutePath);\n\n            final List<ColumnFamilyDescriptor> existingDescriptors = new LinkedList<>();\n            existingDescriptors.add(defaultColumnFamilyDescriptor);\n            existingDescriptors.addAll(extraDescriptors.stream()\n                    .filter(descriptor -> allExisting.stream().anyMatch(existing -> Arrays.equals(existing, descriptor.getName())))\n                    .collect(Collectors.toList()));\n            final List<ColumnFamilyDescriptor> toCreate = extraDescriptors.stream()\n                    .filter(descriptor -> allExisting.stream().noneMatch(existing -> Arrays.equals(existing, descriptor.getName())))\n                    .collect(Collectors.toList());\n            final List<ColumnFamilyHandle> existingColumnFamilies = new ArrayList<>(existingDescriptors.size());\n            db = RocksDB.open(dbOptions, absolutePath, existingDescriptors, existingColumnFamilies);\n            final List<ColumnFamilyHandle> createdColumnFamilies = db.createColumnFamilies(toCreate);\n\n            return mergeColumnFamilyHandleLists(existingColumnFamilies, createdColumnFamilies, allDescriptors);\n\n        } catch (final RocksDBException e) {\n            throw new ProcessorStateException(\"Error opening store \" + name + \" at location \" + dbDir.toString(), e);\n        }\n    }\n\n    /**\n     * match up the existing and created ColumnFamilyHandles with the existing/created ColumnFamilyDescriptors\n     * so that the order of the resultant List matches the order of the allDescriptors argument\n     */\n    private List<ColumnFamilyHandle> mergeColumnFamilyHandleLists(final List<ColumnFamilyHandle> existingColumnFamilyHandles,\n                                                                  final List<ColumnFamilyHandle> createdColumnFamilyHandles,\n                                                                  final List<ColumnFamilyDescriptor> allDescriptors) throws RocksDBException {\n        final List<ColumnFamilyHandle> columnFamilies = new ArrayList<>(allDescriptors.size());\n        int existing = 0;\n        int created = 0;\n\n        while (existing + created < allDescriptors.size()) {\n            final ColumnFamilyHandle existingHandle = existing < existingColumnFamilyHandles.size() ? existingColumnFamilyHandles.get(existing) : null;\n            final ColumnFamilyHandle createdHandle = created < createdColumnFamilyHandles.size() ? createdColumnFamilyHandles.get(created) : null;\n            if (existingHandle != null && Arrays.equals(existingHandle.getDescriptor().getName(), allDescriptors.get(existing + created).getName())) {\n                columnFamilies.add(existingHandle);\n                existing++;\n            } else if (createdHandle != null && Arrays.equals(createdHandle.getDescriptor().getName(), allDescriptors.get(existing + created).getName())) {\n                columnFamilies.add(createdHandle);\n                created++;\n            } else {\n                throw new IllegalStateException(\"Unable to match up column family handles with descriptors.\");\n            }\n        }\n        return columnFamilies;\n    }\n\n    @Override\n    public String name() {\n        return name;\n    }\n\n    @Override\n    public boolean persistent() {\n        return true;\n    }\n\n    @Override\n    public boolean isOpen() {\n        return open;\n    }\n\n    private void validateStoreOpen() {\n        if (!open) {\n            throw new InvalidStateStoreException(\"Store \" + name + \" is currently closed\");\n        }\n    }\n\n    public Snapshot getSnapshot() {\n        return db.getSnapshot();\n    }\n\n    public void releaseSnapshot(final Snapshot snapshot) {\n        db.releaseSnapshot(snapshot);\n    }\n\n    @Override\n    public synchronized void put(final Bytes key,\n                                 final byte[] value) {\n        Objects.requireNonNull(key, \"key cannot be null\");\n        validateStoreOpen();\n        dbAccessor.put(key.get(), value);\n\n        StoreQueryUtils.updatePosition(position, context);\n    }\n\n    @Override\n    public synchronized byte[] putIfAbsent(final Bytes key,\n                                           final byte[] value) {\n        Objects.requireNonNull(key, \"key cannot be null\");\n        final byte[] originalValue = get(key);\n        if (originalValue == null) {\n            put(key, value);\n        }\n        return originalValue;\n    }\n\n    @Override\n    public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n        try (final WriteBatch batch = new WriteBatch()) {\n            dbAccessor.prepareBatch(entries, batch);\n            write(batch);\n            StoreQueryUtils.updatePosition(position, context);\n        } catch (final RocksDBException e) {\n            throw new ProcessorStateException(\"Error while batch writing to store \" + name, e);\n        }\n    }\n\n    @Override\n    public <R> QueryResult<R> query(\n        final Query<R> query,\n        final PositionBound positionBound,\n        final QueryConfig config) {\n\n        return StoreQueryUtils.handleBasicQueries(\n            query,\n            positionBound,\n            config,\n            this,\n            position,\n            context\n        );\n    }\n\n    @Override\n    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix,\n                                                                                    final PS prefixKeySerializer) {\n        if (!autoManagedIterators) {\n            throw new IllegalStateException(\"Must specify openIterators in call to prefixScan()\");\n        }\n        return doPrefixScan(prefix, prefixKeySerializer, openIterators);\n    }\n\n    <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix,\n                                                                             final PS prefixKeySerializer,\n                                                                             final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        if (autoManagedIterators) {\n            throw new IllegalStateException(\"Cannot specify openIterators when using auto-managed iterators\");\n        }\n        return doPrefixScan(prefix, prefixKeySerializer, openIterators);\n    }\n\n    <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> doPrefixScan(final P prefix,\n                                                                               final PS prefixKeySerializer,\n                                                                               final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        validateStoreOpen();\n        Objects.requireNonNull(prefix, \"prefix cannot be null\");\n        Objects.requireNonNull(prefixKeySerializer, \"prefixKeySerializer cannot be null\");\n        final Bytes prefixBytes = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n\n        final ManagedKeyValueIterator<Bytes, byte[]> rocksDbPrefixSeekIterator = dbAccessor.prefixScan(prefixBytes);\n        openIterators.add(rocksDbPrefixSeekIterator);\n        rocksDbPrefixSeekIterator.onClose(() -> openIterators.remove(rocksDbPrefixSeekIterator));\n\n        return rocksDbPrefixSeekIterator;\n    }\n\n    @Override\n    public synchronized byte[] get(final Bytes key) {\n        return get(key, Optional.empty());\n    }\n\n    public synchronized byte[] get(final Bytes key, final ReadOptions readOptions) {\n        return get(key, Optional.of(readOptions));\n    }\n\n    private synchronized byte[] get(final Bytes key, final Optional<ReadOptions> readOptions) {\n        validateStoreOpen();\n        try {\n            return readOptions.isPresent() ? dbAccessor.get(key.get(), readOptions.get()) : dbAccessor.get(key.get());\n        } catch (final RocksDBException e) {\n            // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.\n            throw new ProcessorStateException(\"Error while getting value for key from store \" + name, e);\n        }\n    }\n\n    @Override\n    public synchronized byte[] delete(final Bytes key) {\n        Objects.requireNonNull(key, \"key cannot be null\");\n        final byte[] oldValue;\n        try {\n            oldValue = dbAccessor.getOnly(key.get());\n        } catch (final RocksDBException e) {\n            // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.\n            throw new ProcessorStateException(\"Error while getting value for key from store \" + name, e);\n        }\n        put(key, null);\n        return oldValue;\n    }\n\n    void deleteRange(final Bytes keyFrom, final Bytes keyTo) {\n        Objects.requireNonNull(keyFrom, \"keyFrom cannot be null\");\n        Objects.requireNonNull(keyTo, \"keyTo cannot be null\");\n\n        validateStoreOpen();\n\n        // End of key is exclusive, so we increment it by 1 byte to make keyTo inclusive.\n        // RocksDB's deleteRange() does not support a null upper bound so in the event\n        // of overflow from increment(), the operation cannot be performed and an\n        // IndexOutOfBoundsException will be thrown.\n        dbAccessor.deleteRange(keyFrom.get(), Bytes.increment(keyTo).get());\n    }\n\n    @Override\n    public synchronized KeyValueIterator<Bytes, byte[]> range(final Bytes from,\n                                                              final Bytes to) {\n        if (!autoManagedIterators) {\n            throw new IllegalStateException(\"Must specify openIterators in call to range()\");\n        }\n        return range(from, to, true, openIterators);\n    }\n\n    synchronized KeyValueIterator<Bytes, byte[]> range(final Bytes from,\n                                                       final Bytes to,\n                                                       final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        if (autoManagedIterators) {\n            throw new IllegalStateException(\"Cannot specify openIterators when using auto-managed iterators\");\n        }\n        return range(from, to, true, openIterators);\n    }\n\n    @Override\n    public synchronized KeyValueIterator<Bytes, byte[]> reverseRange(final Bytes from,\n                                                                     final Bytes to) {\n        if (!autoManagedIterators) {\n            throw new IllegalStateException(\"Must specify openIterators in call to reverseRange()\");\n        }\n        return range(from, to, false, openIterators);\n    }\n\n    synchronized KeyValueIterator<Bytes, byte[]> reverseRange(final Bytes from,\n                                                              final Bytes to,\n                                                              final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        if (autoManagedIterators) {\n            throw new IllegalStateException(\"Cannot specify openIterators when using auto-managed iterators\");\n        }\n        return range(from, to, false, openIterators);\n    }\n\n    private KeyValueIterator<Bytes, byte[]> range(final Bytes from,\n                                                  final Bytes to,\n                                                  final boolean forward,\n                                                  final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        if (Objects.nonNull(from) && Objects.nonNull(to) && from.compareTo(to) > 0) {\n            log.warn(\"Returning empty iterator for fetch with invalid key range: from > to. \"\n                    + \"This may be due to range arguments set in the wrong order, \" +\n                    \"or serdes that don't preserve ordering when lexicographically comparing the serialized bytes. \" +\n                    \"Note that the built-in numerical serdes do not follow this for negative numbers\");\n            return KeyValueIterators.emptyIterator();\n        }\n\n        validateStoreOpen();\n\n        final ManagedKeyValueIterator<Bytes, byte[]> rocksDBRangeIterator = dbAccessor.range(from, to, forward);\n        openIterators.add(rocksDBRangeIterator);\n        rocksDBRangeIterator.onClose(() -> openIterators.remove(rocksDBRangeIterator));\n\n        return rocksDBRangeIterator;\n    }\n\n    @Override\n    public synchronized KeyValueIterator<Bytes, byte[]> all() {\n        if (!autoManagedIterators) {\n            throw new IllegalStateException(\"Must specify openIterators in call to all()\");\n        }\n        return all(true, openIterators);\n    }\n\n    synchronized KeyValueIterator<Bytes, byte[]> all(final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        if (autoManagedIterators) {\n            throw new IllegalStateException(\"Cannot specify openIterators when using auto-managed iterators\");\n        }\n        return all(true, openIterators);\n    }\n\n    @Override\n    public KeyValueIterator<Bytes, byte[]> reverseAll() {\n        if (!autoManagedIterators) {\n            throw new IllegalStateException(\"Must specify openIterators in call to reverseAll()\");\n        }\n        return all(false, openIterators);\n    }\n\n    KeyValueIterator<Bytes, byte[]> reverseAll(final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        if (autoManagedIterators) {\n            throw new IllegalStateException(\"Cannot specify openIterators when using auto-managed iterators\");\n        }\n        return all(false, openIterators);\n    }\n\n    private KeyValueIterator<Bytes, byte[]> all(final boolean forward,\n                                                final Set<KeyValueIterator<Bytes, byte[]>> openIterators) {\n        validateStoreOpen();\n        final ManagedKeyValueIterator<Bytes, byte[]> rocksDbIterator = dbAccessor.all(forward);\n        openIterators.add(rocksDbIterator);\n        rocksDbIterator.onClose(() -> openIterators.remove(rocksDbIterator));\n        return rocksDbIterator;\n    }\n\n    /**\n     * Return an approximate count of key-value mappings in this store.\n     *\n     * <code>RocksDB</code> cannot return an exact entry count without doing a\n     * full scan, so this method relies on the <code>rocksdb.estimate-num-keys</code>\n     * property to get an approximate count. The returned size also includes\n     * a count of dirty keys in the store's in-memory cache, which may lead to some\n     * double-counting of entries and inflate the estimate.\n     *\n     * @return an approximate count of key-value mappings in the store.\n     */\n    @Override\n    public long approximateNumEntries() {\n        validateStoreOpen();\n        final long numEntries;\n        try {\n            numEntries = dbAccessor.approximateNumEntries();\n        } catch (final RocksDBException e) {\n            throw new ProcessorStateException(\"Error fetching property from store \" + name, e);\n        }\n        if (isOverflowing(numEntries)) {\n            return Long.MAX_VALUE;\n        }\n        return numEntries;\n    }\n\n    private boolean isOverflowing(final long value) {\n        // RocksDB returns an unsigned 8-byte integer, which could overflow long\n        // and manifest as a negative value.\n        return value < 0;\n    }\n\n    @Override\n    public synchronized void flush() {\n        if (db == null) {\n            return;\n        }\n        try {\n            flush1();\n        } catch (final RocksDBException e) {\n            throw new ProcessorStateException(\"Error while executing flush from store \" + name, e);\n        }\n    }\n\n    private void flush1() throws RocksDBException {\n        dbAccessor.flush();\n    }\n\n    @Override\n    public void addToBatch(final KeyValue<byte[], byte[]> record,\n                           final WriteBatchInterface batch) throws RocksDBException {\n        dbAccessor.addToBatch(record.key, record.value, batch);\n    }\n\n    @Override\n    public void write(final WriteBatchInterface batch) throws RocksDBException {\n        if (batch instanceof WriteBatch) {\n            db.write(wOptions, (WriteBatch) batch);\n        } else if (batch instanceof WriteBatchWithIndex) {\n            db.write(wOptions, (WriteBatchWithIndex) batch);\n        } else {\n            log.error(\"Unknown type of batch {}. This is a bug in Kafka Streams. \" +\n                    \"Please file a bug report at https://issues.apache.org/jira/projects/KAFKA.\",\n                    batch.getClass().getCanonicalName());\n            throw new IllegalStateException(\"Unknown type of batch \" + batch.getClass().getCanonicalName());\n        }\n    }\n\n    @Override\n    public synchronized void close() {\n        if (!open) {\n            return;\n        }\n\n        open = false;\n        closeOpenIterators();\n\n        if (configSetter != null) {\n            configSetter.close(name, userSpecifiedOptions);\n            configSetter = null;\n        }\n\n        metricsRecorder.removeValueProviders(name);\n\n        // Important: do not rearrange the order in which the below objects are closed!\n        // Order of closing must follow: ColumnFamilyHandle > RocksDB > DBOptions > ColumnFamilyOptions\n        dbAccessor.close();\n        db.close();\n        userSpecifiedOptions.close();\n        wOptions.close();\n        fOptions.close();\n        filter.close();\n        cache.close();\n        if (statistics != null) {\n            statistics.close();\n        }\n\n        dbAccessor = null;\n        userSpecifiedOptions = null;\n        wOptions = null;\n        fOptions = null;\n        db = null;\n        filter = null;\n        cache = null;\n        statistics = null;\n    }\n\n    private void closeOpenIterators() {\n        final HashSet<KeyValueIterator<Bytes, byte[]>> iterators;\n        synchronized (openIterators) {\n            iterators = new HashSet<>(openIterators);\n        }\n        if (iterators.size() != 0) {\n            log.warn(\"Closing {} open iterators for store {}\", iterators.size(), name);\n            for (final KeyValueIterator<Bytes, byte[]> iterator : iterators) {\n                iterator.close();\n            }\n        }\n    }\n\n    interface RocksDBAccessor {\n\n        void put(final byte[] key,\n                 final byte[] value);\n\n        void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n                          final WriteBatchInterface batch) throws RocksDBException;\n\n        byte[] get(final byte[] key) throws RocksDBException;\n\n        byte[] get(final byte[] key, ReadOptions readOptions) throws RocksDBException;\n\n        /**\n         * In contrast to get(), we don't migrate the key to new CF.\n         * <p>\n         * Use for get() within delete() -- no need to migrate, as it's deleted anyway\n         */\n        byte[] getOnly(final byte[] key) throws RocksDBException;\n\n        ManagedKeyValueIterator<Bytes, byte[]> range(final Bytes from,\n                                              final Bytes to,\n                                              final boolean forward);\n\n        /**\n         * Deletes keys entries in the range ['from', 'to'], including 'from' and excluding 'to'.\n         */\n        void deleteRange(final byte[] from,\n                         final byte[] to);\n\n        ManagedKeyValueIterator<Bytes, byte[]> all(final boolean forward);\n\n        ManagedKeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix);\n\n        long approximateNumEntries() throws RocksDBException;\n\n        void flush() throws RocksDBException;\n\n        void addToBatch(final byte[] key,\n                        final byte[] value,\n                        final WriteBatchInterface batch) throws RocksDBException;\n\n        void close();\n    }\n\n    class SingleColumnFamilyAccessor implements RocksDBAccessor {\n        private final ColumnFamilyHandle columnFamily;\n\n        SingleColumnFamilyAccessor(final ColumnFamilyHandle columnFamily) {\n            this.columnFamily = columnFamily;\n        }\n\n        @Override\n        public void put(final byte[] key,\n                        final byte[] value) {\n            if (value == null) {\n                try {\n                    db.delete(columnFamily, wOptions, key);\n                } catch (final RocksDBException e) {\n                    // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.\n                    throw new ProcessorStateException(\"Error while removing key from store \" + name, e);\n                }\n            } else {\n                try {\n                    db.put(columnFamily, wOptions, key, value);\n                } catch (final RocksDBException e) {\n                    // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.\n                    throw new ProcessorStateException(\"Error while putting key/value into store \" + name, e);\n                }\n            }\n        }\n\n        @Override\n        public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n                                 final WriteBatchInterface batch) throws RocksDBException {\n            for (final KeyValue<Bytes, byte[]> entry : entries) {\n                Objects.requireNonNull(entry.key, \"key cannot be null\");\n                addToBatch(entry.key.get(), entry.value, batch);\n            }\n        }\n\n        @Override\n        public byte[] get(final byte[] key) throws RocksDBException {\n            return db.get(columnFamily, key);\n        }\n\n        @Override\n        public byte[] get(final byte[] key, final ReadOptions readOptions) throws RocksDBException {\n            return db.get(columnFamily, readOptions, key);\n        }\n\n        @Override\n        public byte[] getOnly(final byte[] key) throws RocksDBException {\n            return db.get(columnFamily, key);\n        }\n\n        @Override\n        public ManagedKeyValueIterator<Bytes, byte[]> range(final Bytes from,\n                                                     final Bytes to,\n                                                     final boolean forward) {\n            return new RocksDBRangeIterator(\n                    name,\n                    db.newIterator(columnFamily),\n                    from,\n                    to,\n                    forward,\n                    true\n            );\n        }\n\n        @Override\n        public void deleteRange(final byte[] from, final byte[] to) {\n            try {\n                db.deleteRange(columnFamily, wOptions, from, to);\n            } catch (final RocksDBException e) {\n                // String format is happening in wrapping stores. So formatted message is thrown from wrapping stores.\n                throw new ProcessorStateException(\"Error while removing key from store \" + name, e);\n            }\n        }\n\n        @Override\n        public ManagedKeyValueIterator<Bytes, byte[]> all(final boolean forward) {\n            final RocksIterator innerIterWithTimestamp = db.newIterator(columnFamily);\n            if (forward) {\n                innerIterWithTimestamp.seekToFirst();\n            } else {\n                innerIterWithTimestamp.seekToLast();\n            }\n            return new RocksDbIterator(name, innerIterWithTimestamp, forward);\n        }\n\n        @Override\n        public ManagedKeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n            final Bytes to = incrementWithoutOverflow(prefix);\n            return new RocksDBRangeIterator(\n                    name,\n                    db.newIterator(columnFamily),\n                    prefix,\n                    to,\n                    true,\n                    false\n            );\n        }\n\n        @Override\n        public long approximateNumEntries() throws RocksDBException {\n            return db.getLongProperty(columnFamily, \"rocksdb.estimate-num-keys\");\n        }\n\n        @Override\n        public void flush() throws RocksDBException {\n            db.flush(fOptions, columnFamily);\n        }\n\n        @Override\n        public void addToBatch(final byte[] key,\n                               final byte[] value,\n                               final WriteBatchInterface batch) throws RocksDBException {\n            if (value == null) {\n                batch.delete(columnFamily, key);\n            } else {\n                batch.put(columnFamily, key, value);\n            }\n        }\n\n        @Override\n        public void close() {\n            columnFamily.close();\n        }\n    }\n\n    void restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> records) {\n        try (final WriteBatch batch = new WriteBatch()) {\n            for (final ConsumerRecord<byte[], byte[]> record : records) {\n                ChangelogRecordDeserializationHelper.applyChecksAndUpdatePosition(\n                    record,\n                    consistencyEnabled,\n                    position\n                );\n                // If version headers are not present or version is V0\n                dbAccessor.addToBatch(record.key(), record.value(), batch);\n            }\n            write(batch);\n        } catch (final RocksDBException e) {\n            throw new ProcessorStateException(\"Error restoring batch to store \" + name, e);\n        }\n\n    }\n\n    // for testing\n    public Options getOptions() {\n        return userSpecifiedOptions;\n    }\n\n    @Override\n    public Position getPosition() {\n        return position;\n    }\n\n    /**\n     * Same as {@link Bytes#increment(Bytes)} but {@code null} is returned instead of throwing\n     * {@code IndexOutOfBoundsException} in the event of overflow.\n     *\n     * @param input bytes to increment\n     * @return A new copy of the incremented byte array, or {@code null} if incrementing would\n     *         result in overflow.\n     */\n    static Bytes incrementWithoutOverflow(final Bytes input) {\n        try {\n            return Bytes.increment(input);\n        } catch (final IndexOutOfBoundsException e) {\n            return null;\n        }\n    }\n}",
                "methodCount": 78
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "openDB",
                            "method_signature": "@SuppressWarnings(\"unchecked\") openDB(final Map<String, Object> configs, final File stateDir)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "setupStatistics",
                            "method_signature": "private setupStatistics(final Map<String, Object> configs, final DBOptions dbOptions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addValueProvidersToMetricsRecorder",
                            "method_signature": "private addValueProvidersToMetricsRecorder()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "openRocksDB",
                            "method_signature": " openRocksDB(final DBOptions dbOptions,\n                     final ColumnFamilyOptions columnFamilyOptions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "openRocksDB",
                            "method_signature": "protected openRocksDB(final DBOptions dbOptions,\n                                                   final ColumnFamilyDescriptor defaultColumnFamilyDescriptor,\n                                                   final ColumnFamilyDescriptor... columnFamilyDescriptors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mergeColumnFamilyHandleLists",
                            "method_signature": "private mergeColumnFamilyHandleLists(final List<ColumnFamilyHandle> existingColumnFamilyHandles,\n                                                                  final List<ColumnFamilyHandle> createdColumnFamilyHandles,\n                                                                  final List<ColumnFamilyDescriptor> allDescriptors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateStoreOpen",
                            "method_signature": "private validateStoreOpen()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "releaseSnapshot",
                            "method_signature": "public releaseSnapshot(final Snapshot snapshot)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "prefixScan",
                            "method_signature": " prefixScan(final P prefix,\n                                                                             final PS prefixKeySerializer,\n                                                                             final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doPrefixScan",
                            "method_signature": " doPrefixScan(final P prefix,\n                                                                               final PS prefixKeySerializer,\n                                                                               final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "get",
                            "method_signature": "public synchronized get(final Bytes key, final ReadOptions readOptions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "get",
                            "method_signature": "private synchronized get(final Bytes key, final Optional<ReadOptions> readOptions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteRange",
                            "method_signature": " deleteRange(final Bytes keyFrom, final Bytes keyTo)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "range",
                            "method_signature": "synchronized range(final Bytes from,\n                                                       final Bytes to,\n                                                       final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "reverseRange",
                            "method_signature": "synchronized reverseRange(final Bytes from,\n                                                              final Bytes to,\n                                                              final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "range",
                            "method_signature": "private range(final Bytes from,\n                                                  final Bytes to,\n                                                  final boolean forward,\n                                                  final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "all",
                            "method_signature": "synchronized all(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "reverseAll",
                            "method_signature": " reverseAll(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "all",
                            "method_signature": "private all(final boolean forward,\n                                                final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isOverflowing",
                            "method_signature": "private isOverflowing(final long value)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flush1",
                            "method_signature": "private flush1()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeOpenIterators",
                            "method_signature": "private closeOpenIterators()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "restoreBatch",
                            "method_signature": " restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "incrementWithoutOverflow",
                            "method_signature": "static incrementWithoutOverflow(final Bytes input)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "incrementWithoutOverflow",
                            "method_signature": "static incrementWithoutOverflow(final Bytes input)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "releaseSnapshot",
                            "method_signature": "public releaseSnapshot(final Snapshot snapshot)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isOverflowing",
                            "method_signature": "private isOverflowing(final long value)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mergeColumnFamilyHandleLists",
                            "method_signature": "private mergeColumnFamilyHandleLists(final List<ColumnFamilyHandle> existingColumnFamilyHandles,\n                                                                  final List<ColumnFamilyHandle> createdColumnFamilyHandles,\n                                                                  final List<ColumnFamilyDescriptor> allDescriptors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "setupStatistics",
                            "method_signature": "private setupStatistics(final Map<String, Object> configs, final DBOptions dbOptions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "get",
                            "method_signature": "public synchronized get(final Bytes key, final ReadOptions readOptions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateStoreOpen",
                            "method_signature": "private validateStoreOpen()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "prefixScan",
                            "method_signature": " prefixScan(final P prefix,\n                                                                             final PS prefixKeySerializer,\n                                                                             final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "reverseAll",
                            "method_signature": " reverseAll(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "reverseRange",
                            "method_signature": "synchronized reverseRange(final Bytes from,\n                                                              final Bytes to,\n                                                              final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addValueProvidersToMetricsRecorder",
                            "method_signature": "private addValueProvidersToMetricsRecorder()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "range",
                            "method_signature": "synchronized range(final Bytes from,\n                                                       final Bytes to,\n                                                       final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flush1",
                            "method_signature": "private flush1()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "restoreBatch",
                            "method_signature": " restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "all",
                            "method_signature": "synchronized all(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "static incrementWithoutOverflow(final Bytes input)": {
                    "first": {
                        "method_name": "incrementWithoutOverflow",
                        "method_signature": "static incrementWithoutOverflow(final Bytes input)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.20931357417217902
                },
                "public releaseSnapshot(final Snapshot snapshot)": {
                    "first": {
                        "method_name": "releaseSnapshot",
                        "method_signature": "public releaseSnapshot(final Snapshot snapshot)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3455677689803033
                },
                "private isOverflowing(final long value)": {
                    "first": {
                        "method_name": "isOverflowing",
                        "method_signature": "private isOverflowing(final long value)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.35544760948784515
                },
                "private mergeColumnFamilyHandleLists(final List<ColumnFamilyHandle> existingColumnFamilyHandles,\n                                                                  final List<ColumnFamilyHandle> createdColumnFamilyHandles,\n                                                                  final List<ColumnFamilyDescriptor> allDescriptors)": {
                    "first": {
                        "method_name": "mergeColumnFamilyHandleLists",
                        "method_signature": "private mergeColumnFamilyHandleLists(final List<ColumnFamilyHandle> existingColumnFamilyHandles,\n                                                                  final List<ColumnFamilyHandle> createdColumnFamilyHandles,\n                                                                  final List<ColumnFamilyDescriptor> allDescriptors)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3557101813924877
                },
                "private setupStatistics(final Map<String, Object> configs, final DBOptions dbOptions)": {
                    "first": {
                        "method_name": "setupStatistics",
                        "method_signature": "private setupStatistics(final Map<String, Object> configs, final DBOptions dbOptions)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.356479123035866
                },
                "public synchronized get(final Bytes key, final ReadOptions readOptions)": {
                    "first": {
                        "method_name": "get",
                        "method_signature": "public synchronized get(final Bytes key, final ReadOptions readOptions)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4043360402719119
                },
                "private validateStoreOpen()": {
                    "first": {
                        "method_name": "validateStoreOpen",
                        "method_signature": "private validateStoreOpen()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.45602205239427257
                },
                " prefixScan(final P prefix,\n                                                                             final PS prefixKeySerializer,\n                                                                             final Set<KeyValueIterator<Bytes, byte[]>> openIterators)": {
                    "first": {
                        "method_name": "prefixScan",
                        "method_signature": " prefixScan(final P prefix,\n                                                                             final PS prefixKeySerializer,\n                                                                             final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.48356151997691393
                },
                " reverseAll(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)": {
                    "first": {
                        "method_name": "reverseAll",
                        "method_signature": " reverseAll(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.48794602087523103
                },
                "synchronized reverseRange(final Bytes from,\n                                                              final Bytes to,\n                                                              final Set<KeyValueIterator<Bytes, byte[]>> openIterators)": {
                    "first": {
                        "method_name": "reverseRange",
                        "method_signature": "synchronized reverseRange(final Bytes from,\n                                                              final Bytes to,\n                                                              final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4934726572204394
                },
                "private addValueProvidersToMetricsRecorder()": {
                    "first": {
                        "method_name": "addValueProvidersToMetricsRecorder",
                        "method_signature": "private addValueProvidersToMetricsRecorder()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5093311322056114
                },
                "synchronized range(final Bytes from,\n                                                       final Bytes to,\n                                                       final Set<KeyValueIterator<Bytes, byte[]>> openIterators)": {
                    "first": {
                        "method_name": "range",
                        "method_signature": "synchronized range(final Bytes from,\n                                                       final Bytes to,\n                                                       final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5124027713115881
                },
                "private flush1()": {
                    "first": {
                        "method_name": "flush1",
                        "method_signature": "private flush1()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5182631935245325
                },
                " restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> records)": {
                    "first": {
                        "method_name": "restoreBatch",
                        "method_signature": " restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> records)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5288050650410469
                },
                "synchronized all(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)": {
                    "first": {
                        "method_name": "all",
                        "method_signature": "synchronized all(final Set<KeyValueIterator<Bytes, byte[]>> openIterators)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5288634316792745
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "e45e032b8d90435de2f338f77a732c88f8cca66e",
        "url": "https://github.com/apache/kafka/commit/e45e032b8d90435de2f338f77a732c88f8cca66e",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public maybeThrowAsyncError() : void extracted from private deliverMessages() : void in class org.apache.kafka.connect.runtime.WorkerSinkTask & moved to class org.apache.kafka.connect.runtime.errors.WorkerErrantRecordReporter",
            "leftSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 595,
                    "endLine": 634,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private deliverMessages() : void"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 605,
                    "endLine": 606,
                    "startColumn": 17,
                    "endColumn": 57,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 604,
                    "endLine": 607,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 604,
                    "endLine": 607,
                    "startColumn": 109,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java",
                    "startLine": 162,
                    "endLine": 166,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public maybeThrowAsyncError() : void"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java",
                    "startLine": 164,
                    "endLine": 164,
                    "startColumn": 13,
                    "endColumn": 103,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java",
                    "startLine": 163,
                    "endLine": 165,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java",
                    "startLine": 163,
                    "endLine": 165,
                    "startColumn": 100,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 595,
                    "endLine": 633,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private deliverMessages() : void"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                    "startLine": 605,
                    "endLine": 605,
                    "startColumn": 17,
                    "endColumn": 66,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "workerErrantRecordReporter.maybeThrowAsyncError()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 567,
        "extraction_results": {
            "success": true,
            "newCommitHash": "1f1c194190a2c0be33606d4db7188eb04925d32a",
            "newBranchName": "extract-maybeThrowAsyncError-deliverMessages-72b7028"
        },
        "telemetry": {
            "id": "8a1609f9-19f0-4f42-88e3-b3b1ab0f411b",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 862,
                "lineStart": 72,
                "lineEnd": 933,
                "bodyLineStart": 72,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
                "sourceCode": "/**\n * {@link WorkerTask} that uses a {@link SinkTask} to export data from Kafka.\n */\nclass WorkerSinkTask extends WorkerTask {\n    private static final Logger log = LoggerFactory.getLogger(WorkerSinkTask.class);\n\n    private final WorkerConfig workerConfig;\n    private final SinkTask task;\n    private final ClusterConfigState configState;\n    private Map<String, String> taskConfig;\n    private final Converter keyConverter;\n    private final Converter valueConverter;\n    private final HeaderConverter headerConverter;\n    private final TransformationChain<SinkRecord> transformationChain;\n    private final SinkTaskMetricsGroup sinkTaskMetricsGroup;\n    private final boolean isTopicTrackingEnabled;\n    private final Consumer<byte[], byte[]> consumer;\n    private WorkerSinkTaskContext context;\n    private final List<SinkRecord> messageBatch;\n    private final Map<TopicPartition, OffsetAndMetadata> lastCommittedOffsets;\n    private final Map<TopicPartition, OffsetAndMetadata> currentOffsets;\n    private final Map<TopicPartition, OffsetAndMetadata> origOffsets;\n    private RuntimeException rebalanceException;\n    private long nextCommit;\n    private int commitSeqno;\n    private long commitStarted;\n    private int commitFailures;\n    private boolean pausedForRedelivery;\n    private boolean committing;\n    private boolean taskStopped;\n    private final WorkerErrantRecordReporter workerErrantRecordReporter;\n    private final Supplier<List<ErrorReporter>> errorReportersSupplier;\n\n    public WorkerSinkTask(ConnectorTaskId id,\n                          SinkTask task,\n                          TaskStatus.Listener statusListener,\n                          TargetState initialState,\n                          WorkerConfig workerConfig,\n                          ClusterConfigState configState,\n                          ConnectMetrics connectMetrics,\n                          Converter keyConverter,\n                          Converter valueConverter,\n                          ErrorHandlingMetrics errorMetrics,\n                          HeaderConverter headerConverter,\n                          TransformationChain<SinkRecord> transformationChain,\n                          Consumer<byte[], byte[]> consumer,\n                          ClassLoader loader,\n                          Time time,\n                          RetryWithToleranceOperator retryWithToleranceOperator,\n                          WorkerErrantRecordReporter workerErrantRecordReporter,\n                          StatusBackingStore statusBackingStore,\n                          Supplier<List<ErrorReporter>> errorReportersSupplier) {\n        super(id, statusListener, initialState, loader, connectMetrics, errorMetrics,\n                retryWithToleranceOperator, time, statusBackingStore);\n\n        this.workerConfig = workerConfig;\n        this.task = task;\n        this.configState = configState;\n        this.keyConverter = keyConverter;\n        this.valueConverter = valueConverter;\n        this.headerConverter = headerConverter;\n        this.transformationChain = transformationChain;\n        this.messageBatch = new ArrayList<>();\n        this.lastCommittedOffsets = new HashMap<>();\n        this.currentOffsets = new HashMap<>();\n        this.origOffsets = new HashMap<>();\n        this.pausedForRedelivery = false;\n        this.rebalanceException = null;\n        this.nextCommit = time.milliseconds() +\n                workerConfig.getLong(WorkerConfig.OFFSET_COMMIT_INTERVAL_MS_CONFIG);\n        this.committing = false;\n        this.commitSeqno = 0;\n        this.commitStarted = -1;\n        this.commitFailures = 0;\n        this.sinkTaskMetricsGroup = new SinkTaskMetricsGroup(id, connectMetrics);\n        this.sinkTaskMetricsGroup.recordOffsetSequenceNumber(commitSeqno);\n        this.consumer = consumer;\n        this.isTopicTrackingEnabled = workerConfig.getBoolean(TOPIC_TRACKING_ENABLE_CONFIG);\n        this.taskStopped = false;\n        this.workerErrantRecordReporter = workerErrantRecordReporter;\n        this.errorReportersSupplier = errorReportersSupplier;\n    }\n\n    @Override\n    public void initialize(TaskConfig taskConfig) {\n        try {\n            this.taskConfig = taskConfig.originalsStrings();\n            this.context = new WorkerSinkTaskContext(consumer, this, configState);\n        } catch (Throwable t) {\n            log.error(\"{} Task failed initialization and will not be started.\", this, t);\n            onFailure(t);\n        }\n    }\n\n    @Override\n    public void stop() {\n        // Offset commit is handled upon exit in work thread\n        super.stop();\n        consumer.wakeup();\n    }\n\n    @Override\n    protected void close() {\n        // FIXME Kafka needs to add a timeout parameter here for us to properly obey the timeout\n        // passed in\n        try {\n            task.stop();\n        } catch (Throwable t) {\n            log.warn(\"Could not stop task\", t);\n        }\n        taskStopped = true;\n        Utils.closeQuietly(consumer, \"consumer\");\n        Utils.closeQuietly(transformationChain, \"transformation chain\");\n        Utils.closeQuietly(retryWithToleranceOperator, \"retry operator\");\n        Utils.closeQuietly(headerConverter, \"header converter\");\n        /*\n            Setting partition count explicitly to 0 to handle the case,\n            when the task fails, which would cause its consumer to leave the group.\n            This would cause onPartitionsRevoked to be invoked in the rebalance listener, but not onPartitionsAssigned,\n            so the metrics for the task (which are still available for failed tasks until they are explicitly revoked\n            from the worker) would become inaccurate.\n        */\n        sinkTaskMetricsGroup.recordPartitionCount(0);\n    }\n\n    @Override\n    public void removeMetrics() {\n        try {\n            sinkTaskMetricsGroup.close();\n        } finally {\n            super.removeMetrics();\n        }\n    }\n\n    @Override\n    public void transitionTo(TargetState state) {\n        super.transitionTo(state);\n        consumer.wakeup();\n    }\n\n    @Override\n    public void execute() {\n        log.info(\"{} Executing sink task\", this);\n        // Make sure any uncommitted data has been committed and the task has\n        // a chance to clean up its state\n        try (UncheckedCloseable suppressible = this::closeAllPartitions) {\n            while (!isStopping())\n                iteration();\n        } catch (WakeupException e) {\n            log.trace(\"Consumer woken up during initial offset commit attempt, \" \n                + \"but succeeded during a later attempt\");\n        }\n    }\n\n    protected void iteration() {\n        final long offsetCommitIntervalMs = workerConfig.getLong(WorkerConfig.OFFSET_COMMIT_INTERVAL_MS_CONFIG);\n\n        try {\n            long now = time.milliseconds();\n\n            // Maybe commit\n            if (!committing && (context.isCommitRequested() || now >= nextCommit)) {\n                commitOffsets(now, false);\n                nextCommit = now + offsetCommitIntervalMs;\n                context.clearCommitRequest();\n            }\n\n            final long commitTimeoutMs = commitStarted + workerConfig.getLong(WorkerConfig.OFFSET_COMMIT_TIMEOUT_MS_CONFIG);\n\n            // Check for timed out commits\n            if (committing && now >= commitTimeoutMs) {\n                log.warn(\"{} Commit of offsets timed out\", this);\n                commitFailures++;\n                committing = false;\n            }\n\n            // And process messages\n            long timeoutMs = Math.max(nextCommit - now, 0);\n            poll(timeoutMs);\n        } catch (WakeupException we) {\n            log.trace(\"{} Consumer woken up\", this);\n\n            if (isStopping())\n                return;\n\n            if (shouldPause()) {\n                pauseAll();\n                onPause();\n                context.requestCommit();\n            } else if (!pausedForRedelivery) {\n                resumeAll();\n                onResume();\n            }\n        }\n    }\n\n    /**\n     * Respond to a previous commit attempt that may or may not have succeeded. Note that due to our use of async commits,\n     * these invocations may come out of order and thus the need for the commit sequence number.\n     *\n     * @param error            the error resulting from the commit, or null if the commit succeeded without error\n     * @param seqno            the sequence number at the time the commit was requested\n     * @param committedOffsets the offsets that were committed; may be null if the commit did not complete successfully\n     *                         or if no new offsets were committed\n     */\n    private void onCommitCompleted(Throwable error, long seqno, Map<TopicPartition, OffsetAndMetadata> committedOffsets) {\n        if (commitSeqno != seqno) {\n            log.debug(\"{} Received out of order commit callback for sequence number {}, but most recent sequence number is {}\",\n                    this, seqno, commitSeqno);\n            sinkTaskMetricsGroup.recordOffsetCommitSkip();\n        } else {\n            long durationMillis = time.milliseconds() - commitStarted;\n            if (error != null) {\n                log.error(\"{} Commit of offsets threw an unexpected exception for sequence number {}: {}\",\n                        this, seqno, committedOffsets, error);\n                commitFailures++;\n                recordCommitFailure(durationMillis, error);\n            } else {\n                log.debug(\"{} Finished offset commit successfully in {} ms for sequence number {}: {}\",\n                        this, durationMillis, seqno, committedOffsets);\n                if (committedOffsets != null) {\n                    log.trace(\"{} Adding to last committed offsets: {}\", this, committedOffsets);\n                    lastCommittedOffsets.putAll(committedOffsets);\n                    log.debug(\"{} Last committed offsets are now {}\", this, committedOffsets);\n                    sinkTaskMetricsGroup.recordCommittedOffsets(committedOffsets);\n                }\n                commitFailures = 0;\n                recordCommitSuccess(durationMillis);\n            }\n            committing = false;\n        }\n    }\n\n    public int commitFailures() {\n        return commitFailures;\n    }\n\n    /**\n     * Initializes and starts the SinkTask.\n     */\n    @Override\n    protected void initializeAndStart() {\n        SinkConnectorConfig.validate(taskConfig);\n        retryWithToleranceOperator.reporters(errorReportersSupplier.get());\n\n        if (SinkConnectorConfig.hasTopicsConfig(taskConfig)) {\n            List<String> topics = SinkConnectorConfig.parseTopicsList(taskConfig);\n            consumer.subscribe(topics, new HandleRebalance());\n            log.debug(\"{} Initializing and starting task for topics {}\", this, Utils.join(topics, \", \"));\n        } else {\n            String topicsRegexStr = taskConfig.get(SinkTask.TOPICS_REGEX_CONFIG);\n            Pattern pattern = Pattern.compile(topicsRegexStr);\n            consumer.subscribe(pattern, new HandleRebalance());\n            log.debug(\"{} Initializing and starting task for topics regex {}\", this, topicsRegexStr);\n        }\n\n        task.initialize(context);\n        task.start(taskConfig);\n        log.info(\"{} Sink task finished initialization and start\", this);\n    }\n\n    /**\n     * Poll for new messages with the given timeout. Should only be invoked by the worker thread.\n     */\n    protected void poll(long timeoutMs) {\n        rewind();\n        long retryTimeout = context.timeout();\n        if (retryTimeout > 0) {\n            timeoutMs = Math.min(timeoutMs, retryTimeout);\n            context.timeout(-1L);\n        }\n\n        log.trace(\"{} Polling consumer with timeout {} ms\", this, timeoutMs);\n        ConsumerRecords<byte[], byte[]> msgs = pollConsumer(timeoutMs);\n        assert messageBatch.isEmpty() || msgs.isEmpty();\n        log.trace(\"{} Polling returned {} messages\", this, msgs.count());\n\n        convertMessages(msgs);\n        deliverMessages();\n    }\n\n    // Visible for testing\n    boolean isCommitting() {\n        return committing;\n    }\n\n    private void doCommitSync(Map<TopicPartition, OffsetAndMetadata> offsets, int seqno) {\n        log.debug(\"{} Committing offsets synchronously using sequence number {}: {}\", this, seqno, offsets);\n        try {\n            consumer.commitSync(offsets);\n            onCommitCompleted(null, seqno, offsets);\n        } catch (WakeupException e) {\n            // retry the commit to ensure offsets get pushed, then propagate the wakeup up to poll\n            doCommitSync(offsets, seqno);\n            throw e;\n        } catch (KafkaException e) {\n            onCommitCompleted(e, seqno, offsets);\n        }\n    }\n\n    private void doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno) {\n        log.debug(\"{} Committing offsets asynchronously using sequence number {}: {}\", this, seqno, offsets);\n        OffsetCommitCallback cb = (tpOffsets, error) -> onCommitCompleted(error, seqno, tpOffsets);\n        consumer.commitAsync(offsets, cb);\n    }\n\n    /**\n     * Starts an offset commit by flushing outstanding messages from the task and then starting\n     * the write commit.\n     */\n    private void doCommit(Map<TopicPartition, OffsetAndMetadata> offsets, boolean closing, int seqno) {\n        if (isCancelled()) {\n            log.debug(\"Skipping final offset commit as task has been cancelled\");\n            return;\n        }\n        if (closing) {\n            doCommitSync(offsets, seqno);\n        } else {\n            doCommitAsync(offsets, seqno);\n        }\n    }\n\n    private void commitOffsets(long now, boolean closing) {\n        commitOffsets(now, closing, consumer.assignment());\n    }\n\n    private void commitOffsets(long now, boolean closing, Collection<TopicPartition> topicPartitions) {\n        log.trace(\"Committing offsets for partitions {}\", topicPartitions);\n        if (workerErrantRecordReporter != null) {\n            log.trace(\"Awaiting reported errors to be completed\");\n            workerErrantRecordReporter.awaitFutures(topicPartitions);\n            log.trace(\"Completed reported errors\");\n        }\n\n        Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = currentOffsets.entrySet().stream()\n            .filter(e -> topicPartitions.contains(e.getKey()))\n            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n        if (offsetsToCommit.isEmpty())\n            return;\n\n        committing = true;\n        commitSeqno += 1;\n        commitStarted = now;\n        sinkTaskMetricsGroup.recordOffsetSequenceNumber(commitSeqno);\n\n        Map<TopicPartition, OffsetAndMetadata> lastCommittedOffsetsForPartitions = this.lastCommittedOffsets.entrySet().stream()\n            .filter(e -> offsetsToCommit.containsKey(e.getKey()))\n            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n        final Map<TopicPartition, OffsetAndMetadata> taskProvidedOffsets;\n        try {\n            log.trace(\"{} Calling task.preCommit with current offsets: {}\", this, offsetsToCommit);\n            taskProvidedOffsets = task.preCommit(new HashMap<>(offsetsToCommit));\n        } catch (Throwable t) {\n            if (closing) {\n                log.warn(\"{} Offset commit failed during close\", this);\n            } else {\n                log.error(\"{} Offset commit failed, rewinding to last committed offsets\", this, t);\n                for (Map.Entry<TopicPartition, OffsetAndMetadata> entry : lastCommittedOffsetsForPartitions.entrySet()) {\n                    log.debug(\"{} Rewinding topic partition {} to offset {}\", this, entry.getKey(), entry.getValue().offset());\n                    consumer.seek(entry.getKey(), entry.getValue().offset());\n                }\n                currentOffsets.putAll(lastCommittedOffsetsForPartitions);\n            }\n            onCommitCompleted(t, commitSeqno, null);\n            return;\n        } finally {\n            if (closing) {\n                log.trace(\"{} Closing the task before committing the offsets: {}\", this, offsetsToCommit);\n                task.close(topicPartitions);\n            }\n        }\n\n        if (taskProvidedOffsets.isEmpty()) {\n            log.debug(\"{} Skipping offset commit, task opted-out by returning no offsets from preCommit\", this);\n            onCommitCompleted(null, commitSeqno, null);\n            return;\n        }\n\n        Collection<TopicPartition> allAssignedTopicPartitions = consumer.assignment();\n        final Map<TopicPartition, OffsetAndMetadata> committableOffsets = new HashMap<>(lastCommittedOffsetsForPartitions);\n        for (Map.Entry<TopicPartition, OffsetAndMetadata> taskProvidedOffsetEntry : taskProvidedOffsets.entrySet()) {\n            final TopicPartition partition = taskProvidedOffsetEntry.getKey();\n            final OffsetAndMetadata taskProvidedOffset = taskProvidedOffsetEntry.getValue();\n            if (committableOffsets.containsKey(partition)) {\n                long taskOffset = taskProvidedOffset.offset();\n                long currentOffset = offsetsToCommit.get(partition).offset();\n                if (taskOffset <= currentOffset) {\n                    committableOffsets.put(partition, taskProvidedOffset);\n                } else {\n                    log.warn(\"{} Ignoring invalid task provided offset {}/{} -- not yet consumed, taskOffset={} currentOffset={}\",\n                        this, partition, taskProvidedOffset, taskOffset, currentOffset);\n                }\n            } else if (!allAssignedTopicPartitions.contains(partition)) {\n                log.warn(\"{} Ignoring invalid task provided offset {}/{} -- partition not assigned, assignment={}\",\n                        this, partition, taskProvidedOffset, allAssignedTopicPartitions);\n            } else {\n                log.debug(\"{} Ignoring task provided offset {}/{} -- partition not requested, requested={}\",\n                        this, partition, taskProvidedOffset, committableOffsets.keySet());\n            }\n        }\n\n        if (committableOffsets.equals(lastCommittedOffsetsForPartitions)) {\n            log.debug(\"{} Skipping offset commit, no change since last commit\", this);\n            onCommitCompleted(null, commitSeqno, null);\n            return;\n        }\n\n        doCommit(committableOffsets, closing, commitSeqno);\n    }\n\n\n    @Override\n    public String toString() {\n        return \"WorkerSinkTask{\" +\n                \"id=\" + id +\n                '}';\n    }\n\n    private ConsumerRecords<byte[], byte[]> pollConsumer(long timeoutMs) {\n        ConsumerRecords<byte[], byte[]> msgs = consumer.poll(Duration.ofMillis(timeoutMs));\n\n        // Exceptions raised from the task during a rebalance should be rethrown to stop the task and mark it as failed\n        if (rebalanceException != null) {\n            RuntimeException e = rebalanceException;\n            rebalanceException = null;\n            throw e;\n        }\n\n        sinkTaskMetricsGroup.recordRead(msgs.count());\n        return msgs;\n    }\n\n    private void convertMessages(ConsumerRecords<byte[], byte[]> msgs) {\n        for (ConsumerRecord<byte[], byte[]> msg : msgs) {\n            log.trace(\"{} Consuming and converting message in topic '{}' partition {} at offset {} and timestamp {}\",\n                    this, msg.topic(), msg.partition(), msg.offset(), msg.timestamp());\n\n            retryWithToleranceOperator.consumerRecord(msg);\n\n            SinkRecord transRecord = convertAndTransformRecord(msg);\n\n            origOffsets.put(\n                    new TopicPartition(msg.topic(), msg.partition()),\n                    new OffsetAndMetadata(msg.offset() + 1)\n            );\n            if (transRecord != null) {\n                messageBatch.add(transRecord);\n            } else {\n                log.trace(\n                        \"{} Converters and transformations returned null, possibly because of too many retries, so \" +\n                                \"dropping record in topic '{}' partition {} at offset {}\",\n                        this, msg.topic(), msg.partition(), msg.offset()\n                );\n            }\n        }\n        sinkTaskMetricsGroup.recordConsumedOffsets(origOffsets);\n    }\n\n    private SinkRecord convertAndTransformRecord(final ConsumerRecord<byte[], byte[]> msg) {\n        SchemaAndValue keyAndSchema = retryWithToleranceOperator.execute(() -> keyConverter.toConnectData(msg.topic(), msg.headers(), msg.key()),\n                Stage.KEY_CONVERTER, keyConverter.getClass());\n\n        SchemaAndValue valueAndSchema = retryWithToleranceOperator.execute(() -> valueConverter.toConnectData(msg.topic(), msg.headers(), msg.value()),\n                Stage.VALUE_CONVERTER, valueConverter.getClass());\n\n        Headers headers = retryWithToleranceOperator.execute(() -> convertHeadersFor(msg), Stage.HEADER_CONVERTER, headerConverter.getClass());\n\n        if (retryWithToleranceOperator.failed()) {\n            return null;\n        }\n\n        Long timestamp = ConnectUtils.checkAndConvertTimestamp(msg.timestamp());\n        SinkRecord origRecord = new SinkRecord(msg.topic(), msg.partition(),\n                keyAndSchema.schema(), keyAndSchema.value(),\n                valueAndSchema.schema(), valueAndSchema.value(),\n                msg.offset(),\n                timestamp,\n                msg.timestampType(),\n                headers);\n        log.trace(\"{} Applying transformations to record in topic '{}' partition {} at offset {} and timestamp {} with key {} and value {}\",\n                this, msg.topic(), msg.partition(), msg.offset(), timestamp, keyAndSchema.value(), valueAndSchema.value());\n        if (isTopicTrackingEnabled) {\n            recordActiveTopic(origRecord.topic());\n        }\n\n        // Apply the transformations\n        SinkRecord transformedRecord = transformationChain.apply(origRecord);\n        if (transformedRecord == null) {\n            return null;\n        }\n        // Error reporting will need to correlate each sink record with the original consumer record\n        return new InternalSinkRecord(msg, transformedRecord);\n    }\n\n    private Headers convertHeadersFor(ConsumerRecord<byte[], byte[]> record) {\n        Headers result = new ConnectHeaders();\n        org.apache.kafka.common.header.Headers recordHeaders = record.headers();\n        if (recordHeaders != null) {\n            String topic = record.topic();\n            for (org.apache.kafka.common.header.Header recordHeader : recordHeaders) {\n                SchemaAndValue schemaAndValue = headerConverter.toConnectHeader(topic, recordHeader.key(), recordHeader.value());\n                result.add(recordHeader.key(), schemaAndValue);\n            }\n        }\n        return result;\n    }\n\n    protected WorkerErrantRecordReporter workerErrantRecordReporter() {\n        return workerErrantRecordReporter;\n    }\n\n    private void resumeAll() {\n        for (TopicPartition tp : consumer.assignment())\n            if (!context.pausedPartitions().contains(tp))\n                consumer.resume(singleton(tp));\n    }\n\n    private void pauseAll() {\n        consumer.pause(consumer.assignment());\n    }\n\n    private void deliverMessages() {\n        // Finally, deliver this batch to the sink\n        try {\n            // Since we reuse the messageBatch buffer, ensure we give the task its own copy\n            log.trace(\"{} Delivering batch of {} messages to task\", this, messageBatch.size());\n            long start = time.milliseconds();\n            task.put(new ArrayList<>(messageBatch));\n            // if errors raised from the operator were swallowed by the task implementation, an\n            // exception needs to be thrown to kill the task indicating the tolerance was exceeded\n            maybeThrowAsyncError();\n            recordBatch(messageBatch.size());\n            sinkTaskMetricsGroup.recordPut(time.milliseconds() - start);\n            currentOffsets.putAll(origOffsets);\n            origOffsets.clear();\n            messageBatch.clear();\n            // If we had paused all consumer topic partitions to try to redeliver data, then we should resume any that\n            // the task had not explicitly paused\n            if (pausedForRedelivery) {\n                if (!shouldPause())\n                    resumeAll();\n                pausedForRedelivery = false;\n            }\n        } catch (RetriableException e) {\n            log.error(\"{} RetriableException from SinkTask:\", this, e);\n            if (!pausedForRedelivery) {\n                // If we're retrying a previous batch, make sure we've paused all topic partitions so we don't get new data,\n                // but will still be able to poll in order to handle user-requested timeouts, keep group membership, etc.\n                pausedForRedelivery = true;\n                pauseAll();\n            }\n            // Let this exit normally, the batch will be reprocessed on the next loop.\n        } catch (Throwable t) {\n            log.error(\"{} Task threw an uncaught and unrecoverable exception. Task is being killed and will not \"\n                    + \"recover until manually restarted. Error: {}\", this, t.getMessage(), t);\n            throw new ConnectException(\"Exiting WorkerSinkTask due to unrecoverable exception.\", t);\n        }\n    }\n\n    private void maybeThrowAsyncError() {\n        if (retryWithToleranceOperator.failed() && !retryWithToleranceOperator.withinToleranceLimits()) {\n            throw new ConnectException(\"Tolerance exceeded in error handler\",\n                retryWithToleranceOperator.error());\n        }\n    }\n\n    private void rewind() {\n        Map<TopicPartition, Long> offsets = context.offsets();\n        if (offsets.isEmpty()) {\n            return;\n        }\n        for (Map.Entry<TopicPartition, Long> entry: offsets.entrySet()) {\n            TopicPartition tp = entry.getKey();\n            Long offset = entry.getValue();\n            if (offset != null) {\n                log.trace(\"{} Rewind {} to offset {}\", this, tp, offset);\n                consumer.seek(tp, offset);\n                lastCommittedOffsets.put(tp, new OffsetAndMetadata(offset));\n                currentOffsets.put(tp, new OffsetAndMetadata(offset));\n            } else {\n                log.warn(\"{} Cannot rewind {} to null offset\", this, tp);\n            }\n        }\n        context.clearOffsets();\n    }\n\n    private void openPartitions(Collection<TopicPartition> partitions) {\n        task.open(partitions);\n    }\n\n    private void closeAllPartitions() {\n        closePartitions(currentOffsets.keySet(), false);\n    }\n\n    private void closePartitions(Collection<TopicPartition> topicPartitions, boolean lost) {\n        if (!lost) {\n            commitOffsets(time.milliseconds(), true, topicPartitions);\n        } else {\n            log.trace(\"{} Closing the task as partitions have been lost: {}\", this, topicPartitions);\n            task.close(topicPartitions);\n            if (workerErrantRecordReporter != null) {\n                log.trace(\"Cancelling reported errors for {}\", topicPartitions);\n                workerErrantRecordReporter.cancelFutures(topicPartitions);\n                log.trace(\"Cancelled all reported errors for {}\", topicPartitions);\n            }\n            origOffsets.keySet().removeAll(topicPartitions);\n            currentOffsets.keySet().removeAll(topicPartitions);\n        }\n        lastCommittedOffsets.keySet().removeAll(topicPartitions);\n    }\n\n    private void updatePartitionCount() {\n        sinkTaskMetricsGroup.recordPartitionCount(consumer.assignment().size());\n    }\n\n    @Override\n    protected void recordBatch(int size) {\n        super.recordBatch(size);\n        sinkTaskMetricsGroup.recordSend(size);\n    }\n\n    @Override\n    protected void recordCommitSuccess(long duration) {\n        super.recordCommitSuccess(duration);\n        sinkTaskMetricsGroup.recordOffsetCommitSuccess();\n    }\n\n    SinkTaskMetricsGroup sinkTaskMetricsGroup() {\n        return sinkTaskMetricsGroup;\n    }\n\n    // Visible for testing\n    long getNextCommit() {\n        return nextCommit;\n    }\n\n    private class HandleRebalance implements ConsumerRebalanceListener {\n        @Override\n        public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n            log.debug(\"{} Partitions assigned {}\", WorkerSinkTask.this, partitions);\n\n            for (TopicPartition tp : partitions) {\n                long pos = consumer.position(tp);\n                lastCommittedOffsets.put(tp, new OffsetAndMetadata(pos));\n                currentOffsets.put(tp, new OffsetAndMetadata(pos));\n                log.debug(\"{} Assigned topic partition {} with offset {}\", WorkerSinkTask.this, tp, pos);\n            }\n            sinkTaskMetricsGroup.assignedOffsets(currentOffsets);\n\n            boolean wasPausedForRedelivery = pausedForRedelivery;\n            pausedForRedelivery = wasPausedForRedelivery && !messageBatch.isEmpty();\n            if (pausedForRedelivery) {\n                // Re-pause here in case we picked up new partitions in the rebalance\n                pauseAll();\n            } else {\n                // If we paused everything for redelivery and all partitions for the failed deliveries have been revoked, make\n                // sure anything we paused that the task didn't request to be paused *and* which we still own is resumed.\n                // Also make sure our tracking of paused partitions is updated to remove any partitions we no longer own.\n                if (wasPausedForRedelivery) {\n                    resumeAll();\n                }\n                // Ensure that the paused partitions contains only assigned partitions and repause as necessary\n                context.pausedPartitions().retainAll(consumer.assignment());\n                if (shouldPause())\n                    pauseAll();\n                else if (!context.pausedPartitions().isEmpty())\n                    consumer.pause(context.pausedPartitions());\n            }\n            updatePartitionCount();\n            if (partitions.isEmpty()) {\n                return;\n            }\n\n            // Instead of invoking the assignment callback on initialization, we guarantee the consumer is ready upon\n            // task start. Since this callback gets invoked during that initial setup before we've started the task, we\n            // need to guard against invoking the user's callback method during that period.\n            if (rebalanceException == null || rebalanceException instanceof WakeupException) {\n                try {\n                    openPartitions(partitions);\n                    // Rewind should be applied only if openPartitions succeeds.\n                    rewind();\n                } catch (RuntimeException e) {\n                    // The consumer swallows exceptions raised in the rebalance listener, so we need to store\n                    // exceptions and rethrow when poll() returns.\n                    rebalanceException = e;\n                }\n            }\n        }\n\n        @Override\n        public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n            onPartitionsRemoved(partitions, false);\n        }\n\n        @Override\n        public void onPartitionsLost(Collection<TopicPartition> partitions) {\n            onPartitionsRemoved(partitions, true);\n        }\n\n        private void onPartitionsRemoved(Collection<TopicPartition> partitions, boolean lost) {\n            if (taskStopped) {\n                log.trace(\"Skipping partition revocation callback as task has already been stopped\");\n                return;\n            }\n            log.debug(\"{} Partitions {}: {}\", WorkerSinkTask.this, lost ? \"lost\" : \"revoked\", partitions);\n\n            if (partitions.isEmpty())\n                return;\n\n            try {\n                closePartitions(partitions, lost);\n                sinkTaskMetricsGroup.clearOffsets(partitions);\n            } catch (RuntimeException e) {\n                // The consumer swallows exceptions raised in the rebalance listener, so we need to store\n                // exceptions and rethrow when poll() returns.\n                rebalanceException = e;\n            }\n\n            // Make sure we don't have any leftover data since offsets for these partitions will be reset to committed positions\n            messageBatch.removeIf(record -> partitions.contains(new TopicPartition(record.topic(), record.kafkaPartition())));\n        }\n    }\n\n    static class SinkTaskMetricsGroup {\n        private final ConnectorTaskId id;\n        private final ConnectMetrics metrics;\n        private final MetricGroup metricGroup;\n        private final Sensor sinkRecordRead;\n        private final Sensor sinkRecordSend;\n        private final Sensor partitionCount;\n        private final Sensor offsetSeqNum;\n        private final Sensor offsetCompletion;\n        private final Sensor offsetCompletionSkip;\n        private final Sensor putBatchTime;\n        private final Sensor sinkRecordActiveCount;\n        private long activeRecords;\n        private Map<TopicPartition, OffsetAndMetadata> consumedOffsets = new HashMap<>();\n        private Map<TopicPartition, OffsetAndMetadata> committedOffsets = new HashMap<>();\n\n        public SinkTaskMetricsGroup(ConnectorTaskId id, ConnectMetrics connectMetrics) {\n            this.metrics = connectMetrics;\n            this.id = id;\n\n            ConnectMetricsRegistry registry = connectMetrics.registry();\n            metricGroup = connectMetrics\n                                  .group(registry.sinkTaskGroupName(), registry.connectorTagName(), id.connector(), registry.taskTagName(),\n                                         Integer.toString(id.task()));\n            // prevent collisions by removing any previously created metrics in this group.\n            metricGroup.close();\n\n            sinkRecordRead = metricGroup.sensor(\"sink-record-read\");\n            sinkRecordRead.add(metricGroup.metricName(registry.sinkRecordReadRate), new Rate());\n            sinkRecordRead.add(metricGroup.metricName(registry.sinkRecordReadTotal), new CumulativeSum());\n\n            sinkRecordSend = metricGroup.sensor(\"sink-record-send\");\n            sinkRecordSend.add(metricGroup.metricName(registry.sinkRecordSendRate), new Rate());\n            sinkRecordSend.add(metricGroup.metricName(registry.sinkRecordSendTotal), new CumulativeSum());\n\n            sinkRecordActiveCount = metricGroup.sensor(\"sink-record-active-count\");\n            sinkRecordActiveCount.add(metricGroup.metricName(registry.sinkRecordActiveCount), new Value());\n            sinkRecordActiveCount.add(metricGroup.metricName(registry.sinkRecordActiveCountMax), new Max());\n            sinkRecordActiveCount.add(metricGroup.metricName(registry.sinkRecordActiveCountAvg), new Avg());\n\n            partitionCount = metricGroup.sensor(\"partition-count\");\n            partitionCount.add(metricGroup.metricName(registry.sinkRecordPartitionCount), new Value());\n\n            offsetSeqNum = metricGroup.sensor(\"offset-seq-number\");\n            offsetSeqNum.add(metricGroup.metricName(registry.sinkRecordOffsetCommitSeqNum), new Value());\n\n            offsetCompletion = metricGroup.sensor(\"offset-commit-completion\");\n            offsetCompletion.add(metricGroup.metricName(registry.sinkRecordOffsetCommitCompletionRate), new Rate());\n            offsetCompletion.add(metricGroup.metricName(registry.sinkRecordOffsetCommitCompletionTotal), new CumulativeSum());\n\n            offsetCompletionSkip = metricGroup.sensor(\"offset-commit-completion-skip\");\n            offsetCompletionSkip.add(metricGroup.metricName(registry.sinkRecordOffsetCommitSkipRate), new Rate());\n            offsetCompletionSkip.add(metricGroup.metricName(registry.sinkRecordOffsetCommitSkipTotal), new CumulativeSum());\n\n            putBatchTime = metricGroup.sensor(\"put-batch-time\");\n            putBatchTime.add(metricGroup.metricName(registry.sinkRecordPutBatchTimeMax), new Max());\n            putBatchTime.add(metricGroup.metricName(registry.sinkRecordPutBatchTimeAvg), new Avg());\n        }\n\n        void computeSinkRecordLag() {\n            Map<TopicPartition, OffsetAndMetadata> consumed = this.consumedOffsets;\n            Map<TopicPartition, OffsetAndMetadata> committed = this.committedOffsets;\n            activeRecords = 0L;\n            for (Map.Entry<TopicPartition, OffsetAndMetadata> committedOffsetEntry : committed.entrySet()) {\n                final TopicPartition partition = committedOffsetEntry.getKey();\n                final OffsetAndMetadata consumedOffsetMeta = consumed.get(partition);\n                if (consumedOffsetMeta != null) {\n                    final OffsetAndMetadata committedOffsetMeta = committedOffsetEntry.getValue();\n                    long consumedOffset = consumedOffsetMeta.offset();\n                    long committedOffset = committedOffsetMeta.offset();\n                    long diff = consumedOffset - committedOffset;\n                    // Connector tasks can return offsets, so make sure nothing wonky happens\n                    activeRecords += Math.max(diff, 0L);\n                }\n            }\n            sinkRecordActiveCount.record(activeRecords);\n        }\n\n        void close() {\n            metricGroup.close();\n        }\n\n        void recordRead(int batchSize) {\n            sinkRecordRead.record(batchSize);\n        }\n\n        void recordSend(int batchSize) {\n            sinkRecordSend.record(batchSize);\n        }\n\n        void recordPut(long duration) {\n            putBatchTime.record(duration);\n        }\n\n        void recordPartitionCount(int assignedPartitionCount) {\n            partitionCount.record(assignedPartitionCount);\n        }\n\n        void recordOffsetSequenceNumber(int seqNum) {\n            offsetSeqNum.record(seqNum);\n        }\n\n        void recordConsumedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets) {\n            consumedOffsets.putAll(offsets);\n            computeSinkRecordLag();\n        }\n\n        void recordCommittedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets) {\n            committedOffsets = offsets;\n            computeSinkRecordLag();\n        }\n\n        void assignedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets) {\n            consumedOffsets = new HashMap<>(offsets);\n            committedOffsets = offsets;\n            computeSinkRecordLag();\n        }\n\n        void clearOffsets(Collection<TopicPartition> topicPartitions) {\n            consumedOffsets.keySet().removeAll(topicPartitions);\n            committedOffsets.keySet().removeAll(topicPartitions);\n            computeSinkRecordLag();\n        }\n\n        void recordOffsetCommitSuccess() {\n            offsetCompletion.record(1.0);\n        }\n\n        void recordOffsetCommitSkip() {\n            offsetCompletionSkip.record(1.0);\n        }\n\n        protected MetricGroup metricGroup() {\n            return metricGroup;\n        }\n    }\n}",
                "methodCount": 56
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "iteration",
                            "method_signature": "protected iteration()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onCommitCompleted",
                            "method_signature": "private onCommitCompleted(Throwable error, long seqno, Map<TopicPartition, OffsetAndMetadata> committedOffsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "poll",
                            "method_signature": "protected poll(long timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doCommitSync",
                            "method_signature": "private doCommitSync(Map<TopicPartition, OffsetAndMetadata> offsets, int seqno)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doCommitAsync",
                            "method_signature": "private doCommitAsync(Map<TopicPartition, OffsetAndMetadata> offsets, final int seqno)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doCommit",
                            "method_signature": "private doCommit(Map<TopicPartition, OffsetAndMetadata> offsets, boolean closing, int seqno)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffsets",
                            "method_signature": "private commitOffsets(long now, boolean closing)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffsets",
                            "method_signature": "private commitOffsets(long now, boolean closing, Collection<TopicPartition> topicPartitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollConsumer",
                            "method_signature": "private pollConsumer(long timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "convertMessages",
                            "method_signature": "private convertMessages(ConsumerRecords<byte[], byte[]> msgs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "convertAndTransformRecord",
                            "method_signature": "private convertAndTransformRecord(final ConsumerRecord<byte[], byte[]> msg)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "convertHeadersFor",
                            "method_signature": "private convertHeadersFor(ConsumerRecord<byte[], byte[]> record)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resumeAll",
                            "method_signature": "private resumeAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pauseAll",
                            "method_signature": "private pauseAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deliverMessages",
                            "method_signature": "private deliverMessages()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeThrowAsyncError",
                            "method_signature": "private maybeThrowAsyncError()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rewind",
                            "method_signature": "private rewind()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "openPartitions",
                            "method_signature": "private openPartitions(Collection<TopicPartition> partitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeAllPartitions",
                            "method_signature": "private closeAllPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closePartitions",
                            "method_signature": "private closePartitions(Collection<TopicPartition> topicPartitions, boolean lost)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updatePartitionCount",
                            "method_signature": "private updatePartitionCount()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onPartitionsRemoved",
                            "method_signature": "private onPartitionsRemoved(Collection<TopicPartition> partitions, boolean lost)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "computeSinkRecordLag",
                            "method_signature": " computeSinkRecordLag()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": " close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordRead",
                            "method_signature": " recordRead(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordSend",
                            "method_signature": " recordSend(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordPut",
                            "method_signature": " recordPut(long duration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordPartitionCount",
                            "method_signature": " recordPartitionCount(int assignedPartitionCount)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetSequenceNumber",
                            "method_signature": " recordOffsetSequenceNumber(int seqNum)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordConsumedOffsets",
                            "method_signature": " recordConsumedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordCommittedOffsets",
                            "method_signature": " recordCommittedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedOffsets",
                            "method_signature": " assignedOffsets(Map<TopicPartition, OffsetAndMetadata> offsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clearOffsets",
                            "method_signature": " clearOffsets(Collection<TopicPartition> topicPartitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetCommitSuccess",
                            "method_signature": " recordOffsetCommitSuccess()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetCommitSkip",
                            "method_signature": " recordOffsetCommitSkip()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "pauseAll",
                            "method_signature": "private pauseAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordPut",
                            "method_signature": " recordPut(long duration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetSequenceNumber",
                            "method_signature": " recordOffsetSequenceNumber(int seqNum)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "convertHeadersFor",
                            "method_signature": "private convertHeadersFor(ConsumerRecord<byte[], byte[]> record)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeThrowAsyncError",
                            "method_signature": "private maybeThrowAsyncError()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": " close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeAllPartitions",
                            "method_signature": "private closeAllPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resumeAll",
                            "method_signature": "private resumeAll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordPartitionCount",
                            "method_signature": " recordPartitionCount(int assignedPartitionCount)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffsets",
                            "method_signature": "private commitOffsets(long now, boolean closing)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetCommitSuccess",
                            "method_signature": " recordOffsetCommitSuccess()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordRead",
                            "method_signature": " recordRead(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordSend",
                            "method_signature": " recordSend(int batchSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordOffsetCommitSkip",
                            "method_signature": " recordOffsetCommitSkip()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rewind",
                            "method_signature": "private rewind()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private pauseAll()": {
                    "first": {
                        "method_name": "pauseAll",
                        "method_signature": "private pauseAll()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.30643395882625846
                },
                " recordPut(long duration)": {
                    "first": {
                        "method_name": "recordPut",
                        "method_signature": " recordPut(long duration)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.30985184734386184
                },
                " recordOffsetSequenceNumber(int seqNum)": {
                    "first": {
                        "method_name": "recordOffsetSequenceNumber",
                        "method_signature": " recordOffsetSequenceNumber(int seqNum)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.32516550856991644
                },
                "private convertHeadersFor(ConsumerRecord<byte[], byte[]> record)": {
                    "first": {
                        "method_name": "convertHeadersFor",
                        "method_signature": "private convertHeadersFor(ConsumerRecord<byte[], byte[]> record)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3278380737788519
                },
                "private maybeThrowAsyncError()": {
                    "first": {
                        "method_name": "maybeThrowAsyncError",
                        "method_signature": "private maybeThrowAsyncError()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3352807770100332
                },
                " close()": {
                    "first": {
                        "method_name": "close",
                        "method_signature": " close()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.33541936655071797
                },
                "private closeAllPartitions()": {
                    "first": {
                        "method_name": "closeAllPartitions",
                        "method_signature": "private closeAllPartitions()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3513300034852954
                },
                "private resumeAll()": {
                    "first": {
                        "method_name": "resumeAll",
                        "method_signature": "private resumeAll()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3789183707565559
                },
                " recordPartitionCount(int assignedPartitionCount)": {
                    "first": {
                        "method_name": "recordPartitionCount",
                        "method_signature": " recordPartitionCount(int assignedPartitionCount)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.408815184847805
                },
                "private commitOffsets(long now, boolean closing)": {
                    "first": {
                        "method_name": "commitOffsets",
                        "method_signature": "private commitOffsets(long now, boolean closing)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4251895998625207
                },
                " recordOffsetCommitSuccess()": {
                    "first": {
                        "method_name": "recordOffsetCommitSuccess",
                        "method_signature": " recordOffsetCommitSuccess()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.44178542710097396
                },
                " recordRead(int batchSize)": {
                    "first": {
                        "method_name": "recordRead",
                        "method_signature": " recordRead(int batchSize)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4473147349366292
                },
                " recordSend(int batchSize)": {
                    "first": {
                        "method_name": "recordSend",
                        "method_signature": " recordSend(int batchSize)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.45038795318655744
                },
                " recordOffsetCommitSkip()": {
                    "first": {
                        "method_name": "recordOffsetCommitSkip",
                        "method_signature": " recordOffsetCommitSkip()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.46758300355079035
                },
                "private rewind()": {
                    "first": {
                        "method_name": "rewind",
                        "method_signature": "private rewind()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.47000183819958635
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "e7fa0edd6351c0949b9082c626597046e9def854",
        "url": "https://github.com/apache/kafka/commit/e7fa0edd6351c0949b9082c626597046e9def854",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public getOrMaybeCreateGroup(groupType Group.GroupType, groupId String) : Group extracted from public testDeleteGroupAllOffsets(groupType Group.GroupType) : void in class org.apache.kafka.coordinator.group.OffsetMetadataManagerTest & moved to class org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2345,
                    "endLine": 2380,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testDeleteGroupAllOffsets(groupType Group.GroupType) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2350,
                    "endLine": 2350,
                    "startColumn": 13,
                    "endColumn": 26,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2356,
                    "endLine": 2356,
                    "startColumn": 13,
                    "endColumn": 27,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2362,
                    "endLine": 2362,
                    "startColumn": 13,
                    "endColumn": 21,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2363,
                    "endLine": 2363,
                    "startColumn": 17,
                    "endColumn": 88,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2351,
                    "endLine": 2354,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2357,
                    "endLine": 2360,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2349,
                    "endLine": 2364,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "SWITCH_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 184,
                    "endLine": 202,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public getOrMaybeCreateGroup(groupType Group.GroupType, groupId String) : Group"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 189,
                    "endLine": 189,
                    "startColumn": 17,
                    "endColumn": 30,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 194,
                    "endLine": 194,
                    "startColumn": 17,
                    "endColumn": 31,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 199,
                    "endLine": 199,
                    "startColumn": 17,
                    "endColumn": 25,
                    "codeElementType": "SWITCH_CASE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 200,
                    "endLine": 200,
                    "startColumn": 21,
                    "endColumn": 92,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 190,
                    "endLine": 193,
                    "startColumn": 21,
                    "endColumn": 23,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 195,
                    "endLine": 198,
                    "startColumn": 21,
                    "endColumn": 23,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 188,
                    "endLine": 201,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "SWITCH_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2391,
                    "endLine": 2412,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testDeleteGroupAllOffsets(groupType Group.GroupType) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 2395,
                    "endLine": 2395,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "context.getOrMaybeCreateGroup(groupType,\"foo\")"
                }
            ],
            "isStatic": false
        },
        "ref_id": 568,
        "extraction_results": {
            "success": true,
            "newCommitHash": "fd74defe96b3f3fc1d3616cb408a0bfdb81ca73a",
            "newBranchName": "extract-getOrMaybeCreateGroup-testDeleteGroupAllOffsets-6ccc19c"
        },
        "telemetry": {
            "id": "c2077ec1-ccb1-4594-9ab4-846187988f6b",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2871,
                "lineStart": 92,
                "lineEnd": 2962,
                "bodyLineStart": 92,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                "sourceCode": "public class OffsetMetadataManagerTest {\n    static class OffsetMetadataManagerTestContext {\n        public static class Builder {\n            private final MockTime time = new MockTime();\n            private final MockCoordinatorTimer<Void, Record> timer = new MockCoordinatorTimer<>(time);\n            private final LogContext logContext = new LogContext();\n            private final SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n            private GroupMetadataManager groupMetadataManager = null;\n            private MetadataImage metadataImage = null;\n            private GroupCoordinatorConfig config = null;\n            private GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n\n            Builder withOffsetMetadataMaxSize(int offsetMetadataMaxSize) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(offsetMetadataMaxSize, 60000L, 24 * 60 * 1000);\n                return this;\n            }\n\n            Builder withOffsetsRetentionMs(long offsetsRetentionMs) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, offsetsRetentionMs);\n                return this;\n            }\n\n            Builder withGroupMetadataManager(GroupMetadataManager groupMetadataManager) {\n                this.groupMetadataManager = groupMetadataManager;\n                return this;\n            }\n\n            OffsetMetadataManagerTestContext build() {\n                if (metadataImage == null) metadataImage = MetadataImage.EMPTY;\n                if (config == null) {\n                    config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, 24 * 60 * 1000);\n                }\n\n                if (groupMetadataManager == null) {\n                    groupMetadataManager = new GroupMetadataManager.Builder()\n                        .withTime(time)\n                        .withTimer(timer)\n                        .withSnapshotRegistry(snapshotRegistry)\n                        .withLogContext(logContext)\n                        .withMetadataImage(metadataImage)\n                        .withConsumerGroupAssignors(Collections.singletonList(new RangeAssignor()))\n                        .withGroupCoordinatorMetricsShard(metrics)\n                        .build();\n                }\n\n                OffsetMetadataManager offsetMetadataManager = new OffsetMetadataManager.Builder()\n                    .withTime(time)\n                    .withLogContext(logContext)\n                    .withSnapshotRegistry(snapshotRegistry)\n                    .withMetadataImage(metadataImage)\n                    .withGroupMetadataManager(groupMetadataManager)\n                    .withGroupCoordinatorConfig(config)\n                    .withGroupCoordinatorMetricsShard(metrics)\n                    .build();\n\n                return new OffsetMetadataManagerTestContext(\n                    time,\n                    timer,\n                    snapshotRegistry,\n                    metrics,\n                    groupMetadataManager,\n                    offsetMetadataManager\n                );\n            }\n        }\n\n        final MockTime time;\n        final MockCoordinatorTimer<Void, Record> timer;\n        final SnapshotRegistry snapshotRegistry;\n        final GroupCoordinatorMetricsShard metrics;\n        final GroupMetadataManager groupMetadataManager;\n        final OffsetMetadataManager offsetMetadataManager;\n\n        long lastCommittedOffset = 0L;\n        long lastWrittenOffset = 0L;\n\n        OffsetMetadataManagerTestContext(\n            MockTime time,\n            MockCoordinatorTimer<Void, Record> timer,\n            SnapshotRegistry snapshotRegistry,\n            GroupCoordinatorMetricsShard metrics,\n            GroupMetadataManager groupMetadataManager,\n            OffsetMetadataManager offsetMetadataManager\n        ) {\n            this.time = time;\n            this.timer = timer;\n            this.snapshotRegistry = snapshotRegistry;\n            this.metrics = metrics;\n            this.groupMetadataManager = groupMetadataManager;\n            this.offsetMetadataManager = offsetMetadataManager;\n        }\n\n        public void commit() {\n            long lastCommittedOffset = this.lastCommittedOffset;\n            this.lastCommittedOffset = lastWrittenOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastCommittedOffset);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, Record> commitOffset(\n            OffsetCommitRequestData request\n        ) {\n            return commitOffset(ApiKeys.OFFSET_COMMIT.latestVersion(), request);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, Record> commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.OFFSET_COMMIT,\n                    version,\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<OffsetCommitResponseData, Record> result = offsetMetadataManager.commitOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public CoordinatorResult<TxnOffsetCommitResponseData, Record> commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.TXN_OFFSET_COMMIT,\n                    ApiKeys.TXN_OFFSET_COMMIT.latestVersion(),\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<TxnOffsetCommitResponseData, Record> result = offsetMetadataManager.commitTransactionalOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(record -> replay(\n                request.producerId(),\n                record\n            ));\n\n            return result;\n        }\n\n        public CoordinatorResult<OffsetDeleteResponseData, Record> deleteOffsets(\n            OffsetDeleteRequestData request\n        ) {\n            CoordinatorResult<OffsetDeleteResponseData, Record> result = offsetMetadataManager.deleteOffsets(request);\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public int deleteAllOffsets(\n            String groupId,\n            List<Record> records\n        ) {\n            List<Record> addedRecords = new ArrayList<>();\n            int numDeletedOffsets = offsetMetadataManager.deleteAllOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return numDeletedOffsets;\n        }\n\n        public boolean cleanupExpiredOffsets(String groupId, List<Record> records) {\n            List<Record> addedRecords = new ArrayList<>();\n            boolean isOffsetsEmptyForGroup = offsetMetadataManager.cleanupExpiredOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return isOffsetsEmptyForGroup;\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            return fetchOffsets(\n                groupId,\n                null,\n                -1,\n                topics,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch)\n                    .setTopics(topics),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        ) {\n            return fetchAllOffsets(\n                groupId,\n                null,\n                -1,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchAllOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<MockCoordinatorTimer.ExpiredTimeout<Void, Record>> sleep(long ms) {\n            time.sleep(ms);\n            List<MockCoordinatorTimer.ExpiredTimeout<Void, Record>> timeouts = timer.poll();\n            timeouts.forEach(timeout -> {\n                if (timeout.result.replayRecords()) {\n                    timeout.result.records().forEach(this::replay);\n                }\n            });\n            return timeouts;\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        ) {\n            commitOffset(\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                time.milliseconds()\n            );\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            commitOffset(\n                RecordBatch.NO_PRODUCER_ID,\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                commitTimestamp\n            );\n        }\n\n        public void commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            replay(producerId, RecordHelpers.newOffsetCommitRecord(\n                groupId,\n                topic,\n                partition,\n                new OffsetAndMetadata(\n                    offset,\n                    OptionalInt.of(leaderEpoch),\n                    \"metadata\",\n                    commitTimestamp,\n                    OptionalLong.empty()\n                ),\n                MetadataVersion.latestTesting()\n            ));\n        }\n\n        public void deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            replay(RecordHelpers.newOffsetCommitTombstoneRecord(\n                groupId,\n                topic,\n                partition\n            ));\n        }\n\n        private ApiMessage messageOrNull(ApiMessageAndVersion apiMessageAndVersion) {\n            if (apiMessageAndVersion == null) {\n                return null;\n            } else {\n                return apiMessageAndVersion.message();\n            }\n        }\n\n        private void replay(\n            Record record\n        ) {\n            replay(\n                RecordBatch.NO_PRODUCER_ID,\n                record\n            );\n        }\n\n        private void replay(\n            long producerId,\n            Record record\n        ) {\n            snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n\n            ApiMessageAndVersion key = record.key();\n            ApiMessageAndVersion value = record.value();\n\n            if (key == null) {\n                throw new IllegalStateException(\"Received a null key in \" + record);\n            }\n\n            switch (key.version()) {\n                case OffsetCommitKey.HIGHEST_SUPPORTED_VERSION:\n                    offsetMetadataManager.replay(\n                        lastWrittenOffset,\n                        producerId,\n                        (OffsetCommitKey) key.message(),\n                        (OffsetCommitValue) messageOrNull(value)\n                    );\n                    break;\n\n                default:\n                    throw new IllegalStateException(\"Received an unknown record type \" + key.version()\n                        + \" in \" + record);\n            }\n\n            lastWrittenOffset++;\n        }\n\n        private void replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        ) {\n            snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n            offsetMetadataManager.replayEndTransactionMarker(producerId, result);\n            lastWrittenOffset++;\n        }\n\n        public void testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        ) {\n            final OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection requestTopicCollection =\n                new OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection(Collections.singletonList(\n                    new OffsetDeleteRequestData.OffsetDeleteRequestTopic()\n                        .setName(topic)\n                        .setPartitions(Collections.singletonList(\n                            new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(partition)\n                        ))\n                ).iterator());\n\n            final OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection expectedResponsePartitionCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection();\n            if (hasOffset(groupId, topic, partition)) {\n                expectedResponsePartitionCollection.add(\n                    new OffsetDeleteResponseData.OffsetDeleteResponsePartition()\n                        .setPartitionIndex(partition)\n                        .setErrorCode(expectedError.code())\n                );\n            }\n\n            final OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection expectedResponseTopicCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection(Collections.singletonList(\n                    new OffsetDeleteResponseData.OffsetDeleteResponseTopic()\n                        .setName(topic)\n                        .setPartitions(expectedResponsePartitionCollection)\n                ).iterator());\n\n            List<Record> expectedRecords = Collections.emptyList();\n            if (hasOffset(groupId, topic, partition) && expectedError == Errors.NONE) {\n                expectedRecords = Collections.singletonList(\n                    RecordHelpers.newOffsetCommitTombstoneRecord(groupId, topic, partition)\n                );\n            }\n\n            final CoordinatorResult<OffsetDeleteResponseData, Record> coordinatorResult = deleteOffsets(\n                new OffsetDeleteRequestData()\n                    .setGroupId(groupId)\n                    .setTopics(requestTopicCollection)\n            );\n\n            assertEquals(new OffsetDeleteResponseData().setTopics(expectedResponseTopicCollection), coordinatorResult.response());\n            assertEquals(expectedRecords, coordinatorResult.records());\n        }\n\n        public boolean hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            return offsetMetadataManager.offset(groupId, topic, partition) != null;\n        }\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testOffsetCommitWithUnknownGroup(short version) {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        Class<? extends Throwable> expectedType;\n        if (version >= 9) {\n            expectedType = GroupIdNotFoundException.class;\n        } else {\n            expectedType = IllegalGenerationException.class;\n        }\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(expectedType, () -> context.commitOffset(\n            version,\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(CoordinatorNotAvailableException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithIllegalGeneration() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(IllegalGenerationException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithUnknownInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member without static id.\n        group.add(mkGenericMember(\"member\", Optional.empty()));\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGroupInstanceId(\"instanceid\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithFencedInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member with static id.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGroupInstanceId(\"old-instance-id\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWhileInCompletingRebalanceState() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(RebalanceInProgressException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithoutMemberIdAndGeneration() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitWithRetentionTime() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.of(context.time.milliseconds() + 1234L)\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testGenericGroupOffsetCommitMaintainsSession() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        // Schedule session timeout. This would be normally done when\n        // the group transitions to stable.\n        context.groupMetadataManager.rescheduleClassicGroupMemberHeartbeat(group, member);\n\n        // Advance time by half of the session timeout. No timeouts are\n        // expired.\n        assertEquals(Collections.emptyList(), context.sleep(5000 / 2));\n\n        // Commit.\n        context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        // Advance time by half of the session timeout. No timeouts are\n        // expired.\n        assertEquals(Collections.emptyList(), context.sleep(5000 / 2));\n\n        // Advance time by half of the session timeout again. The timeout should\n        // expire and the member is removed from the group.\n        List<MockCoordinatorTimer.ExpiredTimeout<Void, Record>> timeouts =\n            context.sleep(5000 / 2);\n        assertEquals(1, timeouts.size());\n        assertFalse(group.hasMemberId(member.memberId()));\n    }\n\n    @Test\n    public void testSimpleGroupOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n\n        // A generic should have been created.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            false\n        );\n        assertNotNull(group);\n        assertEquals(\"foo\", group.groupId());\n    }\n\n    @Test\n    public void testSimpleGroupOffsetCommitWithInstanceId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                // Instance id should be ignored.\n                .setGroupInstanceId(\"instance-id\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnknownMemberIdException.class, () -> context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        OffsetCommitRequestData request = new OffsetCommitRequestData()\n            .setGroupId(\"foo\")\n            .setMemberId(\"member\")\n            .setGenerationIdOrMemberEpoch(9)\n            .setTopics(Collections.singletonList(\n                new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                    .setName(\"bar\")\n                    .setPartitions(Collections.singletonList(\n                        new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                            .setPartitionIndex(0)\n                            .setCommittedOffset(100L)\n                    ))\n            ));\n\n        // Verify that a smaller epoch is rejected.\n        assertThrows(StaleMemberEpochException.class, () -> context.commitOffset(request));\n\n        // Verify that a larger epoch is rejected.\n        request.setGenerationIdOrMemberEpoch(11);\n        assertThrows(StaleMemberEpochException.class, () -> context.commitOffset(request));\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testConsumerGroupOffsetCommitWithVersionSmallerThanVersion9(short version) {\n        // All the newer versions are fine.\n        if (version >= 9) return;\n        // Version 0 does not support MemberId and GenerationIdOrMemberEpoch fields.\n        if (version == 0) return;\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        // Verify that the request is rejected with the correct exception.\n        assertThrows(UnsupportedVersionException.class, () -> context.commitOffset(\n            version,\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(9)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n            )\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitFromAdminClient() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.empty(),\n                    \"\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                                .setCommitTimestamp(context.time.milliseconds())\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupOffsetCommitWithOffsetMetadataTooLarge() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withOffsetMetadataMaxSize(5)\n            .build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(10)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"toolarge\")\n                                .setCommitTimestamp(context.time.milliseconds()),\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(1)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"small\")\n                                .setCommitTimestamp(context.time.milliseconds())\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new OffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitResponseData.OffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.OFFSET_METADATA_TOO_LARGE.code()),\n                            new OffsetCommitResponseData.OffsetCommitResponsePartition()\n                                .setPartitionIndex(1)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                1,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"small\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        CoordinatorResult<TxnOffsetCommitResponseData, Record> result = context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new TxnOffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitResponseData.TxnOffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitResponseData.TxnOffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithUnknownGroupId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        assertThrows(UnknownMemberIdException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testConsumerGroupTransactionalOffsetCommitWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setTargetMemberEpoch(10)\n            .setPreviousMemberEpoch(10)\n            .build()\n        );\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(100)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<TxnOffsetCommitResponseData, Record> result = context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(1)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        );\n\n        assertEquals(\n            new TxnOffsetCommitResponseData()\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitResponseData.TxnOffsetCommitResponseTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitResponseData.TxnOffsetCommitResponsePartition()\n                                .setPartitionIndex(0)\n                                .setErrorCode(Errors.NONE.code())\n                        ))\n                )),\n            result.response()\n        );\n\n        assertEquals(\n            Collections.singletonList(RecordHelpers.newOffsetCommitRecord(\n                \"foo\",\n                \"bar\",\n                0,\n                new OffsetAndMetadata(\n                    100L,\n                    OptionalInt.of(10),\n                    \"metadata\",\n                    context.time.milliseconds(),\n                    OptionalLong.empty()\n                ),\n                MetadataImage.EMPTY.features().metadataVersion()\n            )),\n            result.records()\n        );\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithUnknownGroupId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        assertThrows(UnknownMemberIdException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(10)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupTransactionalOffsetCommitWithIllegalGenerationId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        ClassicGroupMember member = mkGenericMember(\"member\", Optional.empty());\n        group.add(member);\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        assertThrows(IllegalGenerationException.class, () -> context.commitTransactionalOffset(\n            new TxnOffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationId(100)\n                .setTopics(Collections.singletonList(\n                    new TxnOffsetCommitRequestData.TxnOffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Collections.singletonList(\n                            new TxnOffsetCommitRequestData.TxnOffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L)\n                                .setCommittedLeaderEpoch(10)\n                                .setCommittedMetadata(\"metadata\")\n                        ))\n                ))\n        ));\n    }\n\n    @Test\n    public void testGenericGroupFetchOffsetsWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"group\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        List<OffsetFetchResponseData.OffsetFetchResponseTopics> expectedResponse = Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Collections.singletonList(\n                    mkInvalidOffsetPartitionResponse(0)\n                ))\n        );\n\n        assertEquals(expectedResponse, context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsWithUnknownGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        List<OffsetFetchResponseData.OffsetFetchResponseTopics> expectedResponse = Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Collections.singletonList(\n                    mkInvalidOffsetPartitionResponse(0)\n                ))\n        );\n\n        assertEquals(expectedResponse, context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsAtDifferentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        assertEquals(0, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        assertEquals(1, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        assertEquals(2, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n        assertEquals(3, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 111L, 2);\n        assertEquals(4, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 1, 210L, 2);\n        assertEquals(5, context.lastWrittenOffset);\n\n        // Always use the same request.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Arrays.asList(0, 1))\n        );\n\n        // Fetching with 0 should return all invalid offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 0L));\n\n        // Fetching with 1 should return data up to offset 1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 1L));\n\n        // Fetching with 2 should return data up to offset 2.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkInvalidOffsetPartitionResponse(0),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 2L));\n\n        // Fetching with 3 should return data up to offset 3.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 3L));\n\n        // Fetching with 4 should return data up to offset 4.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, 4L));\n\n        // Fetching with 5 should return data up to offset 5.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, 5L));\n\n        // Fetching with Long.MAX_VALUE should return all offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchOffsetsWithPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n\n        context.commit();\n\n        assertEquals(3, context.lastWrittenOffset);\n        assertEquals(3, context.lastCommittedOffset);\n\n        context.commitOffset(10L, \"group\", \"foo\", 1, 111L, 1, context.time.milliseconds());\n        context.commitOffset(10L, \"group\", \"bar\", 0, 201L, 1, context.time.milliseconds());\n        // Note that bar-1 does not exist in the initial commits. UNSTABLE_OFFSET_COMMIT errors\n        // must be returned in this case too.\n        context.commitOffset(10L, \"group\", \"bar\", 1, 211L, 1, context.time.milliseconds());\n\n        // Always use the same request.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> request = Arrays.asList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Arrays.asList(0, 1)),\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"bar\")\n                .setPartitionIndexes(Arrays.asList(0, 1))\n        );\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should return the committed offset for\n        // foo-0 and the UNSTABLE_OFFSET_COMMIT error for foo-1, bar-0 and bar-1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, Errors.UNSTABLE_OFFSET_COMMIT),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n\n        // Fetching offsets without \"require stable\" (lastCommittedOffset) should return the committed\n        // offset for foo-0, foo-1 and bar-0 and the INVALID_OFFSET for bar-1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkInvalidOffsetPartitionResponse(1)\n                ))\n        ), context.fetchOffsets(\"group\", request, context.lastCommittedOffset));\n\n        // Commit the ongoing transaction.\n        context.replayEndTransactionMarker(10L, TransactionResult.COMMIT);\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should not return any errors now.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 201L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 211L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", request, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testGenericGroupFetchAllOffsetsWithDeadGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create a dead group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"group\",\n            true\n        );\n        group.transitionTo(ClassicGroupState.DEAD);\n\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsWithUnknownGroup() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsAtDifferentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        assertEquals(0, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        assertEquals(1, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        assertEquals(2, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n        assertEquals(3, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"foo\", 1, 111L, 2);\n        assertEquals(4, context.lastWrittenOffset);\n        context.commitOffset(\"group\", \"bar\", 1, 210L, 2);\n        assertEquals(5, context.lastWrittenOffset);\n\n        // Fetching with 0 should no offsets.\n        assertEquals(Collections.emptyList(), context.fetchAllOffsets(\"group\", 0L));\n\n        // Fetching with 1 should return data up to offset 1.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 1L));\n\n        // Fetching with 2 should return data up to offset 2.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 2L));\n\n        // Fetching with 3 should return data up to offset 3.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 3L));\n\n        // Fetching with 4 should return data up to offset 4.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", 4L));\n\n        // Fetching with Long.MAX_VALUE should return all offsets.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 210L, 2, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 2, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testFetchAllOffsetsWithPendingTransactionalOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n        context.commitOffset(\"group\", \"foo\", 1, 110L, 1);\n        context.commitOffset(\"group\", \"bar\", 0, 200L, 1);\n\n        context.commit();\n\n        assertEquals(3, context.lastWrittenOffset);\n        assertEquals(3, context.lastCommittedOffset);\n\n        context.commitOffset(10L, \"group\", \"foo\", 1, 111L, 1, context.time.milliseconds());\n        context.commitOffset(10L, \"group\", \"bar\", 0, 201L, 1, context.time.milliseconds());\n        // Note that bar-1 does not exist in the initial commits. The API does not return it at all until\n        // the transaction is committed.\n        context.commitOffset(10L, \"group\", \"bar\", 1, 211L, 1, context.time.milliseconds());\n\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should return the committed offset for\n        // foo-0 and the UNSTABLE_OFFSET_COMMIT error for foo-1 and bar-0.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, Errors.UNSTABLE_OFFSET_COMMIT)\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, Errors.UNSTABLE_OFFSET_COMMIT)\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n\n        // Fetching offsets without \"require stable\" (lastCommittedOffset) should the committed\n        // offset for the foo-0, foo-1 and bar-0.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 200L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 110L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", context.lastCommittedOffset));\n\n        // Commit the ongoing transaction.\n        context.replayEndTransactionMarker(10L, TransactionResult.COMMIT);\n\n        // Fetching offsets with \"require stable\" (Long.MAX_VALUE) should not return any errors now.\n        assertEquals(Arrays.asList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"bar\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 201L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 211L, 1, \"metadata\")\n                )),\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Arrays.asList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\"),\n                    mkOffsetPartitionResponse(1, 111L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithMemberIdAndEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        // Create consumer group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n        // Create member.\n        group.getOrMaybeCreateMember(\"member\", true);\n        // Commit offset.\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", \"member\", 0, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", \"member\", 0, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchFromAdminClient() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        // Create consumer group.\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n        // Create member.\n        group.getOrMaybeCreateMember(\"member\", true);\n        // Commit offset.\n        context.commitOffset(\"group\", \"foo\", 0, 100L, 1);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchOffsets(\"group\", topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertEquals(Collections.singletonList(\n            new OffsetFetchResponseData.OffsetFetchResponseTopics()\n                .setName(\"foo\")\n                .setPartitions(Collections.singletonList(\n                    mkOffsetPartitionResponse(0, 100L, 1, \"metadata\")\n                ))\n        ), context.fetchAllOffsets(\"group\", Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithUnknownMemberId() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        // Fetch offsets cases.\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchOffsets(\"group\", \"\", 0, topics, Long.MAX_VALUE));\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchOffsets(\"group\", \"member\", 0, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets cases.\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchAllOffsets(\"group\", \"\", 0, Long.MAX_VALUE));\n        assertThrows(UnknownMemberIdException.class,\n            () -> context.fetchAllOffsets(\"group\", \"member\", 0, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testConsumerGroupOffsetFetchWithStaleMemberEpoch() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\"group\", true);\n        group.getOrMaybeCreateMember(\"member\", true);\n\n        // Fetch offsets case.\n        List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics = Collections.singletonList(\n            new OffsetFetchRequestData.OffsetFetchRequestTopics()\n                .setName(\"foo\")\n                .setPartitionIndexes(Collections.singletonList(0))\n        );\n\n        // Fetch offsets case.\n        assertThrows(StaleMemberEpochException.class,\n            () -> context.fetchOffsets(\"group\", \"member\", 10, topics, Long.MAX_VALUE));\n\n        // Fetch all offsets case.\n        assertThrows(StaleMemberEpochException.class,\n            () -> context.fetchAllOffsets(\"group\", \"member\", 10, Long.MAX_VALUE));\n    }\n\n    @Test\n    public void testGenericGroupOffsetDelete() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        group.setSubscribedTopics(Optional.of(Collections.emptySet()));\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.NONE);\n    }\n\n    @Test\n    public void testGenericGroupOffsetDeleteWithErrors() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n        group.setSubscribedTopics(Optional.of(Collections.singleton(\"bar\")));\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n\n        // Delete the offset whose topic partition doesn't exist.\n        context.testOffsetDeleteWith(\"foo\", \"bar1\", 0, Errors.NONE);\n        // Delete the offset from the topic that the group is subscribed to.\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.GROUP_SUBSCRIBED_TO_TOPIC);\n    }\n\n    @Test\n    public void testConsumerGroupOffsetDelete() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        assertFalse(group.isSubscribedToTopic(\"bar\"));\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.NONE);\n    }\n\n    @Test\n    public void testConsumerGroupOffsetDeleteWithErrors() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ConsumerGroup group = context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n            \"foo\",\n            true\n        );\n        MetadataImage image = new GroupMetadataManagerTest.MetadataImageBuilder()\n            .addTopic(Uuid.randomUuid(), \"foo\", 1)\n            .addRacks()\n            .build();\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        group.computeSubscriptionMetadata(\n            null,\n            member1,\n            image.topics(),\n            image.cluster()\n        );\n        group.updateMember(member1);\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        assertTrue(group.isSubscribedToTopic(\"bar\"));\n\n        // Delete the offset whose topic partition doesn't exist.\n        context.testOffsetDeleteWith(\"foo\", \"bar1\", 0, Errors.NONE);\n        // Delete the offset from the topic that the group is subscribed to.\n        context.testOffsetDeleteWith(\"foo\", \"bar\", 0, Errors.GROUP_SUBSCRIBED_TO_TOPIC);\n    }\n\n    @ParameterizedTest\n    @EnumSource(Group.GroupType.class)\n    public void testDeleteGroupAllOffsets(Group.GroupType groupType) {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        getOrMaybeCreateGroup(groupType, context);\n        context.commitOffset(\"foo\", \"bar-0\", 0, 100L, 0);\n        context.commitOffset(\"foo\", \"bar-0\", 1, 100L, 0);\n        context.commitOffset(\"foo\", \"bar-1\", 0, 100L, 0);\n\n        List<Record> expectedRecords = Arrays.asList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-1\", 0),\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-0\", 0),\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"foo\", \"bar-0\", 1)\n        );\n\n        List<Record> records = new ArrayList<>();\n        int numDeleteOffsets = context.deleteAllOffsets(\"foo\", records);\n\n        assertEquals(expectedRecords, records);\n        assertEquals(3, numDeleteOffsets);\n    }\n\n    private void getOrMaybeCreateGroup(Group.GroupType groupType, OffsetMetadataManagerTestContext context) {\n        switch (groupType) {\n            case CLASSIC:\n                context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n                    \"foo\",\n                    true\n                );\n                break;\n            case CONSUMER:\n                context.groupMetadataManager.getOrMaybeCreateConsumerGroup(\n                    \"foo\",\n                    true\n                );\n                break;\n            default:\n                throw new IllegalArgumentException(\"Invalid group type: \" + groupType);\n        }\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsGroupHasNoOffsets() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .build();\n\n        List<Record> records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"unknown-group-id\", records));\n        assertEquals(Collections.emptyList(), records);\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsGroupDoesNotExist() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .build();\n\n        when(groupMetadataManager.group(\"unknown-group-id\")).thenThrow(GroupIdNotFoundException.class);\n        context.commitOffset(\"unknown-group-id\", \"topic\", 0, 100L, 0);\n        assertThrows(GroupIdNotFoundException.class, () -> context.cleanupExpiredOffsets(\"unknown-group-id\", new ArrayList<>()));\n    }\n\n    @Test\n    public void testCleanupExpiredOffsetsEmptyOffsetExpirationCondition() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .build();\n\n        context.commitOffset(\"group-id\", \"topic\", 0, 100L, 0);\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.empty());\n\n        List<Record> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(Collections.emptyList(), records);\n    }\n\n    @Test\n    public void testCleanupExpiredOffsets() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .withOffsetsRetentionMs(1000)\n            .build();\n\n        long commitTimestamp = context.time.milliseconds();\n\n        context.commitOffset(\"group-id\", \"firstTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 1, 100L, 0, commitTimestamp + 500);\n\n        context.time.sleep(1000);\n\n        // firstTopic-0: group is still subscribed to firstTopic. Do not expire.\n        // secondTopic-0: should expire as offset retention has passed.\n        // secondTopic-1: has not passed offset retention. Do not expire.\n        List<Record> expectedRecords = Collections.singletonList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.of(\n            new OffsetExpirationConditionImpl(offsetAndMetadata -> offsetAndMetadata.commitTimestampMs)));\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(true);\n        when(group.isSubscribedToTopic(\"secondTopic\")).thenReturn(false);\n\n        List<Record> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Expire secondTopic-1.\n        context.time.sleep(500);\n        expectedRecords = Collections.singletonList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 1)\n        );\n\n        records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Add 2 more commits, then expire all.\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(false);\n        context.commitOffset(\"group-id\", \"firstTopic\", 1, 100L, 0, commitTimestamp + 500);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 101L, 0, commitTimestamp + 500);\n\n        expectedRecords = Arrays.asList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"firstTopic\", 0),\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"firstTopic\", 1),\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n    }\n\n    static private OffsetFetchResponseData.OffsetFetchResponsePartitions mkOffsetPartitionResponse(\n        int partition,\n        long offset,\n        int leaderEpoch,\n        String metadata\n    ) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setCommittedOffset(offset)\n            .setCommittedLeaderEpoch(leaderEpoch)\n            .setMetadata(metadata);\n    }\n\n    static private OffsetFetchResponseData.OffsetFetchResponsePartitions mkInvalidOffsetPartitionResponse(int partition) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setCommittedOffset(INVALID_OFFSET)\n            .setCommittedLeaderEpoch(-1)\n            .setMetadata(\"\");\n    }\n\n    static private OffsetFetchResponseData.OffsetFetchResponsePartitions mkOffsetPartitionResponse(int partition, Errors error) {\n        return new OffsetFetchResponseData.OffsetFetchResponsePartitions()\n            .setPartitionIndex(partition)\n            .setErrorCode(error.code())\n            .setCommittedOffset(INVALID_OFFSET)\n            .setCommittedLeaderEpoch(-1)\n            .setMetadata(\"\");\n    }\n\n    @Test\n    public void testReplay() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            200L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            2L,\n            200L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyReplay(context, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            3L,\n            300L,\n            OptionalInt.of(10),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.of(12345L)\n        ));\n    }\n\n    @Test\n    public void testTransactionalReplay() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        verifyTransactionalReplay(context, 5, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"bar\", \"zar\", 0, new OffsetAndMetadata(\n            2L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 5, \"bar\", \"zar\", 1, new OffsetAndMetadata(\n            3L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 6, \"foo\", \"bar\", 2, new OffsetAndMetadata(\n            4L,\n            102L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        verifyTransactionalReplay(context, 6, \"foo\", \"bar\", 3, new OffsetAndMetadata(\n            5L,\n            102L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n    }\n\n    @Test\n    public void testReplayWithTombstone() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Verify replay adds the offset the map.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Create a tombstone record and replay it to delete the record.\n        context.replay(RecordHelpers.newOffsetCommitTombstoneRecord(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // Verify that the offset is gone.\n        assertNull(context.offsetMetadataManager.offset(\"foo\", \"bar\", 0));\n    }\n\n    @Test\n    public void testReplayTransactionEndMarkerWithCommit() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Add regular offset commit.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            99L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add pending transactional commit for producer id 5.\n        verifyTransactionalReplay(context, 5L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add pending transactional commit for producer id 6.\n        verifyTransactionalReplay(context, 6L, \"foo\", \"bar\", 1, new OffsetAndMetadata(\n            2L,\n            200L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Replaying an end marker with an unknown producer id should not fail.\n        context.replayEndTransactionMarker(1L, TransactionResult.COMMIT);\n\n        // Replaying an end marker to commit transaction of producer id 5.\n        context.replayEndTransactionMarker(5L, TransactionResult.COMMIT);\n\n        // The pending offset is removed...\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            5L,\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // ... and added to the main offset storage.\n        assertEquals(new OffsetAndMetadata(\n            1L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ), context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // Replaying an end marker to abort transaction of producer id 6.\n        context.replayEndTransactionMarker(6L, TransactionResult.ABORT);\n\n        // The pending offset is removed from the pending offsets and\n        // it is not added to the main offset storage.\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            6L,\n            \"foo\",\n            \"bar\",\n            1\n        ));\n        assertNull(context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            1\n        ));\n    }\n\n    @Test\n    public void testReplayTransactionEndMarkerKeepsTheMostRecentCommittedOffset() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Add pending transactional offset commit for producer id 5.\n        verifyTransactionalReplay(context, 5L, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            0L,\n            100L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Add regular offset commit.\n        verifyReplay(context, \"foo\", \"bar\", 0, new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ));\n\n        // Replaying an end marker to commit transaction of producer id 5.\n        context.replayEndTransactionMarker(5L, TransactionResult.COMMIT);\n\n        // The pending offset is removed...\n        assertNull(context.offsetMetadataManager.pendingTransactionalOffset(\n            5L,\n            \"foo\",\n            \"bar\",\n            0\n        ));\n\n        // ... but it is not added to the main storage because the regular\n        // committed offset is more recent.\n        assertEquals(new OffsetAndMetadata(\n            1L,\n            101L,\n            OptionalInt.empty(),\n            \"small\",\n            context.time.milliseconds(),\n            OptionalLong.empty()\n        ), context.offsetMetadataManager.offset(\n            \"foo\",\n            \"bar\",\n            0\n        ));\n    }\n\n    @Test\n    public void testOffsetCommitsSensor() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n\n        // Create an empty group.\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\n            \"foo\",\n            true\n        );\n\n        // Add member.\n        group.add(mkGenericMember(\"member\", Optional.of(\"new-instance-id\")));\n\n        // Transition to next generation.\n        group.transitionTo(ClassicGroupState.PREPARING_REBALANCE);\n        group.initNextGeneration();\n        assertEquals(1, group.generationId());\n        group.transitionTo(ClassicGroupState.STABLE);\n\n        CoordinatorResult<OffsetCommitResponseData, Record> result = context.commitOffset(\n            new OffsetCommitRequestData()\n                .setGroupId(\"foo\")\n                .setMemberId(\"member\")\n                .setGenerationIdOrMemberEpoch(1)\n                .setRetentionTimeMs(1234L)\n                .setTopics(Collections.singletonList(\n                    new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                        .setName(\"bar\")\n                        .setPartitions(Arrays.asList(\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(0)\n                                .setCommittedOffset(100L),\n                            new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                                .setPartitionIndex(1)\n                                .setCommittedOffset(150L)\n                        ))\n                ))\n        );\n\n        verify(context.metrics).record(OFFSET_COMMITS_SENSOR_NAME, 2);\n    }\n\n    @Test\n    public void testOffsetsExpiredSensor() {\n        GroupMetadataManager groupMetadataManager = mock(GroupMetadataManager.class);\n        Group group = mock(Group.class);\n\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder()\n            .withGroupMetadataManager(groupMetadataManager)\n            .withOffsetsRetentionMs(1000)\n            .build();\n\n        long commitTimestamp = context.time.milliseconds();\n\n        context.commitOffset(\"group-id\", \"firstTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 100L, 0, commitTimestamp);\n        context.commitOffset(\"group-id\", \"secondTopic\", 1, 100L, 0, commitTimestamp + 500);\n\n        context.time.sleep(1000);\n\n        // firstTopic-0: group is still subscribed to firstTopic. Do not expire.\n        // secondTopic-0: should expire as offset retention has passed.\n        // secondTopic-1: has not passed offset retention. Do not expire.\n        List<Record> expectedRecords = Collections.singletonList(\n            RecordHelpers.newOffsetCommitTombstoneRecord(\"group-id\", \"secondTopic\", 0)\n        );\n\n        when(groupMetadataManager.group(\"group-id\")).thenReturn(group);\n        when(group.offsetExpirationCondition()).thenReturn(Optional.of(\n            new OffsetExpirationConditionImpl(offsetAndMetadata -> offsetAndMetadata.commitTimestampMs)));\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(true);\n        when(group.isSubscribedToTopic(\"secondTopic\")).thenReturn(false);\n\n        List<Record> records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        assertEquals(expectedRecords, records);\n\n        // Expire secondTopic-1.\n        context.time.sleep(500);\n\n        records = new ArrayList<>();\n        assertFalse(context.cleanupExpiredOffsets(\"group-id\", records));\n        verify(context.metrics, times(2)).record(OFFSET_EXPIRED_SENSOR_NAME, 1);\n\n        // Add 2 more commits, then expire all.\n        when(group.isSubscribedToTopic(\"firstTopic\")).thenReturn(false);\n        context.commitOffset(\"group-id\", \"firstTopic\", 1, 100L, 0, commitTimestamp + 500);\n        context.commitOffset(\"group-id\", \"secondTopic\", 0, 101L, 0, commitTimestamp + 500);\n\n        records = new ArrayList<>();\n        assertTrue(context.cleanupExpiredOffsets(\"group-id\", records));\n        verify(context.metrics).record(OFFSET_EXPIRED_SENSOR_NAME, 3);\n    }\n\n    @Test\n    public void testOffsetDeletionsSensor() {\n        OffsetMetadataManagerTestContext context = new OffsetMetadataManagerTestContext.Builder().build();\n        ClassicGroup group = context.groupMetadataManager.getOrMaybeCreateClassicGroup(\"foo\", true);\n\n        context.commitOffset(\"foo\", \"bar\", 0, 100L, 0);\n        context.commitOffset(\"foo\", \"bar\", 1, 150L, 0);\n        group.setSubscribedTopics(Optional.of(Collections.emptySet()));\n\n        OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection requestTopicCollection =\n            new OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection(Collections.singletonList(\n                new OffsetDeleteRequestData.OffsetDeleteRequestTopic()\n                    .setName(\"bar\")\n                    .setPartitions(Arrays.asList(\n                        new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(0),\n                        new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(1)\n                    ))\n            ).iterator());\n\n        context.deleteOffsets(\n            new OffsetDeleteRequestData()\n                .setGroupId(\"foo\")\n                .setTopics(requestTopicCollection)\n        );\n\n        verify(context.metrics).record(OFFSET_DELETIONS_SENSOR_NAME, 2);\n    }\n\n    private void verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    ) {\n        context.replay(RecordHelpers.newOffsetCommitRecord(\n            groupId,\n            topic,\n            partition,\n            offsetAndMetadata,\n            MetadataImage.EMPTY.features().metadataVersion()\n        ));\n\n        assertEquals(offsetAndMetadata, context.offsetMetadataManager.offset(\n            groupId,\n            topic,\n            partition\n        ));\n    }\n\n    private void verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    ) {\n        context.replay(producerId, RecordHelpers.newOffsetCommitRecord(\n            groupId,\n            topic,\n            partition,\n            offsetAndMetadata,\n            MetadataImage.EMPTY.features().metadataVersion()\n        ));\n\n        assertEquals(offsetAndMetadata, context.offsetMetadataManager.pendingTransactionalOffset(\n            producerId,\n            groupId,\n            topic,\n            partition\n        ));\n    }\n\n    private ClassicGroupMember mkGenericMember(\n        String memberId,\n        Optional<String> groupInstanceId\n    ) {\n        return new ClassicGroupMember(\n            memberId,\n            groupInstanceId,\n            \"client-id\",\n            \"host\",\n            5000,\n            5000,\n            \"consumer\",\n            new JoinGroupRequestData.JoinGroupRequestProtocolCollection(\n                Collections.singletonList(new JoinGroupRequestData.JoinGroupRequestProtocol()\n                    .setName(\"range\")\n                    .setMetadata(new byte[0])\n                ).iterator()\n            )\n        );\n    }\n}",
                "methodCount": 89
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetsRetentionMs",
                            "method_signature": " withOffsetsRetentionMs(long offsetsRetentionMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupMetadataManager",
                            "method_signature": " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public deleteAllOffsets(\n            String groupId,\n            List<Record> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<Record> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            Record record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            Record record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testOffsetDeleteWith",
                            "method_signature": "public testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testOffsetCommitWithUnknownGroup",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testOffsetCommitWithUnknownGroup(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testConsumerGroupOffsetCommitWithVersionSmallerThanVersion9",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testConsumerGroupOffsetCommitWithVersionSmallerThanVersion9(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testDeleteGroupAllOffsets",
                            "method_signature": "@ParameterizedTest\n    @EnumSource(Group.GroupType.class)\n    public testDeleteGroupAllOffsets(Group.GroupType groupType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "private getOrMaybeCreateGroup(Group.GroupType groupType, OffsetMetadataManagerTestContext context)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkOffsetPartitionResponse",
                            "method_signature": "static private mkOffsetPartitionResponse(\n        int partition,\n        long offset,\n        int leaderEpoch,\n        String metadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkInvalidOffsetPartitionResponse",
                            "method_signature": "static private mkInvalidOffsetPartitionResponse(int partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkOffsetPartitionResponse",
                            "method_signature": "static private mkOffsetPartitionResponse(int partition, Errors error)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyReplay",
                            "method_signature": "private verifyReplay(\n        OffsetMetadataManagerTestContext context,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyTransactionalReplay",
                            "method_signature": "private verifyTransactionalReplay(\n        OffsetMetadataManagerTestContext context,\n        long producerId,\n        String groupId,\n        String topic,\n        int partition,\n        OffsetAndMetadata offsetAndMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkGenericMember",
                            "method_signature": "private mkGenericMember(\n        String memberId,\n        Optional<String> groupInstanceId\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            Record record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupMetadataManager",
                            "method_signature": " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkGenericMember",
                            "method_signature": "private mkGenericMember(\n        String memberId,\n        Optional<String> groupInstanceId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetsRetentionMs",
                            "method_signature": " withOffsetsRetentionMs(long offsetsRetentionMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            Record record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mkOffsetPartitionResponse",
                            "method_signature": "static private mkOffsetPartitionResponse(int partition, Errors error)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<Record> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)": {
                    "first": {
                        "method_name": "messageOrNull",
                        "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.1815607539832357
                },
                "private replay(\n            Record record\n        )": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "private replay(\n            Record record\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.27486207791409717
                },
                " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)": {
                    "first": {
                        "method_name": "withGroupMetadataManager",
                        "method_signature": " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.31268322465232795
                },
                "public sleep(long ms)": {
                    "first": {
                        "method_name": "sleep",
                        "method_signature": "public sleep(long ms)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3748306717738077
                },
                "private mkGenericMember(\n        String memberId,\n        Optional<String> groupInstanceId\n    )": {
                    "first": {
                        "method_name": "mkGenericMember",
                        "method_signature": "private mkGenericMember(\n        String memberId,\n        Optional<String> groupInstanceId\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3979626515726731
                },
                "public commit()": {
                    "first": {
                        "method_name": "commit",
                        "method_signature": "public commit()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4846832946850268
                },
                " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)": {
                    "first": {
                        "method_name": "withOffsetMetadataMaxSize",
                        "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5295902043058993
                },
                " withOffsetsRetentionMs(long offsetsRetentionMs)": {
                    "first": {
                        "method_name": "withOffsetsRetentionMs",
                        "method_signature": " withOffsetsRetentionMs(long offsetsRetentionMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5319038879652762
                },
                "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                    "first": {
                        "method_name": "hasOffset",
                        "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.540624935199125
                },
                "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )": {
                    "first": {
                        "method_name": "replayEndTransactionMarker",
                        "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.542963306365468
                },
                "public commitOffset(\n            OffsetCommitRequestData request\n        )": {
                    "first": {
                        "method_name": "commitOffset",
                        "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5509469827082678
                },
                "private replay(\n            long producerId,\n            Record record\n        )": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "private replay(\n            long producerId,\n            Record record\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5524064613788529
                },
                "static private mkOffsetPartitionResponse(int partition, Errors error)": {
                    "first": {
                        "method_name": "mkOffsetPartitionResponse",
                        "method_signature": "static private mkOffsetPartitionResponse(int partition, Errors error)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5602206369140231
                },
                "public cleanupExpiredOffsets(String groupId, List<Record> records)": {
                    "first": {
                        "method_name": "cleanupExpiredOffsets",
                        "method_signature": "public cleanupExpiredOffsets(String groupId, List<Record> records)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5656372252761493
                },
                " build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": " build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5716456623381946
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "0472db2cd39d4f53c3855063117bdae15f4d0d86",
        "url": "https://github.com/apache/kafka/commit/0472db2cd39d4f53c3855063117bdae15f4d0d86",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public combineFutures(futures List<CompletableFuture<T>>, init Supplier<T>, add BiConsumer<T,T>) : CompletableFuture<T> extracted from public consumerGroupDescribe(context RequestContext, groupIds List<String>) : CompletableFuture<List<ConsumerGroupDescribeResponseData.DescribedGroup>> in class org.apache.kafka.coordinator.group.GroupCoordinatorService & moved to class org.apache.kafka.server.util.FutureUtils",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 560,
                    "endLine": 619,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public consumerGroupDescribe(context RequestContext, groupIds List<String>) : CompletableFuture<List<ConsumerGroupDescribeResponseData.DescribedGroup>>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 613,
                    "endLine": 613,
                    "startColumn": 9,
                    "endColumn": 119,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 617,
                    "endLine": 617,
                    "startColumn": 13,
                    "endColumn": 24,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 616,
                    "endLine": 616,
                    "startColumn": 13,
                    "endColumn": 66,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 614,
                    "endLine": 618,
                    "startColumn": 42,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 614,
                    "endLine": 618,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 106,
                    "endLine": 127,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public combineFutures(futures List<CompletableFuture<T>>, init Supplier<T>, add BiConsumer<T,T>) : CompletableFuture<T>"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 121,
                    "endLine": 121,
                    "startColumn": 9,
                    "endColumn": 119,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 125,
                    "endLine": 125,
                    "startColumn": 13,
                    "endColumn": 24,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 124,
                    "endLine": 124,
                    "startColumn": 13,
                    "endColumn": 71,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 122,
                    "endLine": 126,
                    "startColumn": 42,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 122,
                    "endLine": 126,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 534,
                    "endLine": 583,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public consumerGroupDescribe(context RequestContext, groupIds List<String>) : CompletableFuture<List<ConsumerGroupDescribeResponseData.DescribedGroup>>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 582,
                    "endLine": 582,
                    "startColumn": 16,
                    "endColumn": 81,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "FutureUtils.combineFutures(futures,ArrayList::new,List::addAll)"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 123,
                    "endLine": 123,
                    "startColumn": 13,
                    "endColumn": 38,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 569,
        "extraction_results": {
            "success": true,
            "newCommitHash": "194327a1030ebdb592a03af369090178521cf665",
            "newBranchName": "extract-combineFutures-consumerGroupDescribe-016bd68"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "0472db2cd39d4f53c3855063117bdae15f4d0d86",
        "url": "https://github.com/apache/kafka/commit/0472db2cd39d4f53c3855063117bdae15f4d0d86",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public combineFutures(futures List<CompletableFuture<T>>, init Supplier<T>, add BiConsumer<T,T>) : CompletableFuture<T> extracted from public describeGroups(context RequestContext, groupIds List<String>) : CompletableFuture<List<DescribeGroupsResponseData.DescribedGroup>> in class org.apache.kafka.coordinator.group.GroupCoordinatorService & moved to class org.apache.kafka.server.util.FutureUtils",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 621,
                    "endLine": 682,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public describeGroups(context RequestContext, groupIds List<String>) : CompletableFuture<List<DescribeGroupsResponseData.DescribedGroup>>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 676,
                    "endLine": 676,
                    "startColumn": 9,
                    "endColumn": 119,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 680,
                    "endLine": 680,
                    "startColumn": 13,
                    "endColumn": 24,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 679,
                    "endLine": 679,
                    "startColumn": 13,
                    "endColumn": 66,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 677,
                    "endLine": 681,
                    "startColumn": 42,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 677,
                    "endLine": 681,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 106,
                    "endLine": 127,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public combineFutures(futures List<CompletableFuture<T>>, init Supplier<T>, add BiConsumer<T,T>) : CompletableFuture<T>"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 121,
                    "endLine": 121,
                    "startColumn": 9,
                    "endColumn": 119,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 125,
                    "endLine": 125,
                    "startColumn": 13,
                    "endColumn": 24,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 124,
                    "endLine": 124,
                    "startColumn": 13,
                    "endColumn": 71,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 122,
                    "endLine": 126,
                    "startColumn": 42,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 122,
                    "endLine": 126,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 585,
                    "endLine": 636,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public describeGroups(context RequestContext, groupIds List<String>) : CompletableFuture<List<DescribeGroupsResponseData.DescribedGroup>>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 635,
                    "endLine": 635,
                    "startColumn": 16,
                    "endColumn": 81,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "FutureUtils.combineFutures(futures,ArrayList::new,List::addAll)"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 123,
                    "endLine": 123,
                    "startColumn": 13,
                    "endColumn": 38,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 570,
        "extraction_results": {
            "success": true,
            "newCommitHash": "4188c4e8760841b9d5334cfe509818e000a01ad0",
            "newBranchName": "extract-combineFutures-describeGroups-016bd68"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "0472db2cd39d4f53c3855063117bdae15f4d0d86",
        "url": "https://github.com/apache/kafka/commit/0472db2cd39d4f53c3855063117bdae15f4d0d86",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public combineFutures(futures List<CompletableFuture<T>>, init Supplier<T>, add BiConsumer<T,T>) : CompletableFuture<T> extracted from public deleteGroups(context RequestContext, groupIds List<String>, bufferSupplier BufferSupplier) : CompletableFuture<DeleteGroupsResponseData.DeletableGroupResultCollection> in class org.apache.kafka.coordinator.group.GroupCoordinatorService & moved to class org.apache.kafka.server.util.FutureUtils",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 684,
                    "endLine": 745,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public deleteGroups(context RequestContext, groupIds List<String>, bufferSupplier BufferSupplier) : CompletableFuture<DeleteGroupsResponseData.DeletableGroupResultCollection>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 733,
                    "endLine": 733,
                    "startColumn": 9,
                    "endColumn": 119,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 743,
                    "endLine": 743,
                    "startColumn": 13,
                    "endColumn": 24,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 736,
                    "endLine": 742,
                    "startColumn": 13,
                    "endColumn": 15,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 734,
                    "endLine": 744,
                    "startColumn": 43,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 734,
                    "endLine": 744,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 106,
                    "endLine": 127,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public combineFutures(futures List<CompletableFuture<T>>, init Supplier<T>, add BiConsumer<T,T>) : CompletableFuture<T>"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 121,
                    "endLine": 121,
                    "startColumn": 9,
                    "endColumn": 119,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 125,
                    "endLine": 125,
                    "startColumn": 13,
                    "endColumn": 24,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 124,
                    "endLine": 124,
                    "startColumn": 13,
                    "endColumn": 71,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 122,
                    "endLine": 126,
                    "startColumn": 42,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 122,
                    "endLine": 126,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 638,
                    "endLine": 694,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public deleteGroups(context RequestContext, groupIds List<String>, bufferSupplier BufferSupplier) : CompletableFuture<DeleteGroupsResponseData.DeletableGroupResultCollection>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorService.java",
                    "startLine": 690,
                    "endLine": 693,
                    "startColumn": 16,
                    "endColumn": 108,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "FutureUtils.combineFutures(futures,DeleteGroupsResponseData.DeletableGroupResultCollection::new,(accumulator,newResults) -> newResults.forEach(result -> accumulator.add(result.duplicate())))"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/server/util/FutureUtils.java",
                    "startLine": 123,
                    "endLine": 123,
                    "startColumn": 13,
                    "endColumn": 38,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 571,
        "extraction_results": {
            "success": true,
            "newCommitHash": "05e534aa47ab77057acd03014cfa4c015157adbd",
            "newBranchName": "extract-combineFutures-deleteGroups-016bd68"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4f0a40590833a141c78341ce95ffc782747c5ac8",
        "url": "https://github.com/apache/kafka/commit/4f0a40590833a141c78341ce95ffc782747c5ac8",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public tasksMax() : int extracted from public connectorTaskConfigs(connName String, connConfig ConnectorConfig) : List<Map<String,String>> in class org.apache.kafka.connect.runtime.Worker & moved to class org.apache.kafka.connect.runtime.ConnectorConfig",
            "leftSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                    "startLine": 377,
                    "endLine": 414,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public connectorTaskConfigs(connName String, connConfig ConnectorConfig) : List<Map<String,String>>"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                    "startLine": 392,
                    "endLine": 392,
                    "startColumn": 13,
                    "endColumn": 80,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java",
                    "startLine": 295,
                    "endLine": 297,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public tasksMax() : int"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java",
                    "startLine": 296,
                    "endLine": 296,
                    "startColumn": 9,
                    "endColumn": 41,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                    "startLine": 377,
                    "endLine": 422,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public connectorTaskConfigs(connName String, connConfig ConnectorConfig) : List<Map<String,String>>"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                    "startLine": 392,
                    "endLine": 392,
                    "startColumn": 28,
                    "endColumn": 49,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "connConfig.tasksMax()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 572,
        "extraction_results": {
            "success": true,
            "newCommitHash": "617c4dda13dda70600b65f8dc16964d46f996a68",
            "newBranchName": "extract-tasksMax-connectorTaskConfigs-8da6508"
        },
        "telemetry": {
            "id": "5fd267e8-b384-4d28-bd40-4a14382a7065",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2193,
                "lineStart": 128,
                "lineEnd": 2320,
                "bodyLineStart": 128,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
                "sourceCode": "/**\n * <p>\n * Worker runs a (dynamic) set of tasks in a set of threads, doing the work of actually moving\n * data to/from Kafka.\n * </p>\n * <p>\n * Since each task has a dedicated thread, this is mainly just a container for them.\n * </p>\n */\npublic class Worker {\n\n    public static final long CONNECTOR_GRACEFUL_SHUTDOWN_TIMEOUT_MS = TimeUnit.SECONDS.toMillis(5);\n    public static final long EXECUTOR_SHUTDOWN_TERMINATION_TIMEOUT_MS = TimeUnit.SECONDS.toMillis(1);\n\n    private static final Logger log = LoggerFactory.getLogger(Worker.class);\n\n    protected Herder herder;\n    private final ExecutorService executor;\n    private final Time time;\n    private final String workerId;\n    //kafka cluster id\n    private final String kafkaClusterId;\n    private final Plugins plugins;\n    private final ConnectMetrics metrics;\n    private final WorkerMetricsGroup workerMetricsGroup;\n    private ConnectorStatusMetricsGroup connectorStatusMetricsGroup;\n    private final WorkerConfig config;\n    private final Converter internalKeyConverter;\n    private final Converter internalValueConverter;\n    private final OffsetBackingStore globalOffsetBackingStore;\n\n    private final ConcurrentMap<String, WorkerConnector> connectors = new ConcurrentHashMap<>();\n    private final ConcurrentMap<ConnectorTaskId, WorkerTask<?, ?>> tasks = new ConcurrentHashMap<>();\n    private Optional<SourceTaskOffsetCommitter> sourceTaskOffsetCommitter = Optional.empty();\n    private final WorkerConfigTransformer workerConfigTransformer;\n    private final ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy;\n    private final Function<Map<String, Object>, Admin> adminFactory;\n\n    public Worker(\n        String workerId,\n        Time time,\n        Plugins plugins,\n        WorkerConfig config,\n        OffsetBackingStore globalOffsetBackingStore,\n        ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy) {\n        this(workerId, time, plugins, config, globalOffsetBackingStore, Executors.newCachedThreadPool(), connectorClientConfigOverridePolicy, Admin::create);\n    }\n\n    @SuppressWarnings(\"this-escape\")\n    Worker(\n            String workerId,\n            Time time,\n            Plugins plugins,\n            WorkerConfig config,\n            OffsetBackingStore globalOffsetBackingStore,\n            ExecutorService executorService,\n            ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n            Function<Map<String, Object>, Admin> adminFactory\n    ) {\n        this.kafkaClusterId = config.kafkaClusterId();\n        this.metrics = new ConnectMetrics(workerId, config, time, kafkaClusterId);\n        this.executor = executorService;\n        this.workerId = workerId;\n        this.time = time;\n        this.plugins = plugins;\n        this.config = config;\n        this.connectorClientConfigOverridePolicy = connectorClientConfigOverridePolicy;\n        this.workerMetricsGroup = new WorkerMetricsGroup(this.connectors, this.tasks, metrics);\n\n        Map<String, String> internalConverterConfig = Collections.singletonMap(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG, \"false\");\n        this.internalKeyConverter = plugins.newInternalConverter(true, JsonConverter.class.getName(), internalConverterConfig);\n        this.internalValueConverter = plugins.newInternalConverter(false, JsonConverter.class.getName(), internalConverterConfig);\n\n        this.globalOffsetBackingStore = globalOffsetBackingStore;\n\n        this.workerConfigTransformer = initConfigTransformer();\n        this.adminFactory = adminFactory;\n    }\n\n    private WorkerConfigTransformer initConfigTransformer() {\n        final List<String> providerNames = config.getList(WorkerConfig.CONFIG_PROVIDERS_CONFIG);\n        Map<String, ConfigProvider> providerMap = new HashMap<>();\n        for (String providerName : providerNames) {\n            ConfigProvider configProvider = plugins.newConfigProvider(\n                    config,\n                    WorkerConfig.CONFIG_PROVIDERS_CONFIG + \".\" + providerName,\n                    ClassLoaderUsage.PLUGINS\n            );\n            providerMap.put(providerName, configProvider);\n        }\n        return new WorkerConfigTransformer(this, providerMap);\n    }\n\n    public WorkerConfigTransformer configTransformer() {\n        return workerConfigTransformer;\n    }\n\n    protected Herder herder() {\n        return herder;\n    }\n\n    /**\n     * Start worker.\n     */\n    public void start() {\n        log.info(\"Worker starting\");\n\n        globalOffsetBackingStore.start();\n\n        sourceTaskOffsetCommitter = config.exactlyOnceSourceEnabled()\n                ? Optional.empty()\n                : Optional.of(new SourceTaskOffsetCommitter(config));\n\n        connectorStatusMetricsGroup = new ConnectorStatusMetricsGroup(metrics, tasks, herder);\n\n        log.info(\"Worker started\");\n    }\n\n    /**\n     * Stop worker.\n     */\n    public void stop() {\n        log.info(\"Worker stopping\");\n\n        long started = time.milliseconds();\n        long limit = started + config.getLong(WorkerConfig.TASK_SHUTDOWN_GRACEFUL_TIMEOUT_MS_CONFIG);\n\n        if (!connectors.isEmpty()) {\n            log.warn(\"Shutting down connectors {} uncleanly; herder should have shut down connectors before the Worker is stopped\", connectors.keySet());\n            stopAndAwaitConnectors();\n        }\n\n        if (!tasks.isEmpty()) {\n            log.warn(\"Shutting down tasks {} uncleanly; herder should have shut down tasks before the Worker is stopped\", tasks.keySet());\n            stopAndAwaitTasks();\n        }\n\n        long timeoutMs = limit - time.milliseconds();\n        sourceTaskOffsetCommitter.ifPresent(committer -> committer.close(timeoutMs));\n\n        globalOffsetBackingStore.stop();\n        metrics.stop();\n\n        log.info(\"Worker stopped\");\n\n        workerMetricsGroup.close();\n        if (connectorStatusMetricsGroup != null) {\n            connectorStatusMetricsGroup.close();\n        }\n\n        workerConfigTransformer.close();\n        ThreadUtils.shutdownExecutorServiceQuietly(executor, EXECUTOR_SHUTDOWN_TERMINATION_TIMEOUT_MS, TimeUnit.MILLISECONDS);\n    }\n\n    /**\n     * Start a connector managed by this worker.\n     *\n     * @param connName the connector name.\n     * @param connProps the properties of the connector.\n     * @param ctx the connector runtime context.\n     * @param statusListener a listener for the runtime status transitions of the connector.\n     * @param initialState the initial state of the connector.\n     * @param onConnectorStateChange invoked when the initial state change of the connector is completed\n     */\n    public void startConnector(\n            String connName,\n            Map<String, String> connProps,\n            CloseableConnectorContext ctx,\n            ConnectorStatus.Listener statusListener,\n            TargetState initialState,\n            Callback<TargetState> onConnectorStateChange\n    ) {\n        final ConnectorStatus.Listener connectorStatusListener = workerMetricsGroup.wrapStatusListener(statusListener);\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            if (connectors.containsKey(connName)) {\n                onConnectorStateChange.onCompletion(\n                        new ConnectException(\"Connector with name \" + connName + \" already exists\"),\n                        null);\n                return;\n            }\n\n            final WorkerConnector workerConnector;\n            final String connClass = connProps.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n            ClassLoader connectorLoader = plugins.connectorLoader(connClass);\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n                log.info(\"Creating connector {} of type {}\", connName, connClass);\n                final Connector connector = plugins.newConnector(connClass);\n                final ConnectorConfig connConfig;\n                final CloseableOffsetStorageReader offsetReader;\n                final ConnectorOffsetBackingStore offsetStore;\n                if (ConnectUtils.isSinkConnector(connector)) {\n                    connConfig = new SinkConnectorConfig(plugins, connProps);\n                    offsetReader = null;\n                    offsetStore = null;\n                } else {\n                    SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins, connProps, config.topicCreationEnable());\n                    connConfig = sourceConfig;\n\n                    // Set up the offset backing store for this connector instance\n                    offsetStore = config.exactlyOnceSourceEnabled()\n                            ? offsetStoreForExactlyOnceSourceConnector(sourceConfig, connName, connector, null)\n                            : offsetStoreForRegularSourceConnector(sourceConfig, connName, connector, null);\n                    offsetStore.configure(config);\n                    offsetReader = new OffsetStorageReaderImpl(offsetStore, connName, internalKeyConverter, internalValueConverter);\n                }\n                workerConnector = new WorkerConnector(\n                        connName, connector, connConfig, ctx, metrics, connectorStatusListener, offsetReader, offsetStore, connectorLoader);\n                log.info(\"Instantiated connector {} with version {} of type {}\", connName, connector.version(), connector.getClass());\n                workerConnector.transitionTo(initialState, onConnectorStateChange);\n            } catch (Throwable t) {\n                log.error(\"Failed to start connector {}\", connName, t);\n                connectorStatusListener.onFailure(connName, t);\n                onConnectorStateChange.onCompletion(t, null);\n                return;\n            }\n\n            WorkerConnector existing = connectors.putIfAbsent(connName, workerConnector);\n            if (existing != null) {\n                onConnectorStateChange.onCompletion(\n                        new ConnectException(\"Connector with name \" + connName + \" already exists\"),\n                        null);\n                // Don't need to do any cleanup of the WorkerConnector instance (such as calling\n                // shutdown() on it) here because it hasn't actually started running yet\n                return;\n            }\n\n            executor.submit(plugins.withClassLoader(connectorLoader, workerConnector));\n\n            log.info(\"Finished creating connector {}\", connName);\n        }\n    }\n\n    /**\n     * Return true if the connector associated with this worker is a sink connector.\n     *\n     * @param connName the connector name.\n     * @return true if the connector belongs to the worker and is a sink connector.\n     * @throws ConnectException if the worker does not manage a connector with the given name.\n     */\n    public boolean isSinkConnector(String connName) {\n        WorkerConnector workerConnector = connectors.get(connName);\n        if (workerConnector == null)\n            throw new ConnectException(\"Connector \" + connName + \" not found in this worker.\");\n\n        try (LoaderSwap loaderSwap = plugins.withClassLoader(workerConnector.loader())) {\n            return workerConnector.isSinkConnector();\n        }\n    }\n\n    /**\n     * Get a list of updated task properties for the tasks of this connector.\n     *\n     * @param connName the connector name.\n     * @return a list of updated tasks properties.\n     */\n    public List<Map<String, String>> connectorTaskConfigs(String connName, ConnectorConfig connConfig) {\n        List<Map<String, String>> result = new ArrayList<>();\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            log.trace(\"Reconfiguring connector tasks for {}\", connName);\n\n            WorkerConnector workerConnector = connectors.get(connName);\n            if (workerConnector == null)\n                throw new ConnectException(\"Connector \" + connName + \" not found in this worker.\");\n\n            int maxTasks = tasksMax(connConfig);\n            Map<String, String> connOriginals = connConfig.originalsStrings();\n\n            Connector connector = workerConnector.connector();\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(workerConnector.loader())) {\n                String taskClassName = connector.taskClass().getName();\n                for (Map<String, String> taskProps : connector.taskConfigs(maxTasks)) {\n                    // Ensure we don't modify the connector's copy of the config\n                    Map<String, String> taskConfig = new HashMap<>(taskProps);\n                    taskConfig.put(TaskConfig.TASK_CLASS_CONFIG, taskClassName);\n                    if (connOriginals.containsKey(SinkTask.TOPICS_CONFIG)) {\n                        taskConfig.put(SinkTask.TOPICS_CONFIG, connOriginals.get(SinkTask.TOPICS_CONFIG));\n                    }\n                    if (connOriginals.containsKey(SinkTask.TOPICS_REGEX_CONFIG)) {\n                        taskConfig.put(SinkTask.TOPICS_REGEX_CONFIG, connOriginals.get(SinkTask.TOPICS_REGEX_CONFIG));\n                    }\n                    result.add(taskConfig);\n                }\n            }\n        }\n\n        return result;\n    }\n\n    private int tasksMax(ConnectorConfig connConfig) {\n        int maxTasks = connConfig.getInt(ConnectorConfig.TASKS_MAX_CONFIG);\n        return maxTasks;\n    }\n\n    /**\n     * Stop a connector managed by this worker.\n     *\n     * @param connName the connector name.\n     */\n    private void stopConnector(String connName) {\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            WorkerConnector workerConnector = connectors.get(connName);\n            log.info(\"Stopping connector {}\", connName);\n\n            if (workerConnector == null) {\n                log.warn(\"Ignoring stop request for unowned connector {}\", connName);\n                return;\n            }\n\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(workerConnector.loader())) {\n                workerConnector.shutdown();\n            }\n        }\n    }\n\n    private void stopConnectors(Collection<String> ids) {\n        // Herder is responsible for stopping connectors. This is an internal method to sequentially\n        // stop connectors that have not explicitly been stopped.\n        for (String connector: ids)\n            stopConnector(connector);\n    }\n\n    private void awaitStopConnector(String connName, long timeout) {\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            WorkerConnector connector = connectors.remove(connName);\n            if (connector == null) {\n                log.warn(\"Ignoring await stop request for non-present connector {}\", connName);\n                return;\n            }\n\n            if (!connector.awaitShutdown(timeout)) {\n                log.error(\"Connector '{}' failed to properly shut down, has become unresponsive, and \"\n                        + \"may be consuming external resources. Correct the configuration for \"\n                        + \"this connector or remove the connector. After fixing the connector, it \"\n                        + \"may be necessary to restart this worker to release any consumed \"\n                        + \"resources.\", connName);\n                connector.cancel();\n            } else {\n                log.debug(\"Graceful stop of connector {} succeeded.\", connName);\n            }\n        }\n    }\n\n    private void awaitStopConnectors(Collection<String> ids) {\n        long now = time.milliseconds();\n        long deadline = now + CONNECTOR_GRACEFUL_SHUTDOWN_TIMEOUT_MS;\n        for (String id : ids) {\n            long remaining = Math.max(0, deadline - time.milliseconds());\n            awaitStopConnector(id, remaining);\n        }\n    }\n\n    /**\n     * Stop asynchronously all the worker's connectors and await their termination.\n     */\n    public void stopAndAwaitConnectors() {\n        stopAndAwaitConnectors(new ArrayList<>(connectors.keySet()));\n    }\n\n    /**\n     * Stop asynchronously a collection of connectors that belong to this worker and await their\n     * termination.\n     *\n     * @param ids the collection of connectors to be stopped.\n     */\n    public void stopAndAwaitConnectors(Collection<String> ids) {\n        stopConnectors(ids);\n        awaitStopConnectors(ids);\n    }\n\n    /**\n     * Stop a connector that belongs to this worker and await its termination.\n     *\n     * @param connName the name of the connector to be stopped.\n     */\n    public void stopAndAwaitConnector(String connName) {\n        stopConnector(connName);\n        awaitStopConnectors(Collections.singletonList(connName));\n    }\n\n    /**\n     * Get the IDs of the connectors currently running in this worker.\n     *\n     * @return the set of connector IDs.\n     */\n    public Set<String> connectorNames() {\n        return connectors.keySet();\n    }\n\n    /**\n     * Return true if a connector with the given name is managed by this worker and is currently running.\n     *\n     * @param connName the connector name.\n     * @return true if the connector is running, false if the connector is not running or is not manages by this worker.\n     */\n    public boolean isRunning(String connName) {\n        WorkerConnector workerConnector = connectors.get(connName);\n        return workerConnector != null && workerConnector.isRunning();\n    }\n\n    /**\n     * Start a sink task managed by this worker.\n     *\n     * @param id the task ID.\n     * @param configState the most recent {@link ClusterConfigState} known to the worker\n     * @param connProps the connector properties.\n     * @param taskProps the tasks properties.\n     * @param statusListener a listener for the runtime status transitions of the task.\n     * @param initialState the initial state of the connector.\n     * @return true if the task started successfully.\n     */\n    public boolean startSinkTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState\n    ) {\n        return startTask(id, connProps, taskProps, statusListener,\n                new SinkTaskBuilder(id, configState, statusListener, initialState));\n    }\n\n    /**\n     * Start a source task managed by this worker using older behavior that does not provide exactly-once support.\n     *\n     * @param id the task ID.\n     * @param configState the most recent {@link ClusterConfigState} known to the worker\n     * @param connProps the connector properties.\n     * @param taskProps the tasks properties.\n     * @param statusListener a listener for the runtime status transitions of the task.\n     * @param initialState the initial state of the connector.\n     * @return true if the task started successfully.\n     */\n    public boolean startSourceTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState\n    ) {\n        return startTask(id, connProps, taskProps, statusListener,\n                new SourceTaskBuilder(id, configState, statusListener, initialState));\n    }\n\n    /**\n     * Start a source task with exactly-once support managed by this worker.\n     *\n     * @param id the task ID.\n     * @param configState the most recent {@link ClusterConfigState} known to the worker\n     * @param connProps the connector properties.\n     * @param taskProps the tasks properties.\n     * @param statusListener a listener for the runtime status transitions of the task.\n     * @param initialState the initial state of the connector.\n     * @param preProducerCheck a preflight check that should be performed before the task initializes its transactional producer.\n     * @param postProducerCheck a preflight check that should be performed after the task initializes its transactional producer,\n     *                          but before producing any source records or offsets.\n     * @return true if the task started successfully.\n     */\n    public boolean startExactlyOnceSourceTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState,\n            Runnable preProducerCheck,\n            Runnable postProducerCheck\n    ) {\n        return startTask(id, connProps, taskProps, statusListener,\n                new ExactlyOnceSourceTaskBuilder(id, configState, statusListener, initialState, preProducerCheck, postProducerCheck));\n    }\n\n    /**\n     * Start a task managed by this worker.\n     *\n     * @param id the task ID.\n     * @param connProps the connector properties.\n     * @param taskProps the tasks properties.\n     * @param statusListener a listener for the runtime status transitions of the task.\n     * @param taskBuilder the {@link TaskBuilder} used to create the {@link WorkerTask} that manages the lifecycle of the task.\n     * @return true if the task started successfully.\n     */\n    private boolean startTask(\n            ConnectorTaskId id,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TaskBuilder<?, ?> taskBuilder\n    ) {\n        final WorkerTask<?, ?> workerTask;\n        final TaskStatus.Listener taskStatusListener = workerMetricsGroup.wrapStatusListener(statusListener);\n        try (LoggingContext loggingContext = LoggingContext.forTask(id)) {\n            log.info(\"Creating task {}\", id);\n\n            if (tasks.containsKey(id))\n                throw new ConnectException(\"Task already exists in this worker: \" + id);\n\n            connectorStatusMetricsGroup.recordTaskAdded(id);\n            String connType = connProps.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n            ClassLoader connectorLoader = plugins.connectorLoader(connType);\n\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n                final ConnectorConfig connConfig = new ConnectorConfig(plugins, connProps);\n                final TaskConfig taskConfig = new TaskConfig(taskProps);\n                final Class<? extends Task> taskClass = taskConfig.getClass(TaskConfig.TASK_CLASS_CONFIG).asSubclass(Task.class);\n                final Task task = plugins.newTask(taskClass);\n                log.info(\"Instantiated task {} with version {} of type {}\", id, task.version(), taskClass.getName());\n\n                // By maintaining connector's specific class loader for this thread here, we first\n                // search for converters within the connector dependencies.\n                // If any of these aren't found, that means the connector didn't configure specific converters,\n                // so we should instantiate based upon the worker configuration\n                Converter keyConverter = plugins.newConverter(connConfig, WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage\n                                                                                                                           .CURRENT_CLASSLOADER);\n                Converter valueConverter = plugins.newConverter(connConfig, WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.CURRENT_CLASSLOADER);\n                HeaderConverter headerConverter = plugins.newHeaderConverter(connConfig, WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,\n                                                                             ClassLoaderUsage.CURRENT_CLASSLOADER);\n                if (keyConverter == null) {\n                    keyConverter = plugins.newConverter(config, WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);\n                    log.info(\"Set up the key converter {} for task {} using the worker config\", keyConverter.getClass(), id);\n                } else {\n                    log.info(\"Set up the key converter {} for task {} using the connector config\", keyConverter.getClass(), id);\n                }\n                if (valueConverter == null) {\n                    valueConverter = plugins.newConverter(config, WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);\n                    log.info(\"Set up the value converter {} for task {} using the worker config\", valueConverter.getClass(), id);\n                } else {\n                    log.info(\"Set up the value converter {} for task {} using the connector config\", valueConverter.getClass(), id);\n                }\n                if (headerConverter == null) {\n                    headerConverter = plugins.newHeaderConverter(config, WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG, ClassLoaderUsage\n                                                                                                                             .PLUGINS);\n                    log.info(\"Set up the header converter {} for task {} using the worker config\", headerConverter.getClass(), id);\n                } else {\n                    log.info(\"Set up the header converter {} for task {} using the connector config\", headerConverter.getClass(), id);\n                }\n\n                workerTask = taskBuilder\n                        .withTask(task)\n                        .withConnectorConfig(connConfig)\n                        .withKeyConverter(keyConverter)\n                        .withValueConverter(valueConverter)\n                        .withHeaderConverter(headerConverter)\n                        .withClassloader(connectorLoader)\n                        .build();\n\n                workerTask.initialize(taskConfig);\n            } catch (Throwable t) {\n                log.error(\"Failed to start task {}\", id, t);\n                connectorStatusMetricsGroup.recordTaskRemoved(id);\n                taskStatusListener.onFailure(id, t);\n                return false;\n            }\n\n            WorkerTask<?, ?> existing = tasks.putIfAbsent(id, workerTask);\n            if (existing != null)\n                throw new ConnectException(\"Task already exists in this worker: \" + id);\n\n            executor.submit(plugins.withClassLoader(connectorLoader, workerTask));\n            if (workerTask instanceof WorkerSourceTask) {\n                sourceTaskOffsetCommitter.ifPresent(committer -> committer.schedule(id, (WorkerSourceTask) workerTask));\n            }\n            return true;\n        }\n    }\n\n    /**\n     * Using the admin principal for this connector, perform a round of zombie fencing that disables transactional producers\n     * for the specified number of source tasks from sending any more records.\n     * @param connName the name of the connector\n     * @param numTasks the number of tasks to fence out\n     * @param connProps the configuration of the connector; may not be null\n     * @return a {@link KafkaFuture} that will complete when the producers have all been fenced out, or the attempt has failed\n     */\n    public KafkaFuture<Void> fenceZombies(String connName, int numTasks, Map<String, String> connProps) {\n        log.debug(\"Fencing out {} task producers for source connector {}\", numTasks, connName);\n        try (LoggingContext loggingContext = LoggingContext.forConnector(connName)) {\n            String connType = connProps.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n            ClassLoader connectorLoader = plugins.connectorLoader(connType);\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n                final SourceConnectorConfig connConfig = new SourceConnectorConfig(plugins, connProps, config.topicCreationEnable());\n                final Class<? extends Connector> connClass = plugins.connectorClass(\n                        connConfig.getString(ConnectorConfig.CONNECTOR_CLASS_CONFIG));\n\n                Map<String, Object> adminConfig = adminConfigs(\n                        connName,\n                        \"connector-worker-adminclient-\" + connName,\n                        config,\n                        connConfig,\n                        connClass,\n                        connectorClientConfigOverridePolicy,\n                        kafkaClusterId,\n                        ConnectorType.SOURCE);\n                final Admin admin = adminFactory.apply(adminConfig);\n\n                try {\n                    Collection<String> transactionalIds = IntStream.range(0, numTasks)\n                            .mapToObj(i -> new ConnectorTaskId(connName, i))\n                            .map(this::taskTransactionalId)\n                            .collect(Collectors.toList());\n                    FenceProducersOptions fencingOptions = new FenceProducersOptions()\n                            .timeoutMs((int) RestServer.DEFAULT_REST_REQUEST_TIMEOUT_MS);\n                    return admin.fenceProducers(transactionalIds, fencingOptions).all().whenComplete((ignored, error) -> {\n                        if (error == null)\n                            log.debug(\"Finished fencing out {} task producers for source connector {}\", numTasks, connName);\n                        Utils.closeQuietly(admin, \"Zombie fencing admin for connector \" + connName);\n                    });\n                } catch (Exception e) {\n                    Utils.closeQuietly(admin, \"Zombie fencing admin for connector \" + connName);\n                    throw e;\n                }\n            }\n        }\n    }\n\n    static Map<String, Object> exactlyOnceSourceTaskProducerConfigs(ConnectorTaskId id,\n                                                              WorkerConfig config,\n                                                              ConnectorConfig connConfig,\n                                                              Class<? extends Connector>  connectorClass,\n                                                              ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                              String clusterId) {\n        Map<String, Object> result = baseProducerConfigs(id.connector(), \"connector-producer-\" + id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy, clusterId);\n        // The base producer properties forcibly disable idempotence; remove it from those properties\n        // if not explicitly requested by the user\n        boolean connectorProducerIdempotenceConfigured = connConfig.originals().containsKey(\n                ConnectorConfig.CONNECTOR_CLIENT_PRODUCER_OVERRIDES_PREFIX + ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG\n        );\n        if (!connectorProducerIdempotenceConfigured) {\n            boolean workerProducerIdempotenceConfigured = config.originals().containsKey(\n                    \"producer.\" + ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG\n            );\n            if (!workerProducerIdempotenceConfigured) {\n                result.remove(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG);\n            }\n        }\n        ConnectUtils.ensureProperty(\n                result, ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\",\n                \"for connectors when exactly-once source support is enabled\",\n                false\n        );\n        String transactionalId = taskTransactionalId(config.groupId(), id.connector(), id.task());\n        ConnectUtils.ensureProperty(\n                result, ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId,\n                \"for connectors when exactly-once source support is enabled\",\n                true\n        );\n        return result;\n    }\n\n    static Map<String, Object> baseProducerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector>  connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId) {\n        Map<String, Object> producerProps = new HashMap<>();\n        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, config.bootstrapServers());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n        // These settings will execute infinite retries on retriable exceptions. They *may* be overridden via configs passed to the worker,\n        // but this may compromise the delivery guarantees of Kafka Connect.\n        producerProps.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, Long.toString(Long.MAX_VALUE));\n        // By default, Connect disables idempotent behavior for all producers, even though idempotence became\n        // default for Kafka producers. This is to ensure Connect continues to work with many Kafka broker versions, including older brokers that do not support\n        // idempotent producers or require explicit steps to enable them (e.g. adding the IDEMPOTENT_WRITE ACL to brokers older than 2.8).\n        // These settings might change when https://cwiki.apache.org/confluence/display/KAFKA/KIP-318%3A+Make+Kafka+Connect+Source+idempotent\n        // gets approved and scheduled for release.\n        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"false\");\n        producerProps.put(ProducerConfig.ACKS_CONFIG, \"all\");\n        producerProps.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, \"1\");\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.toString(Integer.MAX_VALUE));\n        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, defaultClientId);\n        // User-specified overrides\n        producerProps.putAll(config.originalsWithPrefix(\"producer.\"));\n        //add client metrics.context properties\n        ConnectUtils.addMetricsContextProperties(producerProps, config, clusterId);\n\n        // Connector-specified overrides\n        Map<String, Object> producerOverrides =\n            connectorClientConfigOverrides(connName, connConfig, connectorClass, ConnectorConfig.CONNECTOR_CLIENT_PRODUCER_OVERRIDES_PREFIX,\n                                           ConnectorType.SOURCE, ConnectorClientConfigRequest.ClientType.PRODUCER,\n                                           connectorClientConfigOverridePolicy);\n        producerProps.putAll(producerOverrides);\n\n        return producerProps;\n    }\n\n    static Map<String, Object> exactlyOnceSourceOffsetsConsumerConfigs(String connName,\n                                                                       String defaultClientId,\n                                                                       WorkerConfig config,\n                                                                       ConnectorConfig connConfig,\n                                                                       Class<? extends Connector> connectorClass,\n                                                                       ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                       String clusterId) {\n        Map<String, Object> result = baseConsumerConfigs(\n                connName, defaultClientId, config, connConfig, connectorClass,\n                connectorClientConfigOverridePolicy, clusterId, ConnectorType.SOURCE);\n        ConnectUtils.ensureProperty(\n                result, ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.name().toLowerCase(Locale.ROOT),\n                \"for source connectors' offset consumers when exactly-once source support is enabled\",\n                false\n        );\n        return result;\n    }\n\n    static Map<String, Object> regularSourceOffsetsConsumerConfigs(String connName,\n                                                                   String defaultClientId,\n                                                                   WorkerConfig config,\n                                                                   ConnectorConfig connConfig,\n                                                                   Class<? extends Connector> connectorClass,\n                                                                   ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                   String clusterId) {\n        Map<String, Object> result = baseConsumerConfigs(\n                connName, defaultClientId, config, connConfig, connectorClass,\n                connectorClientConfigOverridePolicy, clusterId, ConnectorType.SOURCE);\n        // Users can disable this if they want to since the task isn't exactly-once anyways\n        result.putIfAbsent(\n                ConsumerConfig.ISOLATION_LEVEL_CONFIG,\n                IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT));\n        return result;\n    }\n\n    static Map<String, Object> baseConsumerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector> connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId,\n                                               ConnectorType connectorType) {\n        // Include any unknown worker configs so consumer configs can be set globally on the worker\n        // and through to the task\n        Map<String, Object> consumerProps = new HashMap<>();\n\n        consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, SinkUtils.consumerGroupId(connName));\n        consumerProps.put(ConsumerConfig.CLIENT_ID_CONFIG, defaultClientId);\n        consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, config.bootstrapServers());\n        consumerProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false\");\n        consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");\n\n        consumerProps.putAll(config.originalsWithPrefix(\"consumer.\"));\n        //add client metrics.context properties\n        ConnectUtils.addMetricsContextProperties(consumerProps, config, clusterId);\n        // Connector-specified overrides\n        Map<String, Object> consumerOverrides =\n            connectorClientConfigOverrides(connName, connConfig, connectorClass, ConnectorConfig.CONNECTOR_CLIENT_CONSUMER_OVERRIDES_PREFIX,\n                                           connectorType, ConnectorClientConfigRequest.ClientType.CONSUMER,\n                                           connectorClientConfigOverridePolicy);\n        consumerProps.putAll(consumerOverrides);\n\n        return consumerProps;\n    }\n\n    static Map<String, Object> adminConfigs(String connName,\n                                            String defaultClientId,\n                                            WorkerConfig config,\n                                            ConnectorConfig connConfig,\n                                            Class<? extends Connector> connectorClass,\n                                            ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                            String clusterId,\n                                            ConnectorType connectorType) {\n        Map<String, Object> adminProps = new HashMap<>();\n        // Use the top-level worker configs to retain backwards compatibility with older releases which\n        // did not require a prefix for connector admin client configs in the worker configuration file\n        // Ignore configs that begin with \"admin.\" since those will be added next (with the prefix stripped)\n        // and those that begin with \"producer.\" and \"consumer.\", since we know they aren't intended for\n        // the admin client\n        Map<String, Object> nonPrefixedWorkerConfigs = config.originals().entrySet().stream()\n                .filter(e -> !e.getKey().startsWith(\"admin.\")\n                        && !e.getKey().startsWith(\"producer.\")\n                        && !e.getKey().startsWith(\"consumer.\"))\n                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n        adminProps.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, config.bootstrapServers());\n        adminProps.put(AdminClientConfig.CLIENT_ID_CONFIG, defaultClientId);\n        adminProps.putAll(nonPrefixedWorkerConfigs);\n\n        // Admin client-specific overrides in the worker config\n        adminProps.putAll(config.originalsWithPrefix(\"admin.\"));\n\n        // Connector-specified overrides\n        Map<String, Object> adminOverrides =\n                connectorClientConfigOverrides(connName, connConfig, connectorClass, ConnectorConfig.CONNECTOR_CLIENT_ADMIN_OVERRIDES_PREFIX,\n                        connectorType, ConnectorClientConfigRequest.ClientType.ADMIN,\n                        connectorClientConfigOverridePolicy);\n        adminProps.putAll(adminOverrides);\n\n        //add client metrics.context properties\n        ConnectUtils.addMetricsContextProperties(adminProps, config, clusterId);\n\n        return adminProps;\n    }\n\n    private static Map<String, Object> connectorClientConfigOverrides(String connName,\n                                                                      ConnectorConfig connConfig,\n                                                                      Class<? extends Connector> connectorClass,\n                                                                      String clientConfigPrefix,\n                                                                      ConnectorType connectorType,\n                                                                      ConnectorClientConfigRequest.ClientType clientType,\n                                                                      ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy) {\n        Map<String, Object> clientOverrides = connConfig.originalsWithPrefix(clientConfigPrefix);\n        ConnectorClientConfigRequest connectorClientConfigRequest = new ConnectorClientConfigRequest(\n            connName,\n            connectorType,\n            connectorClass,\n            clientOverrides,\n            clientType\n        );\n        List<ConfigValue> configValues = connectorClientConfigOverridePolicy.validate(connectorClientConfigRequest);\n        List<ConfigValue> errorConfigs = configValues.stream().\n            filter(configValue -> configValue.errorMessages().size() > 0).collect(Collectors.toList());\n        // These should be caught when the herder validates the connector configuration, but just in case\n        if (errorConfigs.size() > 0) {\n            throw new ConnectException(\"Client Config Overrides not allowed \" + errorConfigs);\n        }\n        return clientOverrides;\n    }\n\n    private String taskTransactionalId(ConnectorTaskId id) {\n        return taskTransactionalId(config.groupId(), id.connector(), id.task());\n    }\n\n    /**\n     * @return the {@link ProducerConfig#TRANSACTIONAL_ID_CONFIG transactional ID} to use for a task that writes\n     * records and/or offsets in a transaction. Not to be confused with {@link DistributedConfig#transactionalProducerId()},\n     * which is not used by tasks at all, but instead, by the worker itself.\n     */\n    public static String taskTransactionalId(String groupId, String connector, int taskId) {\n        return String.format(\"%s-%s-%d\", groupId, connector, taskId);\n    }\n\n    ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {\n        return new ErrorHandlingMetrics(id, metrics);\n    }\n\n    private List<ErrorReporter<ConsumerRecord<byte[], byte[]>>> sinkTaskReporters(ConnectorTaskId id, SinkConnectorConfig connConfig,\n                                                     ErrorHandlingMetrics errorHandlingMetrics,\n                                                     Class<? extends Connector> connectorClass) {\n        ArrayList<ErrorReporter<ConsumerRecord<byte[], byte[]>>> reporters = new ArrayList<>();\n        LogReporter<ConsumerRecord<byte[], byte[]>> logReporter = new LogReporter.Sink(id, connConfig, errorHandlingMetrics);\n        reporters.add(logReporter);\n\n        // check if topic for dead letter queue exists\n        String topic = connConfig.dlqTopicName();\n        if (topic != null && !topic.isEmpty()) {\n            Map<String, Object> producerProps = baseProducerConfigs(id.connector(), \"connector-dlq-producer-\" + id, config, connConfig, connectorClass,\n                                                                connectorClientConfigOverridePolicy, kafkaClusterId);\n            Map<String, Object> adminProps = adminConfigs(id.connector(), \"connector-dlq-adminclient-\", config, connConfig, connectorClass, connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SINK);\n            DeadLetterQueueReporter reporter = DeadLetterQueueReporter.createAndSetup(adminProps, id, connConfig, producerProps, errorHandlingMetrics);\n\n            reporters.add(reporter);\n        }\n\n        return reporters;\n    }\n\n    private List<ErrorReporter<SourceRecord>> sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,\n                                                       ErrorHandlingMetrics errorHandlingMetrics) {\n        List<ErrorReporter<SourceRecord>> reporters = new ArrayList<>();\n        LogReporter<SourceRecord> logReporter = new LogReporter.Source(id, connConfig, errorHandlingMetrics);\n        reporters.add(logReporter);\n\n        return reporters;\n    }\n\n    private WorkerErrantRecordReporter createWorkerErrantRecordReporter(\n        SinkConnectorConfig connConfig,\n        RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n        Converter keyConverter,\n        Converter valueConverter,\n        HeaderConverter headerConverter\n    ) {\n        // check if errant record reporter topic is configured\n        if (connConfig.enableErrantRecordReporter()) {\n            return new WorkerErrantRecordReporter(retryWithToleranceOperator, keyConverter, valueConverter, headerConverter);\n        }\n        return null;\n    }\n\n    private void stopTask(ConnectorTaskId taskId) {\n        try (LoggingContext loggingContext = LoggingContext.forTask(taskId)) {\n            WorkerTask<?, ?> task = tasks.get(taskId);\n            if (task == null) {\n                log.warn(\"Ignoring stop request for unowned task {}\", taskId);\n                return;\n            }\n\n            log.info(\"Stopping task {}\", task.id());\n            if (task instanceof WorkerSourceTask)\n                sourceTaskOffsetCommitter.ifPresent(committer -> committer.remove(task.id()));\n\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(task.loader())) {\n                task.stop();\n            }\n        }\n    }\n\n    private void stopTasks(Collection<ConnectorTaskId> ids) {\n        // Herder is responsible for stopping tasks. This is an internal method to sequentially\n        // stop the tasks that have not explicitly been stopped.\n        for (ConnectorTaskId taskId : ids) {\n            stopTask(taskId);\n        }\n    }\n\n    private void awaitStopTask(ConnectorTaskId taskId, long timeout) {\n        try (LoggingContext loggingContext = LoggingContext.forTask(taskId)) {\n            WorkerTask<?, ?> task = tasks.remove(taskId);\n            if (task == null) {\n                log.warn(\"Ignoring await stop request for non-present task {}\", taskId);\n                return;\n            }\n\n            if (!task.awaitStop(timeout)) {\n                log.error(\"Graceful stop of task {} failed.\", task.id());\n                task.cancel();\n            } else {\n                log.debug(\"Graceful stop of task {} succeeded.\", task.id());\n            }\n\n            try {\n                task.removeMetrics();\n            } finally {\n                connectorStatusMetricsGroup.recordTaskRemoved(taskId);\n            }\n        }\n    }\n\n    private void awaitStopTasks(Collection<ConnectorTaskId> ids) {\n        long now = time.milliseconds();\n        long deadline = now + config.getLong(WorkerConfig.TASK_SHUTDOWN_GRACEFUL_TIMEOUT_MS_CONFIG);\n        for (ConnectorTaskId id : ids) {\n            long remaining = Math.max(0, deadline - time.milliseconds());\n            awaitStopTask(id, remaining);\n        }\n    }\n\n    /**\n     * Stop asynchronously all the worker's tasks and await their termination.\n     */\n    public void stopAndAwaitTasks() {\n        stopAndAwaitTasks(new ArrayList<>(tasks.keySet()));\n    }\n\n    /**\n     * Stop asynchronously a collection of tasks that belong to this worker and await their termination.\n     *\n     * @param ids the collection of tasks to be stopped.\n     */\n    public void stopAndAwaitTasks(Collection<ConnectorTaskId> ids) {\n        stopTasks(ids);\n        awaitStopTasks(ids);\n    }\n\n    /**\n     * Stop a task that belongs to this worker and await its termination.\n     *\n     * @param taskId the ID of the task to be stopped.\n     */\n    public void stopAndAwaitTask(ConnectorTaskId taskId) {\n        stopTask(taskId);\n        awaitStopTasks(Collections.singletonList(taskId));\n    }\n\n    /**\n     * Get the IDs of the tasks currently running in this worker.\n     */\n    public Set<ConnectorTaskId> taskIds() {\n        return tasks.keySet();\n    }\n\n    public Converter getInternalKeyConverter() {\n        return internalKeyConverter;\n    }\n\n    public Converter getInternalValueConverter() {\n        return internalValueConverter;\n    }\n\n    public Plugins getPlugins() {\n        return plugins;\n    }\n\n    public String workerId() {\n        return workerId;\n    }\n\n    /**\n     * Returns whether this worker is configured to allow source connectors to create the topics\n     * that they use with custom configurations, if these topics don't already exist.\n     *\n     * @return true if topic creation by source connectors is allowed; false otherwise\n     */\n    public boolean isTopicCreationEnabled() {\n        return config.topicCreationEnable();\n    }\n\n    /**\n     * Get the {@link ConnectMetrics} that uses Kafka Metrics and manages the JMX reporter.\n     * @return the Connect-specific metrics; never null\n     */\n    public ConnectMetrics metrics() {\n        return metrics;\n    }\n\n    public void setTargetState(String connName, TargetState state, Callback<TargetState> stateChangeCallback) {\n        log.info(\"Setting connector {} state to {}\", connName, state);\n\n        WorkerConnector workerConnector = connectors.get(connName);\n        if (workerConnector != null) {\n            try (LoaderSwap loaderSwap = plugins.withClassLoader(workerConnector.loader())) {\n                workerConnector.transitionTo(state, stateChangeCallback);\n            }\n        }\n\n        for (Map.Entry<ConnectorTaskId, WorkerTask<?, ?>> taskEntry : tasks.entrySet()) {\n            if (taskEntry.getKey().connector().equals(connName)) {\n                WorkerTask<?, ?> workerTask = taskEntry.getValue();\n                try (LoaderSwap loaderSwap = plugins.withClassLoader(workerTask.loader())) {\n                    workerTask.transitionTo(state);\n                }\n            }\n        }\n    }\n\n    /**\n     * Get the current offsets for a connector. This method is asynchronous and the passed callback is completed when the\n     * request finishes processing.\n     *\n     * @param connName the name of the connector whose offsets are to be retrieved\n     * @param connectorConfig the connector's configurations\n     * @param cb callback to invoke upon completion of the request\n     */\n    public void connectorOffsets(String connName, Map<String, String> connectorConfig, Callback<ConnectorOffsets> cb) {\n        String connectorClassOrAlias = connectorConfig.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n        ClassLoader connectorLoader = plugins.connectorLoader(connectorClassOrAlias);\n\n        try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n            Connector connector = plugins.newConnector(connectorClassOrAlias);\n            if (ConnectUtils.isSinkConnector(connector)) {\n                log.debug(\"Fetching offsets for sink connector: {}\", connName);\n                sinkConnectorOffsets(connName, connector, connectorConfig, cb);\n            } else {\n                log.debug(\"Fetching offsets for source connector: {}\", connName);\n                sourceConnectorOffsets(connName, connector, connectorConfig, cb);\n            }\n        }\n    }\n\n    /**\n     * Get the current consumer group offsets for a sink connector.\n     * <p>\n     * Visible for testing.\n     *\n     * @param connName the name of the sink connector whose offsets are to be retrieved\n     * @param connector the sink connector\n     * @param connectorConfig the sink connector's configurations\n     * @param cb callback to invoke upon completion of the request\n     */\n    void sinkConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                              Callback<ConnectorOffsets> cb) {\n        Map<String, Object> adminConfig = adminConfigs(\n                connName,\n                \"connector-worker-adminclient-\" + connName,\n                config,\n                new SinkConnectorConfig(plugins, connectorConfig),\n                connector.getClass(),\n                connectorClientConfigOverridePolicy,\n                kafkaClusterId,\n                ConnectorType.SINK);\n        String groupId = (String) baseConsumerConfigs(\n                connName, \"connector-consumer-\", config, new SinkConnectorConfig(plugins, connectorConfig),\n                connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SINK).get(ConsumerConfig.GROUP_ID_CONFIG);\n        Admin admin = adminFactory.apply(adminConfig);\n        try {\n            ListConsumerGroupOffsetsOptions listOffsetsOptions = new ListConsumerGroupOffsetsOptions()\n                    .timeoutMs((int) RestServer.DEFAULT_REST_REQUEST_TIMEOUT_MS);\n            ListConsumerGroupOffsetsResult listConsumerGroupOffsetsResult = admin.listConsumerGroupOffsets(groupId, listOffsetsOptions);\n            listConsumerGroupOffsetsResult.partitionsToOffsetAndMetadata().whenComplete((result, error) -> {\n                if (error != null) {\n                    log.error(\"Failed to retrieve consumer group offsets for sink connector {}\", connName, error);\n                    cb.onCompletion(new ConnectException(\"Failed to retrieve consumer group offsets for sink connector \" + connName, error), null);\n                } else {\n                    ConnectorOffsets offsets = SinkUtils.consumerGroupOffsetsToConnectorOffsets(result);\n                    cb.onCompletion(null, offsets);\n                }\n                Utils.closeQuietly(admin, \"Offset fetch admin for sink connector \" + connName);\n            });\n        } catch (Throwable t) {\n            Utils.closeQuietly(admin, \"Offset fetch admin for sink connector \" + connName);\n            log.error(\"Failed to retrieve consumer group offsets for sink connector {}\", connName, t);\n            cb.onCompletion(new ConnectException(\"Failed to retrieve consumer group offsets for sink connector \" + connName, t), null);\n        }\n    }\n\n    /**\n     * Get the current offsets for a source connector.\n     *\n     * @param connName the name of the source connector whose offsets are to be retrieved\n     * @param connector the source connector\n     * @param connectorConfig the source connector's configurations\n     * @param cb callback to invoke upon completion of the request\n     */\n    private void sourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                        Callback<ConnectorOffsets> cb) {\n        SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins, connectorConfig, config.topicCreationEnable());\n        ConnectorOffsetBackingStore offsetStore = config.exactlyOnceSourceEnabled()\n                ? offsetStoreForExactlyOnceSourceConnector(sourceConfig, connName, connector, null)\n                : offsetStoreForRegularSourceConnector(sourceConfig, connName, connector, null);\n        CloseableOffsetStorageReader offsetReader = new OffsetStorageReaderImpl(offsetStore, connName, internalKeyConverter, internalValueConverter);\n        sourceConnectorOffsets(connName, offsetStore, offsetReader, cb);\n    }\n\n    // Visible for testing\n    void sourceConnectorOffsets(String connName, ConnectorOffsetBackingStore offsetStore,\n                                CloseableOffsetStorageReader offsetReader, Callback<ConnectorOffsets> cb) {\n        executor.submit(() -> {\n            try {\n                offsetStore.configure(config);\n                offsetStore.start();\n                Set<Map<String, Object>> connectorPartitions = offsetStore.connectorPartitions(connName);\n                List<ConnectorOffset> connectorOffsets = offsetReader.offsets(connectorPartitions).entrySet().stream()\n                        .map(entry -> new ConnectorOffset(entry.getKey(), entry.getValue()))\n                        .collect(Collectors.toList());\n                cb.onCompletion(null, new ConnectorOffsets(connectorOffsets));\n            } catch (Throwable t) {\n                log.error(\"Failed to retrieve offsets for source connector {}\", connName, t);\n                cb.onCompletion(ConnectUtils.maybeWrap(t, \"Failed to retrieve offsets for source connector \" + connName), null);\n            } finally {\n                Utils.closeQuietly(offsetReader, \"Offset reader for connector \" + connName);\n                Utils.closeQuietly(offsetStore::stop, \"Offset store for connector \" + connName);\n            }\n        });\n    }\n\n    /**\n     * Modify (alter / reset) a connector's offsets.\n     *\n     * @param connName the name of the connector whose offsets are to be modified\n     * @param connectorConfig the connector's configurations\n     * @param offsets a mapping from partitions (either source partitions for source connectors, or Kafka topic\n     *                partitions for sink connectors) to offsets that need to be written; this should be {@code null}\n     *                for offsets reset requests\n     * @param cb callback to invoke upon completion\n     */\n    public void modifyConnectorOffsets(String connName, Map<String, String> connectorConfig,\n                                      Map<Map<String, ?>, Map<String, ?>> offsets, Callback<Message> cb) {\n        String connectorClassOrAlias = connectorConfig.get(ConnectorConfig.CONNECTOR_CLASS_CONFIG);\n        ClassLoader connectorLoader = plugins.connectorLoader(connectorClassOrAlias);\n        Connector connector;\n\n        try (LoaderSwap loaderSwap = plugins.withClassLoader(connectorLoader)) {\n            connector = plugins.newConnector(connectorClassOrAlias);\n            if (ConnectUtils.isSinkConnector(connector)) {\n                log.debug(\"Modifying offsets for sink connector: {}\", connName);\n                modifySinkConnectorOffsets(connName, connector, connectorConfig, offsets, connectorLoader, cb);\n            } else {\n                log.debug(\"Modifying offsets for source connector: {}\", connName);\n                modifySourceConnectorOffsets(connName, connector, connectorConfig, offsets, connectorLoader, cb);\n            }\n        }\n    }\n\n    /**\n     * Modify (alter / reset) a sink connector's consumer group offsets.\n     * <p>\n     * Visible for testing.\n     *\n     * @param connName the name of the sink connector whose offsets are to be modified\n     * @param connector an instance of the sink connector\n     * @param connectorConfig the sink connector's configuration\n     * @param offsets a mapping from topic partitions to offsets that need to be written; this should be {@code null}\n     *                for offsets reset requests\n     * @param connectorLoader the connector plugin's classloader to be used as the thread context classloader\n     * @param cb callback to invoke upon completion\n     */\n    void modifySinkConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                    Map<Map<String, ?>, Map<String, ?>> offsets, ClassLoader connectorLoader, Callback<Message> cb) {\n        executor.submit(plugins.withClassLoader(connectorLoader, () -> {\n            try {\n                Timer timer = time.timer(Duration.ofMillis(RestServer.DEFAULT_REST_REQUEST_TIMEOUT_MS));\n                boolean isReset = offsets == null;\n                SinkConnectorConfig sinkConnectorConfig = new SinkConnectorConfig(plugins, connectorConfig);\n                Class<? extends Connector> sinkConnectorClass = connector.getClass();\n                Map<String, Object> adminConfig = adminConfigs(\n                        connName,\n                        \"connector-worker-adminclient-\" + connName,\n                        config,\n                        sinkConnectorConfig,\n                        sinkConnectorClass,\n                        connectorClientConfigOverridePolicy,\n                        kafkaClusterId,\n                        ConnectorType.SINK);\n\n                String groupId = (String) baseConsumerConfigs(\n                        connName, \"connector-consumer-\", config, sinkConnectorConfig,\n                        sinkConnectorClass, connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SINK).get(ConsumerConfig.GROUP_ID_CONFIG);\n\n                Admin admin = adminFactory.apply(adminConfig);\n\n                try {\n                    Map<TopicPartition, Long> offsetsToWrite;\n                    if (isReset) {\n                        offsetsToWrite = new HashMap<>();\n                        ListConsumerGroupOffsetsOptions listConsumerGroupOffsetsOptions = new ListConsumerGroupOffsetsOptions()\n                                .timeoutMs((int) timer.remainingMs());\n                        try {\n                            admin.listConsumerGroupOffsets(groupId, listConsumerGroupOffsetsOptions)\n                                    .partitionsToOffsetAndMetadata()\n                                    .get(timer.remainingMs(), TimeUnit.MILLISECONDS)\n                                    .forEach((topicPartition, offsetAndMetadata) -> offsetsToWrite.put(topicPartition, null));\n\n                            timer.update();\n                            log.debug(\"Found the following topic partitions (to reset offsets) for sink connector {} and consumer group ID {}: {}\",\n                                    connName, groupId, offsetsToWrite.keySet());\n                        } catch (Exception e) {\n                            Utils.closeQuietly(admin, \"Offset reset admin for sink connector \" + connName);\n                            log.error(\"Failed to list offsets prior to resetting offsets for sink connector {}\", connName, e);\n                            cb.onCompletion(new ConnectException(\"Failed to list offsets prior to resetting offsets for sink connector \" + connName, e), null);\n                            return;\n                        }\n                    } else {\n                        offsetsToWrite = SinkUtils.parseSinkConnectorOffsets(offsets);\n                    }\n\n                    boolean alterOffsetsResult;\n                    try {\n                        alterOffsetsResult = ((SinkConnector) connector).alterOffsets(connectorConfig, offsetsToWrite);\n                    } catch (UnsupportedOperationException e) {\n                        log.error(\"Failed to modify offsets for connector {} because it doesn't support external modification of offsets\",\n                                connName, e);\n                        throw new ConnectException(\"Failed to modify offsets for connector \" + connName + \" because it doesn't support external \" +\n                                \"modification of offsets\", e);\n                    }\n                    updateTimerAndCheckExpiry(timer, \"Timed out while calling the 'alterOffsets' method for sink connector \" + connName);\n\n                    if (isReset) {\n                        resetSinkConnectorOffsets(connName, groupId, admin, cb, alterOffsetsResult, timer);\n                    } else {\n                        alterSinkConnectorOffsets(connName, groupId, admin, offsetsToWrite, cb, alterOffsetsResult, timer);\n                    }\n                } catch (Throwable t) {\n                    Utils.closeQuietly(admin, \"Offset modification admin for sink connector \" + connName);\n                    throw t;\n                }\n            } catch (Throwable t) {\n                log.error(\"Failed to modify offsets for sink connector {}\", connName, t);\n                cb.onCompletion(ConnectUtils.maybeWrap(t, \"Failed to modify offsets for sink connector \" + connName), null);\n            }\n        }));\n    }\n\n    /**\n     * Alter a sink connector's consumer group offsets. This is done via calls to {@link Admin#alterConsumerGroupOffsets}\n     * and / or {@link Admin#deleteConsumerGroupOffsets}.\n     *\n     * @param connName the name of the sink connector whose offsets are to be altered\n     * @param groupId the sink connector's consumer group ID\n     * @param admin the {@link Admin admin client} to be used for altering the consumer group offsets; will be closed after use\n     * @param offsetsToWrite a mapping from topic partitions to offsets that need to be written; may not be null or empty\n     * @param cb callback to invoke upon completion\n     * @param alterOffsetsResult the result of the call to {@link SinkConnector#alterOffsets} for the connector\n     * @param timer {@link Timer} to bound the total runtime of admin client requests\n     */\n    private void alterSinkConnectorOffsets(String connName, String groupId, Admin admin, Map<TopicPartition, Long> offsetsToWrite,\n                                           Callback<Message> cb, boolean alterOffsetsResult, Timer timer) {\n        List<KafkaFuture<Void>> adminFutures = new ArrayList<>();\n\n        Map<TopicPartition, OffsetAndMetadata> offsetsToAlter = offsetsToWrite.entrySet()\n                .stream()\n                .filter(entry -> entry.getValue() != null)\n                .collect(Collectors.toMap(Map.Entry::getKey, e -> new OffsetAndMetadata(e.getValue())));\n\n        if (!offsetsToAlter.isEmpty()) {\n            log.debug(\"Committing the following consumer group offsets using an admin client for sink connector {}: {}.\",\n                    connName, offsetsToAlter);\n            AlterConsumerGroupOffsetsOptions alterConsumerGroupOffsetsOptions = new AlterConsumerGroupOffsetsOptions()\n                    .timeoutMs((int) timer.remainingMs());\n            AlterConsumerGroupOffsetsResult alterConsumerGroupOffsetsResult = admin.alterConsumerGroupOffsets(groupId, offsetsToAlter,\n                    alterConsumerGroupOffsetsOptions);\n\n            adminFutures.add(alterConsumerGroupOffsetsResult.all());\n        }\n\n        Set<TopicPartition> partitionsToReset = offsetsToWrite.entrySet()\n                .stream()\n                .filter(entry -> entry.getValue() == null)\n                .map(Map.Entry::getKey)\n                .collect(Collectors.toSet());\n\n        if (!partitionsToReset.isEmpty()) {\n            log.debug(\"Deleting the consumer group offsets for the following topic partitions using an admin client for sink connector {}: {}.\",\n                    connName, partitionsToReset);\n            DeleteConsumerGroupOffsetsOptions deleteConsumerGroupOffsetsOptions = new DeleteConsumerGroupOffsetsOptions()\n                    .timeoutMs((int) timer.remainingMs());\n            DeleteConsumerGroupOffsetsResult deleteConsumerGroupOffsetsResult = admin.deleteConsumerGroupOffsets(groupId, partitionsToReset,\n                    deleteConsumerGroupOffsetsOptions);\n\n            adminFutures.add(deleteConsumerGroupOffsetsResult.all());\n        }\n\n        @SuppressWarnings(\"rawtypes\")\n        KafkaFuture<Void> compositeAdminFuture = KafkaFuture.allOf(adminFutures.toArray(new KafkaFuture[0]));\n\n        compositeAdminFuture.whenComplete((ignored, error) -> {\n            if (error != null) {\n                // When a consumer group is non-empty, only group members can commit offsets. An attempt to alter offsets via the admin client\n                // will result in an UnknownMemberIdException if the consumer group is non-empty (i.e. if the sink tasks haven't stopped\n                // completely or if the connector is resumed while the alter offsets request is being processed). Similarly, an attempt to\n                // delete consumer group offsets for a non-empty consumer group will result in a GroupSubscribedToTopicException\n                if (error instanceof UnknownMemberIdException || error instanceof GroupSubscribedToTopicException) {\n                    String errorMsg = \"Failed to alter consumer group offsets for connector \" + connName + \" either because its tasks \" +\n                            \"haven't stopped completely yet or the connector was resumed before the request to alter its offsets could be successfully \" +\n                            \"completed. If the connector is in a stopped state, this operation can be safely retried. If it doesn't eventually succeed, the \" +\n                            \"Connect cluster may need to be restarted to get rid of the zombie sink tasks.\";\n                    log.error(errorMsg, error);\n                    cb.onCompletion(new ConnectException(errorMsg, error), null);\n                } else {\n                    log.error(\"Failed to alter consumer group offsets for connector {}\", connName, error);\n                    cb.onCompletion(new ConnectException(\"Failed to alter consumer group offsets for connector \" + connName, error), null);\n                }\n            } else {\n                completeModifyOffsetsCallback(alterOffsetsResult, false, cb);\n            }\n        }).whenComplete((ignored, ignoredError) -> {\n            // errors originating from the original future are handled in the prior whenComplete invocation which isn't expected to throw\n            // an exception itself, and we can thus ignore the error here\n            Utils.closeQuietly(admin, \"Offset alter admin for sink connector \" + connName);\n        });\n    }\n\n    /**\n     * Reset a sink connector's consumer group offsets. This is done by deleting the consumer group via a call to\n     * {@link Admin#deleteConsumerGroups}\n     *\n     * @param connName the name of the sink connector whose offsets are to be reset\n     * @param groupId the sink connector's consumer group ID\n     * @param admin the {@link Admin admin client} to be used for resetting the consumer group offsets; will be closed after use\n     * @param cb callback to invoke upon completion\n     * @param alterOffsetsResult the result of the call to {@link SinkConnector#alterOffsets} for the connector\n     * @param timer {@link Timer} to bound the total runtime of admin client requests\n     */\n    private void resetSinkConnectorOffsets(String connName, String groupId, Admin admin, Callback<Message> cb, boolean alterOffsetsResult, Timer timer) {\n        DeleteConsumerGroupsOptions deleteConsumerGroupsOptions = new DeleteConsumerGroupsOptions().timeoutMs((int) timer.remainingMs());\n\n        admin.deleteConsumerGroups(Collections.singleton(groupId), deleteConsumerGroupsOptions)\n                .all()\n                .whenComplete((ignored, error) -> {\n                    // We treat GroupIdNotFoundException as a non-error here because resetting a connector's offsets is expected to be an idempotent operation\n                    // and the consumer group could have already been deleted in a prior offsets reset request\n                    if (error != null && !(error instanceof GroupIdNotFoundException)) {\n                        // When a consumer group is non-empty, attempts to delete it via the admin client result in a GroupNotEmptyException. This can occur\n                        // if the sink tasks haven't stopped completely or if the connector is resumed while the reset offsets request is being processed\n                        if (error instanceof GroupNotEmptyException) {\n                            String errorMsg = \"Failed to reset consumer group offsets for connector \" + connName + \" either because its tasks \" +\n                                    \"haven't stopped completely yet or the connector was resumed before the request to reset its offsets could be successfully \" +\n                                    \"completed. If the connector is in a stopped state, this operation can be safely retried. If it doesn't eventually succeed, the \" +\n                                    \"Connect cluster may need to be restarted to get rid of the zombie sink tasks.\";\n                            log.error(errorMsg, error);\n                            cb.onCompletion(new ConnectException(errorMsg, error), null);\n                        } else {\n                            log.error(\"Failed to reset consumer group offsets for sink connector {}\", connName, error);\n                            cb.onCompletion(new ConnectException(\"Failed to reset consumer group offsets for sink connector \" + connName, error), null);\n                        }\n                    } else {\n                        completeModifyOffsetsCallback(alterOffsetsResult, true, cb);\n                    }\n                }).whenComplete((ignored, ignoredError) -> {\n                    // errors originating from the original future are handled in the prior whenComplete invocation which isn't expected to throw\n                    // an exception itself, and we can thus ignore the error here\n                    Utils.closeQuietly(admin, \"Offset reset admin for sink connector \" + connName);\n                });\n    }\n\n    /**\n     * Modify (alter / reset) a source connector's offsets.\n     *\n     * @param connName the name of the source connector whose offsets are to be modified\n     * @param connector an instance of the source connector\n     * @param connectorConfig the source connector's configuration\n     * @param offsets a mapping from partitions to offsets that need to be written; this should be {@code null} for\n     *                offsets reset requests\n     * @param connectorLoader the connector plugin's classloader to be used as the thread context classloader\n     * @param cb callback to invoke upon completion\n     */\n    private void modifySourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                              Map<Map<String, ?>, Map<String, ?>> offsets, ClassLoader connectorLoader, Callback<Message> cb) {\n        SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins, connectorConfig, config.topicCreationEnable());\n        Map<String, Object> producerProps = config.exactlyOnceSourceEnabled()\n                ? exactlyOnceSourceTaskProducerConfigs(new ConnectorTaskId(connName, 0), config, sourceConfig,\n                connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId)\n                : baseProducerConfigs(connName, \"connector-offset-producer-\" + connName, config, sourceConfig,\n                connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId);\n        KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(producerProps);\n\n        ConnectorOffsetBackingStore offsetStore = config.exactlyOnceSourceEnabled()\n                ? offsetStoreForExactlyOnceSourceConnector(sourceConfig, connName, connector, producer)\n                : offsetStoreForRegularSourceConnector(sourceConfig, connName, connector, producer);\n        offsetStore.configure(config);\n\n        OffsetStorageWriter offsetWriter = new OffsetStorageWriter(offsetStore, connName, internalKeyConverter, internalValueConverter);\n        modifySourceConnectorOffsets(connName, connector, connectorConfig, offsets, offsetStore, producer, offsetWriter, connectorLoader, cb);\n    }\n\n    // Visible for testing\n    void modifySourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                      Map<Map<String, ?>, Map<String, ?>> offsets, ConnectorOffsetBackingStore offsetStore,\n                                      KafkaProducer<byte[], byte[]> producer, OffsetStorageWriter offsetWriter,\n                                      ClassLoader connectorLoader, Callback<Message> cb) {\n        executor.submit(plugins.withClassLoader(connectorLoader, () -> {\n            try {\n                Timer timer = time.timer(Duration.ofMillis(RestServer.DEFAULT_REST_REQUEST_TIMEOUT_MS));\n                // This reads to the end of the offsets topic and can be a potentially time-consuming operation\n                offsetStore.start();\n                updateTimerAndCheckExpiry(timer, \"Timed out while trying to read to the end of the offsets topic prior to modifying \" +\n                        \"offsets for source connector \" + connName);\n                Map<Map<String, ?>, Map<String, ?>> offsetsToWrite;\n\n                // If the offsets argument is null, it indicates an offsets reset operation - i.e. a null offset should\n                // be written for every source partition of the connector\n                boolean isReset;\n                if (offsets == null) {\n                    isReset = true;\n                    offsetsToWrite = new HashMap<>();\n                    offsetStore.connectorPartitions(connName).forEach(partition -> offsetsToWrite.put(partition, null));\n                    log.debug(\"Found the following partitions (to reset offsets) for source connector {}: {}\", connName, offsetsToWrite.keySet());\n                } else {\n                    isReset = false;\n                    offsetsToWrite = offsets;\n                }\n\n                Map<Map<String, ?>, Map<String, ?>> normalizedOffsets = normalizeSourceConnectorOffsets(offsetsToWrite);\n\n                boolean alterOffsetsResult;\n                try {\n                    alterOffsetsResult = ((SourceConnector) connector).alterOffsets(connectorConfig, normalizedOffsets);\n                } catch (UnsupportedOperationException e) {\n                    log.error(\"Failed to modify offsets for connector {} because it doesn't support external modification of offsets\",\n                            connName, e);\n                    throw new ConnectException(\"Failed to modify offsets for connector \" + connName + \" because it doesn't support external \" +\n                            \"modification of offsets\", e);\n                }\n                updateTimerAndCheckExpiry(timer, \"Timed out while calling the 'alterOffsets' method for source connector \" + connName);\n\n                // This should only occur for an offsets reset request when there are no source partitions found for the source connector in the\n                // offset store - either because there was a prior attempt to reset offsets or if there are no offsets committed by this source\n                // connector so far\n                if (normalizedOffsets.isEmpty()) {\n                    log.info(\"No offsets found for source connector {} - this can occur due to a prior attempt to reset offsets or if the \" +\n                            \"source connector hasn't committed any offsets yet\", connName);\n                    completeModifyOffsetsCallback(alterOffsetsResult, isReset, cb);\n                    return;\n                }\n\n                // The modifySourceConnectorOffsets method should only be called after all the connector's tasks have been stopped, and it's\n                // safe to write offsets via an offset writer\n                normalizedOffsets.forEach(offsetWriter::offset);\n\n                // We can call begin flush without a timeout because this newly created single-purpose offset writer can't do concurrent\n                // offset writes. We can also ignore the return value since it returns false if and only if there is no data to be flushed,\n                // and we've just put some data in the previous statement\n                offsetWriter.beginFlush();\n\n                if (config.exactlyOnceSourceEnabled()) {\n                    producer.initTransactions();\n                    producer.beginTransaction();\n                }\n                log.debug(\"Committing the following offsets for source connector {}: {}\", connName, normalizedOffsets);\n                FutureCallback<Void> offsetWriterCallback = new FutureCallback<>();\n                offsetWriter.doFlush(offsetWriterCallback);\n                if (config.exactlyOnceSourceEnabled()) {\n                    producer.commitTransaction();\n                }\n\n                try {\n                    offsetWriterCallback.get(timer.remainingMs(), TimeUnit.MILLISECONDS);\n                } catch (ExecutionException e) {\n                    throw new ConnectException(\"Failed to modify offsets for source connector \" + connName, e.getCause());\n                } catch (TimeoutException e) {\n                    throw new ConnectException(\"Timed out while attempting to modify offsets for source connector \" + connName, e);\n                } catch (InterruptedException e) {\n                    throw new ConnectException(\"Unexpectedly interrupted while attempting to modify offsets for source connector \" + connName, e);\n                }\n\n                completeModifyOffsetsCallback(alterOffsetsResult, isReset, cb);\n            } catch (Throwable t) {\n                log.error(\"Failed to modify offsets for source connector {}\", connName, t);\n                cb.onCompletion(ConnectUtils.maybeWrap(t, \"Failed to modify offsets for source connector \" + connName), null);\n            } finally {\n                Utils.closeQuietly(offsetStore::stop, \"Offset store for offset modification request for connector \" + connName);\n            }\n        }));\n    }\n\n    /**\n     * \"Normalize\" source connector offsets by serializing and deserializing them using the internal {@link JsonConverter}.\n     * This is done in order to prevent type mismatches between the offsets passed to {@link SourceConnector#alterOffsets(Map, Map)}\n     * and the offsets that connectors and tasks retrieve via an instance of {@link OffsetStorageReader}.\n     * <p>\n     * Visible for testing.\n     *\n     * @param originalOffsets the offsets that are to be normalized\n     * @return the normalized offsets\n     */\n    @SuppressWarnings(\"unchecked\")\n    Map<Map<String, ?>, Map<String, ?>> normalizeSourceConnectorOffsets(Map<Map<String, ?>, Map<String, ?>> originalOffsets) {\n        Map<Map<String, ?>, Map<String, ?>> normalizedOffsets = new HashMap<>();\n        for (Map.Entry<Map<String, ?>, Map<String, ?>> entry : originalOffsets.entrySet()) {\n            OffsetUtils.validateFormat(entry.getKey());\n            OffsetUtils.validateFormat(entry.getValue());\n            byte[] serializedKey = internalKeyConverter.fromConnectData(\"\", null, entry.getKey());\n            byte[] serializedValue = internalValueConverter.fromConnectData(\"\", null, entry.getValue());\n            Object deserializedKey = internalKeyConverter.toConnectData(\"\", serializedKey).value();\n            Object deserializedValue = internalValueConverter.toConnectData(\"\", serializedValue).value();\n            normalizedOffsets.put((Map<String, ?>) deserializedKey, (Map<String, ?>) deserializedValue);\n        }\n\n        return normalizedOffsets;\n    }\n\n    /**\n     * Update the provided timer, check if it's expired and throw a {@link ConnectException} with the provided error\n     * message if it is.\n     *\n     * @param timer {@link Timer} to check\n     * @param errorMessageIfExpired error message indicating the cause for the timer expiry\n     * @throws ConnectException if the timer has expired\n     */\n    private void updateTimerAndCheckExpiry(Timer timer, String errorMessageIfExpired) {\n        timer.update();\n        if (timer.isExpired()) {\n            log.error(errorMessageIfExpired);\n            throw new ConnectException(errorMessageIfExpired);\n        }\n    }\n\n    /**\n     * Complete the alter / reset offsets callback with a potential-success or a definite-success message.\n     *\n     * @param alterOffsetsResult the result of the call to {@link SinkConnector#alterOffsets} / {@link SourceConnector#alterOffsets}\n     * @param isReset whether this callback if for an offsets reset operation\n     * @param cb the callback to complete\n     *\n     * @see <a href=\"https://cwiki.apache.org/confluence/display/KAFKA/KIP-875%3A+First-class+offsets+support+in+Kafka+Connect\">KIP-875</a>\n     */\n    private void completeModifyOffsetsCallback(boolean alterOffsetsResult, boolean isReset, Callback<Message> cb) {\n        String modificationType = isReset ? \"reset\" : \"altered\";\n        if (alterOffsetsResult) {\n            cb.onCompletion(null, new Message(\"The offsets for this connector have been \" + modificationType + \" successfully\"));\n        } else {\n            cb.onCompletion(null, new Message(\"The Connect framework-managed offsets for this connector have been \" +\n                    modificationType + \" successfully. However, if this connector manages offsets externally, they will need to be \" +\n                    \"manually \" + modificationType + \" in the system that the connector uses.\"));\n        }\n    }\n\n    ConnectorStatusMetricsGroup connectorStatusMetricsGroup() {\n        return connectorStatusMetricsGroup;\n    }\n\n    WorkerMetricsGroup workerMetricsGroup() {\n        return workerMetricsGroup;\n    }\n\n    abstract class TaskBuilder<T, R extends ConnectRecord<R>> {\n\n        private final ConnectorTaskId id;\n        private final ClusterConfigState configState;\n        private final TaskStatus.Listener statusListener;\n        private final TargetState initialState;\n\n        private Task task = null;\n        private ConnectorConfig connectorConfig = null;\n        private Converter keyConverter = null;\n        private Converter valueConverter = null;\n        private HeaderConverter headerConverter = null;\n        private ClassLoader classLoader = null;\n\n        public TaskBuilder(ConnectorTaskId id,\n                           ClusterConfigState configState,\n                           TaskStatus.Listener statusListener,\n                           TargetState initialState) {\n            this.id = id;\n            this.configState = configState;\n            this.statusListener = statusListener;\n            this.initialState = initialState;\n        }\n\n        public TaskBuilder<T, R> withTask(Task task) {\n            this.task = task;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withConnectorConfig(ConnectorConfig connectorConfig) {\n            this.connectorConfig = connectorConfig;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withKeyConverter(Converter keyConverter) {\n            this.keyConverter = keyConverter;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withValueConverter(Converter valueConverter) {\n            this.valueConverter = valueConverter;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withHeaderConverter(HeaderConverter headerConverter) {\n            this.headerConverter = headerConverter;\n            return this;\n        }\n\n        public TaskBuilder<T, R> withClassloader(ClassLoader classLoader) {\n            this.classLoader = classLoader;\n            return this;\n        }\n\n        public WorkerTask<T, R> build() {\n            Objects.requireNonNull(task, \"Task cannot be null\");\n            Objects.requireNonNull(connectorConfig, \"Connector config used by task cannot be null\");\n            Objects.requireNonNull(keyConverter, \"Key converter used by task cannot be null\");\n            Objects.requireNonNull(valueConverter, \"Value converter used by task cannot be null\");\n            Objects.requireNonNull(headerConverter, \"Header converter used by task cannot be null\");\n            Objects.requireNonNull(classLoader, \"Classloader used by task cannot be null\");\n\n            ErrorHandlingMetrics errorHandlingMetrics = errorHandlingMetrics(id);\n            final Class<? extends Connector> connectorClass = plugins.connectorClass(\n                    connectorConfig.getString(ConnectorConfig.CONNECTOR_CLASS_CONFIG));\n\n            RetryWithToleranceOperator<T> retryWithToleranceOperator = new RetryWithToleranceOperator<>(connectorConfig.errorRetryTimeout(),\n                    connectorConfig.errorMaxDelayInMillis(), connectorConfig.errorToleranceType(), Time.SYSTEM, errorHandlingMetrics);\n\n            TransformationChain<T, R> transformationChain = new TransformationChain<>(connectorConfig.<R>transformationStages(), retryWithToleranceOperator);\n            log.info(\"Initializing: {}\", transformationChain);\n\n            return doBuild(task, id, configState, statusListener, initialState,\n                    connectorConfig, keyConverter, valueConverter, headerConverter, classLoader,\n                    retryWithToleranceOperator, transformationChain,\n                    errorHandlingMetrics, connectorClass);\n        }\n\n        abstract WorkerTask<T, R> doBuild(\n                Task task,\n                ConnectorTaskId id,\n                ClusterConfigState configState,\n                TaskStatus.Listener statusListener,\n                TargetState initialState,\n                ConnectorConfig connectorConfig,\n                Converter keyConverter,\n                Converter valueConverter,\n                HeaderConverter headerConverter,\n                ClassLoader classLoader,\n                RetryWithToleranceOperator<T> retryWithToleranceOperator,\n                TransformationChain<T, R> transformationChain,\n                ErrorHandlingMetrics errorHandlingMetrics,\n                Class<? extends Connector> connectorClass\n        );\n\n    }\n\n    class SinkTaskBuilder extends TaskBuilder<ConsumerRecord<byte[], byte[]>, SinkRecord> {\n        public SinkTaskBuilder(ConnectorTaskId id,\n                               ClusterConfigState configState,\n                               TaskStatus.Listener statusListener,\n                               TargetState initialState) {\n            super(id, configState, statusListener, initialState);\n        }\n\n        @Override\n        public WorkerTask<ConsumerRecord<byte[], byte[]>, SinkRecord> doBuild(\n                Task task,\n                ConnectorTaskId id,\n                ClusterConfigState configState,\n                TaskStatus.Listener statusListener,\n                TargetState initialState,\n                ConnectorConfig connectorConfig,\n                Converter keyConverter,\n                Converter valueConverter,\n                HeaderConverter headerConverter,\n                ClassLoader classLoader,\n                RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n                TransformationChain<ConsumerRecord<byte[], byte[]>, SinkRecord> transformationChain,\n                ErrorHandlingMetrics errorHandlingMetrics,\n                Class<? extends Connector> connectorClass\n        ) {\n            SinkConnectorConfig sinkConfig = new SinkConnectorConfig(plugins, connectorConfig.originalsStrings());\n            WorkerErrantRecordReporter workerErrantRecordReporter = createWorkerErrantRecordReporter(sinkConfig, retryWithToleranceOperator,\n                    keyConverter, valueConverter, headerConverter);\n\n            Map<String, Object> consumerProps = baseConsumerConfigs(\n                    id.connector(),  \"connector-consumer-\" + id, config, connectorConfig, connectorClass,\n                    connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SINK);\n            KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n            return new WorkerSinkTask(id, (SinkTask) task, statusListener, initialState, config, configState, metrics, keyConverter,\n                    valueConverter, errorHandlingMetrics, headerConverter, transformationChain, consumer, classLoader, time,\n                    retryWithToleranceOperator, workerErrantRecordReporter, herder.statusBackingStore(),\n                    () -> sinkTaskReporters(id, sinkConfig, errorHandlingMetrics, connectorClass));\n        }\n    }\n\n    class SourceTaskBuilder extends TaskBuilder<SourceRecord, SourceRecord> {\n        public SourceTaskBuilder(ConnectorTaskId id,\n                               ClusterConfigState configState,\n                               TaskStatus.Listener statusListener,\n                               TargetState initialState) {\n            super(id, configState, statusListener, initialState);\n        }\n\n        @Override\n        public WorkerTask<SourceRecord, SourceRecord> doBuild(\n                Task task,\n                ConnectorTaskId id,\n                ClusterConfigState configState,\n                TaskStatus.Listener statusListener,\n                TargetState initialState,\n                ConnectorConfig connectorConfig,\n                Converter keyConverter,\n                Converter valueConverter,\n                HeaderConverter headerConverter,\n                ClassLoader classLoader,\n                RetryWithToleranceOperator<SourceRecord> retryWithToleranceOperator,\n                TransformationChain<SourceRecord, SourceRecord> transformationChain,\n                ErrorHandlingMetrics errorHandlingMetrics,\n                Class<? extends Connector> connectorClass\n        ) {\n            SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins,\n                    connectorConfig.originalsStrings(), config.topicCreationEnable());\n\n            Map<String, Object> producerProps = baseProducerConfigs(id.connector(), \"connector-producer-\" + id, config, sourceConfig, connectorClass,\n                    connectorClientConfigOverridePolicy, kafkaClusterId);\n            KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(producerProps);\n\n            TopicAdmin topicAdmin = null;\n            final boolean topicCreationEnabled = sourceConnectorTopicCreationEnabled(sourceConfig);\n            if (topicCreationEnabled || regularSourceTaskUsesConnectorSpecificOffsetsStore(sourceConfig)) {\n                Map<String, Object> adminOverrides = adminConfigs(id.connector(), \"connector-adminclient-\" + id, config,\n                        sourceConfig, connectorClass, connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SOURCE);\n                topicAdmin = new TopicAdmin(adminOverrides);\n            }\n\n            Map<String, TopicCreationGroup> topicCreationGroups = topicCreationEnabled\n                    ? TopicCreationGroup.configuredGroups(sourceConfig)\n                    : null;\n\n            // Set up the offset backing store for this task instance\n            ConnectorOffsetBackingStore offsetStore = offsetStoreForRegularSourceTask(\n                    id, sourceConfig, connectorClass, producer, producerProps, topicAdmin);\n            offsetStore.configure(config);\n\n            CloseableOffsetStorageReader offsetReader = new OffsetStorageReaderImpl(offsetStore, id.connector(), internalKeyConverter, internalValueConverter);\n            OffsetStorageWriter offsetWriter = new OffsetStorageWriter(offsetStore, id.connector(), internalKeyConverter, internalValueConverter);\n\n            // Note we pass the configState as it performs dynamic transformations under the covers\n            return new WorkerSourceTask(id, (SourceTask) task, statusListener, initialState, keyConverter, valueConverter, errorHandlingMetrics,\n                    headerConverter, transformationChain, producer, topicAdmin, topicCreationGroups,\n                    offsetReader, offsetWriter, offsetStore, config, configState, metrics, classLoader, time,\n                    retryWithToleranceOperator, herder.statusBackingStore(), executor, () -> sourceTaskReporters(id, sourceConfig, errorHandlingMetrics));\n        }\n    }\n\n    class ExactlyOnceSourceTaskBuilder extends TaskBuilder<SourceRecord, SourceRecord> {\n        private final Runnable preProducerCheck;\n        private final Runnable postProducerCheck;\n\n        public ExactlyOnceSourceTaskBuilder(ConnectorTaskId id,\n                                            ClusterConfigState configState,\n                                            TaskStatus.Listener statusListener,\n                                            TargetState initialState,\n                                            Runnable preProducerCheck,\n                                            Runnable postProducerCheck) {\n            super(id, configState, statusListener, initialState);\n            this.preProducerCheck = preProducerCheck;\n            this.postProducerCheck = postProducerCheck;\n        }\n\n        @Override\n        public WorkerTask<SourceRecord, SourceRecord> doBuild(\n                Task task,\n                ConnectorTaskId id,\n                ClusterConfigState configState,\n                TaskStatus.Listener statusListener,\n                TargetState initialState,\n                ConnectorConfig connectorConfig,\n                Converter keyConverter,\n                Converter valueConverter,\n                HeaderConverter headerConverter,\n                ClassLoader classLoader,\n                RetryWithToleranceOperator<SourceRecord> retryWithToleranceOperator,\n                TransformationChain<SourceRecord, SourceRecord> transformationChain,\n                ErrorHandlingMetrics errorHandlingMetrics,\n                Class<? extends Connector> connectorClass\n        ) {\n            SourceConnectorConfig sourceConfig = new SourceConnectorConfig(plugins,\n                    connectorConfig.originalsStrings(), config.topicCreationEnable());\n            Map<String, Object> producerProps = exactlyOnceSourceTaskProducerConfigs(\n                    id, config, sourceConfig, connectorClass,\n                    connectorClientConfigOverridePolicy, kafkaClusterId);\n            KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(producerProps);\n\n            // Create a topic admin that the task will use for its offsets topic and, potentially, automatic topic creation\n            Map<String, Object> adminOverrides = adminConfigs(id.connector(), \"connector-adminclient-\" + id, config,\n                    sourceConfig, connectorClass, connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SOURCE);\n            TopicAdmin topicAdmin = new TopicAdmin(adminOverrides);\n\n            Map<String, TopicCreationGroup> topicCreationGroups = sourceConnectorTopicCreationEnabled(sourceConfig)\n                    ? TopicCreationGroup.configuredGroups(sourceConfig)\n                    : null;\n\n            // Set up the offset backing store for this task instance\n            ConnectorOffsetBackingStore offsetStore = offsetStoreForExactlyOnceSourceTask(\n                    id, sourceConfig, connectorClass, producer, producerProps, topicAdmin);\n            offsetStore.configure(config);\n\n            CloseableOffsetStorageReader offsetReader = new OffsetStorageReaderImpl(offsetStore, id.connector(), internalKeyConverter, internalValueConverter);\n            OffsetStorageWriter offsetWriter = new OffsetStorageWriter(offsetStore, id.connector(), internalKeyConverter, internalValueConverter);\n\n            // Note we pass the configState as it performs dynamic transformations under the covers\n            return new ExactlyOnceWorkerSourceTask(id, (SourceTask) task, statusListener, initialState, keyConverter, valueConverter,\n                    headerConverter, transformationChain, producer, topicAdmin, topicCreationGroups,\n                    offsetReader, offsetWriter, offsetStore, config, configState, metrics, errorHandlingMetrics, classLoader, time, retryWithToleranceOperator,\n                    herder.statusBackingStore(), sourceConfig, executor, preProducerCheck, postProducerCheck,\n                    () -> sourceTaskReporters(id, sourceConfig, errorHandlingMetrics));\n        }\n    }\n\n    /**\n     * Builds and returns an offset backing store for a regular source connector (i.e. when exactly-once support for source connectors is disabled).\n     * The offset backing store will either be just the worker's global offset backing store (if the connector doesn't define a connector-specific\n     * offset topic via its configs), just a connector-specific offset backing store (if the connector defines a connector-specific offsets\n     * topic which appears to be the same as the worker's global offset topic) or a combination of both the worker's global offset backing store\n     * and a connector-specific offset backing store.\n     * <p>\n     * Visible for testing.\n     * @param sourceConfig the source connector's config\n     * @param connName the source connector's name\n     * @param connector the source connector\n     * @param producer the Kafka producer for the offset backing store; may be {@code null} if a read-only offset backing store is required\n     * @return An offset backing store for a regular source connector\n     */\n    ConnectorOffsetBackingStore offsetStoreForRegularSourceConnector(\n            SourceConnectorConfig sourceConfig,\n            String connName,\n            Connector connector,\n            Producer<byte[], byte[]> producer\n    ) {\n        String connectorSpecificOffsetsTopic = sourceConfig.offsetsTopic();\n\n        Map<String, Object> producerProps = baseProducerConfigs(connName, \"connector-producer-\" + connName, config, sourceConfig, connector.getClass(),\n                connectorClientConfigOverridePolicy, kafkaClusterId);\n\n        // We use a connector-specific store (i.e., a dedicated KafkaOffsetBackingStore for this connector)\n        // if the worker supports per-connector offsets topics (which may be the case in distributed but not standalone mode, for example)\n        // and if the connector is explicitly configured with an offsets topic\n        final boolean usesConnectorSpecificStore = connectorSpecificOffsetsTopic != null\n                && config.connectorOffsetsTopicsPermitted();\n\n        if (usesConnectorSpecificStore) {\n            Map<String, Object> consumerProps = regularSourceOffsetsConsumerConfigs(\n                        connName, \"connector-consumer-\" + connName, config, sourceConfig, connector.getClass(),\n                        connectorClientConfigOverridePolicy, kafkaClusterId);\n            KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n            Map<String, Object> adminOverrides = adminConfigs(connName, \"connector-adminclient-\" + connName, config,\n                    sourceConfig, connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SOURCE);\n\n            TopicAdmin admin = new TopicAdmin(adminOverrides);\n\n            KafkaOffsetBackingStore connectorStore = producer == null\n                    ? KafkaOffsetBackingStore.readOnlyStore(connectorSpecificOffsetsTopic, consumer, admin, internalKeyConverter)\n                    : KafkaOffsetBackingStore.readWriteStore(connectorSpecificOffsetsTopic, producer, consumer, admin, internalKeyConverter);\n\n            // If the connector's offsets topic is the same as the worker-global offsets topic, there's no need to construct\n            // an offset store that has a primary and a secondary store which both read from that same topic.\n            // So, if the user has explicitly configured the connector with a connector-specific offsets topic\n            // but we know that that topic is the same as the worker-global offsets topic, we ignore the worker-global\n            // offset store and build a store backed exclusively by a connector-specific offsets store.\n            // It may seem reasonable to instead build a store backed exclusively by the worker-global offset store, but that\n            // would prevent users from being able to customize the config properties used for the Kafka clients that\n            // access the offsets topic, and we would not be able to establish reasonable defaults like setting\n            // isolation.level=read_committed for the offsets topic consumer for this connector\n            if (sameOffsetTopicAsWorker(connectorSpecificOffsetsTopic, producerProps)) {\n                return ConnectorOffsetBackingStore.withOnlyConnectorStore(\n                        () -> LoggingContext.forConnector(connName),\n                        connectorStore,\n                        connectorSpecificOffsetsTopic,\n                        admin\n                );\n            } else {\n                return ConnectorOffsetBackingStore.withConnectorAndWorkerStores(\n                        () -> LoggingContext.forConnector(connName),\n                        globalOffsetBackingStore,\n                        connectorStore,\n                        connectorSpecificOffsetsTopic,\n                        admin\n                );\n            }\n        } else {\n            Utils.closeQuietly(producer, \"Unused producer for offset store\");\n            return ConnectorOffsetBackingStore.withOnlyWorkerStore(\n                    () -> LoggingContext.forConnector(connName),\n                    globalOffsetBackingStore,\n                    config.offsetsTopic()\n            );\n        }\n    }\n\n    /**\n     * Builds and returns an offset backing store for an exactly-once source connector. The offset backing store will either be just\n     * a connector-specific offset backing store (if the connector's offsets topic is the same as the worker's global offset topic)\n     * or a combination of both the worker's global offset backing store and a connector-specific offset backing store.\n     * <p>\n     * Visible for testing.\n     * @param sourceConfig the source connector's config\n     * @param connName the source connector's name\n     * @param connector the source connector\n     * @param producer the Kafka producer for the offset backing store; may be {@code null} if a read-only offset backing store is required\n     * @return An offset backing store for an exactly-once source connector\n     */\n    ConnectorOffsetBackingStore offsetStoreForExactlyOnceSourceConnector(\n            SourceConnectorConfig sourceConfig,\n            String connName,\n            Connector connector,\n            Producer<byte[], byte[]> producer\n    ) {\n        String connectorSpecificOffsetsTopic = Optional.ofNullable(sourceConfig.offsetsTopic()).orElse(config.offsetsTopic());\n\n        Map<String, Object> producerProps = baseProducerConfigs(connName, \"connector-producer-\" + connName, config, sourceConfig, connector.getClass(),\n                connectorClientConfigOverridePolicy, kafkaClusterId);\n\n        Map<String, Object> consumerProps = exactlyOnceSourceOffsetsConsumerConfigs(\n                    connName, \"connector-consumer-\" + connName, config, sourceConfig, connector.getClass(),\n                    connectorClientConfigOverridePolicy, kafkaClusterId);\n        KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n        Map<String, Object> adminOverrides = adminConfigs(connName, \"connector-adminclient-\" + connName, config,\n                sourceConfig, connector.getClass(), connectorClientConfigOverridePolicy, kafkaClusterId, ConnectorType.SOURCE);\n\n        TopicAdmin admin = new TopicAdmin(adminOverrides);\n\n        KafkaOffsetBackingStore connectorStore = producer == null\n                ? KafkaOffsetBackingStore.readOnlyStore(connectorSpecificOffsetsTopic, consumer, admin, internalKeyConverter)\n                : KafkaOffsetBackingStore.readWriteStore(connectorSpecificOffsetsTopic, producer, consumer, admin, internalKeyConverter);\n\n        // If the connector's offsets topic is the same as the worker-global offsets topic, there's no need to construct\n        // an offset store that has a primary and a secondary store which both read from that same topic.\n        // So, even if the user has explicitly configured the connector with a connector-specific offsets topic,\n        // if we know that that topic is the same as the worker-global offsets topic, we ignore the worker-global\n        // offset store and build a store backed exclusively by a connector-specific offsets store.\n        // It may seem reasonable to instead build a store backed exclusively by the worker-global offset store, but that\n        // would prevent users from being able to customize the config properties used for the Kafka clients that\n        // access the offsets topic, and may lead to confusion for them when tasks are created for the connector\n        // since they will all have their own dedicated offsets stores anyways\n        if (sameOffsetTopicAsWorker(connectorSpecificOffsetsTopic, producerProps)) {\n            return ConnectorOffsetBackingStore.withOnlyConnectorStore(\n                    () -> LoggingContext.forConnector(connName),\n                    connectorStore,\n                    connectorSpecificOffsetsTopic,\n                    admin\n            );\n        } else {\n            return ConnectorOffsetBackingStore.withConnectorAndWorkerStores(\n                    () -> LoggingContext.forConnector(connName),\n                    globalOffsetBackingStore,\n                    connectorStore,\n                    connectorSpecificOffsetsTopic,\n                    admin\n            );\n        }\n    }\n\n    // Visible for testing\n    ConnectorOffsetBackingStore offsetStoreForRegularSourceTask(\n            ConnectorTaskId id,\n            SourceConnectorConfig sourceConfig,\n            Class<? extends Connector> connectorClass,\n            Producer<byte[], byte[]> producer,\n            Map<String, Object> producerProps,\n            TopicAdmin topicAdmin\n    ) {\n        String connectorSpecificOffsetsTopic = sourceConfig.offsetsTopic();\n\n        if (regularSourceTaskUsesConnectorSpecificOffsetsStore(sourceConfig)) {\n            Objects.requireNonNull(topicAdmin, \"Source tasks require a non-null topic admin when configured to use their own offsets topic\");\n\n            Map<String, Object> consumerProps = regularSourceOffsetsConsumerConfigs(\n                    id.connector(), \"connector-consumer-\" + id, config, sourceConfig, connectorClass,\n                    connectorClientConfigOverridePolicy, kafkaClusterId);\n            KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n            KafkaOffsetBackingStore connectorStore =\n                    KafkaOffsetBackingStore.readWriteStore(sourceConfig.offsetsTopic(), producer, consumer, topicAdmin, internalKeyConverter);\n\n            // If the connector's offsets topic is the same as the worker-global offsets topic, there's no need to construct\n            // an offset store that has a primary and a secondary store which both read from that same topic.\n            // So, if the user has (implicitly or explicitly) configured the connector with a connector-specific offsets topic\n            // but we know that that topic is the same as the worker-global offsets topic, we ignore the worker-global\n            // offset store and build a store backed exclusively by a connector-specific offsets store.\n            // It may seem reasonable to instead build a store backed exclusively by the worker-global offset store, but that\n            // would prevent users from being able to customize the config properties used for the Kafka clients that\n            // access the offsets topic, and we would not be able to establish reasonable defaults like setting\n            // isolation.level=read_committed for the offsets topic consumer for this task\n            if (sameOffsetTopicAsWorker(sourceConfig.offsetsTopic(), producerProps)) {\n                return ConnectorOffsetBackingStore.withOnlyConnectorStore(\n                        () -> LoggingContext.forTask(id),\n                        connectorStore,\n                        connectorSpecificOffsetsTopic,\n                        topicAdmin\n                );\n            } else {\n                return ConnectorOffsetBackingStore.withConnectorAndWorkerStores(\n                        () -> LoggingContext.forTask(id),\n                        globalOffsetBackingStore,\n                        connectorStore,\n                        connectorSpecificOffsetsTopic,\n                        topicAdmin\n                );\n            }\n        } else {\n            return ConnectorOffsetBackingStore.withOnlyWorkerStore(\n                    () -> LoggingContext.forTask(id),\n                    globalOffsetBackingStore,\n                    config.offsetsTopic()\n            );\n        }\n    }\n\n    // Visible for testing\n    ConnectorOffsetBackingStore offsetStoreForExactlyOnceSourceTask(\n            ConnectorTaskId id,\n            SourceConnectorConfig sourceConfig,\n            Class<? extends Connector> connectorClass,\n            Producer<byte[], byte[]> producer,\n            Map<String, Object> producerProps,\n            TopicAdmin topicAdmin\n    ) {\n        Objects.requireNonNull(topicAdmin, \"Source tasks require a non-null topic admin when exactly-once support is enabled\");\n\n        Map<String, Object> consumerProps = exactlyOnceSourceOffsetsConsumerConfigs(\n                id.connector(), \"connector-consumer-\" + id, config, sourceConfig, connectorClass,\n                connectorClientConfigOverridePolicy, kafkaClusterId);\n        KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(consumerProps);\n\n        String connectorOffsetsTopic = Optional.ofNullable(sourceConfig.offsetsTopic()).orElse(config.offsetsTopic());\n\n        KafkaOffsetBackingStore connectorStore =\n                KafkaOffsetBackingStore.readWriteStore(connectorOffsetsTopic, producer, consumer, topicAdmin, internalKeyConverter);\n\n        // If the connector's offsets topic is the same as the worker-global offsets topic, there's no need to construct\n        // an offset store that has a primary and a secondary store which both read from that same topic.\n        // So, if the user has (implicitly or explicitly) configured the connector with a connector-specific offsets topic\n        // but we know that that topic is the same as the worker-global offsets topic, we ignore the worker-global\n        // offset store and build a store backed exclusively by a connector-specific offsets store.\n        // We cannot under any circumstances build an offset store backed exclusively by the worker-global offset store\n        // as that would prevent us from being able to write source records and source offset information for the task\n        // with the same producer, and therefore, in the same transaction.\n        if (sameOffsetTopicAsWorker(connectorOffsetsTopic, producerProps)) {\n            return ConnectorOffsetBackingStore.withOnlyConnectorStore(\n                    () -> LoggingContext.forTask(id),\n                    connectorStore,\n                    connectorOffsetsTopic,\n                    topicAdmin\n            );\n        } else {\n            return ConnectorOffsetBackingStore.withConnectorAndWorkerStores(\n                    () -> LoggingContext.forTask(id),\n                    globalOffsetBackingStore,\n                    connectorStore,\n                    connectorOffsetsTopic,\n                    topicAdmin\n            );\n        }\n    }\n\n    /**\n     * Gives a best-effort guess for whether the given offsets topic is the same topic as the worker-global offsets topic.\n     * Even if the name of the topic is the same as the name of the worker's offsets topic, the two may still be different topics\n     * if the connector is configured to produce to a different Kafka cluster than the one that hosts the worker's offsets topic.\n     * @param offsetsTopic the name of the offsets topic for the connector\n     * @param producerProps the producer configuration for the connector\n     * @return whether it appears that the connector's offsets topic is the same topic as the worker-global offsets topic.\n     * If {@code true}, it is guaranteed that the two are the same;\n     * if {@code false}, it is likely but not guaranteed that the two are not the same\n     */\n    private boolean sameOffsetTopicAsWorker(String offsetsTopic, Map<String, Object> producerProps) {\n        // We can check the offset topic name and the Kafka cluster's bootstrap servers,\n        // although this isn't exact and can lead to some false negatives if the user\n        // provides an overridden bootstrap servers value for their producer that is different than\n        // the worker's but still resolves to the same Kafka cluster used by the worker.\n        // At the moment this is probably adequate, especially since we don't want to put\n        // a network ping to a remote Kafka cluster inside the herder's tick thread (which is where this\n        // logic takes place right now) in case that takes a while.\n        Set<String> workerBootstrapServers = new HashSet<>(config.getList(BOOTSTRAP_SERVERS_CONFIG));\n        Set<String> producerBootstrapServers = new HashSet<>();\n        try {\n            String rawBootstrapServers = producerProps.getOrDefault(BOOTSTRAP_SERVERS_CONFIG, \"\").toString();\n            @SuppressWarnings(\"unchecked\")\n            List<String> parsedBootstrapServers = (List<String>) ConfigDef.parseType(BOOTSTRAP_SERVERS_CONFIG, rawBootstrapServers, ConfigDef.Type.LIST);\n            producerBootstrapServers.addAll(parsedBootstrapServers);\n        } catch (Exception e) {\n            // Should never happen by this point, but if it does, make sure to present a readable error message to the user\n            throw new ConnectException(\"Failed to parse bootstrap servers property in producer config\", e);\n        }\n        return offsetsTopic.equals(config.offsetsTopic())\n                && workerBootstrapServers.equals(producerBootstrapServers);\n    }\n\n    private boolean regularSourceTaskUsesConnectorSpecificOffsetsStore(SourceConnectorConfig sourceConfig) {\n        // We use a connector-specific store (i.e., a dedicated KafkaOffsetBackingStore for this task)\n        // if the worker supports per-connector offsets topics (which may be the case in distributed mode but not standalone, for example)\n        // and the user has explicitly specified an offsets topic for the connector\n        return sourceConfig.offsetsTopic() != null && config.connectorOffsetsTopicsPermitted();\n    }\n\n    private boolean sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig) {\n        return config.topicCreationEnable() && sourceConfig.usesTopicCreation();\n    }\n\n    static class ConnectorStatusMetricsGroup {\n        private final ConnectMetrics connectMetrics;\n        private final ConnectMetricsRegistry registry;\n        private final ConcurrentMap<String, MetricGroup> connectorStatusMetrics = new ConcurrentHashMap<>();\n        private final Herder herder;\n        private final ConcurrentMap<ConnectorTaskId, WorkerTask<?, ?>> tasks;\n\n\n        protected ConnectorStatusMetricsGroup(\n                ConnectMetrics connectMetrics, ConcurrentMap<ConnectorTaskId, WorkerTask<?, ?>> tasks, Herder herder) {\n            this.connectMetrics = connectMetrics;\n            this.registry = connectMetrics.registry();\n            this.tasks = tasks;\n            this.herder = herder;\n        }\n\n        protected ConnectMetrics.LiteralSupplier<Long> taskCounter(String connName) {\n            return now -> tasks.keySet()\n                .stream()\n                .filter(taskId -> taskId.connector().equals(connName))\n                .count();\n        }\n\n        protected ConnectMetrics.LiteralSupplier<Long> taskStatusCounter(String connName, TaskStatus.State state) {\n            return now -> tasks.values()\n                .stream()\n                .filter(task ->\n                    task.id().connector().equals(connName) &&\n                    herder.taskStatus(task.id()).state().equalsIgnoreCase(state.toString()))\n                .count();\n        }\n\n        protected synchronized void recordTaskAdded(ConnectorTaskId connectorTaskId) {\n            if (connectorStatusMetrics.containsKey(connectorTaskId.connector())) {\n                return;\n            }\n\n            String connName = connectorTaskId.connector();\n\n            MetricGroup metricGroup = connectMetrics.group(registry.workerGroupName(),\n                registry.connectorTagName(), connName);\n\n            metricGroup.addValueMetric(registry.connectorTotalTaskCount, taskCounter(connName));\n            for (Map.Entry<MetricNameTemplate, TaskStatus.State> statusMetric : registry.connectorStatusMetrics\n                .entrySet()) {\n                metricGroup.addValueMetric(statusMetric.getKey(), taskStatusCounter(connName,\n                    statusMetric.getValue()));\n            }\n            connectorStatusMetrics.put(connectorTaskId.connector(), metricGroup);\n        }\n\n        protected synchronized void recordTaskRemoved(ConnectorTaskId connectorTaskId) {\n            // Unregister connector task count metric if we remove the last task of the connector\n            if (tasks.keySet().stream().noneMatch(id -> id.connector().equals(connectorTaskId.connector()))) {\n                connectorStatusMetrics.get(connectorTaskId.connector()).close();\n                connectorStatusMetrics.remove(connectorTaskId.connector());\n            }\n        }\n\n        protected synchronized void close() {\n            for (MetricGroup metricGroup: connectorStatusMetrics.values()) {\n                metricGroup.close();\n            }\n        }\n\n        protected MetricGroup metricGroup(String connectorId) {\n            return connectorStatusMetrics.get(connectorId);\n        }\n    }\n\n}",
                "methodCount": 97
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "initConfigTransformer",
                            "method_signature": "private initConfigTransformer()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "start",
                            "method_signature": "public start()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stop",
                            "method_signature": "public stop()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "startConnector",
                            "method_signature": "public startConnector(\n            String connName,\n            Map<String, String> connProps,\n            CloseableConnectorContext ctx,\n            ConnectorStatus.Listener statusListener,\n            TargetState initialState,\n            Callback<TargetState> onConnectorStateChange\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isSinkConnector",
                            "method_signature": "public isSinkConnector(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "connectorTaskConfigs",
                            "method_signature": "public connectorTaskConfigs(String connName, ConnectorConfig connConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tasksMax",
                            "method_signature": "private tasksMax(ConnectorConfig connConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopConnector",
                            "method_signature": "private stopConnector(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopConnectors",
                            "method_signature": "private stopConnectors(Collection<String> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitStopConnector",
                            "method_signature": "private awaitStopConnector(String connName, long timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitStopConnectors",
                            "method_signature": "private awaitStopConnectors(Collection<String> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopAndAwaitConnectors",
                            "method_signature": "public stopAndAwaitConnectors()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopAndAwaitConnectors",
                            "method_signature": "public stopAndAwaitConnectors(Collection<String> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopAndAwaitConnector",
                            "method_signature": "public stopAndAwaitConnector(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "connectorNames",
                            "method_signature": "public connectorNames()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunning",
                            "method_signature": "public isRunning(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "startSinkTask",
                            "method_signature": "public startSinkTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "startSourceTask",
                            "method_signature": "public startSourceTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "startExactlyOnceSourceTask",
                            "method_signature": "public startExactlyOnceSourceTask(\n            ConnectorTaskId id,\n            ClusterConfigState configState,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TargetState initialState,\n            Runnable preProducerCheck,\n            Runnable postProducerCheck\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "startTask",
                            "method_signature": "private startTask(\n            ConnectorTaskId id,\n            Map<String, String> connProps,\n            Map<String, String> taskProps,\n            TaskStatus.Listener statusListener,\n            TaskBuilder<?, ?> taskBuilder\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fenceZombies",
                            "method_signature": "public fenceZombies(String connName, int numTasks, Map<String, String> connProps)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "exactlyOnceSourceTaskProducerConfigs",
                            "method_signature": "static exactlyOnceSourceTaskProducerConfigs(ConnectorTaskId id,\n                                                              WorkerConfig config,\n                                                              ConnectorConfig connConfig,\n                                                              Class<? extends Connector>  connectorClass,\n                                                              ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                              String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "baseProducerConfigs",
                            "method_signature": "static baseProducerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector>  connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "exactlyOnceSourceOffsetsConsumerConfigs",
                            "method_signature": "static exactlyOnceSourceOffsetsConsumerConfigs(String connName,\n                                                                       String defaultClientId,\n                                                                       WorkerConfig config,\n                                                                       ConnectorConfig connConfig,\n                                                                       Class<? extends Connector> connectorClass,\n                                                                       ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                       String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "regularSourceOffsetsConsumerConfigs",
                            "method_signature": "static regularSourceOffsetsConsumerConfigs(String connName,\n                                                                   String defaultClientId,\n                                                                   WorkerConfig config,\n                                                                   ConnectorConfig connConfig,\n                                                                   Class<? extends Connector> connectorClass,\n                                                                   ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                                                   String clusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "baseConsumerConfigs",
                            "method_signature": "static baseConsumerConfigs(String connName,\n                                               String defaultClientId,\n                                               WorkerConfig config,\n                                               ConnectorConfig connConfig,\n                                               Class<? extends Connector> connectorClass,\n                                               ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                               String clusterId,\n                                               ConnectorType connectorType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "adminConfigs",
                            "method_signature": "static adminConfigs(String connName,\n                                            String defaultClientId,\n                                            WorkerConfig config,\n                                            ConnectorConfig connConfig,\n                                            Class<? extends Connector> connectorClass,\n                                            ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy,\n                                            String clusterId,\n                                            ConnectorType connectorType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "connectorClientConfigOverrides",
                            "method_signature": "private static connectorClientConfigOverrides(String connName,\n                                                                      ConnectorConfig connConfig,\n                                                                      Class<? extends Connector> connectorClass,\n                                                                      String clientConfigPrefix,\n                                                                      ConnectorType connectorType,\n                                                                      ConnectorClientConfigRequest.ClientType clientType,\n                                                                      ConnectorClientConfigOverridePolicy connectorClientConfigOverridePolicy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskTransactionalId",
                            "method_signature": "private taskTransactionalId(ConnectorTaskId id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskTransactionalId",
                            "method_signature": "public static taskTransactionalId(String groupId, String connector, int taskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "errorHandlingMetrics",
                            "method_signature": " errorHandlingMetrics(ConnectorTaskId id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sinkTaskReporters",
                            "method_signature": "private sinkTaskReporters(ConnectorTaskId id, SinkConnectorConfig connConfig,\n                                                     ErrorHandlingMetrics errorHandlingMetrics,\n                                                     Class<? extends Connector> connectorClass)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceTaskReporters",
                            "method_signature": "private sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,\n                                                       ErrorHandlingMetrics errorHandlingMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createWorkerErrantRecordReporter",
                            "method_signature": "private createWorkerErrantRecordReporter(\n        SinkConnectorConfig connConfig,\n        RetryWithToleranceOperator<ConsumerRecord<byte[], byte[]>> retryWithToleranceOperator,\n        Converter keyConverter,\n        Converter valueConverter,\n        HeaderConverter headerConverter\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopTask",
                            "method_signature": "private stopTask(ConnectorTaskId taskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopTasks",
                            "method_signature": "private stopTasks(Collection<ConnectorTaskId> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitStopTask",
                            "method_signature": "private awaitStopTask(ConnectorTaskId taskId, long timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitStopTasks",
                            "method_signature": "private awaitStopTasks(Collection<ConnectorTaskId> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopAndAwaitTasks",
                            "method_signature": "public stopAndAwaitTasks()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopAndAwaitTasks",
                            "method_signature": "public stopAndAwaitTasks(Collection<ConnectorTaskId> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopAndAwaitTask",
                            "method_signature": "public stopAndAwaitTask(ConnectorTaskId taskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskIds",
                            "method_signature": "public taskIds()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isTopicCreationEnabled",
                            "method_signature": "public isTopicCreationEnabled()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "setTargetState",
                            "method_signature": "public setTargetState(String connName, TargetState state, Callback<TargetState> stateChangeCallback)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "connectorOffsets",
                            "method_signature": "public connectorOffsets(String connName, Map<String, String> connectorConfig, Callback<ConnectorOffsets> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sinkConnectorOffsets",
                            "method_signature": " sinkConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                              Callback<ConnectorOffsets> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceConnectorOffsets",
                            "method_signature": "private sourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                        Callback<ConnectorOffsets> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceConnectorOffsets",
                            "method_signature": " sourceConnectorOffsets(String connName, ConnectorOffsetBackingStore offsetStore,\n                                CloseableOffsetStorageReader offsetReader, Callback<ConnectorOffsets> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "modifyConnectorOffsets",
                            "method_signature": "public modifyConnectorOffsets(String connName, Map<String, String> connectorConfig,\n                                      Map<Map<String, ?>, Map<String, ?>> offsets, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "modifySinkConnectorOffsets",
                            "method_signature": " modifySinkConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                    Map<Map<String, ?>, Map<String, ?>> offsets, ClassLoader connectorLoader, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "alterSinkConnectorOffsets",
                            "method_signature": "private alterSinkConnectorOffsets(String connName, String groupId, Admin admin, Map<TopicPartition, Long> offsetsToWrite,\n                                           Callback<Message> cb, boolean alterOffsetsResult, Timer timer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetSinkConnectorOffsets",
                            "method_signature": "private resetSinkConnectorOffsets(String connName, String groupId, Admin admin, Callback<Message> cb, boolean alterOffsetsResult, Timer timer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "modifySourceConnectorOffsets",
                            "method_signature": "private modifySourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                              Map<Map<String, ?>, Map<String, ?>> offsets, ClassLoader connectorLoader, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "modifySourceConnectorOffsets",
                            "method_signature": " modifySourceConnectorOffsets(String connName, Connector connector, Map<String, String> connectorConfig,\n                                      Map<Map<String, ?>, Map<String, ?>> offsets, ConnectorOffsetBackingStore offsetStore,\n                                      KafkaProducer<byte[], byte[]> producer, OffsetStorageWriter offsetWriter,\n                                      ClassLoader connectorLoader, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "normalizeSourceConnectorOffsets",
                            "method_signature": "@SuppressWarnings(\"unchecked\") normalizeSourceConnectorOffsets(Map<Map<String, ?>, Map<String, ?>> originalOffsets)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateTimerAndCheckExpiry",
                            "method_signature": "private updateTimerAndCheckExpiry(Timer timer, String errorMessageIfExpired)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeModifyOffsetsCallback",
                            "method_signature": "private completeModifyOffsetsCallback(boolean alterOffsetsResult, boolean isReset, Callback<Message> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTask",
                            "method_signature": "public withTask(Task task)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConnectorConfig",
                            "method_signature": "public withConnectorConfig(ConnectorConfig connectorConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withKeyConverter",
                            "method_signature": "public withKeyConverter(Converter keyConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withValueConverter",
                            "method_signature": "public withValueConverter(Converter valueConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withHeaderConverter",
                            "method_signature": "public withHeaderConverter(HeaderConverter headerConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withClassloader",
                            "method_signature": "public withClassloader(ClassLoader classLoader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "offsetStoreForRegularSourceConnector",
                            "method_signature": " offsetStoreForRegularSourceConnector(\n            SourceConnectorConfig sourceConfig,\n            String connName,\n            Connector connector,\n            Producer<byte[], byte[]> producer\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "offsetStoreForExactlyOnceSourceConnector",
                            "method_signature": " offsetStoreForExactlyOnceSourceConnector(\n            SourceConnectorConfig sourceConfig,\n            String connName,\n            Connector connector,\n            Producer<byte[], byte[]> producer\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "offsetStoreForRegularSourceTask",
                            "method_signature": " offsetStoreForRegularSourceTask(\n            ConnectorTaskId id,\n            SourceConnectorConfig sourceConfig,\n            Class<? extends Connector> connectorClass,\n            Producer<byte[], byte[]> producer,\n            Map<String, Object> producerProps,\n            TopicAdmin topicAdmin\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "offsetStoreForExactlyOnceSourceTask",
                            "method_signature": " offsetStoreForExactlyOnceSourceTask(\n            ConnectorTaskId id,\n            SourceConnectorConfig sourceConfig,\n            Class<? extends Connector> connectorClass,\n            Producer<byte[], byte[]> producer,\n            Map<String, Object> producerProps,\n            TopicAdmin topicAdmin\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sameOffsetTopicAsWorker",
                            "method_signature": "private sameOffsetTopicAsWorker(String offsetsTopic, Map<String, Object> producerProps)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "regularSourceTaskUsesConnectorSpecificOffsetsStore",
                            "method_signature": "private regularSourceTaskUsesConnectorSpecificOffsetsStore(SourceConnectorConfig sourceConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceConnectorTopicCreationEnabled",
                            "method_signature": "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskCounter",
                            "method_signature": "protected taskCounter(String connName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskStatusCounter",
                            "method_signature": "protected taskStatusCounter(String connName, TaskStatus.State state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordTaskAdded",
                            "method_signature": "protected synchronized recordTaskAdded(ConnectorTaskId connectorTaskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "recordTaskRemoved",
                            "method_signature": "protected synchronized recordTaskRemoved(ConnectorTaskId connectorTaskId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "protected synchronized close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metricGroup",
                            "method_signature": "protected metricGroup(String connectorId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "withClassloader",
                            "method_signature": "public withClassloader(ClassLoader classLoader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withTask",
                            "method_signature": "public withTask(Task task)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withValueConverter",
                            "method_signature": "public withValueConverter(Converter valueConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withHeaderConverter",
                            "method_signature": "public withHeaderConverter(HeaderConverter headerConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateTimerAndCheckExpiry",
                            "method_signature": "private updateTimerAndCheckExpiry(Timer timer, String errorMessageIfExpired)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "protected synchronized close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initConfigTransformer",
                            "method_signature": "private initConfigTransformer()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withKeyConverter",
                            "method_signature": "public withKeyConverter(Converter keyConverter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metricGroup",
                            "method_signature": "protected metricGroup(String connectorId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitStopConnectors",
                            "method_signature": "private awaitStopConnectors(Collection<String> ids)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "errorHandlingMetrics",
                            "method_signature": " errorHandlingMetrics(ConnectorTaskId id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "stopAndAwaitTasks",
                            "method_signature": "public stopAndAwaitTasks()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceTaskReporters",
                            "method_signature": "private sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,\n                                                       ErrorHandlingMetrics errorHandlingMetrics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sourceConnectorTopicCreationEnabled",
                            "method_signature": "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "taskTransactionalId",
                            "method_signature": "private taskTransactionalId(ConnectorTaskId id)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "public withClassloader(ClassLoader classLoader)": {
                    "first": {
                        "method_name": "withClassloader",
                        "method_signature": "public withClassloader(ClassLoader classLoader)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.26881763973623357
                },
                "public withTask(Task task)": {
                    "first": {
                        "method_name": "withTask",
                        "method_signature": "public withTask(Task task)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.27930230433267045
                },
                "public withValueConverter(Converter valueConverter)": {
                    "first": {
                        "method_name": "withValueConverter",
                        "method_signature": "public withValueConverter(Converter valueConverter)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.31894351139862714
                },
                "public withHeaderConverter(HeaderConverter headerConverter)": {
                    "first": {
                        "method_name": "withHeaderConverter",
                        "method_signature": "public withHeaderConverter(HeaderConverter headerConverter)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.333849716148804
                },
                "private updateTimerAndCheckExpiry(Timer timer, String errorMessageIfExpired)": {
                    "first": {
                        "method_name": "updateTimerAndCheckExpiry",
                        "method_signature": "private updateTimerAndCheckExpiry(Timer timer, String errorMessageIfExpired)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3462610809913813
                },
                "protected synchronized close()": {
                    "first": {
                        "method_name": "close",
                        "method_signature": "protected synchronized close()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3651933899257402
                },
                "private initConfigTransformer()": {
                    "first": {
                        "method_name": "initConfigTransformer",
                        "method_signature": "private initConfigTransformer()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3751603123720797
                },
                "public withKeyConverter(Converter keyConverter)": {
                    "first": {
                        "method_name": "withKeyConverter",
                        "method_signature": "public withKeyConverter(Converter keyConverter)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3849464074375511
                },
                "protected metricGroup(String connectorId)": {
                    "first": {
                        "method_name": "metricGroup",
                        "method_signature": "protected metricGroup(String connectorId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.39833805691163465
                },
                "private awaitStopConnectors(Collection<String> ids)": {
                    "first": {
                        "method_name": "awaitStopConnectors",
                        "method_signature": "private awaitStopConnectors(Collection<String> ids)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4151234979514168
                },
                " errorHandlingMetrics(ConnectorTaskId id)": {
                    "first": {
                        "method_name": "errorHandlingMetrics",
                        "method_signature": " errorHandlingMetrics(ConnectorTaskId id)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4163833878944241
                },
                "public stopAndAwaitTasks()": {
                    "first": {
                        "method_name": "stopAndAwaitTasks",
                        "method_signature": "public stopAndAwaitTasks()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.417509354016988
                },
                "private sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,\n                                                       ErrorHandlingMetrics errorHandlingMetrics)": {
                    "first": {
                        "method_name": "sourceTaskReporters",
                        "method_signature": "private sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,\n                                                       ErrorHandlingMetrics errorHandlingMetrics)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4426974514879247
                },
                "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)": {
                    "first": {
                        "method_name": "sourceConnectorTopicCreationEnabled",
                        "method_signature": "private sourceConnectorTopicCreationEnabled(SourceConnectorConfig sourceConfig)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4463049258915329
                },
                "private taskTransactionalId(ConnectorTaskId id)": {
                    "first": {
                        "method_name": "taskTransactionalId",
                        "method_signature": "private taskTransactionalId(ConnectorTaskId id)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.44651389019177473
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "d24abe0edebad37e554adea47408c3063037f744",
        "url": "https://github.com/apache/kafka/commit/d24abe0edebad37e554adea47408c3063037f744",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public assertNoOrEmptyResult(timeouts List<MockCoordinatorTimer.ExpiredTimeout<Void,Record>>) : void extracted from public testLastJoiningMembersAreKickedOutWhenRejoiningGroupWithMaxSize() : void in class org.apache.kafka.coordinator.group.GroupMetadataManagerTest & moved to class org.apache.kafka.coordinator.group.GroupMetadataManagerTestContext",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java",
                    "startLine": 5158,
                    "endLine": 5217,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testLastJoiningMembersAreKickedOutWhenRejoiningGroupWithMaxSize() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java",
                    "startLine": 10901,
                    "endLine": 10901,
                    "startColumn": 9,
                    "endColumn": 42,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java",
                    "startLine": 10902,
                    "endLine": 10902,
                    "startColumn": 9,
                    "endColumn": 81,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java",
                    "startLine": 10902,
                    "endLine": 10902,
                    "startColumn": 37,
                    "endColumn": 79,
                    "codeElementType": "LAMBDA_EXPRESSION_BODY",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 106,
                    "endLine": 109,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public assertNoOrEmptyResult(timeouts List<MockCoordinatorTimer.ExpiredTimeout<Void,Record>>) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 107,
                    "endLine": 107,
                    "startColumn": 9,
                    "endColumn": 42,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 108,
                    "endLine": 108,
                    "startColumn": 9,
                    "endColumn": 81,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 108,
                    "endLine": 108,
                    "startColumn": 37,
                    "endColumn": 79,
                    "codeElementType": "LAMBDA_EXPRESSION_BODY",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java",
                    "startLine": 4040,
                    "endLine": 4097,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testLastJoiningMembersAreKickedOutWhenRejoiningGroupWithMaxSize() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java",
                    "startLine": 4085,
                    "endLine": 4085,
                    "startColumn": 9,
                    "endColumn": 84,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "GroupMetadataManagerTestContext.assertNoOrEmptyResult(context.sleep(10000))"
                }
            ],
            "isStatic": true
        },
        "ref_id": 573,
        "extraction_results": {
            "success": true,
            "newCommitHash": "d67ed1ff166294681b2b562d9437a2ba61f11db9",
            "newBranchName": "extract-assertNoOrEmptyResult-testLastJoiningMembersAreKickedOutWhenRejoiningGroupWithMaxSize-be6653c"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "7dbdc15c668dfb5a4a91c79f339c22fb7178c368",
        "url": "https://github.com/apache/kafka/commit/7dbdc15c668dfb5a4a91c79f339c22fb7178c368",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public maybeCheckpoint() : void extracted from package pollAndUpdate() : void in class org.apache.kafka.streams.processor.internals.GlobalStreamThread.StateConsumer & moved to class org.apache.kafka.streams.processor.internals.GlobalStateUpdateTask",
            "leftSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 265,
                    "endLine": 275,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package pollAndUpdate() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 270,
                    "endLine": 270,
                    "startColumn": 13,
                    "endColumn": 50,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 273,
                    "endLine": 273,
                    "startColumn": 17,
                    "endColumn": 33,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 272,
                    "endLine": 272,
                    "startColumn": 17,
                    "endColumn": 46,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 271,
                    "endLine": 274,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 271,
                    "endLine": 271,
                    "startColumn": 17,
                    "endColumn": 49,
                    "codeElementType": "INFIX_EXPRESSION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 271,
                    "endLine": 274,
                    "startColumn": 51,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java",
                    "startLine": 163,
                    "endLine": 170,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public maybeCheckpoint() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java",
                    "startLine": 165,
                    "endLine": 165,
                    "startColumn": 9,
                    "endColumn": 46,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java",
                    "startLine": 168,
                    "endLine": 168,
                    "startColumn": 13,
                    "endColumn": 29,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java",
                    "startLine": 167,
                    "endLine": 167,
                    "startColumn": 13,
                    "endColumn": 26,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java",
                    "startLine": 166,
                    "endLine": 169,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java",
                    "startLine": 166,
                    "endLine": 166,
                    "startColumn": 13,
                    "endColumn": 45,
                    "codeElementType": "INFIX_EXPRESSION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java",
                    "startLine": 166,
                    "endLine": 169,
                    "startColumn": 129,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 256,
                    "endLine": 262,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package pollAndUpdate() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                    "startLine": 261,
                    "endLine": 261,
                    "startColumn": 13,
                    "endColumn": 46,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "stateMaintainer.maybeCheckpoint()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 574,
        "extraction_results": {
            "success": true,
            "newCommitHash": "1d75be12be5762c23b7a31c0b37559f1ba8a719b",
            "newBranchName": "extract-maybeCheckpoint-pollAndUpdate-2c0cab3"
        },
        "telemetry": {
            "id": "87b55397-2548-4190-a01e-fbd828fa8c5b",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 65,
                "lineStart": 228,
                "lineEnd": 292,
                "bodyLineStart": 228,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java",
                "sourceCode": "static class StateConsumer {\n        private final Consumer<byte[], byte[]> globalConsumer;\n        private final GlobalStateMaintainer stateMaintainer;\n        private final Time time;\n        private final Duration pollTime;\n        private final long flushInterval;\n        private final Logger log;\n\n        private long lastFlush;\n\n        StateConsumer(final LogContext logContext,\n                      final Consumer<byte[], byte[]> globalConsumer,\n                      final GlobalStateMaintainer stateMaintainer,\n                      final Time time,\n                      final Duration pollTime,\n                      final long flushInterval) {\n            this.log = logContext.logger(getClass());\n            this.globalConsumer = globalConsumer;\n            this.stateMaintainer = stateMaintainer;\n            this.time = time;\n            this.pollTime = pollTime;\n            this.flushInterval = flushInterval;\n        }\n\n        /**\n         * @throws IllegalStateException If store gets registered after initialized is already finished\n         * @throws StreamsException      if the store's change log does not contain the partition\n         */\n        void initialize() {\n            final Map<TopicPartition, Long> partitionOffsets = stateMaintainer.initialize();\n            globalConsumer.assign(partitionOffsets.keySet());\n            for (final Map.Entry<TopicPartition, Long> entry : partitionOffsets.entrySet()) {\n                globalConsumer.seek(entry.getKey(), entry.getValue());\n            }\n            lastFlush = time.milliseconds();\n        }\n\n        void pollAndUpdate() {\n            final ConsumerRecords<byte[], byte[]> received = globalConsumer.poll(pollTime);\n            for (final ConsumerRecord<byte[], byte[]> record : received) {\n                stateMaintainer.update(record);\n            }\n            final long now = time.milliseconds();\n            maybeCheckpoint(now);\n        }\n\n        private void maybeCheckpoint(long now) {\n            if (now - flushInterval >= lastFlush) {\n                stateMaintainer.flushState();\n                lastFlush = now;\n            }\n        }\n\n        public void close(final boolean wipeStateStore) throws IOException {\n            try {\n                globalConsumer.close();\n            } catch (final RuntimeException e) {\n                // just log an error if the consumer throws an exception during close\n                // so we can always attempt to close the state stores.\n                log.error(\"Failed to close global consumer due to the following error:\", e);\n            }\n\n            stateMaintainer.close(wipeStateStore);\n        }\n    }",
                "methodCount": 5
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "initialize",
                            "method_signature": " initialize()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollAndUpdate",
                            "method_signature": " pollAndUpdate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCheckpoint",
                            "method_signature": "private maybeCheckpoint(long now)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public close(final boolean wipeStateStore)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "close",
                            "method_signature": "public close(final boolean wipeStateStore)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCheckpoint",
                            "method_signature": "private maybeCheckpoint(long now)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": " initialize()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollAndUpdate",
                            "method_signature": " pollAndUpdate()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "public close(final boolean wipeStateStore)": {
                    "first": {
                        "method_name": "close",
                        "method_signature": "public close(final boolean wipeStateStore)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5061086326701493
                },
                "private maybeCheckpoint(long now)": {
                    "first": {
                        "method_name": "maybeCheckpoint",
                        "method_signature": "private maybeCheckpoint(long now)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5505768178561731
                },
                " initialize()": {
                    "first": {
                        "method_name": "initialize",
                        "method_signature": " initialize()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6231106328011317
                },
                " pollAndUpdate()": {
                    "first": {
                        "method_name": "pollAndUpdate",
                        "method_signature": " pollAndUpdate()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6346169038834558
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "b6183a41342c765daec9c88f9d2723a221131960",
        "url": "https://github.com/apache/kafka/commit/b6183a41342c765daec9c88f9d2723a221131960",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public fromArgs(args String[]) : ConsumerGroupCommandOptions extracted from public testDescribeGroupCliWithGroupDescribe(quorum String) : void in class org.apache.kafka.tools.consumer.group.AuthorizerIntegrationTest & moved to class org.apache.kafka.tools.consumer.group.ConsumerGroupCommandOptions",
            "leftSideLocations": [
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/AuthorizerIntegrationTest.java",
                    "startLine": 36,
                    "endLine": 46,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testDescribeGroupCliWithGroupDescribe(quorum String) : void"
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/AuthorizerIntegrationTest.java",
                    "startLine": 42,
                    "endLine": 42,
                    "startColumn": 9,
                    "endColumn": 127,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 120,
                    "endLine": 124,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public fromArgs(args String[]) : ConsumerGroupCommandOptions"
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 121,
                    "endLine": 121,
                    "startColumn": 9,
                    "endColumn": 82,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/AuthorizerIntegrationTest.java",
                    "startLine": 33,
                    "endLine": 44,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testDescribeGroupCliWithGroupDescribe(quorum String) : void"
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/AuthorizerIntegrationTest.java",
                    "startLine": 40,
                    "endLine": 40,
                    "startColumn": 44,
                    "endColumn": 89,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ConsumerGroupCommandOptions.fromArgs(cgcArgs)"
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 122,
                    "endLine": 122,
                    "startColumn": 9,
                    "endColumn": 26,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 123,
                    "endLine": 123,
                    "startColumn": 9,
                    "endColumn": 21,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 575,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a7fa737b2cc8ece8e16aef27445f356bd1a14c8e",
            "newBranchName": "extract-fromArgs-testDescribeGroupCliWithGroupDescribe-34d365f"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "b6183a41342c765daec9c88f9d2723a221131960",
        "url": "https://github.com/apache/kafka/commit/b6183a41342c765daec9c88f9d2723a221131960",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public fromArgs(args String[]) : ConsumerGroupCommandOptions extracted from package getConsumerGroupService(args String[]) : ConsumerGroupCommand.ConsumerGroupService in class org.apache.kafka.tools.consumer.group.ConsumerGroupCommandTest & moved to class org.apache.kafka.tools.consumer.group.ConsumerGroupCommandOptions",
            "leftSideLocations": [
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandTest.java",
                    "startLine": 132,
                    "endLine": 141,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package getConsumerGroupService(args String[]) : ConsumerGroupCommand.ConsumerGroupService"
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandTest.java",
                    "startLine": 134,
                    "endLine": 137,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandTest.java",
                    "startLine": 133,
                    "endLine": 133,
                    "startColumn": 9,
                    "endColumn": 124,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 120,
                    "endLine": 124,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public fromArgs(args String[]) : ConsumerGroupCommandOptions"
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 123,
                    "endLine": 123,
                    "startColumn": 9,
                    "endColumn": 21,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 121,
                    "endLine": 121,
                    "startColumn": 9,
                    "endColumn": 82,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandTest.java",
                    "startLine": 130,
                    "endLine": 139,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package getConsumerGroupService(args String[]) : ConsumerGroupCommand.ConsumerGroupService"
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandTest.java",
                    "startLine": 131,
                    "endLine": 131,
                    "startColumn": 44,
                    "endColumn": 86,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ConsumerGroupCommandOptions.fromArgs(args)"
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 122,
                    "endLine": 122,
                    "startColumn": 9,
                    "endColumn": 26,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 576,
        "extraction_results": {
            "success": true,
            "newCommitHash": "7af27c6d699e8995dba400666005069a2daa5be0",
            "newBranchName": "extract-fromArgs-getConsumerGroupService-34d365f"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "b6183a41342c765daec9c88f9d2723a221131960",
        "url": "https://github.com/apache/kafka/commit/b6183a41342c765daec9c88f9d2723a221131960",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public fromArgs(args String[]) : ConsumerGroupCommandOptions extracted from private prepareConsumerGroupService() : ConsumerGroupCommand.ConsumerGroupService in class org.apache.kafka.tools.consumer.group.SaslClientsWithInvalidCredentialsTest & moved to class org.apache.kafka.tools.consumer.group.ConsumerGroupCommandOptions",
            "leftSideLocations": [
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/SaslClientsWithInvalidCredentialsTest.java",
                    "startLine": 158,
                    "endLine": 169,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private prepareConsumerGroupService() : ConsumerGroupCommand.ConsumerGroupService"
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/SaslClientsWithInvalidCredentialsTest.java",
                    "startLine": 167,
                    "endLine": 167,
                    "startColumn": 9,
                    "endColumn": 127,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 120,
                    "endLine": 124,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public fromArgs(args String[]) : ConsumerGroupCommandOptions"
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 121,
                    "endLine": 121,
                    "startColumn": 9,
                    "endColumn": 82,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/SaslClientsWithInvalidCredentialsTest.java",
                    "startLine": 156,
                    "endLine": 167,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private prepareConsumerGroupService() : ConsumerGroupCommand.ConsumerGroupService"
                },
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/consumer/group/SaslClientsWithInvalidCredentialsTest.java",
                    "startLine": 165,
                    "endLine": 165,
                    "startColumn": 44,
                    "endColumn": 89,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ConsumerGroupCommandOptions.fromArgs(cgcArgs)"
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 122,
                    "endLine": 122,
                    "startColumn": 9,
                    "endColumn": 26,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "tools/src/main/java/org/apache/kafka/tools/consumer/group/ConsumerGroupCommandOptions.java",
                    "startLine": 123,
                    "endLine": 123,
                    "startColumn": 9,
                    "endColumn": 21,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 577,
        "extraction_results": {
            "success": true,
            "newCommitHash": "1b78c84f6d0cc6ae9d5dad0f09e845a861e84805",
            "newBranchName": "extract-fromArgs-prepareConsumerGroupService-34d365f"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "71bcac3b6ada9872dcdec48f72372d6c5b041c0a",
        "url": "https://github.com/apache/kafka/commit/71bcac3b6ada9872dcdec48f72372d6c5b041c0a",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public castToStringObjectMap(inputMap Map<?,?>) : Map<String,Object> extracted from public AbstractConfig(definition ConfigDef, originals Map<?,?>, configProviderProps Map<String,?>, doLog boolean) in class org.apache.kafka.common.config.AbstractConfig & moved to class org.apache.kafka.common.utils.Utils",
            "leftSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java",
                    "startLine": 65,
                    "endLine": 121,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public AbstractConfig(definition ConfigDef, originals Map<?,?>, configProviderProps Map<String,?>, doLog boolean)"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java",
                    "startLine": 109,
                    "endLine": 109,
                    "startColumn": 17,
                    "endColumn": 113,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java",
                    "startLine": 107,
                    "endLine": 109,
                    "startColumn": 9,
                    "endColumn": 113,
                    "codeElementType": "ENHANCED_FOR_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java",
                    "startLine": 108,
                    "endLine": 109,
                    "startColumn": 13,
                    "endColumn": 113,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1508,
                    "endLine": 1525,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public castToStringObjectMap(inputMap Map<?,?>) : Map<String,Object>"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1521,
                    "endLine": 1521,
                    "startColumn": 17,
                    "endColumn": 118,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1516,
                    "endLine": 1523,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "ENHANCED_FOR_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1517,
                    "endLine": 1522,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java",
                    "startLine": 70,
                    "endLine": 123,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public AbstractConfig(definition ConfigDef, originals Map<?,?>, configProviderProps Map<String,?>, doLog boolean)"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java",
                    "startLine": 111,
                    "endLine": 111,
                    "startColumn": 43,
                    "endColumn": 81,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "Utils.castToStringObjectMap(originals)"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1515,
                    "endLine": 1515,
                    "startColumn": 9,
                    "endColumn": 66,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1518,
                    "endLine": 1518,
                    "startColumn": 17,
                    "endColumn": 52,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1519,
                    "endLine": 1519,
                    "startColumn": 17,
                    "endColumn": 46,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1524,
                    "endLine": 1524,
                    "startColumn": 9,
                    "endColumn": 20,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1517,
                    "endLine": 1520,
                    "startColumn": 51,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1520,
                    "endLine": 1522,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
                    "startLine": 1516,
                    "endLine": 1523,
                    "startColumn": 59,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 578,
        "extraction_results": {
            "success": false
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "a1ca788c993e3b217a0904c3f4768f0682dc840b",
        "url": "https://github.com/apache/kafka/commit/a1ca788c993e3b217a0904c3f4768f0682dc840b",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method package adminCall(callable Callable<T>, errMsg Supplier<String>) : T extracted from private listTopicAclBindings() : Optional<Collection<AclBinding>> in class org.apache.kafka.connect.mirror.MirrorSourceConnector & moved to class org.apache.kafka.connect.mirror.MirrorUtils",
            "leftSideLocations": [
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java",
                    "startLine": 525,
                    "endLine": 546,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private listTopicAclBindings() : Optional<Collection<AclBinding>>"
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java",
                    "startLine": 542,
                    "endLine": 542,
                    "startColumn": 17,
                    "endColumn": 25,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java",
                    "startLine": 540,
                    "endLine": 540,
                    "startColumn": 17,
                    "endColumn": 41,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 327,
                    "endLine": 342,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "package adminCall(callable Callable<T>, errMsg Supplier<String>) : T"
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 338,
                    "endLine": 338,
                    "startColumn": 13,
                    "endColumn": 21,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 330,
                    "endLine": 330,
                    "startColumn": 13,
                    "endColumn": 36,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java",
                    "startLine": 543,
                    "endLine": 569,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private listTopicAclBindings() : Optional<Collection<AclBinding>>"
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java",
                    "startLine": 545,
                    "endLine": 568,
                    "startColumn": 16,
                    "endColumn": 10,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "adminCall(() -> {\n  Collection<AclBinding> bindings;\n  try {\n    bindings=sourceAdminClient.describeAcls(ANY_TOPIC_ACL).values().get();\n  }\n catch (  ExecutionException e) {\n    if (e.getCause() instanceof SecurityDisabledException) {\n      if (noAclAuthorizer.compareAndSet(false,true)) {\n        log.info(\"No ACL authorizer is configured on the source Kafka cluster, so no topic ACL syncing will take place. \" + \"Consider disabling topic ACL syncing by setting \" + SYNC_TOPIC_ACLS_ENABLED + \" to 'false'.\");\n      }\n else {\n        log.debug(\"Source-side ACL authorizer still not found; skipping topic ACL sync\");\n      }\n      return Optional.empty();\n    }\n else {\n      throw e;\n    }\n  }\n  return Optional.of(bindings);\n}\n,() -> \"describe ACLs on \" + config.sourceClusterAlias() + \" cluster\")"
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 332,
                    "endLine": 332,
                    "startColumn": 13,
                    "endColumn": 44,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 336,
                    "endLine": 336,
                    "startColumn": 17,
                    "endColumn": 109,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 340,
                    "endLine": 340,
                    "startColumn": 13,
                    "endColumn": 43,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 329,
                    "endLine": 341,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "TRY_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 335,
                    "endLine": 337,
                    "startColumn": 67,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 333,
                    "endLine": 337,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 331,
                    "endLine": 339,
                    "startColumn": 11,
                    "endColumn": 10,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java",
                    "startLine": 339,
                    "endLine": 341,
                    "startColumn": 11,
                    "endColumn": 10,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 579,
        "extraction_results": {
            "success": true,
            "newCommitHash": "edafad4bfaf5f07b8323dc99a2100711712341ed",
            "newBranchName": "extract-adminCall-listTopicAclBindings-f7eb962"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "ebeef4eb64a36d46565629d2a370eeb1635a7293",
        "url": "https://github.com/apache/kafka/commit/ebeef4eb64a36d46565629d2a370eeb1635a7293",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method private shouldInitialize() : boolean extracted from public resetInitializingPositions() : void in class org.apache.kafka.clients.consumer.internals.SubscriptionState & moved to class org.apache.kafka.clients.consumer.internals.SubscriptionState.TopicPartitionState",
            "leftSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 763,
                    "endLine": 776,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public resetInitializingPositions() : void"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 766,
                    "endLine": 766,
                    "startColumn": 17,
                    "endColumn": 75,
                    "codeElementType": "IF_STATEMENT_CONDITION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 1102,
                    "endLine": 1110,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "private shouldInitialize() : boolean"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 1109,
                    "endLine": 1109,
                    "startColumn": 13,
                    "endColumn": 94,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 764,
                    "endLine": 789,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public resetInitializingPositions() : void"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                    "startLine": 779,
                    "endLine": 779,
                    "startColumn": 17,
                    "endColumn": 50,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "partitionState.shouldInitialize()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 580,
        "extraction_results": {
            "success": true,
            "newCommitHash": "fa9a5d390256649011d80f3a1cd77fecfe7dda76",
            "newBranchName": "extract-shouldInitialize-resetInitializingPositions-a1ca788"
        },
        "telemetry": {
            "id": "23d89008-fa2e-4fa9-a5e7-370a97c757ad",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1250,
                "lineStart": 54,
                "lineEnd": 1303,
                "bodyLineStart": 54,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java",
                "sourceCode": "/**\n * A class for tracking the topics, partitions, and offsets for the consumer. A partition\n * is \"assigned\" either directly with {@link #assignFromUser(Set)} (manual assignment)\n * or with {@link #assignFromSubscribed(Collection)} (automatic assignment from subscription).\n * <p>\n * Once assigned, the partition is not considered \"fetchable\" until its initial position has\n * been set with {@link #seekValidated(TopicPartition, FetchPosition)}. Fetchable partitions\n * track a position which is the last offset that has been returned to the user. You can\n * suspend fetching from a partition through {@link #pause(TopicPartition)} without affecting the consumed\n * position. The partition will remain unfetchable until the {@link #resume(TopicPartition)} is\n * used. You can also query the pause state independently with {@link #isPaused(TopicPartition)}.\n * <p>\n * Note that pause state as well as the consumed positions are not preserved when partition\n * assignment is changed whether directly by the user or through a group rebalance.\n * <p>\n * Thread Safety: this class is thread-safe.\n */\npublic class SubscriptionState {\n    private static final String SUBSCRIPTION_EXCEPTION_MESSAGE =\n            \"Subscription to topics, partitions and pattern are mutually exclusive\";\n\n    private final Logger log;\n\n    private enum SubscriptionType {\n        NONE, AUTO_TOPICS, AUTO_PATTERN, USER_ASSIGNED\n    }\n\n    /* the type of subscription */\n    private SubscriptionType subscriptionType;\n\n    /* the pattern user has requested */\n    private Pattern subscribedPattern;\n\n    /* the list of topics the user has requested */\n    private Set<String> subscription;\n\n    /* The list of topics the group has subscribed to. This may include some topics which are not part\n     * of `subscription` for the leader of a group since it is responsible for detecting metadata changes\n     * which require a group rebalance. */\n    private Set<String> groupSubscription;\n\n    /* the partitions that are currently assigned, note that the order of partition matters (see FetchBuilder for more details) */\n    private final PartitionStates<TopicPartitionState> assignment;\n\n    /* Default offset reset strategy */\n    private final OffsetResetStrategy defaultResetStrategy;\n\n    /* User-provided listener to be invoked when assignment changes */\n    private Optional<ConsumerRebalanceListener> rebalanceListener;\n\n    private int assignmentId = 0;\n\n    @Override\n    public synchronized String toString() {\n        return \"SubscriptionState{\" +\n            \"type=\" + subscriptionType +\n            \", subscribedPattern=\" + subscribedPattern +\n            \", subscription=\" + String.join(\",\", subscription) +\n            \", groupSubscription=\" + String.join(\",\", groupSubscription) +\n            \", defaultResetStrategy=\" + defaultResetStrategy +\n            \", assignment=\" + assignment.partitionStateValues() + \" (id=\" + assignmentId + \")}\";\n    }\n\n    public synchronized String prettyString() {\n        switch (subscriptionType) {\n            case NONE:\n                return \"None\";\n            case AUTO_TOPICS:\n                return \"Subscribe(\" + String.join(\",\", subscription) + \")\";\n            case AUTO_PATTERN:\n                return \"Subscribe(\" + subscribedPattern + \")\";\n            case USER_ASSIGNED:\n                return \"Assign(\" + assignedPartitions() + \" , id=\" + assignmentId + \")\";\n            default:\n                throw new IllegalStateException(\"Unrecognized subscription type: \" + subscriptionType);\n        }\n    }\n\n    public SubscriptionState(LogContext logContext, OffsetResetStrategy defaultResetStrategy) {\n        this.log = logContext.logger(this.getClass());\n        this.defaultResetStrategy = defaultResetStrategy;\n        this.subscription = new TreeSet<>(); // use a sorted set for better logging\n        this.assignment = new PartitionStates<>();\n        this.groupSubscription = new HashSet<>();\n        this.subscribedPattern = null;\n        this.subscriptionType = SubscriptionType.NONE;\n    }\n\n    /**\n     * Monotonically increasing id which is incremented after every assignment change. This can\n     * be used to check when an assignment has changed.\n     *\n     * @return The current assignment Id\n     */\n    synchronized int assignmentId() {\n        return assignmentId;\n    }\n\n    /**\n     * This method sets the subscription type if it is not already set (i.e. when it is NONE),\n     * or verifies that the subscription type is equal to the give type when it is set (i.e.\n     * when it is not NONE)\n     * @param type The given subscription type\n     */\n    private void setSubscriptionType(SubscriptionType type) {\n        if (this.subscriptionType == SubscriptionType.NONE)\n            this.subscriptionType = type;\n        else if (this.subscriptionType != type)\n            throw new IllegalStateException(SUBSCRIPTION_EXCEPTION_MESSAGE);\n    }\n\n    public synchronized boolean subscribe(Set<String> topics, Optional<ConsumerRebalanceListener> listener) {\n        registerRebalanceListener(listener);\n        setSubscriptionType(SubscriptionType.AUTO_TOPICS);\n        return changeSubscription(topics);\n    }\n\n    public synchronized void subscribe(Pattern pattern, Optional<ConsumerRebalanceListener> listener) {\n        registerRebalanceListener(listener);\n        setSubscriptionType(SubscriptionType.AUTO_PATTERN);\n        this.subscribedPattern = pattern;\n    }\n\n    public synchronized boolean subscribeFromPattern(Set<String> topics) {\n        if (subscriptionType != SubscriptionType.AUTO_PATTERN)\n            throw new IllegalArgumentException(\"Attempt to subscribe from pattern while subscription type set to \" +\n                    subscriptionType);\n\n        return changeSubscription(topics);\n    }\n\n    private boolean changeSubscription(Set<String> topicsToSubscribe) {\n        if (subscription.equals(topicsToSubscribe))\n            return false;\n\n        subscription = topicsToSubscribe;\n        return true;\n    }\n\n    /**\n     * Set the current group subscription. This is used by the group leader to ensure\n     * that it receives metadata updates for all topics that the group is interested in.\n     *\n     * @param topics All topics from the group subscription\n     * @return true if the group subscription contains topics which are not part of the local subscription\n     */\n    synchronized boolean groupSubscribe(Collection<String> topics) {\n        if (!hasAutoAssignedPartitions())\n            throw new IllegalStateException(SUBSCRIPTION_EXCEPTION_MESSAGE);\n        groupSubscription = new HashSet<>(topics);\n        return !subscription.containsAll(groupSubscription);\n    }\n\n    /**\n     * Reset the group's subscription to only contain topics subscribed by this consumer.\n     */\n    synchronized void resetGroupSubscription() {\n        groupSubscription = Collections.emptySet();\n    }\n\n    /**\n     * Change the assignment to the specified partitions provided by the user,\n     * note this is different from {@link #assignFromSubscribed(Collection)}\n     * whose input partitions are provided from the subscribed topics.\n     */\n    public synchronized boolean assignFromUser(Set<TopicPartition> partitions) {\n        setSubscriptionType(SubscriptionType.USER_ASSIGNED);\n\n        if (this.assignment.partitionSet().equals(partitions))\n            return false;\n\n        assignmentId++;\n\n        // update the subscribed topics\n        Set<String> manualSubscribedTopics = new HashSet<>();\n        Map<TopicPartition, TopicPartitionState> partitionToState = new HashMap<>();\n        for (TopicPartition partition : partitions) {\n            TopicPartitionState state = assignment.stateValue(partition);\n            if (state == null)\n                state = new TopicPartitionState();\n            partitionToState.put(partition, state);\n\n            manualSubscribedTopics.add(partition.topic());\n        }\n\n        this.assignment.set(partitionToState);\n        return changeSubscription(manualSubscribedTopics);\n    }\n\n    /**\n     * @return true if assignments matches subscription, otherwise false\n     */\n    public synchronized boolean checkAssignmentMatchedSubscription(Collection<TopicPartition> assignments) {\n        for (TopicPartition topicPartition : assignments) {\n            if (this.subscribedPattern != null) {\n                if (!this.subscribedPattern.matcher(topicPartition.topic()).matches()) {\n                    log.info(\"Assigned partition {} for non-subscribed topic regex pattern; subscription pattern is {}\",\n                        topicPartition,\n                        this.subscribedPattern);\n\n                    return false;\n                }\n            } else {\n                if (!this.subscription.contains(topicPartition.topic())) {\n                    log.info(\"Assigned partition {} for non-subscribed topic; subscription is {}\", topicPartition, this.subscription);\n\n                    return false;\n                }\n            }\n        }\n\n        return true;\n    }\n\n    /**\n     * Change the assignment to the specified partitions returned from the coordinator, note this is\n     * different from {@link #assignFromUser(Set)} which directly set the assignment from user inputs.\n     */\n    public synchronized void assignFromSubscribed(Collection<TopicPartition> assignments) {\n        if (!this.hasAutoAssignedPartitions())\n            throw new IllegalArgumentException(\"Attempt to dynamically assign partitions while manual assignment in use\");\n\n        Map<TopicPartition, TopicPartitionState> assignedPartitionStates = new HashMap<>(assignments.size());\n        for (TopicPartition tp : assignments) {\n            TopicPartitionState state = this.assignment.stateValue(tp);\n            if (state == null)\n                state = new TopicPartitionState();\n            assignedPartitionStates.put(tp, state);\n        }\n\n        assignmentId++;\n        this.assignment.set(assignedPartitionStates);\n    }\n\n    private void registerRebalanceListener(Optional<ConsumerRebalanceListener> listener) {\n        this.rebalanceListener = Objects.requireNonNull(listener, \"RebalanceListener cannot be null\");\n    }\n\n    /**\n     * Check whether pattern subscription is in use.\n     *\n     */\n    synchronized boolean hasPatternSubscription() {\n        return this.subscriptionType == SubscriptionType.AUTO_PATTERN;\n    }\n\n    public synchronized boolean hasNoSubscriptionOrUserAssignment() {\n        return this.subscriptionType == SubscriptionType.NONE;\n    }\n\n    public synchronized void unsubscribe() {\n        this.subscription = Collections.emptySet();\n        this.groupSubscription = Collections.emptySet();\n        this.assignment.clear();\n        this.subscribedPattern = null;\n        this.subscriptionType = SubscriptionType.NONE;\n        this.assignmentId++;\n    }\n\n    /**\n     * Check whether a topic matches a subscribed pattern.\n     *\n     * @return true if pattern subscription is in use and the topic matches the subscribed pattern, false otherwise\n     */\n    synchronized boolean matchesSubscribedPattern(String topic) {\n        Pattern pattern = this.subscribedPattern;\n        if (hasPatternSubscription() && pattern != null)\n            return pattern.matcher(topic).matches();\n        return false;\n    }\n\n    public synchronized Set<String> subscription() {\n        if (hasAutoAssignedPartitions())\n            return this.subscription;\n        return Collections.emptySet();\n    }\n\n    public synchronized Set<TopicPartition> pausedPartitions() {\n        return collectPartitions(TopicPartitionState::isPaused);\n    }\n\n    /**\n     * Get the subscription topics for which metadata is required. For the leader, this will include\n     * the union of the subscriptions of all group members. For followers, it is just that member's\n     * subscription. This is used when querying topic metadata to detect the metadata changes which would\n     * require rebalancing. The leader fetches metadata for all topics in the group so that it\n     * can do the partition assignment (which requires at least partition counts for all topics\n     * to be assigned).\n     *\n     * @return The union of all subscribed topics in the group if this member is the leader\n     *   of the current generation; otherwise it returns the same set as {@link #subscription()}\n     */\n    synchronized Set<String> metadataTopics() {\n        if (groupSubscription.isEmpty())\n            return subscription;\n        else if (groupSubscription.containsAll(subscription))\n            return groupSubscription;\n        else {\n            // When subscription changes `groupSubscription` may be outdated, ensure that\n            // new subscription topics are returned.\n            Set<String> topics = new HashSet<>(groupSubscription);\n            topics.addAll(subscription);\n            return topics;\n        }\n    }\n\n    synchronized boolean needsMetadata(String topic) {\n        return subscription.contains(topic) || groupSubscription.contains(topic);\n    }\n\n    private TopicPartitionState assignedState(TopicPartition tp) {\n        TopicPartitionState state = this.assignment.stateValue(tp);\n        if (state == null)\n            throw new IllegalStateException(\"No current assignment for partition \" + tp);\n        return state;\n    }\n\n    private TopicPartitionState assignedStateOrNull(TopicPartition tp) {\n        return this.assignment.stateValue(tp);\n    }\n\n    public synchronized void seekValidated(TopicPartition tp, FetchPosition position) {\n        assignedState(tp).seekValidated(position);\n    }\n\n    public void seek(TopicPartition tp, long offset) {\n        seekValidated(tp, new FetchPosition(offset));\n    }\n\n    public void seekUnvalidated(TopicPartition tp, FetchPosition position) {\n        assignedState(tp).seekUnvalidated(position);\n    }\n\n    synchronized void maybeSeekUnvalidated(TopicPartition tp, FetchPosition position, OffsetResetStrategy requestedResetStrategy) {\n        TopicPartitionState state = assignedStateOrNull(tp);\n        if (state == null) {\n            log.debug(\"Skipping reset of partition {} since it is no longer assigned\", tp);\n        } else if (!state.awaitingReset()) {\n            log.debug(\"Skipping reset of partition {} since reset is no longer needed\", tp);\n        } else if (requestedResetStrategy != state.resetStrategy) {\n            log.debug(\"Skipping reset of partition {} since an alternative reset has been requested\", tp);\n        } else {\n            log.info(\"Resetting offset for partition {} to position {}.\", tp, position);\n            state.seekUnvalidated(position);\n        }\n    }\n\n    /**\n     * @return a modifiable copy of the currently assigned partitions\n     */\n    public synchronized Set<TopicPartition> assignedPartitions() {\n        return new HashSet<>(this.assignment.partitionSet());\n    }\n\n    /**\n     * @return a modifiable copy of the currently assigned partitions as a list\n     */\n    public synchronized List<TopicPartition> assignedPartitionsList() {\n        return new ArrayList<>(this.assignment.partitionSet());\n    }\n\n    /**\n     * Provides the number of assigned partitions in a thread safe manner.\n     * @return the number of assigned partitions.\n     */\n    synchronized int numAssignedPartitions() {\n        return this.assignment.size();\n    }\n\n    // Visible for testing\n    public synchronized List<TopicPartition> fetchablePartitions(Predicate<TopicPartition> isAvailable) {\n        // Since this is in the hot-path for fetching, we do this instead of using java.util.stream API\n        List<TopicPartition> result = new ArrayList<>();\n        assignment.forEach((topicPartition, topicPartitionState) -> {\n            // Cheap check is first to avoid evaluating the predicate if possible\n            if (topicPartitionState.isFetchable() && isAvailable.test(topicPartition)) {\n                result.add(topicPartition);\n            }\n        });\n        return result;\n    }\n\n    public synchronized boolean hasAutoAssignedPartitions() {\n        return this.subscriptionType == SubscriptionType.AUTO_TOPICS || this.subscriptionType == SubscriptionType.AUTO_PATTERN;\n    }\n\n    public synchronized void position(TopicPartition tp, FetchPosition position) {\n        assignedState(tp).position(position);\n    }\n\n    /**\n     * Enter the offset validation state if the leader for this partition is known to support a usable version of the\n     * OffsetsForLeaderEpoch API. If the leader node does not support the API, simply complete the offset validation.\n     *\n     * @param apiVersions supported API versions\n     * @param tp topic partition to validate\n     * @param leaderAndEpoch leader epoch of the topic partition\n     * @return true if we enter the offset validation state\n     */\n    public synchronized boolean maybeValidatePositionForCurrentLeader(ApiVersions apiVersions,\n                                                                      TopicPartition tp,\n                                                                      Metadata.LeaderAndEpoch leaderAndEpoch) {\n        TopicPartitionState state = assignedStateOrNull(tp);\n        if (state == null) {\n            log.debug(\"Skipping validating position for partition {} which is not currently assigned.\", tp);\n            return false;\n        }\n        if (leaderAndEpoch.leader.isPresent()) {\n            NodeApiVersions nodeApiVersions = apiVersions.get(leaderAndEpoch.leader.get().idString());\n            if (nodeApiVersions == null || hasUsableOffsetForLeaderEpochVersion(nodeApiVersions)) {\n                return state.maybeValidatePosition(leaderAndEpoch);\n            } else {\n                // If the broker does not support a newer version of OffsetsForLeaderEpoch, we skip validation\n                state.updatePositionLeaderNoValidation(leaderAndEpoch);\n                return false;\n            }\n        } else {\n            return state.maybeValidatePosition(leaderAndEpoch);\n        }\n    }\n\n    /**\n     * Attempt to complete validation with the end offset returned from the OffsetForLeaderEpoch request.\n     * @return Log truncation details if detected and no reset policy is defined.\n     */\n    public synchronized Optional<LogTruncation> maybeCompleteValidation(TopicPartition tp,\n                                                                        FetchPosition requestPosition,\n                                                                        EpochEndOffset epochEndOffset) {\n        TopicPartitionState state = assignedStateOrNull(tp);\n        if (state == null) {\n            log.debug(\"Skipping completed validation for partition {} which is not currently assigned.\", tp);\n        } else if (!state.awaitingValidation()) {\n            log.debug(\"Skipping completed validation for partition {} which is no longer expecting validation.\", tp);\n        } else {\n            SubscriptionState.FetchPosition currentPosition = state.position;\n            if (!currentPosition.equals(requestPosition)) {\n                log.debug(\"Skipping completed validation for partition {} since the current position {} \" +\n                          \"no longer matches the position {} when the request was sent\",\n                          tp, currentPosition, requestPosition);\n            } else if (epochEndOffset.endOffset() == UNDEFINED_EPOCH_OFFSET ||\n                        epochEndOffset.leaderEpoch() == UNDEFINED_EPOCH) {\n                if (hasDefaultOffsetResetPolicy()) {\n                    log.info(\"Truncation detected for partition {} at offset {}, resetting offset\",\n                             tp, currentPosition);\n                    requestOffsetReset(tp);\n                } else {\n                    log.warn(\"Truncation detected for partition {} at offset {}, but no reset policy is set\",\n                             tp, currentPosition);\n                    return Optional.of(new LogTruncation(tp, requestPosition, Optional.empty()));\n                }\n            } else if (epochEndOffset.endOffset() < currentPosition.offset) {\n                if (hasDefaultOffsetResetPolicy()) {\n                    SubscriptionState.FetchPosition newPosition = new SubscriptionState.FetchPosition(\n                            epochEndOffset.endOffset(), Optional.of(epochEndOffset.leaderEpoch()),\n                            currentPosition.currentLeader);\n                    log.info(\"Truncation detected for partition {} at offset {}, resetting offset to \" +\n                             \"the first offset known to diverge {}\", tp, currentPosition, newPosition);\n                    state.seekValidated(newPosition);\n                } else {\n                    OffsetAndMetadata divergentOffset = new OffsetAndMetadata(epochEndOffset.endOffset(),\n                        Optional.of(epochEndOffset.leaderEpoch()), null);\n                    log.warn(\"Truncation detected for partition {} at offset {} (the end offset from the \" +\n                             \"broker is {}), but no reset policy is set\", tp, currentPosition, divergentOffset);\n                    return Optional.of(new LogTruncation(tp, requestPosition, Optional.of(divergentOffset)));\n                }\n            } else {\n                state.completeValidation();\n            }\n        }\n\n        return Optional.empty();\n    }\n\n    public synchronized boolean awaitingValidation(TopicPartition tp) {\n        return assignedState(tp).awaitingValidation();\n    }\n\n    public synchronized void completeValidation(TopicPartition tp) {\n        assignedState(tp).completeValidation();\n    }\n\n    public synchronized FetchPosition validPosition(TopicPartition tp) {\n        return assignedState(tp).validPosition();\n    }\n\n    public synchronized FetchPosition position(TopicPartition tp) {\n        return assignedState(tp).position;\n    }\n\n    public synchronized FetchPosition positionOrNull(TopicPartition tp) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state == null) {\n            return null;\n        }\n        return assignedState(tp).position;\n    }\n\n    public synchronized Long partitionLag(TopicPartition tp, IsolationLevel isolationLevel) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        if (topicPartitionState.position == null) {\n            return null;\n        } else if (isolationLevel == IsolationLevel.READ_COMMITTED) {\n            return topicPartitionState.lastStableOffset == null ? null : topicPartitionState.lastStableOffset - topicPartitionState.position.offset;\n        } else {\n            return topicPartitionState.highWatermark == null ? null : topicPartitionState.highWatermark - topicPartitionState.position.offset;\n        }\n    }\n\n    public synchronized Long partitionEndOffset(TopicPartition tp, IsolationLevel isolationLevel) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        if (isolationLevel == IsolationLevel.READ_COMMITTED) {\n            return topicPartitionState.lastStableOffset;\n        } else {\n            return topicPartitionState.highWatermark;\n        }\n    }\n\n    public synchronized void requestPartitionEndOffset(TopicPartition tp) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        topicPartitionState.requestEndOffset();\n    }\n\n    public synchronized boolean partitionEndOffsetRequested(TopicPartition tp) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        return topicPartitionState.endOffsetRequested();\n    }\n\n    synchronized Long partitionLead(TopicPartition tp) {\n        TopicPartitionState topicPartitionState = assignedState(tp);\n        return topicPartitionState.logStartOffset == null ? null : topicPartitionState.position.offset - topicPartitionState.logStartOffset;\n    }\n\n    synchronized void updateHighWatermark(TopicPartition tp, long highWatermark) {\n        assignedState(tp).highWatermark(highWatermark);\n    }\n\n    synchronized boolean tryUpdatingHighWatermark(TopicPartition tp, long highWatermark) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state != null) {\n            assignedState(tp).highWatermark(highWatermark);\n            return true;\n        }\n        return false;\n    }\n\n    synchronized boolean tryUpdatingLogStartOffset(TopicPartition tp, long highWatermark) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state != null) {\n            assignedState(tp).logStartOffset(highWatermark);\n            return true;\n        }\n        return false;\n    }\n\n    synchronized void updateLastStableOffset(TopicPartition tp, long lastStableOffset) {\n        assignedState(tp).lastStableOffset(lastStableOffset);\n    }\n\n    synchronized boolean tryUpdatingLastStableOffset(TopicPartition tp, long lastStableOffset) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state != null) {\n            assignedState(tp).lastStableOffset(lastStableOffset);\n            return true;\n        }\n        return false;\n    }\n\n    /**\n     * Set the preferred read replica with a lease timeout. After this time, the replica will no longer be valid and\n     * {@link #preferredReadReplica(TopicPartition, long)} will return an empty result.\n     *\n     * @param tp The topic partition\n     * @param preferredReadReplicaId The preferred read replica\n     * @param timeMs The time at which this preferred replica is no longer valid\n     */\n    public synchronized void updatePreferredReadReplica(TopicPartition tp, int preferredReadReplicaId, LongSupplier timeMs) {\n        assignedState(tp).updatePreferredReadReplica(preferredReadReplicaId, timeMs);\n    }\n\n    /**\n     * Tries to set the preferred read replica with a lease timeout. After this time, the replica will no longer be valid and\n     * {@link #preferredReadReplica(TopicPartition, long)} will return an empty result. If the preferred replica of\n     * the partition could not be updated (e.g. because the partition is not assigned) this method will return\n     * {@code false}, otherwise it will return {@code true}.\n     *\n     * @param tp The topic partition\n     * @param preferredReadReplicaId The preferred read replica\n     * @param timeMs The time at which this preferred replica is no longer valid\n     * @return {@code true} if the preferred read replica was updated, {@code false} otherwise.\n     */\n    public synchronized boolean tryUpdatingPreferredReadReplica(TopicPartition tp,\n                                                             int preferredReadReplicaId,\n                                                             LongSupplier timeMs) {\n        final TopicPartitionState state = assignedStateOrNull(tp);\n        if (state != null) {\n            assignedState(tp).updatePreferredReadReplica(preferredReadReplicaId, timeMs);\n            return true;\n        }\n        return false;\n    }\n\n    /**\n     * Get the preferred read replica\n     *\n     * @param tp The topic partition\n     * @param timeMs The current time\n     * @return Returns the current preferred read replica, if it has been set and if it has not expired.\n     */\n    public synchronized Optional<Integer> preferredReadReplica(TopicPartition tp, long timeMs) {\n        final TopicPartitionState topicPartitionState = assignedStateOrNull(tp);\n        if (topicPartitionState == null) {\n            return Optional.empty();\n        } else {\n            return topicPartitionState.preferredReadReplica(timeMs);\n        }\n    }\n\n    /**\n     * Unset the preferred read replica. This causes the fetcher to go back to the leader for fetches.\n     *\n     * @param tp The topic partition\n     * @return the removed preferred read replica if set, Empty otherwise.\n     */\n    public synchronized Optional<Integer> clearPreferredReadReplica(TopicPartition tp) {\n        final TopicPartitionState topicPartitionState = assignedStateOrNull(tp);\n        if (topicPartitionState == null) {\n            return Optional.empty();\n        } else {\n            return topicPartitionState.clearPreferredReadReplica();\n        }\n    }\n\n    public synchronized Map<TopicPartition, OffsetAndMetadata> allConsumed() {\n        Map<TopicPartition, OffsetAndMetadata> allConsumed = new HashMap<>();\n        assignment.forEach((topicPartition, partitionState) -> {\n            if (partitionState.hasValidPosition())\n                allConsumed.put(topicPartition, new OffsetAndMetadata(partitionState.position.offset,\n                        partitionState.position.offsetEpoch, \"\"));\n        });\n        return allConsumed;\n    }\n\n    public synchronized void requestOffsetReset(TopicPartition partition, OffsetResetStrategy offsetResetStrategy) {\n        assignedState(partition).reset(offsetResetStrategy);\n    }\n\n    public synchronized void requestOffsetReset(Collection<TopicPartition> partitions, OffsetResetStrategy offsetResetStrategy) {\n        partitions.forEach(tp -> {\n            log.info(\"Seeking to {} offset of partition {}\", offsetResetStrategy, tp);\n            assignedState(tp).reset(offsetResetStrategy);\n        });\n    }\n\n    public void requestOffsetReset(TopicPartition partition) {\n        requestOffsetReset(partition, defaultResetStrategy);\n    }\n\n    public synchronized void requestOffsetResetIfPartitionAssigned(TopicPartition partition) {\n        final TopicPartitionState state = assignedStateOrNull(partition);\n        if (state != null) {\n            state.reset(defaultResetStrategy);\n        }\n    }\n\n\n    synchronized void setNextAllowedRetry(Set<TopicPartition> partitions, long nextAllowResetTimeMs) {\n        for (TopicPartition partition : partitions) {\n            assignedState(partition).setNextAllowedRetry(nextAllowResetTimeMs);\n        }\n    }\n\n    boolean hasDefaultOffsetResetPolicy() {\n        return defaultResetStrategy != OffsetResetStrategy.NONE;\n    }\n\n    public synchronized boolean isOffsetResetNeeded(TopicPartition partition) {\n        return assignedState(partition).awaitingReset();\n    }\n\n    public synchronized OffsetResetStrategy resetStrategy(TopicPartition partition) {\n        return assignedState(partition).resetStrategy();\n    }\n\n    public synchronized boolean hasAllFetchPositions() {\n        // Since this is in the hot-path for fetching, we do this instead of using java.util.stream API\n        Iterator<TopicPartitionState> it = assignment.stateIterator();\n        while (it.hasNext()) {\n            if (!it.next().hasValidPosition()) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    public synchronized Set<TopicPartition> initializingPartitions() {\n        return collectPartitions(state -> shouldInitialize(state) && !state.pendingOnAssignedCallback);\n    }\n\n    private Set<TopicPartition> collectPartitions(Predicate<TopicPartitionState> filter) {\n        Set<TopicPartition> result = new HashSet<>();\n        assignment.forEach((topicPartition, topicPartitionState) -> {\n            if (filter.test(topicPartitionState)) {\n                result.add(topicPartition);\n            }\n        });\n        return result;\n    }\n\n\n    public synchronized void resetInitializingPositions() {\n        final Set<TopicPartition> partitionsWithNoOffsets = new HashSet<>();\n        assignment.forEach((tp, partitionState) -> {\n            if (shouldInitialize(partitionState)) {\n                if (defaultResetStrategy == OffsetResetStrategy.NONE)\n                    partitionsWithNoOffsets.add(tp);\n                else\n                    requestOffsetReset(tp);\n            }\n        });\n\n        if (!partitionsWithNoOffsets.isEmpty())\n            throw new NoOffsetForPartitionException(partitionsWithNoOffsets);\n    }\n\n    private boolean shouldInitialize(TopicPartitionState partitionState) {\n        return partitionState.fetchState.equals(FetchStates.INITIALIZING);\n    }\n\n    public synchronized Set<TopicPartition> partitionsNeedingReset(long nowMs) {\n        return collectPartitions(state -> state.awaitingReset() && !state.awaitingRetryBackoff(nowMs));\n    }\n\n    public synchronized Set<TopicPartition> partitionsNeedingValidation(long nowMs) {\n        return collectPartitions(state -> state.awaitingValidation() && !state.awaitingRetryBackoff(nowMs));\n    }\n\n    public synchronized boolean isAssigned(TopicPartition tp) {\n        return assignment.contains(tp);\n    }\n\n    public synchronized boolean isPaused(TopicPartition tp) {\n        TopicPartitionState assignedOrNull = assignedStateOrNull(tp);\n        return assignedOrNull != null && assignedOrNull.isPaused();\n    }\n\n    synchronized boolean isFetchable(TopicPartition tp) {\n        TopicPartitionState assignedOrNull = assignedStateOrNull(tp);\n        return assignedOrNull != null && assignedOrNull.isFetchable();\n    }\n\n    public synchronized boolean hasValidPosition(TopicPartition tp) {\n        TopicPartitionState assignedOrNull = assignedStateOrNull(tp);\n        return assignedOrNull != null && assignedOrNull.hasValidPosition();\n    }\n\n    public synchronized void pause(TopicPartition tp) {\n        assignedState(tp).pause();\n    }\n\n    public synchronized void markPendingRevocation(Set<TopicPartition> tps) {\n        tps.forEach(tp -> assignedState(tp).markPendingRevocation());\n    }\n\n    // Visible for testing\n    synchronized void markPendingOnAssignedCallback(Collection<TopicPartition> tps,\n                                                    boolean pendingOnAssignedCallback) {\n        tps.forEach(tp -> assignedState(tp).markPendingOnAssignedCallback(pendingOnAssignedCallback));\n    }\n\n    /**\n     * Change the assignment to the specified partitions returned from the coordinator and mark\n     * them as awaiting onPartitionsAssigned callback. This will ensure that the partitions are\n     * included in the assignment, but are not fetchable or initialize positions while the\n     * callback runs. This is expected to be used by the async consumer.\n     *\n     * @param fullAssignment  Full collection of partitions assigned. Includes previously owned\n     *                        and newly added partitions.\n     * @param addedPartitions Subset of the fullAssignment containing the added partitions. These\n     *                        are not fetchable until the onPartitionsAssigned callback completes.\n     */\n    public synchronized void assignFromSubscribedAwaitingCallback(Collection<TopicPartition> fullAssignment,\n                                                                  Collection<TopicPartition> addedPartitions) {\n        assignFromSubscribed(fullAssignment);\n        markPendingOnAssignedCallback(addedPartitions, true);\n    }\n\n    /**\n     * Enable fetching and updating positions for the given partitions that were added to the\n     * assignment, but waiting for the onPartitionsAssigned callback to complete. This is\n     * expected to be used by the async consumer.\n     */\n    public synchronized void enablePartitionsAwaitingCallback(Collection<TopicPartition> partitions) {\n        markPendingOnAssignedCallback(partitions, false);\n    }\n\n    public synchronized void resume(TopicPartition tp) {\n        assignedState(tp).resume();\n    }\n\n    synchronized void requestFailed(Set<TopicPartition> partitions, long nextRetryTimeMs) {\n        for (TopicPartition partition : partitions) {\n            // by the time the request failed, the assignment may no longer\n            // contain this partition any more, in which case we would just ignore.\n            final TopicPartitionState state = assignedStateOrNull(partition);\n            if (state != null)\n                state.requestFailed(nextRetryTimeMs);\n        }\n    }\n\n    synchronized void movePartitionToEnd(TopicPartition tp) {\n        assignment.moveToEnd(tp);\n    }\n\n    public synchronized Optional<ConsumerRebalanceListener> rebalanceListener() {\n        return rebalanceListener;\n    }\n\n    private static class TopicPartitionState {\n\n        private FetchState fetchState;\n        private FetchPosition position; // last consumed position\n\n        private Long highWatermark; // the high watermark from last fetch\n        private Long logStartOffset; // the log start offset\n        private Long lastStableOffset;\n        private boolean paused;  // whether this partition has been paused by the user\n        private boolean pendingRevocation;\n        private boolean pendingOnAssignedCallback;\n        private OffsetResetStrategy resetStrategy;  // the strategy to use if the offset needs resetting\n        private Long nextRetryTimeMs;\n        private Integer preferredReadReplica;\n        private Long preferredReadReplicaExpireTimeMs;\n        private boolean endOffsetRequested;\n        \n        TopicPartitionState() {\n            this.paused = false;\n            this.pendingRevocation = false;\n            this.pendingOnAssignedCallback = false;\n            this.endOffsetRequested = false;\n            this.fetchState = FetchStates.INITIALIZING;\n            this.position = null;\n            this.highWatermark = null;\n            this.logStartOffset = null;\n            this.lastStableOffset = null;\n            this.resetStrategy = null;\n            this.nextRetryTimeMs = null;\n            this.preferredReadReplica = null;\n        }\n\n        public boolean endOffsetRequested() {\n            return endOffsetRequested;\n        }\n\n        public void requestEndOffset() {\n            endOffsetRequested = true;\n        }\n\n        private void transitionState(FetchState newState, Runnable runIfTransitioned) {\n            FetchState nextState = this.fetchState.transitionTo(newState);\n            if (nextState.equals(newState)) {\n                this.fetchState = nextState;\n                runIfTransitioned.run();\n                if (this.position == null && nextState.requiresPosition()) {\n                    throw new IllegalStateException(\"Transitioned subscription state to \" + nextState + \", but position is null\");\n                } else if (!nextState.requiresPosition()) {\n                    this.position = null;\n                }\n            }\n        }\n\n        private Optional<Integer> preferredReadReplica(long timeMs) {\n            if (preferredReadReplicaExpireTimeMs != null && timeMs > preferredReadReplicaExpireTimeMs) {\n                preferredReadReplica = null;\n                return Optional.empty();\n            } else {\n                return Optional.ofNullable(preferredReadReplica);\n            }\n        }\n\n        private void updatePreferredReadReplica(int preferredReadReplica, LongSupplier timeMs) {\n            if (this.preferredReadReplica == null || preferredReadReplica != this.preferredReadReplica) {\n                this.preferredReadReplica = preferredReadReplica;\n                this.preferredReadReplicaExpireTimeMs = timeMs.getAsLong();\n            }\n        }\n\n        private Optional<Integer> clearPreferredReadReplica() {\n            if (preferredReadReplica != null) {\n                int removedReplicaId = this.preferredReadReplica;\n                this.preferredReadReplica = null;\n                this.preferredReadReplicaExpireTimeMs = null;\n                return Optional.of(removedReplicaId);\n            } else {\n                return Optional.empty();\n            }\n        }\n\n        private void reset(OffsetResetStrategy strategy) {\n            transitionState(FetchStates.AWAIT_RESET, () -> {\n                this.resetStrategy = strategy;\n                this.nextRetryTimeMs = null;\n            });\n        }\n\n        /**\n         * Check if the position exists and needs to be validated. If so, enter the AWAIT_VALIDATION state. This method\n         * also will update the position with the current leader and epoch.\n         *\n         * @param currentLeaderAndEpoch leader and epoch to compare the offset with\n         * @return true if the position is now awaiting validation\n         */\n        private boolean maybeValidatePosition(Metadata.LeaderAndEpoch currentLeaderAndEpoch) {\n            if (this.fetchState.equals(FetchStates.AWAIT_RESET)) {\n                return false;\n            }\n\n            if (!currentLeaderAndEpoch.leader.isPresent()) {\n                return false;\n            }\n\n            if (position != null && !position.currentLeader.equals(currentLeaderAndEpoch)) {\n                FetchPosition newPosition = new FetchPosition(position.offset, position.offsetEpoch, currentLeaderAndEpoch);\n                validatePosition(newPosition);\n                preferredReadReplica = null;\n            }\n            return this.fetchState.equals(FetchStates.AWAIT_VALIDATION);\n        }\n\n        /**\n         * For older versions of the API, we cannot perform offset validation so we simply transition directly to FETCHING\n         */\n        private void updatePositionLeaderNoValidation(Metadata.LeaderAndEpoch currentLeaderAndEpoch) {\n            if (position != null) {\n                transitionState(FetchStates.FETCHING, () -> {\n                    this.position = new FetchPosition(position.offset, position.offsetEpoch, currentLeaderAndEpoch);\n                    this.nextRetryTimeMs = null;\n                });\n            }\n        }\n\n        private void validatePosition(FetchPosition position) {\n            if (position.offsetEpoch.isPresent() && position.currentLeader.epoch.isPresent()) {\n                transitionState(FetchStates.AWAIT_VALIDATION, () -> {\n                    this.position = position;\n                    this.nextRetryTimeMs = null;\n                });\n            } else {\n                // If we have no epoch information for the current position, then we can skip validation\n                transitionState(FetchStates.FETCHING, () -> {\n                    this.position = position;\n                    this.nextRetryTimeMs = null;\n                });\n            }\n        }\n\n        /**\n         * Clear the awaiting validation state and enter fetching.\n         */\n        private void completeValidation() {\n            if (hasPosition()) {\n                transitionState(FetchStates.FETCHING, () -> this.nextRetryTimeMs = null);\n            }\n        }\n\n        private boolean awaitingValidation() {\n            return fetchState.equals(FetchStates.AWAIT_VALIDATION);\n        }\n\n        private boolean awaitingRetryBackoff(long nowMs) {\n            return nextRetryTimeMs != null && nowMs < nextRetryTimeMs;\n        }\n\n        private boolean awaitingReset() {\n            return fetchState.equals(FetchStates.AWAIT_RESET);\n        }\n\n        private void setNextAllowedRetry(long nextAllowedRetryTimeMs) {\n            this.nextRetryTimeMs = nextAllowedRetryTimeMs;\n        }\n\n        private void requestFailed(long nextAllowedRetryTimeMs) {\n            this.nextRetryTimeMs = nextAllowedRetryTimeMs;\n        }\n\n        private boolean hasValidPosition() {\n            return fetchState.hasValidPosition();\n        }\n\n        private boolean hasPosition() {\n            return position != null;\n        }\n\n        private boolean isPaused() {\n            return paused;\n        }\n\n        private void seekValidated(FetchPosition position) {\n            transitionState(FetchStates.FETCHING, () -> {\n                this.position = position;\n                this.resetStrategy = null;\n                this.nextRetryTimeMs = null;\n            });\n        }\n\n        private void seekUnvalidated(FetchPosition fetchPosition) {\n            seekValidated(fetchPosition);\n            validatePosition(fetchPosition);\n        }\n\n        private void position(FetchPosition position) {\n            if (!hasValidPosition())\n                throw new IllegalStateException(\"Cannot set a new position without a valid current position\");\n            this.position = position;\n        }\n\n        private FetchPosition validPosition() {\n            if (hasValidPosition()) {\n                return position;\n            } else {\n                return null;\n            }\n        }\n\n        private void pause() {\n            this.paused = true;\n        }\n\n        private void markPendingRevocation() {\n            this.pendingRevocation = true;\n        }\n\n        private void markPendingOnAssignedCallback(boolean pendingOnAssignedCallback) {\n            this.pendingOnAssignedCallback = pendingOnAssignedCallback;\n        }\n\n        private void resume() {\n            this.paused = false;\n        }\n\n        private boolean isFetchable() {\n            return !paused && !pendingRevocation && !pendingOnAssignedCallback && hasValidPosition();\n        }\n\n        private void highWatermark(Long highWatermark) {\n            this.highWatermark = highWatermark;\n            this.endOffsetRequested = false;\n        }\n\n        private void logStartOffset(Long logStartOffset) {\n            this.logStartOffset = logStartOffset;\n        }\n\n        private void lastStableOffset(Long lastStableOffset) {\n            this.lastStableOffset = lastStableOffset;\n            this.endOffsetRequested = false;\n        }\n\n        private OffsetResetStrategy resetStrategy() {\n            return resetStrategy;\n        }\n    }\n\n    /**\n     * The fetch state of a partition. This class is used to determine valid state transitions and expose the some of\n     * the behavior of the current fetch state. Actual state variables are stored in the {@link TopicPartitionState}.\n     */\n    interface FetchState {\n        default FetchState transitionTo(FetchState newState) {\n            if (validTransitions().contains(newState)) {\n                return newState;\n            } else {\n                return this;\n            }\n        }\n\n        /**\n         * Return the valid states which this state can transition to\n         */\n        Collection<FetchState> validTransitions();\n\n        /**\n         * Test if this state requires a position to be set\n         */\n        boolean requiresPosition();\n\n        /**\n         * Test if this state is considered to have a valid position which can be used for fetching\n         */\n        boolean hasValidPosition();\n    }\n\n    /**\n     * An enumeration of all the possible fetch states. The state transitions are encoded in the values returned by\n     * {@link FetchState#validTransitions}.\n     */\n    enum FetchStates implements FetchState {\n        INITIALIZING() {\n            @Override\n            public Collection<FetchState> validTransitions() {\n                return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET, FetchStates.AWAIT_VALIDATION);\n            }\n\n            @Override\n            public boolean requiresPosition() {\n                return false;\n            }\n\n            @Override\n            public boolean hasValidPosition() {\n                return false;\n            }\n        },\n\n        FETCHING() {\n            @Override\n            public Collection<FetchState> validTransitions() {\n                return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET, FetchStates.AWAIT_VALIDATION);\n            }\n\n            @Override\n            public boolean requiresPosition() {\n                return true;\n            }\n\n            @Override\n            public boolean hasValidPosition() {\n                return true;\n            }\n        },\n\n        AWAIT_RESET() {\n            @Override\n            public Collection<FetchState> validTransitions() {\n                return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET);\n            }\n\n            @Override\n            public boolean requiresPosition() {\n                return false;\n            }\n\n            @Override\n            public boolean hasValidPosition() {\n                return false;\n            }\n        },\n\n        AWAIT_VALIDATION() {\n            @Override\n            public Collection<FetchState> validTransitions() {\n                return Arrays.asList(FetchStates.FETCHING, FetchStates.AWAIT_RESET, FetchStates.AWAIT_VALIDATION);\n            }\n\n            @Override\n            public boolean requiresPosition() {\n                return true;\n            }\n\n            @Override\n            public boolean hasValidPosition() {\n                return false;\n            }\n        }\n    }\n\n    /**\n     * Represents the position of a partition subscription.\n     *\n     * This includes the offset and epoch from the last record in\n     * the batch from a FetchResponse. It also includes the leader epoch at the time the batch was consumed.\n     */\n    public static class FetchPosition {\n        public final long offset;\n        final Optional<Integer> offsetEpoch;\n        final Metadata.LeaderAndEpoch currentLeader;\n\n        FetchPosition(long offset) {\n            this(offset, Optional.empty(), Metadata.LeaderAndEpoch.noLeaderOrEpoch());\n        }\n\n        public FetchPosition(long offset, Optional<Integer> offsetEpoch, Metadata.LeaderAndEpoch currentLeader) {\n            this.offset = offset;\n            this.offsetEpoch = Objects.requireNonNull(offsetEpoch);\n            this.currentLeader = Objects.requireNonNull(currentLeader);\n        }\n\n        @Override\n        public boolean equals(Object o) {\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n            FetchPosition that = (FetchPosition) o;\n            return offset == that.offset &&\n                    offsetEpoch.equals(that.offsetEpoch) &&\n                    currentLeader.equals(that.currentLeader);\n        }\n\n        @Override\n        public int hashCode() {\n            return Objects.hash(offset, offsetEpoch, currentLeader);\n        }\n\n        @Override\n        public String toString() {\n            return \"FetchPosition{\" +\n                    \"offset=\" + offset +\n                    \", offsetEpoch=\" + offsetEpoch +\n                    \", currentLeader=\" + currentLeader +\n                    '}';\n        }\n    }\n\n    public static class LogTruncation {\n        public final TopicPartition topicPartition;\n        public final FetchPosition fetchPosition;\n        public final Optional<OffsetAndMetadata> divergentOffsetOpt;\n\n        public LogTruncation(TopicPartition topicPartition,\n                             FetchPosition fetchPosition,\n                             Optional<OffsetAndMetadata> divergentOffsetOpt) {\n            this.topicPartition = topicPartition;\n            this.fetchPosition = fetchPosition;\n            this.divergentOffsetOpt = divergentOffsetOpt;\n        }\n\n        @Override\n        public String toString() {\n            StringBuilder bldr = new StringBuilder()\n                .append(\"(partition=\")\n                .append(topicPartition)\n                .append(\", fetchOffset=\")\n                .append(fetchPosition.offset)\n                .append(\", fetchEpoch=\")\n                .append(fetchPosition.offsetEpoch);\n\n            if (divergentOffsetOpt.isPresent()) {\n                OffsetAndMetadata divergentOffset = divergentOffsetOpt.get();\n                bldr.append(\", divergentOffset=\")\n                    .append(divergentOffset.offset())\n                    .append(\", divergentEpoch=\")\n                    .append(divergentOffset.leaderEpoch());\n            } else {\n                bldr.append(\", divergentOffset=unknown\")\n                    .append(\", divergentEpoch=unknown\");\n            }\n\n            return bldr.append(\")\").toString();\n\n        }\n    }\n}",
                "methodCount": 141
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "prettyString",
                            "method_signature": "public synchronized prettyString()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "subscribe",
                            "method_signature": "public synchronized subscribe(Set<String> topics, Optional<ConsumerRebalanceListener> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "subscribe",
                            "method_signature": "public synchronized subscribe(Pattern pattern, Optional<ConsumerRebalanceListener> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "subscribeFromPattern",
                            "method_signature": "public synchronized subscribeFromPattern(Set<String> topics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "changeSubscription",
                            "method_signature": "private changeSubscription(Set<String> topicsToSubscribe)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "groupSubscribe",
                            "method_signature": "synchronized groupSubscribe(Collection<String> topics)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignFromUser",
                            "method_signature": "public synchronized assignFromUser(Set<TopicPartition> partitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "checkAssignmentMatchedSubscription",
                            "method_signature": "public synchronized checkAssignmentMatchedSubscription(Collection<TopicPartition> assignments)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignFromSubscribed",
                            "method_signature": "public synchronized assignFromSubscribed(Collection<TopicPartition> assignments)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasPatternSubscription",
                            "method_signature": "synchronized hasPatternSubscription()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasNoSubscriptionOrUserAssignment",
                            "method_signature": "public synchronized hasNoSubscriptionOrUserAssignment()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unsubscribe",
                            "method_signature": "public synchronized unsubscribe()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "matchesSubscribedPattern",
                            "method_signature": "synchronized matchesSubscribedPattern(String topic)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "subscription",
                            "method_signature": "public synchronized subscription()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pausedPartitions",
                            "method_signature": "public synchronized pausedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metadataTopics",
                            "method_signature": "synchronized metadataTopics()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "needsMetadata",
                            "method_signature": "synchronized needsMetadata(String topic)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedState",
                            "method_signature": "private assignedState(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedStateOrNull",
                            "method_signature": "private assignedStateOrNull(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seekValidated",
                            "method_signature": "public synchronized seekValidated(TopicPartition tp, FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seek",
                            "method_signature": "public seek(TopicPartition tp, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seekUnvalidated",
                            "method_signature": "public seekUnvalidated(TopicPartition tp, FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSeekUnvalidated",
                            "method_signature": "synchronized maybeSeekUnvalidated(TopicPartition tp, FetchPosition position, OffsetResetStrategy requestedResetStrategy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedPartitions",
                            "method_signature": "public synchronized assignedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignedPartitionsList",
                            "method_signature": "public synchronized assignedPartitionsList()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "numAssignedPartitions",
                            "method_signature": "synchronized numAssignedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchablePartitions",
                            "method_signature": "public synchronized fetchablePartitions(Predicate<TopicPartition> isAvailable)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasAutoAssignedPartitions",
                            "method_signature": "public synchronized hasAutoAssignedPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "position",
                            "method_signature": "public synchronized position(TopicPartition tp, FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeValidatePositionForCurrentLeader",
                            "method_signature": "public synchronized maybeValidatePositionForCurrentLeader(ApiVersions apiVersions,\n                                                                      TopicPartition tp,\n                                                                      Metadata.LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCompleteValidation",
                            "method_signature": "public synchronized maybeCompleteValidation(TopicPartition tp,\n                                                                        FetchPosition requestPosition,\n                                                                        EpochEndOffset epochEndOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingValidation",
                            "method_signature": "public synchronized awaitingValidation(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeValidation",
                            "method_signature": "public synchronized completeValidation(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validPosition",
                            "method_signature": "public synchronized validPosition(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "positionOrNull",
                            "method_signature": "public synchronized positionOrNull(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionLag",
                            "method_signature": "public synchronized partitionLag(TopicPartition tp, IsolationLevel isolationLevel)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionEndOffset",
                            "method_signature": "public synchronized partitionEndOffset(TopicPartition tp, IsolationLevel isolationLevel)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestPartitionEndOffset",
                            "method_signature": "public synchronized requestPartitionEndOffset(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionEndOffsetRequested",
                            "method_signature": "public synchronized partitionEndOffsetRequested(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionLead",
                            "method_signature": "synchronized partitionLead(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateHighWatermark",
                            "method_signature": "synchronized updateHighWatermark(TopicPartition tp, long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tryUpdatingHighWatermark",
                            "method_signature": "synchronized tryUpdatingHighWatermark(TopicPartition tp, long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tryUpdatingLogStartOffset",
                            "method_signature": "synchronized tryUpdatingLogStartOffset(TopicPartition tp, long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateLastStableOffset",
                            "method_signature": "synchronized updateLastStableOffset(TopicPartition tp, long lastStableOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tryUpdatingLastStableOffset",
                            "method_signature": "synchronized tryUpdatingLastStableOffset(TopicPartition tp, long lastStableOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updatePreferredReadReplica",
                            "method_signature": "public synchronized updatePreferredReadReplica(TopicPartition tp, int preferredReadReplicaId, LongSupplier timeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tryUpdatingPreferredReadReplica",
                            "method_signature": "public synchronized tryUpdatingPreferredReadReplica(TopicPartition tp,\n                                                             int preferredReadReplicaId,\n                                                             LongSupplier timeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "preferredReadReplica",
                            "method_signature": "public synchronized preferredReadReplica(TopicPartition tp, long timeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clearPreferredReadReplica",
                            "method_signature": "public synchronized clearPreferredReadReplica(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allConsumed",
                            "method_signature": "public synchronized allConsumed()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestOffsetReset",
                            "method_signature": "public synchronized requestOffsetReset(TopicPartition partition, OffsetResetStrategy offsetResetStrategy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestOffsetReset",
                            "method_signature": "public synchronized requestOffsetReset(Collection<TopicPartition> partitions, OffsetResetStrategy offsetResetStrategy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestOffsetReset",
                            "method_signature": "public requestOffsetReset(TopicPartition partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestOffsetResetIfPartitionAssigned",
                            "method_signature": "public synchronized requestOffsetResetIfPartitionAssigned(TopicPartition partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "setNextAllowedRetry",
                            "method_signature": "synchronized setNextAllowedRetry(Set<TopicPartition> partitions, long nextAllowResetTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasDefaultOffsetResetPolicy",
                            "method_signature": " hasDefaultOffsetResetPolicy()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isOffsetResetNeeded",
                            "method_signature": "public synchronized isOffsetResetNeeded(TopicPartition partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetStrategy",
                            "method_signature": "public synchronized resetStrategy(TopicPartition partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasAllFetchPositions",
                            "method_signature": "public synchronized hasAllFetchPositions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initializingPartitions",
                            "method_signature": "public synchronized initializingPartitions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "collectPartitions",
                            "method_signature": "private collectPartitions(Predicate<TopicPartitionState> filter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetInitializingPositions",
                            "method_signature": "public synchronized resetInitializingPositions()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shouldInitialize",
                            "method_signature": "private shouldInitialize(TopicPartitionState partitionState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionsNeedingReset",
                            "method_signature": "public synchronized partitionsNeedingReset(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionsNeedingValidation",
                            "method_signature": "public synchronized partitionsNeedingValidation(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isAssigned",
                            "method_signature": "public synchronized isAssigned(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isPaused",
                            "method_signature": "public synchronized isPaused(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isFetchable",
                            "method_signature": "synchronized isFetchable(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasValidPosition",
                            "method_signature": "public synchronized hasValidPosition(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pause",
                            "method_signature": "public synchronized pause(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "markPendingRevocation",
                            "method_signature": "public synchronized markPendingRevocation(Set<TopicPartition> tps)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "markPendingOnAssignedCallback",
                            "method_signature": "synchronized markPendingOnAssignedCallback(Collection<TopicPartition> tps,\n                                                    boolean pendingOnAssignedCallback)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assignFromSubscribedAwaitingCallback",
                            "method_signature": "public synchronized assignFromSubscribedAwaitingCallback(Collection<TopicPartition> fullAssignment,\n                                                                  Collection<TopicPartition> addedPartitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "enablePartitionsAwaitingCallback",
                            "method_signature": "public synchronized enablePartitionsAwaitingCallback(Collection<TopicPartition> partitions)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resume",
                            "method_signature": "public synchronized resume(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestFailed",
                            "method_signature": "synchronized requestFailed(Set<TopicPartition> partitions, long nextRetryTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "movePartitionToEnd",
                            "method_signature": "synchronized movePartitionToEnd(TopicPartition tp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionState",
                            "method_signature": "private transitionState(FetchState newState, Runnable runIfTransitioned)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "preferredReadReplica",
                            "method_signature": "private preferredReadReplica(long timeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updatePreferredReadReplica",
                            "method_signature": "private updatePreferredReadReplica(int preferredReadReplica, LongSupplier timeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clearPreferredReadReplica",
                            "method_signature": "private clearPreferredReadReplica()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "reset",
                            "method_signature": "private reset(OffsetResetStrategy strategy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeValidatePosition",
                            "method_signature": "private maybeValidatePosition(Metadata.LeaderAndEpoch currentLeaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updatePositionLeaderNoValidation",
                            "method_signature": "private updatePositionLeaderNoValidation(Metadata.LeaderAndEpoch currentLeaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validatePosition",
                            "method_signature": "private validatePosition(FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeValidation",
                            "method_signature": "private completeValidation()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingValidation",
                            "method_signature": "private awaitingValidation()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingRetryBackoff",
                            "method_signature": "private awaitingRetryBackoff(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingReset",
                            "method_signature": "private awaitingReset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasValidPosition",
                            "method_signature": "private hasValidPosition()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasPosition",
                            "method_signature": "private hasPosition()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seekValidated",
                            "method_signature": "private seekValidated(FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seekUnvalidated",
                            "method_signature": "private seekUnvalidated(FetchPosition fetchPosition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "position",
                            "method_signature": "private position(FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validPosition",
                            "method_signature": "private validPosition()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isFetchable",
                            "method_signature": "private isFetchable()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "highWatermark",
                            "method_signature": "private highWatermark(Long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "lastStableOffset",
                            "method_signature": "private lastStableOffset(Long lastStableOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionTo",
                            "method_signature": "default transitionTo(FetchState newState)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "hasPosition",
                            "method_signature": "private hasPosition()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingRetryBackoff",
                            "method_signature": "private awaitingRetryBackoff(long nowMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clearPreferredReadReplica",
                            "method_signature": "private clearPreferredReadReplica()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionTo",
                            "method_signature": "default transitionTo(FetchState newState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "awaitingValidation",
                            "method_signature": "private awaitingValidation()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "preferredReadReplica",
                            "method_signature": "private preferredReadReplica(long timeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seekUnvalidated",
                            "method_signature": "private seekUnvalidated(FetchPosition fetchPosition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "highWatermark",
                            "method_signature": "private highWatermark(Long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updatePreferredReadReplica",
                            "method_signature": "private updatePreferredReadReplica(int preferredReadReplica, LongSupplier timeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "prettyString",
                            "method_signature": "public synchronized prettyString()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validPosition",
                            "method_signature": "private validPosition()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeValidation",
                            "method_signature": "private completeValidation()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "position",
                            "method_signature": "private position(FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "changeSubscription",
                            "method_signature": "private changeSubscription(Set<String> topicsToSubscribe)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "seekValidated",
                            "method_signature": "private seekValidated(FetchPosition position)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private hasPosition()": {
                    "first": {
                        "method_name": "hasPosition",
                        "method_signature": "private hasPosition()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2723158527288302
                },
                "private awaitingRetryBackoff(long nowMs)": {
                    "first": {
                        "method_name": "awaitingRetryBackoff",
                        "method_signature": "private awaitingRetryBackoff(long nowMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.32066329072422967
                },
                "private clearPreferredReadReplica()": {
                    "first": {
                        "method_name": "clearPreferredReadReplica",
                        "method_signature": "private clearPreferredReadReplica()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.33472678062096806
                },
                "default transitionTo(FetchState newState)": {
                    "first": {
                        "method_name": "transitionTo",
                        "method_signature": "default transitionTo(FetchState newState)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.33607585160528053
                },
                "private awaitingValidation()": {
                    "first": {
                        "method_name": "awaitingValidation",
                        "method_signature": "private awaitingValidation()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3362139540110143
                },
                "private preferredReadReplica(long timeMs)": {
                    "first": {
                        "method_name": "preferredReadReplica",
                        "method_signature": "private preferredReadReplica(long timeMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3564958641777372
                },
                "private seekUnvalidated(FetchPosition fetchPosition)": {
                    "first": {
                        "method_name": "seekUnvalidated",
                        "method_signature": "private seekUnvalidated(FetchPosition fetchPosition)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3607037753862881
                },
                "private highWatermark(Long highWatermark)": {
                    "first": {
                        "method_name": "highWatermark",
                        "method_signature": "private highWatermark(Long highWatermark)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3630372942146014
                },
                "private updatePreferredReadReplica(int preferredReadReplica, LongSupplier timeMs)": {
                    "first": {
                        "method_name": "updatePreferredReadReplica",
                        "method_signature": "private updatePreferredReadReplica(int preferredReadReplica, LongSupplier timeMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3779082815031921
                },
                "public synchronized prettyString()": {
                    "first": {
                        "method_name": "prettyString",
                        "method_signature": "public synchronized prettyString()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.37999363829823524
                },
                "private validPosition()": {
                    "first": {
                        "method_name": "validPosition",
                        "method_signature": "private validPosition()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.38657211306612005
                },
                "private completeValidation()": {
                    "first": {
                        "method_name": "completeValidation",
                        "method_signature": "private completeValidation()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.39326397458786083
                },
                "private position(FetchPosition position)": {
                    "first": {
                        "method_name": "position",
                        "method_signature": "private position(FetchPosition position)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4031273057968271
                },
                "private changeSubscription(Set<String> topicsToSubscribe)": {
                    "first": {
                        "method_name": "changeSubscription",
                        "method_signature": "private changeSubscription(Set<String> topicsToSubscribe)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41488376787414816
                },
                "private seekValidated(FetchPosition position)": {
                    "first": {
                        "method_name": "seekValidated",
                        "method_signature": "private seekValidated(FetchPosition position)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41662685811938416
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "459da4795a511f6933e940fcf105a824bd9e589c",
        "url": "https://github.com/apache/kafka/commit/459da4795a511f6933e940fcf105a824bd9e589c",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public hasAnyInflightRequest(currentTimeMs long) : boolean extracted from private pollFollowerAsObserver(state FollowerState, currentTimeMs long) : long in class org.apache.kafka.raft.KafkaRaftClient & moved to class org.apache.kafka.raft.RequestManager",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 2157,
                    "endLine": 2178,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private pollFollowerAsObserver(state FollowerState, currentTimeMs long) : long"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 2166,
                    "endLine": 2166,
                    "startColumn": 13,
                    "endColumn": 87,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 63,
                    "endLine": 94,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public hasAnyInflightRequest(currentTimeMs long) : boolean"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 78,
                    "endLine": 78,
                    "startColumn": 13,
                    "endColumn": 58,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 2220,
                    "endLine": 2243,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private pollFollowerAsObserver(state FollowerState, currentTimeMs long) : long"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 2235,
                    "endLine": 2235,
                    "startColumn": 25,
                    "endColumn": 76,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "requestManager.hasAnyInflightRequest(currentTimeMs)"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 74,
                    "endLine": 74,
                    "startColumn": 9,
                    "endColumn": 32,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 76,
                    "endLine": 76,
                    "startColumn": 9,
                    "endColumn": 78,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 81,
                    "endLine": 81,
                    "startColumn": 17,
                    "endColumn": 35,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 84,
                    "endLine": 84,
                    "startColumn": 17,
                    "endColumn": 35,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 88,
                    "endLine": 88,
                    "startColumn": 17,
                    "endColumn": 31,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 89,
                    "endLine": 89,
                    "startColumn": 17,
                    "endColumn": 23,
                    "codeElementType": "BREAK_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 93,
                    "endLine": 93,
                    "startColumn": 9,
                    "endColumn": 23,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 79,
                    "endLine": 82,
                    "startColumn": 63,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 82,
                    "endLine": 85,
                    "startColumn": 69,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 85,
                    "endLine": 90,
                    "startColumn": 70,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 85,
                    "endLine": 90,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 82,
                    "endLine": 90,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 77,
                    "endLine": 91,
                    "startColumn": 36,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 77,
                    "endLine": 91,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "WHILE_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 79,
                    "endLine": 90,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 581,
        "extraction_results": {
            "success": true,
            "newCommitHash": "f62b73b6006f87b0004fd8ae26fec3980db04ca8",
            "newBranchName": "extract-hasAnyInflightRequest-pollFollowerAsObserver-8a882a7"
        },
        "telemetry": {
            "id": "812ebb4f-278a-4053-adbf-e19e8c16476e",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2693,
                "lineStart": 109,
                "lineEnd": 2801,
                "bodyLineStart": 109,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                "sourceCode": "/**\n * This class implements a Kafkaesque version of the Raft protocol. Leader election\n * is more or less pure Raft, but replication is driven by replica fetching and we use Kafka's\n * log reconciliation protocol to truncate the log to a common point following each leader\n * election.\n *\n * Like Zookeeper, this protocol distinguishes between voters and observers. Voters are\n * the only ones who are eligible to handle protocol requests and they are the only ones\n * who take part in elections. The protocol does not yet support dynamic quorum changes.\n *\n * These are the APIs in this protocol:\n *\n * 1) {@link VoteRequestData}: Sent by valid voters when their election timeout expires and they\n *    become a candidate. This request includes the last offset in the log which electors use\n *    to tell whether or not to grant the vote.\n *\n * 2) {@link BeginQuorumEpochRequestData}: Sent by the leader of an epoch only to valid voters to\n *    assert its leadership of the new epoch. This request will be retried indefinitely for\n *    each voter until it acknowledges the request or a new election occurs.\n *\n *    This is not needed in usual Raft because the leader can use an empty data push\n *    to achieve the same purpose. The Kafka Raft implementation, however, is driven by\n *    fetch requests from followers, so there must be a way to find the new leader after\n *    an election has completed.\n *\n * 3) {@link EndQuorumEpochRequestData}: Sent by the leader of an epoch to valid voters in order to\n *    gracefully resign from the current epoch. This causes remaining voters to immediately\n *    begin a new election.\n *\n * 4) {@link FetchRequestData}: This is the same as the usual Fetch API in Kafka, but we add snapshot\n *    check before responding, and we also piggyback some additional metadata on responses (i.e. current\n *    leader and epoch). Unlike partition replication, we also piggyback truncation detection on this API\n *    rather than through a separate truncation state.\n *\n * 5) {@link FetchSnapshotRequestData}: Sent by the follower to the epoch leader in order to fetch a snapshot.\n *    This happens when a FetchResponse includes a snapshot ID due to the follower's log end offset being less\n *    than the leader's log start offset. This API is similar to the Fetch API since the snapshot is stored\n *    as FileRecords, but we use {@link UnalignedRecords} in FetchSnapshotResponse because the records\n *    are not necessarily offset-aligned.\n */\nfinal public class KafkaRaftClient<T> implements RaftClient<T> {\n    private static final int RETRY_BACKOFF_BASE_MS = 100;\n    public static final int MAX_FETCH_WAIT_MS = 500;\n    public static final int MAX_BATCH_SIZE_BYTES = 8 * 1024 * 1024;\n    public static final int MAX_FETCH_SIZE_BYTES = MAX_BATCH_SIZE_BYTES;\n\n    private final OptionalInt nodeId;\n    private final Uuid nodeDirectoryId;\n    private final AtomicReference<GracefulShutdown> shutdown = new AtomicReference<>();\n    private final LogContext logContext;\n    private final Logger logger;\n    private final Time time;\n    private final int fetchMaxWaitMs;\n    private final String clusterId;\n    private final NetworkChannel channel;\n    private final ReplicatedLog log;\n    private final Random random;\n    private final FuturePurgatory<Long> appendPurgatory;\n    private final FuturePurgatory<Long> fetchPurgatory;\n    private final RecordSerde<T> serde;\n    private final MemoryPool memoryPool;\n    private final RaftMessageQueue messageQueue;\n    private final QuorumConfig quorumConfig;\n    private final RaftMetadataLogCleanerManager snapshotCleaner;\n\n    private final Map<Listener<T>, ListenerContext> listenerContexts = new IdentityHashMap<>();\n    private final ConcurrentLinkedQueue<Registration<T>> pendingRegistrations = new ConcurrentLinkedQueue<>();\n\n    // These components need to be initialized by the method initialize() because they depend on\n    // the voter set\n    /*\n     * The key invariant for the kraft control record state machine is that it has always read to\n     * the LEO. This is achieved by:\n     *\n     * 1. reading the entire partition (snapshot and log) at start up,\n     * 2. updating the state when a snapshot is replaced, because of FETCH_SNAPSHOT, on the\n     *    followers\n     * 3. updating the state when the leader (call to append()) or follower (FETCH) appends to the\n     *    log\n     * 4. truncate new entries when a follower truncates their log\n     * 5. truncate old entries when a snapshot gets generated\n     */\n    private volatile KRaftControlRecordStateMachine partitionState;\n    private volatile KafkaRaftMetrics kafkaRaftMetrics;\n    private volatile QuorumState quorum;\n    private volatile RequestManager requestManager;\n\n    /**\n     * Create a new instance.\n     *\n     * Note that if the node ID is empty, then the client will behave as a\n     * non-participating observer.\n     */\n    public KafkaRaftClient(\n        OptionalInt nodeId,\n        Uuid nodeDirectoryId,\n        RecordSerde<T> serde,\n        NetworkChannel channel,\n        ReplicatedLog log,\n        Time time,\n        ExpirationService expirationService,\n        LogContext logContext,\n        String clusterId,\n        QuorumConfig quorumConfig\n    ) {\n        this(\n            nodeId,\n            nodeDirectoryId,\n            serde,\n            channel,\n            new BlockingMessageQueue(),\n            log,\n            new BatchMemoryPool(5, MAX_BATCH_SIZE_BYTES),\n            time,\n            expirationService,\n            MAX_FETCH_WAIT_MS,\n            clusterId,\n            logContext,\n            new Random(),\n            quorumConfig\n        );\n    }\n\n    KafkaRaftClient(\n        OptionalInt nodeId,\n        Uuid nodeDirectoryId,\n        RecordSerde<T> serde,\n        NetworkChannel channel,\n        RaftMessageQueue messageQueue,\n        ReplicatedLog log,\n        MemoryPool memoryPool,\n        Time time,\n        ExpirationService expirationService,\n        int fetchMaxWaitMs,\n        String clusterId,\n        LogContext logContext,\n        Random random,\n        QuorumConfig quorumConfig\n    ) {\n        this.nodeId = nodeId;\n        this.nodeDirectoryId = nodeDirectoryId;\n        this.logContext = logContext;\n        this.serde = serde;\n        this.channel = channel;\n        this.messageQueue = messageQueue;\n        this.log = log;\n        this.memoryPool = memoryPool;\n        this.fetchPurgatory = new ThresholdPurgatory<>(expirationService);\n        this.appendPurgatory = new ThresholdPurgatory<>(expirationService);\n        this.time = time;\n        this.clusterId = clusterId;\n        this.fetchMaxWaitMs = fetchMaxWaitMs;\n        this.logger = logContext.logger(KafkaRaftClient.class);\n        this.random = random;\n        this.quorumConfig = quorumConfig;\n        this.snapshotCleaner = new RaftMetadataLogCleanerManager(logger, time, 60000, log::maybeClean);\n    }\n\n    private void updateFollowerHighWatermark(\n        FollowerState state,\n        OptionalLong highWatermarkOpt\n    ) {\n        highWatermarkOpt.ifPresent(highWatermark -> {\n            long newHighWatermark = Math.min(endOffset().offset(), highWatermark);\n            if (state.updateHighWatermark(OptionalLong.of(newHighWatermark))) {\n                logger.debug(\"Follower high watermark updated to {}\", newHighWatermark);\n                log.updateHighWatermark(new LogOffsetMetadata(newHighWatermark));\n                updateListenersProgress(newHighWatermark);\n            }\n        });\n    }\n\n    private void updateLeaderEndOffsetAndTimestamp(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n\n        if (state.updateLocalState(endOffsetMetadata)) {\n            onUpdateLeaderHighWatermark(state, currentTimeMs);\n        }\n\n        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n    }\n\n    private void onUpdateLeaderHighWatermark(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        state.highWatermark().ifPresent(highWatermark -> {\n            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n            log.updateHighWatermark(highWatermark);\n\n            // After updating the high watermark, we first clear the append\n            // purgatory so that we have an opportunity to route the pending\n            // records still held in memory directly to the listener\n            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n\n            // It is also possible that the high watermark is being updated\n            // for the first time following the leader election, so we need\n            // to give lagging listeners an opportunity to catch up as well\n            updateListenersProgress(highWatermark.offset);\n        });\n    }\n\n    private void updateListenersProgress(long highWatermark) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.nextExpectedOffset().ifPresent(nextExpectedOffset -> {\n                // Send snapshot to the listener, if the listener is at the beginning of the log and there is a snapshot,\n                // or the listener is trying to read an offset for which there isn't a segment in the log.\n                if (nextExpectedOffset < highWatermark &&\n                    ((nextExpectedOffset == 0 && latestSnapshot().isPresent()) ||\n                     nextExpectedOffset < log.startOffset())\n                ) {\n                    SnapshotReader<T> snapshot = latestSnapshot().orElseThrow(() -> new IllegalStateException(\n                        String.format(\n                            \"Snapshot expected since next offset of %s is %d, log start offset is %d and high-watermark is %d\",\n                            listenerContext.listenerName(),\n                            nextExpectedOffset,\n                            log.startOffset(),\n                            highWatermark\n                        )\n                    ));\n                    listenerContext.fireHandleSnapshot(snapshot);\n                }\n            });\n\n            // Re-read the expected offset in case the snapshot had to be reloaded\n            listenerContext.nextExpectedOffset().ifPresent(nextExpectedOffset -> {\n                if (nextExpectedOffset < highWatermark) {\n                    LogFetchInfo readInfo = log.read(nextExpectedOffset, Isolation.COMMITTED);\n                    listenerContext.fireHandleCommit(nextExpectedOffset, readInfo.records);\n                }\n            });\n        }\n    }\n\n    private Optional<SnapshotReader<T>> latestSnapshot() {\n        return log.latestSnapshot().map(reader ->\n            RecordsSnapshotReader.of(reader,\n                serde,\n                BufferSupplier.create(),\n                MAX_BATCH_SIZE_BYTES,\n                true /* Validate batch CRC*/\n            )\n        );\n    }\n\n    private void maybeFireHandleCommit(long baseOffset, int epoch, long appendTimestamp, int sizeInBytes, List<T> records) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.nextExpectedOffset().ifPresent(nextOffset -> {\n                if (nextOffset == baseOffset) {\n                    listenerContext.fireHandleCommit(baseOffset, epoch, appendTimestamp, sizeInBytes, records);\n                }\n            });\n        }\n    }\n\n    private void maybeFireLeaderChange(LeaderState<T> state) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.maybeFireLeaderChange(quorum.leaderAndEpoch(), state.epochStartOffset());\n        }\n    }\n\n    private void maybeFireLeaderChange() {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.maybeFireLeaderChange(quorum.leaderAndEpoch());\n        }\n    }\n\n    public void initialize(\n        Map<Integer, InetSocketAddress> voterAddresses,\n        String listenerName,\n        QuorumStateStore quorumStateStore,\n        Metrics metrics\n    ) {\n        partitionState = new KRaftControlRecordStateMachine(\n            Optional.of(VoterSet.fromInetSocketAddresses(listenerName, voterAddresses)),\n            log,\n            serde,\n            BufferSupplier.create(),\n            MAX_BATCH_SIZE_BYTES,\n            logContext\n        );\n        // Read the entire log\n        logger.info(\"Reading KRaft snapshot and log as part of the initialization\");\n        partitionState.updateState();\n\n        VoterSet lastVoterSet = partitionState.lastVoterSet();\n        requestManager = new RequestManager(\n            lastVoterSet.voterIds(),\n            quorumConfig.retryBackoffMs(),\n            quorumConfig.requestTimeoutMs(),\n            random\n        );\n\n        quorum = new QuorumState(\n            nodeId,\n            nodeDirectoryId,\n            partitionState::lastVoterSet,\n            partitionState::lastKraftVersion,\n            quorumConfig.electionTimeoutMs(),\n            quorumConfig.fetchTimeoutMs(),\n            quorumStateStore,\n            time,\n            logContext,\n            random\n        );\n\n        kafkaRaftMetrics = new KafkaRaftMetrics(metrics, \"raft\", quorum);\n        // All Raft voters are statically configured and known at startup\n        // so there are no unknown voter connections. Report this metric as 0.\n        kafkaRaftMetrics.updateNumUnknownVoterConnections(0);\n\n        for (Integer voterId : lastVoterSet.voterIds()) {\n            channel.updateEndpoint(voterId, lastVoterSet.voterAddress(voterId, listenerName).get());\n        }\n\n        quorum.initialize(new OffsetAndEpoch(log.endOffset().offset, log.lastFetchedEpoch()));\n\n        long currentTimeMs = time.milliseconds();\n        if (quorum.isLeader()) {\n            throw new IllegalStateException(\"Voter cannot initialize as a Leader\");\n        } else if (quorum.isCandidate()) {\n            onBecomeCandidate(currentTimeMs);\n        } else if (quorum.isFollower()) {\n            onBecomeFollower(currentTimeMs);\n        }\n\n        // When there is only a single voter, become candidate immediately\n        if (quorum.isOnlyVoter() && !quorum.isCandidate()) {\n            transitionToCandidate(currentTimeMs);\n        }\n    }\n\n    @Override\n    public void register(Listener<T> listener) {\n        pendingRegistrations.add(Registration.register(listener));\n        wakeup();\n    }\n\n    @Override\n    public void unregister(Listener<T> listener) {\n        pendingRegistrations.add(Registration.unregister(listener));\n        // No need to wake up the polling thread. It is a removal so the updates can be\n        // delayed until the polling thread wakes up for other reasons.\n    }\n\n    @Override\n    public LeaderAndEpoch leaderAndEpoch() {\n        if (isInitialized()) {\n            return quorum.leaderAndEpoch();\n        } else {\n            return LeaderAndEpoch.UNKNOWN;\n        }\n    }\n\n    @Override\n    public OptionalInt nodeId() {\n        return nodeId;\n    }\n\n    private OffsetAndEpoch endOffset() {\n        return new OffsetAndEpoch(log.endOffset().offset, log.lastFetchedEpoch());\n    }\n\n    private void resetConnections() {\n        requestManager.resetAll();\n    }\n\n    private void onBecomeLeader(long currentTimeMs) {\n        long endOffset = log.endOffset().offset;\n\n        BatchAccumulator<T> accumulator = new BatchAccumulator<>(\n            quorum.epoch(),\n            endOffset,\n            quorumConfig.appendLingerMs(),\n            MAX_BATCH_SIZE_BYTES,\n            memoryPool,\n            time,\n            Compression.NONE,\n            serde\n        );\n\n        LeaderState<T> state = quorum.transitionToLeader(endOffset, accumulator);\n\n        log.initializeLeaderEpoch(quorum.epoch());\n\n        // The high watermark can only be advanced once we have written a record\n        // from the new leader's epoch. Hence we write a control message immediately\n        // to ensure there is no delay committing pending data.\n        state.appendLeaderChangeMessage(currentTimeMs);\n\n        resetConnections();\n        kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n    }\n\n    private void flushLeaderLog(LeaderState<T> state, long currentTimeMs) {\n        // We update the end offset before flushing so that parked fetches can return sooner.\n        updateLeaderEndOffsetAndTimestamp(state, currentTimeMs);\n        log.flush(false);\n    }\n\n    private boolean maybeTransitionToLeader(CandidateState state, long currentTimeMs) {\n        if (state.isVoteGranted()) {\n            onBecomeLeader(currentTimeMs);\n            return true;\n        } else {\n            return false;\n        }\n    }\n\n    private void onBecomeCandidate(long currentTimeMs) {\n        CandidateState state = quorum.candidateStateOrThrow();\n        if (!maybeTransitionToLeader(state, currentTimeMs)) {\n            resetConnections();\n            kafkaRaftMetrics.updateElectionStartMs(currentTimeMs);\n        }\n    }\n\n    private void transitionToCandidate(long currentTimeMs) {\n        quorum.transitionToCandidate();\n        maybeFireLeaderChange();\n        onBecomeCandidate(currentTimeMs);\n    }\n\n    private void transitionToUnattached(int epoch) {\n        quorum.transitionToUnattached(epoch);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void transitionToResigned(List<Integer> preferredSuccessors) {\n        fetchPurgatory.completeAllExceptionally(\n            Errors.NOT_LEADER_OR_FOLLOWER.exception(\"Not handling request since this node is resigning\"));\n        quorum.transitionToResigned(preferredSuccessors);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void transitionToVoted(ReplicaKey candidateKey, int epoch) {\n        quorum.transitionToVoted(epoch, candidateKey);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void onBecomeFollower(long currentTimeMs) {\n        kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n\n        resetConnections();\n\n        // After becoming a follower, we need to complete all pending fetches so that\n        // they can be re-sent to the leader without waiting for their expirations\n        fetchPurgatory.completeAllExceptionally(new NotLeaderOrFollowerException(\n            \"Cannot process the fetch request because the node is no longer the leader.\"));\n\n        // Clearing the append purgatory should complete all futures exceptionally since this node is no longer the leader\n        appendPurgatory.completeAllExceptionally(new NotLeaderOrFollowerException(\n            \"Failed to receive sufficient acknowledgments for this append before leader change.\"));\n    }\n\n    private void transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    ) {\n        quorum.transitionToFollower(epoch, leaderId);\n        maybeFireLeaderChange();\n        onBecomeFollower(currentTimeMs);\n    }\n\n    private VoteResponseData buildVoteResponse(Errors partitionLevelError, boolean voteGranted) {\n        return VoteResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel(),\n            voteGranted);\n    }\n\n    /**\n     * Handle a Vote request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#INVALID_REQUEST} if the last epoch or offset are invalid\n     */\n    private VoteResponseData handleVoteRequest(\n        RaftRequest.Inbound requestMetadata\n    ) {\n        VoteRequestData request = (VoteRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new VoteResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat individual topic partition mismatches as invalid requests\n            return new VoteResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        VoteRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int candidateId = partitionRequest.candidateId();\n        int candidateEpoch = partitionRequest.candidateEpoch();\n\n        int lastEpoch = partitionRequest.lastOffsetEpoch();\n        long lastEpochEndOffset = partitionRequest.lastOffset();\n        if (lastEpochEndOffset < 0 || lastEpoch < 0 || lastEpoch >= candidateEpoch) {\n            return buildVoteResponse(Errors.INVALID_REQUEST, false);\n        }\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(candidateId, candidateEpoch);\n        if (errorOpt.isPresent()) {\n            return buildVoteResponse(errorOpt.get(), false);\n        }\n\n        if (candidateEpoch > quorum.epoch()) {\n            transitionToUnattached(candidateEpoch);\n        }\n\n        OffsetAndEpoch lastEpochEndOffsetAndEpoch = new OffsetAndEpoch(lastEpochEndOffset, lastEpoch);\n        ReplicaKey candidateKey = ReplicaKey.of(candidateId, Optional.empty());\n        boolean voteGranted = quorum.canGrantVote(\n            candidateKey,\n            lastEpochEndOffsetAndEpoch.compareTo(endOffset()) >= 0\n        );\n\n        if (voteGranted && quorum.isUnattached()) {\n            transitionToVoted(candidateKey, candidateEpoch);\n        }\n\n        logger.info(\"Vote request {} with epoch {} is {}\", request, candidateEpoch, voteGranted ? \"granted\" : \"rejected\");\n        return buildVoteResponse(Errors.NONE, voteGranted);\n    }\n\n    private boolean handleVoteResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        int remoteNodeId = responseMetadata.sourceId();\n        VoteResponseData response = (VoteResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        VoteResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors error = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (error == Errors.NONE) {\n            if (quorum.isLeader()) {\n                logger.debug(\"Ignoring vote response {} since we already became leader for epoch {}\",\n                    partitionResponse, quorum.epoch());\n            } else if (quorum.isCandidate()) {\n                CandidateState state = quorum.candidateStateOrThrow();\n                if (partitionResponse.voteGranted()) {\n                    state.recordGrantedVote(remoteNodeId);\n                    maybeTransitionToLeader(state, currentTimeMs);\n                } else {\n                    state.recordRejectedVote(remoteNodeId);\n\n                    // If our vote is rejected, we go immediately to the random backoff. This\n                    // ensures that we are not stuck waiting for the election timeout when the\n                    // vote has become gridlocked.\n                    if (state.isVoteRejected() && !state.isBackingOff()) {\n                        logger.info(\"Insufficient remaining votes to become leader (rejected by {}). \" +\n                            \"We will backoff before retrying election again\", state.rejectingVoters());\n\n                        state.startBackingOff(\n                            currentTimeMs,\n                            binaryExponentialElectionBackoffMs(state.retries())\n                        );\n                    }\n                }\n            } else {\n                logger.debug(\"Ignoring vote response {} since we are no longer a candidate in epoch {}\",\n                    partitionResponse, quorum.epoch());\n            }\n            return true;\n        } else {\n            return handleUnexpectedError(error, responseMetadata);\n        }\n    }\n\n    private int binaryExponentialElectionBackoffMs(int retries) {\n        if (retries <= 0) {\n            throw new IllegalArgumentException(\"Retries \" + retries + \" should be larger than zero\");\n        }\n        // upper limit exponential co-efficients at 20 to avoid overflow\n        return Math.min(RETRY_BACKOFF_BASE_MS * random.nextInt(2 << Math.min(20, retries - 1)),\n                quorumConfig.electionBackoffMaxMs());\n    }\n\n    private int strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors) {\n        if (positionInSuccessors <= 0 || positionInSuccessors >= totalNumSuccessors) {\n            throw new IllegalArgumentException(\"Position \" + positionInSuccessors + \" should be larger than zero\" +\n                    \" and smaller than total number of successors \" + totalNumSuccessors);\n        }\n\n        int retryBackOffBaseMs = quorumConfig.electionBackoffMaxMs() >> (totalNumSuccessors - 1);\n        return Math.min(quorumConfig.electionBackoffMaxMs(), retryBackOffBaseMs << (positionInSuccessors - 1));\n    }\n\n    private BeginQuorumEpochResponseData buildBeginQuorumEpochResponse(Errors partitionLevelError) {\n        return BeginQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel());\n    }\n\n    /**\n     * Handle a BeginEpoch request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     */\n    private BeginQuorumEpochResponseData handleBeginQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        BeginQuorumEpochRequestData request = (BeginQuorumEpochRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new BeginQuorumEpochResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return new BeginQuorumEpochResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        BeginQuorumEpochRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int requestLeaderId = partitionRequest.leaderId();\n        int requestEpoch = partitionRequest.leaderEpoch();\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(requestLeaderId, requestEpoch);\n        if (errorOpt.isPresent()) {\n            return buildBeginQuorumEpochResponse(errorOpt.get());\n        }\n\n        maybeTransition(OptionalInt.of(requestLeaderId), requestEpoch, currentTimeMs);\n        return buildBeginQuorumEpochResponse(Errors.NONE);\n    }\n\n    private boolean handleBeginQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        int remoteNodeId = responseMetadata.sourceId();\n        BeginQuorumEpochResponseData response = (BeginQuorumEpochResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        BeginQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors partitionError = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            partitionError, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (partitionError == Errors.NONE) {\n            if (quorum.isLeader()) {\n                LeaderState<T> state = quorum.leaderStateOrThrow();\n                state.addAcknowledgementFrom(remoteNodeId);\n            } else {\n                logger.debug(\"Ignoring BeginQuorumEpoch response {} since \" +\n                    \"this node is not the leader anymore\", response);\n            }\n            return true;\n        } else {\n            return handleUnexpectedError(partitionError, responseMetadata);\n        }\n    }\n\n    private EndQuorumEpochResponseData buildEndQuorumEpochResponse(Errors partitionLevelError) {\n        return EndQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel());\n    }\n\n    /**\n     * Handle an EndEpoch request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     */\n    private EndQuorumEpochResponseData handleEndQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        EndQuorumEpochRequestData request = (EndQuorumEpochRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new EndQuorumEpochResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return new EndQuorumEpochResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        EndQuorumEpochRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int requestEpoch = partitionRequest.leaderEpoch();\n        int requestLeaderId = partitionRequest.leaderId();\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(requestLeaderId, requestEpoch);\n        if (errorOpt.isPresent()) {\n            return buildEndQuorumEpochResponse(errorOpt.get());\n        }\n        maybeTransition(OptionalInt.of(requestLeaderId), requestEpoch, currentTimeMs);\n\n        if (quorum.isFollower()) {\n            FollowerState state = quorum.followerStateOrThrow();\n            if (state.leaderId() == requestLeaderId) {\n                List<Integer> preferredSuccessors = partitionRequest.preferredSuccessors();\n                long electionBackoffMs = endEpochElectionBackoff(preferredSuccessors);\n                logger.debug(\"Overriding follower fetch timeout to {} after receiving \" +\n                    \"EndQuorumEpoch request from leader {} in epoch {}\", electionBackoffMs,\n                    requestLeaderId, requestEpoch);\n                state.overrideFetchTimeout(currentTimeMs, electionBackoffMs);\n            }\n        }\n        return buildEndQuorumEpochResponse(Errors.NONE);\n    }\n\n    private long endEpochElectionBackoff(List<Integer> preferredSuccessors) {\n        // Based on the priority inside the preferred successors, choose the corresponding delayed\n        // election backoff time based on strict exponential mechanism so that the most up-to-date\n        // voter has a higher chance to be elected. If the node's priority is highest, become\n        // candidate immediately instead of waiting for next poll.\n        int position = preferredSuccessors.indexOf(quorum.localIdOrThrow());\n        if (position <= 0) {\n            return 0;\n        } else {\n            return strictExponentialElectionBackoffMs(position, preferredSuccessors.size());\n        }\n    }\n\n    private boolean handleEndQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        EndQuorumEpochResponseData response = (EndQuorumEpochResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        EndQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors partitionError = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            partitionError, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (partitionError == Errors.NONE) {\n            ResignedState resignedState = quorum.resignedStateOrThrow();\n            resignedState.acknowledgeResignation(responseMetadata.sourceId());\n            return true;\n        } else {\n            return handleUnexpectedError(partitionError, responseMetadata);\n        }\n    }\n\n    private FetchResponseData buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    ) {\n        return RaftUtil.singletonFetchResponse(log.topicPartition(), log.topicId(), Errors.NONE, partitionData -> {\n            partitionData\n                .setRecords(records)\n                .setErrorCode(error.code())\n                .setLogStartOffset(log.startOffset())\n                .setHighWatermark(\n                    highWatermark.map(offsetMetadata -> offsetMetadata.offset).orElse(-1L)\n                );\n\n            partitionData.currentLeader()\n                .setLeaderEpoch(quorum.epoch())\n                .setLeaderId(quorum.leaderIdOrSentinel());\n\n            switch (validOffsetAndEpoch.kind()) {\n                case DIVERGING:\n                    partitionData.divergingEpoch()\n                        .setEpoch(validOffsetAndEpoch.offsetAndEpoch().epoch())\n                        .setEndOffset(validOffsetAndEpoch.offsetAndEpoch().offset());\n                    break;\n                case SNAPSHOT:\n                    partitionData.snapshotId()\n                        .setEpoch(validOffsetAndEpoch.offsetAndEpoch().epoch())\n                        .setEndOffset(validOffsetAndEpoch.offsetAndEpoch().offset());\n                    break;\n                default:\n            }\n        });\n    }\n\n    private FetchResponseData buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    ) {\n        return buildFetchResponse(\n            error,\n            MemoryRecords.EMPTY,\n            ValidOffsetAndEpoch.valid(),\n            highWatermark\n        );\n    }\n\n    private boolean hasValidClusterId(String requestClusterId) {\n        // We don't enforce the cluster id if it is not provided.\n        if (requestClusterId == null) {\n            return true;\n        }\n        return clusterId.equals(requestClusterId);\n    }\n\n    /**\n     * Handle a Fetch request. The fetch offset and last fetched epoch are always\n     * validated against the current log. In the case that they do not match, the response will\n     * indicate the diverging offset/epoch. A follower is expected to truncate its log in this\n     * case and resend the fetch.\n     *\n     * This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *     but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INVALID_REQUEST} if the request epoch is larger than the leader's current epoch\n     *     or if either the fetch offset or the last fetched epoch is invalid\n     */\n    private CompletableFuture<FetchResponseData> handleFetchRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        FetchRequestData request = (FetchRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return completedFuture(new FetchResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code()));\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition(), log.topicId())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return completedFuture(new FetchResponseData().setErrorCode(Errors.INVALID_REQUEST.code()));\n        }\n        // If the ID is valid, we can set the topic name.\n        request.topics().get(0).setTopic(log.topicPartition().topic());\n\n        FetchRequestData.FetchPartition fetchPartition = request.topics().get(0).partitions().get(0);\n        if (request.maxWaitMs() < 0\n            || fetchPartition.fetchOffset() < 0\n            || fetchPartition.lastFetchedEpoch() < 0\n            || fetchPartition.lastFetchedEpoch() > fetchPartition.currentLeaderEpoch()) {\n            return completedFuture(\n                buildEmptyFetchResponse(Errors.INVALID_REQUEST, Optional.empty())\n            );\n        }\n\n        int replicaId = FetchRequest.replicaId(request);\n        FetchResponseData response = tryCompleteFetchRequest(replicaId, fetchPartition, currentTimeMs);\n        FetchResponseData.PartitionData partitionResponse =\n            response.responses().get(0).partitions().get(0);\n\n        if (partitionResponse.errorCode() != Errors.NONE.code()\n            || FetchResponse.recordsSize(partitionResponse) > 0\n            || request.maxWaitMs() == 0\n            || isPartitionDiverged(partitionResponse)\n            || isPartitionSnapshotted(partitionResponse)) {\n            // Reply immediately if any of the following is true\n            // 1. The response contains an error\n            // 2. There are records in the response\n            // 3. The fetching replica doesn't want to wait for the partition to contain new data\n            // 4. The fetching replica needs to truncate because the log diverged\n            // 5. The fetching replica needs to fetch a snapshot\n            return completedFuture(response);\n        }\n\n        CompletableFuture<Long> future = fetchPurgatory.await(\n            fetchPartition.fetchOffset(),\n            request.maxWaitMs());\n\n        return future.handle((completionTimeMs, exception) -> {\n            if (exception != null) {\n                Throwable cause = exception instanceof ExecutionException ?\n                    exception.getCause() : exception;\n\n                Errors error = Errors.forException(cause);\n                if (error == Errors.REQUEST_TIMED_OUT) {\n                    // Note that for this case the calling thread is the expiration service thread and not the\n                    // polling thread.\n                    //\n                    // If the fetch request timed out in purgatory, it means no new data is available,\n                    // just return the original fetch response.\n                    return response;\n                } else {\n                    // If there was any error other than REQUEST_TIMED_OUT, return it.\n                    logger.info(\"Failed to handle fetch from {} at {} due to {}\",\n                        replicaId, fetchPartition.fetchOffset(), error);\n                    return buildEmptyFetchResponse(error, Optional.empty());\n                }\n            }\n\n            // FIXME: `completionTimeMs`, which can be null\n            logger.trace(\"Completing delayed fetch from {} starting at offset {} at {}\",\n                replicaId, fetchPartition.fetchOffset(), completionTimeMs);\n\n            // It is safe to call tryCompleteFetchRequest because only the polling thread completes this\n            // future successfully. This is true because only the polling thread appends record batches to\n            // the log from maybeAppendBatches.\n            return tryCompleteFetchRequest(replicaId, fetchPartition, time.milliseconds());\n        });\n    }\n\n    private FetchResponseData tryCompleteFetchRequest(\n        int replicaId,\n        FetchRequestData.FetchPartition request,\n        long currentTimeMs\n    ) {\n        try {\n            Optional<Errors> errorOpt = validateLeaderOnlyRequest(request.currentLeaderEpoch());\n            if (errorOpt.isPresent()) {\n                return buildEmptyFetchResponse(errorOpt.get(), Optional.empty());\n            }\n\n            long fetchOffset = request.fetchOffset();\n            int lastFetchedEpoch = request.lastFetchedEpoch();\n            LeaderState<T> state = quorum.leaderStateOrThrow();\n\n            Optional<OffsetAndEpoch> latestSnapshotId = log.latestSnapshotId();\n            final ValidOffsetAndEpoch validOffsetAndEpoch;\n            if (fetchOffset == 0 && latestSnapshotId.isPresent()) {\n                // If the follower has an empty log and a snapshot exist, it is always more efficient\n                // to reply with a snapshot id (FETCH_SNAPSHOT) instead of fetching from the log segments.\n                validOffsetAndEpoch = ValidOffsetAndEpoch.snapshot(latestSnapshotId.get());\n            } else {\n                validOffsetAndEpoch = log.validateOffsetAndEpoch(fetchOffset, lastFetchedEpoch);\n            }\n\n            final Records records;\n            if (validOffsetAndEpoch.kind() == ValidOffsetAndEpoch.Kind.VALID) {\n                LogFetchInfo info = log.read(fetchOffset, Isolation.UNCOMMITTED);\n\n                if (state.updateReplicaState(replicaId, currentTimeMs, info.startOffsetMetadata)) {\n                    onUpdateLeaderHighWatermark(state, currentTimeMs);\n                }\n\n                records = info.records;\n            } else {\n                records = MemoryRecords.EMPTY;\n            }\n\n            return buildFetchResponse(Errors.NONE, records, validOffsetAndEpoch, state.highWatermark());\n        } catch (Exception e) {\n            logger.error(\"Caught unexpected error in fetch completion of request {}\", request, e);\n            return buildEmptyFetchResponse(Errors.UNKNOWN_SERVER_ERROR, Optional.empty());\n        }\n    }\n\n    private static boolean isPartitionDiverged(FetchResponseData.PartitionData partitionResponseData) {\n        FetchResponseData.EpochEndOffset divergingEpoch = partitionResponseData.divergingEpoch();\n\n        return divergingEpoch.epoch() != -1 || divergingEpoch.endOffset() != -1;\n    }\n\n    private static boolean isPartitionSnapshotted(FetchResponseData.PartitionData partitionResponseData) {\n        FetchResponseData.SnapshotId snapshotId = partitionResponseData.snapshotId();\n\n        return snapshotId.epoch() != -1 || snapshotId.endOffset() != -1;\n    }\n\n    private static OptionalInt optionalLeaderId(int leaderIdOrNil) {\n        if (leaderIdOrNil < 0)\n            return OptionalInt.empty();\n        return OptionalInt.of(leaderIdOrNil);\n    }\n\n    private static String listenerName(Listener<?> listener) {\n        return String.format(\"%s@%d\", listener.getClass().getTypeName(), System.identityHashCode(listener));\n    }\n\n    private boolean handleFetchResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        FetchResponseData response = (FetchResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!RaftUtil.hasValidTopicPartition(response, log.topicPartition(), log.topicId())) {\n            return false;\n        }\n        // If the ID is valid, we can set the topic name.\n        response.responses().get(0).setTopic(log.topicPartition().topic());\n\n        FetchResponseData.PartitionData partitionResponse =\n            response.responses().get(0).partitions().get(0);\n\n        FetchResponseData.LeaderIdAndEpoch currentLeaderIdAndEpoch = partitionResponse.currentLeader();\n        OptionalInt responseLeaderId = optionalLeaderId(currentLeaderIdAndEpoch.leaderId());\n        int responseEpoch = currentLeaderIdAndEpoch.leaderEpoch();\n        Errors error = Errors.forCode(partitionResponse.errorCode());\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        }\n\n        FollowerState state = quorum.followerStateOrThrow();\n        if (error == Errors.NONE) {\n            FetchResponseData.EpochEndOffset divergingEpoch = partitionResponse.divergingEpoch();\n            if (divergingEpoch.epoch() >= 0) {\n                // The leader is asking us to truncate before continuing\n                final OffsetAndEpoch divergingOffsetAndEpoch = new OffsetAndEpoch(\n                    divergingEpoch.endOffset(), divergingEpoch.epoch());\n\n                state.highWatermark().ifPresent(highWatermark -> {\n                    if (divergingOffsetAndEpoch.offset() < highWatermark.offset) {\n                        throw new KafkaException(\"The leader requested truncation to offset \" +\n                            divergingOffsetAndEpoch.offset() + \", which is below the current high watermark\" +\n                            \" \" + highWatermark);\n                    }\n                });\n\n                long truncationOffset = log.truncateToEndOffset(divergingOffsetAndEpoch);\n                logger.info(\n                    \"Truncated to offset {} from Fetch response from leader {}\",\n                    truncationOffset,\n                    quorum.leaderIdOrSentinel()\n                );\n\n                // Update the internal listener to the new end offset\n                partitionState.truncateNewEntries(truncationOffset);\n            } else if (partitionResponse.snapshotId().epoch() >= 0 ||\n                       partitionResponse.snapshotId().endOffset() >= 0) {\n                // The leader is asking us to fetch a snapshot\n\n                if (partitionResponse.snapshotId().epoch() < 0) {\n                    logger.error(\n                        \"The leader sent a snapshot id with a valid end offset {} but with an invalid epoch {}\",\n                        partitionResponse.snapshotId().endOffset(),\n                        partitionResponse.snapshotId().epoch()\n                    );\n                    return false;\n                } else if (partitionResponse.snapshotId().endOffset() < 0) {\n                    logger.error(\n                        \"The leader sent a snapshot id with a valid epoch {} but with an invalid end offset {}\",\n                        partitionResponse.snapshotId().epoch(),\n                        partitionResponse.snapshotId().endOffset()\n                    );\n                    return false;\n                } else {\n                    final OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n                        partitionResponse.snapshotId().endOffset(),\n                        partitionResponse.snapshotId().epoch()\n                    );\n\n                    // Do not validate the snapshot id against the local replicated log since this\n                    // snapshot is expected to reference offsets and epochs greater than the log\n                    // end offset and high-watermark.\n                    state.setFetchingSnapshot(log.createNewSnapshotUnchecked(snapshotId));\n                    logger.info(\n                        \"Fetching snapshot {} from Fetch response from leader {}\",\n                        snapshotId,\n                        quorum.leaderIdOrSentinel()\n                    );\n                }\n            } else {\n                Records records = FetchResponse.recordsOrFail(partitionResponse);\n                if (records.sizeInBytes() > 0) {\n                    appendAsFollower(records);\n                }\n\n                OptionalLong highWatermark = partitionResponse.highWatermark() < 0 ?\n                    OptionalLong.empty() : OptionalLong.of(partitionResponse.highWatermark());\n                updateFollowerHighWatermark(state, highWatermark);\n            }\n\n            state.resetFetchTimeout(currentTimeMs);\n            return true;\n        } else {\n            return handleUnexpectedError(error, responseMetadata);\n        }\n    }\n\n    private void appendAsFollower(\n        Records records\n    ) {\n        LogAppendInfo info = log.appendAsFollower(records);\n        if (quorum.isVoter()) {\n            // the leader only requires that voters have flushed their log before sending\n            // a Fetch request\n            log.flush(false);\n        }\n\n        partitionState.updateState();\n\n        OffsetAndEpoch endOffset = endOffset();\n        kafkaRaftMetrics.updateFetchedRecords(info.lastOffset - info.firstOffset + 1);\n        kafkaRaftMetrics.updateLogEnd(endOffset);\n        logger.trace(\"Follower end offset updated to {} after append\", endOffset);\n    }\n\n    private LogAppendInfo appendAsLeader(\n        Records records\n    ) {\n        LogAppendInfo info = log.appendAsLeader(records, quorum.epoch());\n\n        partitionState.updateState();\n\n        OffsetAndEpoch endOffset = endOffset();\n        kafkaRaftMetrics.updateAppendRecords(info.lastOffset - info.firstOffset + 1);\n        kafkaRaftMetrics.updateLogEnd(endOffset);\n        logger.trace(\"Leader appended records at base offset {}, new end offset is {}\", info.firstOffset, endOffset);\n        return info;\n    }\n\n    private DescribeQuorumResponseData handleDescribeQuorumRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        DescribeQuorumRequestData describeQuorumRequestData = (DescribeQuorumRequestData) requestMetadata.data;\n        if (!hasValidTopicPartition(describeQuorumRequestData, log.topicPartition())) {\n            return DescribeQuorumRequest.getPartitionLevelErrorResponse(\n                describeQuorumRequestData, Errors.UNKNOWN_TOPIC_OR_PARTITION);\n        }\n\n        if (!quorum.isLeader()) {\n            return DescribeQuorumResponse.singletonErrorResponse(\n                log.topicPartition(),\n                Errors.NOT_LEADER_OR_FOLLOWER\n            );\n        }\n\n        LeaderState<T> leaderState = quorum.leaderStateOrThrow();\n        return DescribeQuorumResponse.singletonResponse(\n            log.topicPartition(),\n            leaderState.describeQuorum(currentTimeMs)\n        );\n    }\n\n    /**\n     * Handle a FetchSnapshot request, similar to the Fetch request but we use {@link UnalignedRecords}\n     * in response because the records are not necessarily offset-aligned.\n     *\n     * This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *     but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INVALID_REQUEST} if the request epoch is larger than the leader's current epoch\n     *     or if either the fetch offset or the last fetched epoch is invalid\n     * - {@link Errors#SNAPSHOT_NOT_FOUND} if the request snapshot id does not exists\n     * - {@link Errors#POSITION_OUT_OF_RANGE} if the request snapshot offset out of range\n     */\n    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(data.clusterId())) {\n            return new FetchSnapshotResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n            return FetchSnapshotResponse.withTopLevelError(Errors.INVALID_REQUEST);\n        }\n\n        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n            .forTopicPartition(data, log.topicPartition());\n        if (!partitionSnapshotOpt.isPresent()) {\n            // The Raft client assumes that there is only one topic partition.\n            TopicPartition unknownTopicPartition = new TopicPartition(\n                data.topics().get(0).name(),\n                data.topics().get(0).partitions().get(0).partition()\n            );\n\n            return FetchSnapshotResponse.singleton(\n                unknownTopicPartition,\n                responsePartitionSnapshot -> responsePartitionSnapshot\n                    .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code())\n            );\n        }\n\n        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n                partitionSnapshot.currentLeaderEpoch()\n        );\n        if (leaderValidation.isPresent()) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(leaderValidation.get().code())\n            );\n        }\n\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n            partitionSnapshot.snapshotId().endOffset(),\n            partitionSnapshot.snapshotId().epoch()\n        );\n        Optional<RawSnapshotReader> snapshotOpt = log.readSnapshot(snapshotId);\n        if (!snapshotOpt.isPresent()) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code())\n            );\n        }\n\n        RawSnapshotReader snapshot = snapshotOpt.get();\n        long snapshotSize = snapshot.sizeInBytes();\n        if (partitionSnapshot.position() < 0 || partitionSnapshot.position() >= snapshotSize) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(Errors.POSITION_OUT_OF_RANGE.code())\n            );\n        }\n\n        if (partitionSnapshot.position() > Integer.MAX_VALUE) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Trying to fetch a snapshot with size (%d) and a position (%d) larger than %d\",\n                    snapshotSize,\n                    partitionSnapshot.position(),\n                    Integer.MAX_VALUE\n                )\n            );\n        }\n\n        int maxSnapshotSize;\n        try {\n            maxSnapshotSize = Math.toIntExact(snapshotSize);\n        } catch (ArithmeticException e) {\n            maxSnapshotSize = Integer.MAX_VALUE;\n        }\n\n        UnalignedRecords records = snapshot.slice(partitionSnapshot.position(), Math.min(data.maxBytes(), maxSnapshotSize));\n\n        LeaderState<T> state = quorum.leaderStateOrThrow();\n        state.updateCheckQuorumForFollowingVoter(data.replicaId(), currentTimeMs);\n\n        return FetchSnapshotResponse.singleton(\n            log.topicPartition(),\n            responsePartitionSnapshot -> {\n                addQuorumLeader(responsePartitionSnapshot)\n                    .snapshotId()\n                    .setEndOffset(snapshotId.offset())\n                    .setEpoch(snapshotId.epoch());\n\n                return responsePartitionSnapshot\n                    .setSize(snapshotSize)\n                    .setPosition(partitionSnapshot.position())\n                    .setUnalignedRecords(records);\n            }\n        );\n    }\n\n    private boolean handleFetchSnapshotResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        FetchSnapshotResponseData data = (FetchSnapshotResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(data.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n            return false;\n        }\n\n        Optional<FetchSnapshotResponseData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotResponse\n            .forTopicPartition(data, log.topicPartition());\n        if (!partitionSnapshotOpt.isPresent()) {\n            return false;\n        }\n\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n\n        FetchSnapshotResponseData.LeaderIdAndEpoch currentLeaderIdAndEpoch = partitionSnapshot.currentLeader();\n        OptionalInt responseLeaderId = optionalLeaderId(currentLeaderIdAndEpoch.leaderId());\n        int responseEpoch = currentLeaderIdAndEpoch.leaderEpoch();\n        Errors error = Errors.forCode(partitionSnapshot.errorCode());\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        }\n\n        FollowerState state = quorum.followerStateOrThrow();\n\n        if (Errors.forCode(partitionSnapshot.errorCode()) == Errors.SNAPSHOT_NOT_FOUND ||\n            partitionSnapshot.snapshotId().endOffset() < 0 ||\n            partitionSnapshot.snapshotId().epoch() < 0) {\n\n            /* The leader deleted the snapshot before the follower could download it. Start over by\n             * resetting the fetching snapshot state and sending another fetch request.\n             */\n            logger.info(\n                \"Leader doesn't know about snapshot id {}, returned error {} and snapshot id {}\",\n                state.fetchingSnapshot(),\n                partitionSnapshot.errorCode(),\n                partitionSnapshot.snapshotId()\n            );\n            state.setFetchingSnapshot(Optional.empty());\n            state.resetFetchTimeout(currentTimeMs);\n            return true;\n        }\n\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n            partitionSnapshot.snapshotId().endOffset(),\n            partitionSnapshot.snapshotId().epoch()\n        );\n\n        RawSnapshotWriter snapshot;\n        if (state.fetchingSnapshot().isPresent()) {\n            snapshot = state.fetchingSnapshot().get();\n        } else {\n            throw new IllegalStateException(\n                String.format(\"Received unexpected fetch snapshot response: %s\", partitionSnapshot)\n            );\n        }\n\n        if (!snapshot.snapshotId().equals(snapshotId)) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Received fetch snapshot response with an invalid id. Expected %s; Received %s\",\n                    snapshot.snapshotId(),\n                    snapshotId\n                )\n            );\n        }\n        if (snapshot.sizeInBytes() != partitionSnapshot.position()) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Received fetch snapshot response with an invalid position. Expected %d; Received %d\",\n                    snapshot.sizeInBytes(),\n                    partitionSnapshot.position()\n                )\n            );\n        }\n\n        final UnalignedMemoryRecords records;\n        if (partitionSnapshot.unalignedRecords() instanceof MemoryRecords) {\n            records = new UnalignedMemoryRecords(((MemoryRecords) partitionSnapshot.unalignedRecords()).buffer());\n        } else if (partitionSnapshot.unalignedRecords() instanceof UnalignedMemoryRecords) {\n            records = (UnalignedMemoryRecords) partitionSnapshot.unalignedRecords();\n        } else {\n            throw new IllegalStateException(String.format(\"Received unexpected fetch snapshot response: %s\", partitionSnapshot));\n        }\n        snapshot.append(records);\n\n        if (snapshot.sizeInBytes() == partitionSnapshot.size()) {\n            // Finished fetching the snapshot.\n            snapshot.freeze();\n            state.setFetchingSnapshot(Optional.empty());\n\n            if (log.truncateToLatestSnapshot()) {\n                logger.info(\n                    \"Fully truncated the log at ({}, {}) after downloading snapshot {} from leader {}\",\n                    log.endOffset(),\n                    log.lastFetchedEpoch(),\n                    snapshot.snapshotId(),\n                    quorum.leaderIdOrSentinel()\n                );\n\n                // This will always reload the snapshot because the internal next offset\n                // is always less than the snapshot id just downloaded.\n                partitionState.updateState();\n\n                updateFollowerHighWatermark(state, OptionalLong.of(log.highWatermark().offset));\n            } else {\n                throw new IllegalStateException(\n                    String.format(\n                        \"Full log truncation expected but didn't happen. Snapshot of %s, log end offset %s, last fetched %d\",\n                        snapshot.snapshotId(),\n                        log.endOffset(),\n                        log.lastFetchedEpoch()\n                    )\n                );\n            }\n        }\n\n        state.resetFetchTimeout(currentTimeMs);\n        return true;\n    }\n\n    private boolean hasConsistentLeader(int epoch, OptionalInt leaderId) {\n        // Only elected leaders are sent in the request/response header, so if we have an elected\n        // leaderId, it should be consistent with what is in the message.\n        if (leaderId.isPresent() && leaderId.getAsInt() == quorum.localIdOrSentinel()) {\n            // The response indicates that we should be the leader, so we verify that is the case\n            return quorum.isLeader();\n        } else {\n            return epoch != quorum.epoch()\n                || !leaderId.isPresent()\n                || !quorum.leaderId().isPresent()\n                || leaderId.equals(quorum.leaderId());\n        }\n    }\n\n    /**\n     * Handle response errors that are common across request types.\n     *\n     * @param error Error from the received response\n     * @param leaderId Optional leaderId from the response\n     * @param epoch Epoch received from the response\n     * @param currentTimeMs Current epoch time in milliseconds\n     * @return Optional value indicating whether the error was handled here and the outcome of\n     *    that handling. Specifically:\n     *\n     *    - Optional.empty means that the response was not handled here and the custom\n     *        API handler should be applied\n     *    - Optional.of(true) indicates that the response was successfully handled here and\n     *        the request does not need to be retried\n     *    - Optional.of(false) indicates that the response was handled here, but that the request\n     *        will need to be retried\n     */\n    private Optional<Boolean> maybeHandleCommonResponse(\n        Errors error,\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    ) {\n        if (epoch < quorum.epoch() || error == Errors.UNKNOWN_LEADER_EPOCH) {\n            // We have a larger epoch, so the response is no longer relevant\n            return Optional.of(true);\n        } else if (epoch > quorum.epoch()\n            || error == Errors.FENCED_LEADER_EPOCH\n            || error == Errors.NOT_LEADER_OR_FOLLOWER) {\n\n            // The response indicates that the request had a stale epoch, but we need\n            // to validate the epoch from the response against our current state.\n            maybeTransition(leaderId, epoch, currentTimeMs);\n            return Optional.of(true);\n        } else if (epoch == quorum.epoch()\n            && leaderId.isPresent()\n            && !quorum.hasLeader()) {\n\n            // Since we are transitioning to Follower, we will only forward the\n            // request to the handler if there is no error. Otherwise, we will let\n            // the request be retried immediately (if needed) after the transition.\n            // This handling allows an observer to discover the leader and append\n            // to the log in the same Fetch request.\n            transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n            if (error == Errors.NONE) {\n                return Optional.empty();\n            } else {\n                return Optional.of(true);\n            }\n        } else if (error == Errors.BROKER_NOT_AVAILABLE) {\n            return Optional.of(false);\n        } else if (error == Errors.INCONSISTENT_GROUP_PROTOCOL) {\n            // For now we treat this as a fatal error. Once we have support for quorum\n            // reassignment, this error could suggest that either we or the recipient of\n            // the request just has stale voter information, which means we can retry\n            // after backing off.\n            throw new IllegalStateException(\"Received error indicating inconsistent voter sets\");\n        } else if (error == Errors.INVALID_REQUEST) {\n            throw new IllegalStateException(\"Received unexpected invalid request error\");\n        }\n\n        return Optional.empty();\n    }\n\n    private void maybeTransition(\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    ) {\n        if (!hasConsistentLeader(epoch, leaderId)) {\n            throw new IllegalStateException(\"Received request or response with leader \" + leaderId +\n                \" and epoch \" + epoch + \" which is inconsistent with current leader \" +\n                quorum.leaderId() + \" and epoch \" + quorum.epoch());\n        } else if (epoch > quorum.epoch()) {\n            if (leaderId.isPresent()) {\n                transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n            } else {\n                transitionToUnattached(epoch);\n            }\n        } else if (leaderId.isPresent() && !quorum.hasLeader()) {\n            // The request or response indicates the leader of the current epoch,\n            // which is currently unknown\n            transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n        }\n    }\n\n    private boolean handleTopLevelError(Errors error, RaftResponse.Inbound response) {\n        if (error == Errors.BROKER_NOT_AVAILABLE) {\n            return false;\n        } else if (error == Errors.CLUSTER_AUTHORIZATION_FAILED) {\n            throw new ClusterAuthorizationException(\"Received cluster authorization error in response \" + response);\n        } else {\n            return handleUnexpectedError(error, response);\n        }\n    }\n\n    private boolean handleUnexpectedError(Errors error, RaftResponse.Inbound response) {\n        logger.error(\"Unexpected error {} in {} response: {}\",\n            error, ApiKeys.forId(response.data.apiKey()), response);\n        return false;\n    }\n\n    private void handleResponse(RaftResponse.Inbound response, long currentTimeMs) {\n        // The response epoch matches the local epoch, so we can handle the response\n        ApiKeys apiKey = ApiKeys.forId(response.data.apiKey());\n        final boolean handledSuccessfully;\n\n        switch (apiKey) {\n            case FETCH:\n                handledSuccessfully = handleFetchResponse(response, currentTimeMs);\n                break;\n\n            case VOTE:\n                handledSuccessfully = handleVoteResponse(response, currentTimeMs);\n                break;\n\n            case BEGIN_QUORUM_EPOCH:\n                handledSuccessfully = handleBeginQuorumEpochResponse(response, currentTimeMs);\n                break;\n\n            case END_QUORUM_EPOCH:\n                handledSuccessfully = handleEndQuorumEpochResponse(response, currentTimeMs);\n                break;\n\n            case FETCH_SNAPSHOT:\n                handledSuccessfully = handleFetchSnapshotResponse(response, currentTimeMs);\n                break;\n\n            default:\n                throw new IllegalArgumentException(\"Received unexpected response type: \" + apiKey);\n        }\n\n        ConnectionState connection = requestManager.getOrCreate(response.sourceId());\n        if (handledSuccessfully) {\n            connection.onResponseReceived(response.correlationId);\n        } else {\n            connection.onResponseError(response.correlationId, currentTimeMs);\n        }\n    }\n\n    /**\n     * Validate common state for requests to establish leadership.\n     *\n     * These include the Vote, BeginQuorumEpoch and EndQuorumEpoch RPCs. If an error is present in\n     * the returned value, it should be returned in the response.\n     */\n    private Optional<Errors> validateVoterOnlyRequest(int remoteNodeId, int requestEpoch) {\n        if (requestEpoch < quorum.epoch()) {\n            return Optional.of(Errors.FENCED_LEADER_EPOCH);\n        } else if (remoteNodeId < 0) {\n            return Optional.of(Errors.INVALID_REQUEST);\n        } else {\n            return Optional.empty();\n        }\n    }\n\n    /**\n     * Validate a request which is intended for the current quorum leader.\n     * If an error is present in the returned value, it should be returned\n     * in the response.\n     */\n    private Optional<Errors> validateLeaderOnlyRequest(int requestEpoch) {\n        if (requestEpoch < quorum.epoch()) {\n            return Optional.of(Errors.FENCED_LEADER_EPOCH);\n        } else if (requestEpoch > quorum.epoch()) {\n            return Optional.of(Errors.UNKNOWN_LEADER_EPOCH);\n        } else if (!quorum.isLeader()) {\n            // In general, non-leaders do not expect to receive requests\n            // matching their own epoch, but it is possible when observers\n            // are using the Fetch API to find the result of an election.\n            return Optional.of(Errors.NOT_LEADER_OR_FOLLOWER);\n        } else if (shutdown.get() != null) {\n            return Optional.of(Errors.BROKER_NOT_AVAILABLE);\n        } else {\n            return Optional.empty();\n        }\n    }\n\n    private void handleRequest(RaftRequest.Inbound request, long currentTimeMs) {\n        ApiKeys apiKey = ApiKeys.forId(request.data.apiKey());\n        final CompletableFuture<? extends ApiMessage> responseFuture;\n\n        switch (apiKey) {\n            case FETCH:\n                responseFuture = handleFetchRequest(request, currentTimeMs);\n                break;\n\n            case VOTE:\n                responseFuture = completedFuture(handleVoteRequest(request));\n                break;\n\n            case BEGIN_QUORUM_EPOCH:\n                responseFuture = completedFuture(handleBeginQuorumEpochRequest(request, currentTimeMs));\n                break;\n\n            case END_QUORUM_EPOCH:\n                responseFuture = completedFuture(handleEndQuorumEpochRequest(request, currentTimeMs));\n                break;\n\n            case DESCRIBE_QUORUM:\n                responseFuture = completedFuture(handleDescribeQuorumRequest(request, currentTimeMs));\n                break;\n\n            case FETCH_SNAPSHOT:\n                responseFuture = completedFuture(handleFetchSnapshotRequest(request, currentTimeMs));\n                break;\n\n            default:\n                throw new IllegalArgumentException(\"Unexpected request type \" + apiKey);\n        }\n\n        responseFuture.whenComplete((response, exception) -> {\n            final ApiMessage message;\n            if (response != null) {\n                message = response;\n            } else {\n                message = RaftUtil.errorResponse(apiKey, Errors.forException(exception));\n            }\n\n            RaftResponse.Outbound responseMessage = new RaftResponse.Outbound(request.correlationId(), message);\n            request.completion.complete(responseMessage);\n            logger.trace(\"Sent response {} to inbound request {}\", responseMessage, request);\n        });\n    }\n\n    private void handleInboundMessage(RaftMessage message, long currentTimeMs) {\n        logger.trace(\"Received inbound message {}\", message);\n\n        if (message instanceof RaftRequest.Inbound) {\n            RaftRequest.Inbound request = (RaftRequest.Inbound) message;\n            handleRequest(request, currentTimeMs);\n        } else if (message instanceof RaftResponse.Inbound) {\n            RaftResponse.Inbound response = (RaftResponse.Inbound) message;\n            ConnectionState connection = requestManager.getOrCreate(response.sourceId());\n            if (connection.isResponseExpected(response.correlationId)) {\n                handleResponse(response, currentTimeMs);\n            } else {\n                logger.debug(\"Ignoring response {} since it is no longer needed\", response);\n            }\n        } else {\n            throw new IllegalArgumentException(\"Unexpected message \" + message);\n        }\n    }\n\n    /**\n     * Attempt to send a request. Return the time to wait before the request can be retried.\n     */\n    private long maybeSendRequest(\n        long currentTimeMs,\n        int destinationId,\n        Supplier<ApiMessage> requestSupplier\n    )  {\n        ConnectionState connection = requestManager.getOrCreate(destinationId);\n\n        if (connection.isBackingOff(currentTimeMs)) {\n            long remainingBackoffMs = connection.remainingBackoffMs(currentTimeMs);\n            logger.debug(\"Connection for {} is backing off for {} ms\", destinationId, remainingBackoffMs);\n            return remainingBackoffMs;\n        }\n\n        if (connection.isReady(currentTimeMs)) {\n            int correlationId = channel.newCorrelationId();\n            ApiMessage request = requestSupplier.get();\n\n            RaftRequest.Outbound requestMessage = new RaftRequest.Outbound(\n                correlationId,\n                request,\n                destinationId,\n                currentTimeMs\n            );\n\n            requestMessage.completion.whenComplete((response, exception) -> {\n                if (exception != null) {\n                    ApiKeys api = ApiKeys.forId(request.apiKey());\n                    Errors error = Errors.forException(exception);\n                    ApiMessage errorResponse = RaftUtil.errorResponse(api, error);\n\n                    response = new RaftResponse.Inbound(\n                        correlationId,\n                        errorResponse,\n                        destinationId\n                    );\n                }\n\n                messageQueue.add(response);\n            });\n\n            channel.send(requestMessage);\n            logger.trace(\"Sent outbound request: {}\", requestMessage);\n            connection.onRequestSent(correlationId, currentTimeMs);\n            return Long.MAX_VALUE;\n        }\n\n        return connection.remainingRequestTimeMs(currentTimeMs);\n    }\n\n    private EndQuorumEpochRequestData buildEndQuorumEpochRequest(\n        ResignedState state\n    ) {\n        return EndQuorumEpochRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow(),\n            state.preferredSuccessors()\n        );\n    }\n\n    private long maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    ) {\n        long minBackoffMs = Long.MAX_VALUE;\n        for (Integer destinationId : destinationIds) {\n            long backoffMs = maybeSendRequest(currentTimeMs, destinationId, requestSupplier);\n            if (backoffMs < minBackoffMs) {\n                minBackoffMs = backoffMs;\n            }\n        }\n        return minBackoffMs;\n    }\n\n    private BeginQuorumEpochRequestData buildBeginQuorumEpochRequest() {\n        return BeginQuorumEpochRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow()\n        );\n    }\n\n    private VoteRequestData buildVoteRequest() {\n        OffsetAndEpoch endOffset = endOffset();\n        return VoteRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow(),\n            endOffset.epoch(),\n            endOffset.offset()\n        );\n    }\n\n    private FetchRequestData buildFetchRequest() {\n        FetchRequestData request = RaftUtil.singletonFetchRequest(log.topicPartition(), log.topicId(), fetchPartition -> {\n            fetchPartition\n                .setCurrentLeaderEpoch(quorum.epoch())\n                .setLastFetchedEpoch(log.lastFetchedEpoch())\n                .setFetchOffset(log.endOffset().offset);\n        });\n        return request\n            .setMaxBytes(MAX_FETCH_SIZE_BYTES)\n            .setMaxWaitMs(fetchMaxWaitMs)\n            .setClusterId(clusterId)\n            .setReplicaState(new FetchRequestData.ReplicaState().setReplicaId(quorum.localIdOrSentinel()));\n    }\n\n    private long maybeSendAnyVoterFetch(long currentTimeMs) {\n        OptionalInt readyVoterIdOpt = requestManager.findReadyVoter(currentTimeMs);\n        if (readyVoterIdOpt.isPresent()) {\n            return maybeSendRequest(\n                currentTimeMs,\n                readyVoterIdOpt.getAsInt(),\n                this::buildFetchRequest\n            );\n        } else {\n            return requestManager.backoffBeforeAvailableVoter(currentTimeMs);\n        }\n    }\n\n    private FetchSnapshotRequestData buildFetchSnapshotRequest(OffsetAndEpoch snapshotId, long snapshotSize) {\n        FetchSnapshotRequestData.SnapshotId requestSnapshotId = new FetchSnapshotRequestData.SnapshotId()\n            .setEpoch(snapshotId.epoch())\n            .setEndOffset(snapshotId.offset());\n\n        FetchSnapshotRequestData request = FetchSnapshotRequest.singleton(\n            clusterId,\n            quorum().localIdOrSentinel(),\n            log.topicPartition(),\n            snapshotPartition -> snapshotPartition\n                .setCurrentLeaderEpoch(quorum.epoch())\n                .setSnapshotId(requestSnapshotId)\n                .setPosition(snapshotSize)\n        );\n\n        return request.setReplicaId(quorum.localIdOrSentinel());\n    }\n\n    private FetchSnapshotResponseData.PartitionSnapshot addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    ) {\n        partitionSnapshot.currentLeader()\n            .setLeaderEpoch(quorum.epoch())\n            .setLeaderId(quorum.leaderIdOrSentinel());\n\n        return partitionSnapshot;\n    }\n\n    public boolean isRunning() {\n        GracefulShutdown gracefulShutdown = shutdown.get();\n        return gracefulShutdown == null || !gracefulShutdown.isFinished();\n    }\n\n    public boolean isShuttingDown() {\n        GracefulShutdown gracefulShutdown = shutdown.get();\n        return gracefulShutdown != null && !gracefulShutdown.isFinished();\n    }\n\n    private void appendBatch(\n        LeaderState<T> state,\n        BatchAccumulator.CompletedBatch<T> batch,\n        long appendTimeMs\n    ) {\n        try {\n            int epoch = state.epoch();\n            LogAppendInfo info = appendAsLeader(batch.data);\n            OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(info.lastOffset, epoch);\n            CompletableFuture<Long> future = appendPurgatory.await(\n                offsetAndEpoch.offset() + 1, Integer.MAX_VALUE);\n\n            future.whenComplete((commitTimeMs, exception) -> {\n                if (exception != null) {\n                    logger.debug(\"Failed to commit {} records up to last offset {}\", batch.numRecords, offsetAndEpoch, exception);\n                } else {\n                    long elapsedTime = Math.max(0, commitTimeMs - appendTimeMs);\n                    double elapsedTimePerRecord = (double) elapsedTime / batch.numRecords;\n                    kafkaRaftMetrics.updateCommitLatency(elapsedTimePerRecord, appendTimeMs);\n                    logger.debug(\"Completed commit of {} records up to last offset {}\", batch.numRecords, offsetAndEpoch);\n                    batch.records.ifPresent(records -> {\n                        maybeFireHandleCommit(batch.baseOffset, epoch, batch.appendTimestamp(), batch.sizeInBytes(), records);\n                    });\n                }\n            });\n        } finally {\n            batch.release();\n        }\n    }\n\n    private long maybeAppendBatches(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        long timeUntilDrain = state.accumulator().timeUntilDrain(currentTimeMs);\n        if (timeUntilDrain <= 0) {\n            List<BatchAccumulator.CompletedBatch<T>> batches = state.accumulator().drain();\n            Iterator<BatchAccumulator.CompletedBatch<T>> iterator = batches.iterator();\n\n            try {\n                while (iterator.hasNext()) {\n                    BatchAccumulator.CompletedBatch<T> batch = iterator.next();\n                    appendBatch(state, batch, currentTimeMs);\n                }\n                flushLeaderLog(state, currentTimeMs);\n            } finally {\n                // Release and discard any batches which failed to be appended\n                while (iterator.hasNext()) {\n                    iterator.next().release();\n                }\n            }\n        }\n        return timeUntilDrain;\n    }\n\n    private long pollResigned(long currentTimeMs) {\n        ResignedState state = quorum.resignedStateOrThrow();\n        long endQuorumBackoffMs = maybeSendRequests(\n            currentTimeMs,\n            state.unackedVoters(),\n            () -> buildEndQuorumEpochRequest(state)\n        );\n\n        GracefulShutdown shutdown = this.shutdown.get();\n        final long stateTimeoutMs;\n        if (shutdown != null) {\n            // If we are shutting down, then we will remain in the resigned state\n            // until either the shutdown expires or an election bumps the epoch\n            stateTimeoutMs = shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            stateTimeoutMs = 0L;\n        } else {\n            stateTimeoutMs = state.remainingElectionTimeMs(currentTimeMs);\n        }\n\n        return Math.min(stateTimeoutMs, endQuorumBackoffMs);\n    }\n\n    private long pollLeader(long currentTimeMs) {\n        LeaderState<T> state = quorum.leaderStateOrThrow();\n        maybeFireLeaderChange(state);\n\n        long timeUntilCheckQuorumExpires = state.timeUntilCheckQuorumExpires(currentTimeMs);\n        if (shutdown.get() != null || state.isResignRequested() || timeUntilCheckQuorumExpires == 0) {\n            transitionToResigned(state.nonLeaderVotersByDescendingFetchOffset());\n            return 0L;\n        }\n\n        long timeUntilFlush = maybeAppendBatches(\n            state,\n            currentTimeMs\n        );\n\n        long timeUntilSend = maybeSendRequests(\n            currentTimeMs,\n            state.nonAcknowledgingVoters(),\n            this::buildBeginQuorumEpochRequest\n        );\n\n        return Math.min(timeUntilFlush, Math.min(timeUntilSend, timeUntilCheckQuorumExpires));\n    }\n\n    private long maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    ) {\n        // Continue sending Vote requests as long as we still have a chance to win the election\n        if (!state.isVoteRejected()) {\n            return maybeSendRequests(\n                currentTimeMs,\n                state.unrecordedVoters(),\n                this::buildVoteRequest\n            );\n        }\n        return Long.MAX_VALUE;\n    }\n\n    private long pollCandidate(long currentTimeMs) {\n        CandidateState state = quorum.candidateStateOrThrow();\n        GracefulShutdown shutdown = this.shutdown.get();\n\n        if (shutdown != null) {\n            // If we happen to shutdown while we are a candidate, we will continue\n            // with the current election until one of the following conditions is met:\n            //  1) we are elected as leader (which allows us to resign)\n            //  2) another leader is elected\n            //  3) the shutdown timer expires\n            long minRequestBackoffMs = maybeSendVoteRequests(state, currentTimeMs);\n            return Math.min(shutdown.remainingTimeMs(), minRequestBackoffMs);\n        } else if (state.isBackingOff()) {\n            if (state.isBackoffComplete(currentTimeMs)) {\n                logger.info(\"Re-elect as candidate after election backoff has completed\");\n                transitionToCandidate(currentTimeMs);\n                return 0L;\n            }\n            return state.remainingBackoffMs(currentTimeMs);\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            long backoffDurationMs = binaryExponentialElectionBackoffMs(state.retries());\n            logger.info(\"Election has timed out, backing off for {}ms before becoming a candidate again\",\n                backoffDurationMs);\n            state.startBackingOff(currentTimeMs, backoffDurationMs);\n            return backoffDurationMs;\n        } else {\n            long minRequestBackoffMs = maybeSendVoteRequests(state, currentTimeMs);\n            return Math.min(minRequestBackoffMs, state.remainingElectionTimeMs(currentTimeMs));\n        }\n    }\n\n    private long pollFollower(long currentTimeMs) {\n        FollowerState state = quorum.followerStateOrThrow();\n        if (quorum.isVoter()) {\n            return pollFollowerAsVoter(state, currentTimeMs);\n        } else {\n            return pollFollowerAsObserver(state, currentTimeMs);\n        }\n    }\n\n    private long pollFollowerAsVoter(FollowerState state, long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown != null) {\n            // If we are a follower, then we can shutdown immediately. We want to\n            // skip the transition to candidate in any case.\n            return 0;\n        } else if (state.hasFetchTimeoutExpired(currentTimeMs)) {\n            logger.info(\"Become candidate due to fetch timeout\");\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            long backoffMs = maybeSendFetchOrFetchSnapshot(state, currentTimeMs);\n\n            return Math.min(backoffMs, state.remainingFetchTimeMs(currentTimeMs));\n        }\n    }\n\n    private long pollFollowerAsObserver(FollowerState state, long currentTimeMs) {\n        if (state.hasFetchTimeoutExpired(currentTimeMs)) {\n            return maybeSendAnyVoterFetch(currentTimeMs);\n        } else {\n            final long backoffMs;\n\n            // If the current leader is backing off due to some failure or if the\n            // request has timed out, then we attempt to send the Fetch to another\n            // voter in order to discover if there has been a leader change.\n            ConnectionState connection = hasAnyInflightRequest(state);\n            if (connection.hasRequestTimedOut(currentTimeMs)) {\n                backoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n                connection.reset();\n            } else if (connection.isBackingOff(currentTimeMs)) {\n                backoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n            } else {\n                backoffMs = maybeSendFetchOrFetchSnapshot(state, currentTimeMs);\n            }\n\n            return Math.min(backoffMs, state.remainingFetchTimeMs(currentTimeMs));\n        }\n    }\n\n    private ConnectionState hasAnyInflightRequest(FollowerState state) {\n        ConnectionState connection = requestManager.getOrCreate(state.leaderId());\n        return connection;\n    }\n\n    private long maybeSendFetchOrFetchSnapshot(FollowerState state, long currentTimeMs) {\n        final Supplier<ApiMessage> requestSupplier;\n\n        if (state.fetchingSnapshot().isPresent()) {\n            RawSnapshotWriter snapshot = state.fetchingSnapshot().get();\n            long snapshotSize = snapshot.sizeInBytes();\n\n            requestSupplier = () -> buildFetchSnapshotRequest(snapshot.snapshotId(), snapshotSize);\n        } else {\n            requestSupplier = this::buildFetchRequest;\n        }\n\n        return maybeSendRequest(currentTimeMs, state.leaderId(), requestSupplier);\n    }\n\n    private long pollVoted(long currentTimeMs) {\n        VotedState state = quorum.votedStateOrThrow();\n        GracefulShutdown shutdown = this.shutdown.get();\n\n        if (shutdown != null) {\n            // If shutting down, then remain in this state until either the\n            // shutdown completes or an epoch bump forces another state transition\n            return shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            return state.remainingElectionTimeMs(currentTimeMs);\n        }\n    }\n\n    private long pollUnattached(long currentTimeMs) {\n        UnattachedState state = quorum.unattachedStateOrThrow();\n        if (quorum.isVoter()) {\n            return pollUnattachedAsVoter(state, currentTimeMs);\n        } else {\n            return pollUnattachedAsObserver(state, currentTimeMs);\n        }\n    }\n\n    private long pollUnattachedAsVoter(UnattachedState state, long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown != null) {\n            // If shutting down, then remain in this state until either the\n            // shutdown completes or an epoch bump forces another state transition\n            return shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            return state.remainingElectionTimeMs(currentTimeMs);\n        }\n    }\n\n    private long pollUnattachedAsObserver(UnattachedState state, long currentTimeMs) {\n        long fetchBackoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n        return Math.min(fetchBackoffMs, state.remainingElectionTimeMs(currentTimeMs));\n    }\n\n    private long pollCurrentState(long currentTimeMs) {\n        if (quorum.isLeader()) {\n            return pollLeader(currentTimeMs);\n        } else if (quorum.isCandidate()) {\n            return pollCandidate(currentTimeMs);\n        } else if (quorum.isFollower()) {\n            return pollFollower(currentTimeMs);\n        } else if (quorum.isVoted()) {\n            return pollVoted(currentTimeMs);\n        } else if (quorum.isUnattached()) {\n            return pollUnattached(currentTimeMs);\n        } else if (quorum.isResigned()) {\n            return pollResigned(currentTimeMs);\n        } else {\n            throw new IllegalStateException(\"Unexpected quorum state \" + quorum);\n        }\n    }\n\n    private void pollListeners() {\n        // Apply all of the pending registration\n        while (true) {\n            Registration<T> registration = pendingRegistrations.poll();\n            if (registration == null) {\n                break;\n            }\n\n            processRegistration(registration);\n        }\n\n        // Check listener progress to see if reads are expected\n        quorum.highWatermark().ifPresent(highWatermarkMetadata -> {\n            updateListenersProgress(highWatermarkMetadata.offset);\n        });\n\n        // Notify the new listeners of the latest leader and epoch\n        Optional<LeaderState<T>> leaderState = quorum.maybeLeaderState();\n        if (leaderState.isPresent()) {\n            maybeFireLeaderChange(leaderState.get());\n        } else {\n            maybeFireLeaderChange();\n        }\n    }\n\n    private void processRegistration(Registration<T> registration) {\n        Listener<T> listener = registration.listener();\n        Registration.Ops ops = registration.ops();\n\n        if (ops == Registration.Ops.REGISTER) {\n            if (listenerContexts.putIfAbsent(listener, new ListenerContext(listener)) != null) {\n                logger.error(\"Attempting to add a listener that already exists: {}\", listenerName(listener));\n            } else {\n                logger.info(\"Registered the listener {}\", listenerName(listener));\n            }\n        } else {\n            if (listenerContexts.remove(listener) == null) {\n                logger.error(\"Attempting to remove a listener that doesn't exists: {}\", listenerName(listener));\n            } else {\n                logger.info(\"Unregistered the listener {}\", listenerName(listener));\n            }\n        }\n    }\n\n    private boolean maybeCompleteShutdown(long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown == null) {\n            return false;\n        }\n\n        shutdown.update(currentTimeMs);\n        if (shutdown.hasTimedOut()) {\n            shutdown.failWithTimeout();\n            return true;\n        }\n\n        if (quorum.isObserver()\n            || quorum.isOnlyVoter()\n            || quorum.hasRemoteLeader()\n        ) {\n            shutdown.complete();\n            return true;\n        }\n\n        return false;\n    }\n\n    /**\n     * A simple timer based log cleaner\n     */\n    private static class RaftMetadataLogCleanerManager {\n        private final Logger logger;\n        private final Timer timer;\n        private final long delayMs;\n        private final Runnable cleaner;\n\n        RaftMetadataLogCleanerManager(Logger logger, Time time, long delayMs, Runnable cleaner) {\n            this.logger = logger;\n            this.timer = time.timer(delayMs);\n            this.delayMs = delayMs;\n            this.cleaner = cleaner;\n        }\n\n        public long maybeClean(long currentTimeMs) {\n            timer.update(currentTimeMs);\n            if (timer.isExpired()) {\n                try {\n                    cleaner.run();\n                } catch (Throwable t) {\n                    logger.error(\"Had an error during log cleaning\", t);\n                }\n                timer.reset(delayMs);\n            }\n            return timer.remainingMs();\n        }\n    }\n\n    private void wakeup() {\n        messageQueue.wakeup();\n    }\n\n    /**\n     * Handle an inbound request. The response will be returned through\n     * {@link RaftRequest.Inbound#completion}.\n     *\n     * @param request The inbound request\n     */\n    public void handle(RaftRequest.Inbound request) {\n        messageQueue.add(Objects.requireNonNull(request));\n    }\n\n    /**\n     * Poll for new events. This allows the client to handle inbound\n     * requests and send any needed outbound requests.\n     */\n    public void poll() {\n        if (!isInitialized()) {\n            throw new IllegalStateException(\"Replica needs to be initialized before polling\");\n        }\n\n        long startPollTimeMs = time.milliseconds();\n        if (maybeCompleteShutdown(startPollTimeMs)) {\n            return;\n        }\n\n        long pollStateTimeoutMs = pollCurrentState(startPollTimeMs);\n        long cleaningTimeoutMs = snapshotCleaner.maybeClean(startPollTimeMs);\n        long pollTimeoutMs = Math.min(pollStateTimeoutMs, cleaningTimeoutMs);\n\n        long startWaitTimeMs = time.milliseconds();\n        kafkaRaftMetrics.updatePollStart(startWaitTimeMs);\n\n        RaftMessage message = messageQueue.poll(pollTimeoutMs);\n\n        long endWaitTimeMs = time.milliseconds();\n        kafkaRaftMetrics.updatePollEnd(endWaitTimeMs);\n\n        if (message != null) {\n            handleInboundMessage(message, endWaitTimeMs);\n        }\n\n        pollListeners();\n    }\n\n    @Override\n    public long scheduleAppend(int epoch, List<T> records) {\n        return append(epoch, records, OptionalLong.empty(), false);\n    }\n\n    @Override\n    public long scheduleAtomicAppend(int epoch, OptionalLong requiredBaseOffset, List<T> records) {\n        return append(epoch, records, requiredBaseOffset, true);\n    }\n\n    private long append(int epoch, List<T> records, OptionalLong requiredBaseOffset, boolean isAtomic) {\n        if (!isInitialized()) {\n            throw new NotLeaderException(\"Append failed because the replica is not the current leader\");\n        }\n\n        LeaderState<T> leaderState = quorum.<T>maybeLeaderState().orElseThrow(\n            () -> new NotLeaderException(\"Append failed because the replica is not the current leader\")\n        );\n\n        BatchAccumulator<T> accumulator = leaderState.accumulator();\n        boolean isFirstAppend = accumulator.isEmpty();\n        final long offset = accumulator.append(epoch, records, requiredBaseOffset, isAtomic);\n\n        // Wakeup the network channel if either this is the first append\n        // or the accumulator is ready to drain now. Checking for the first\n        // append ensures that we give the IO thread a chance to observe\n        // the linger timeout so that it can schedule its own wakeup in case\n        // there are no additional appends.\n        if (isFirstAppend || accumulator.needsDrain(time.milliseconds())) {\n            wakeup();\n        }\n        return offset;\n    }\n\n    @Override\n    public CompletableFuture<Void> shutdown(int timeoutMs) {\n        logger.info(\"Beginning graceful shutdown\");\n        CompletableFuture<Void> shutdownComplete = new CompletableFuture<>();\n        shutdown.set(new GracefulShutdown(timeoutMs, shutdownComplete));\n        wakeup();\n        return shutdownComplete;\n    }\n\n    @Override\n    public void resign(int epoch) {\n        if (epoch < 0) {\n            throw new IllegalArgumentException(\"Attempt to resign from an invalid negative epoch \" + epoch);\n        } else if (!isInitialized()) {\n            throw new IllegalStateException(\"Replica needs to be initialized before resigning\");\n        } else if (!quorum.isVoter()) {\n            throw new IllegalStateException(\"Attempt to resign by a non-voter\");\n        }\n\n        LeaderAndEpoch leaderAndEpoch = leaderAndEpoch();\n        int currentEpoch = leaderAndEpoch.epoch();\n\n        if (epoch > currentEpoch) {\n            throw new IllegalArgumentException(\"Attempt to resign from epoch \" + epoch +\n                \" which is larger than the current epoch \" + currentEpoch);\n        } else if (epoch < currentEpoch) {\n            // If the passed epoch is smaller than the current epoch, then it might mean\n            // that the listener has not been notified about a leader change that already\n            // took place. In this case, we consider the call as already fulfilled and\n            // take no further action.\n            logger.debug(\"Ignoring call to resign from epoch {} since it is smaller than the \" +\n                \"current epoch {}\", epoch, currentEpoch);\n        } else if (!leaderAndEpoch.isLeader(quorum.localIdOrThrow())) {\n            throw new IllegalArgumentException(\"Cannot resign from epoch \" + epoch +\n                \" since we are not the leader\");\n        } else {\n            // Note that if we transition to another state before we have a chance to\n            // request resignation, then we consider the call fulfilled.\n            Optional<LeaderState<Object>> leaderStateOpt = quorum.maybeLeaderState();\n            if (!leaderStateOpt.isPresent()) {\n                logger.debug(\"Ignoring call to resign from epoch {} since this node is \" +\n                    \"no longer the leader\", epoch);\n                return;\n            }\n\n            LeaderState<Object> leaderState = leaderStateOpt.get();\n            if (leaderState.epoch() != epoch) {\n                logger.debug(\"Ignoring call to resign from epoch {} since it is smaller than the \" +\n                    \"current epoch {}\", epoch, leaderState.epoch());\n            } else {\n                logger.info(\"Received user request to resign from the current epoch {}\", currentEpoch);\n                leaderState.requestResign();\n                wakeup();\n            }\n        }\n    }\n\n    @Override\n    public Optional<SnapshotWriter<T>> createSnapshot(\n        OffsetAndEpoch snapshotId,\n        long lastContainedLogTimestamp\n    ) {\n        if (!isInitialized()) {\n            throw new IllegalStateException(\"Cannot create snapshot before the replica has been initialized\");\n        }\n\n        return log.createNewSnapshot(snapshotId).map(writer -> {\n            long lastContainedLogOffset = snapshotId.offset() - 1;\n\n            RawSnapshotWriter wrappedWriter = new NotifyingRawSnapshotWriter(writer, offsetAndEpoch -> {\n                // Trim the state in the internal listener up to the new snapshot\n                partitionState.truncateOldEntries(offsetAndEpoch.offset());\n            });\n\n            return new RecordsSnapshotWriter.Builder()\n                .setLastContainedLogTimestamp(lastContainedLogTimestamp)\n                .setTime(time)\n                .setMaxBatchSize(MAX_BATCH_SIZE_BYTES)\n                .setMemoryPool(memoryPool)\n                .setRawSnapshotWriter(wrappedWriter)\n                .setKraftVersion(partitionState.kraftVersionAtOffset(lastContainedLogOffset))\n                .setVoterSet(partitionState.voterSetAtOffset(lastContainedLogOffset))\n                .build(serde);\n        });\n    }\n\n    @Override\n    public Optional<OffsetAndEpoch> latestSnapshotId() {\n        return log.latestSnapshotId();\n    }\n\n    @Override\n    public long logEndOffset() {\n        return log.endOffset().offset;\n    }\n\n    @Override\n    public void close() {\n        log.flush(true);\n        if (kafkaRaftMetrics != null) {\n            kafkaRaftMetrics.close();\n        }\n        if (memoryPool instanceof BatchMemoryPool) {\n            BatchMemoryPool batchMemoryPool = (BatchMemoryPool) memoryPool;\n            batchMemoryPool.releaseRetained();\n        }\n    }\n\n    @Override\n    public OptionalLong highWatermark() {\n        if (isInitialized() && quorum.highWatermark().isPresent()) {\n            return OptionalLong.of(quorum.highWatermark().get().offset);\n        } else {\n            return OptionalLong.empty();\n        }\n    }\n\n    public Optional<Node> voterNode(int id, String listener) {\n        return partitionState.lastVoterSet().voterNode(id, listener);\n    }\n\n    // Visible only for test\n    QuorumState quorum() {\n        // It's okay to return null since this method is only called by tests\n        return quorum;\n    }\n\n    private boolean isInitialized() {\n        return partitionState != null && quorum != null && requestManager != null && kafkaRaftMetrics != null;\n    }\n\n    private class GracefulShutdown {\n        final Timer finishTimer;\n        final CompletableFuture<Void> completeFuture;\n\n        public GracefulShutdown(long shutdownTimeoutMs,\n                                CompletableFuture<Void> completeFuture) {\n            this.finishTimer = time.timer(shutdownTimeoutMs);\n            this.completeFuture = completeFuture;\n        }\n\n        public void update(long currentTimeMs) {\n            finishTimer.update(currentTimeMs);\n        }\n\n        public boolean hasTimedOut() {\n            return finishTimer.isExpired();\n        }\n\n        public boolean isFinished() {\n            return completeFuture.isDone();\n        }\n\n        public long remainingTimeMs() {\n            return finishTimer.remainingMs();\n        }\n\n        public void failWithTimeout() {\n            logger.warn(\"Graceful shutdown timed out after {}ms\", finishTimer.timeoutMs());\n            completeFuture.completeExceptionally(\n                new TimeoutException(\"Timeout expired before graceful shutdown completed\"));\n        }\n\n        public void complete() {\n            logger.info(\"Graceful shutdown completed\");\n            completeFuture.complete(null);\n        }\n    }\n\n    private static final class Registration<T> {\n        private final Ops ops;\n        private final Listener<T> listener;\n\n        private Registration(Ops ops, Listener<T> listener) {\n            this.ops = ops;\n            this.listener = listener;\n        }\n\n        private Ops ops() {\n            return ops;\n        }\n\n        private Listener<T> listener() {\n            return listener;\n        }\n\n        private enum Ops {\n            REGISTER, UNREGISTER\n        }\n\n        private static <T> Registration<T> register(Listener<T> listener) {\n            return new Registration<>(Ops.REGISTER, listener);\n        }\n\n        private static <T> Registration<T> unregister(Listener<T> listener) {\n            return new Registration<>(Ops.UNREGISTER, listener);\n        }\n    }\n\n    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n        private final RaftClient.Listener<T> listener;\n        // This field is used only by the Raft IO thread\n        private LeaderAndEpoch lastFiredLeaderChange = LeaderAndEpoch.UNKNOWN;\n\n        // These fields are visible to both the Raft IO thread and the listener\n        // and are protected through synchronization on this ListenerContext instance\n        private BatchReader<T> lastSent = null;\n        private long nextOffset = 0;\n\n        private ListenerContext(Listener<T> listener) {\n            this.listener = listener;\n        }\n\n        /**\n         * Get the last acked offset, which is one greater than the offset of the\n         * last record which was acked by the state machine.\n         */\n        private synchronized long nextOffset() {\n            return nextOffset;\n        }\n\n        /**\n         * Get the next expected offset, which might be larger than the last acked\n         * offset if there are inflight batches which have not been acked yet.\n         * Note that when fetching from disk, we may not know the last offset of\n         * inflight data until it has been processed by the state machine. In this case,\n         * we delay sending additional data until the state machine has read to the\n         * end and the last offset is determined.\n         */\n        private synchronized OptionalLong nextExpectedOffset() {\n            if (lastSent != null) {\n                OptionalLong lastSentOffset = lastSent.lastOffset();\n                if (lastSentOffset.isPresent()) {\n                    return OptionalLong.of(lastSentOffset.getAsLong() + 1);\n                } else {\n                    return OptionalLong.empty();\n                }\n            } else {\n                return OptionalLong.of(nextOffset);\n            }\n        }\n\n        /**\n         * This API is used when the Listener needs to be notified of a new snapshot. This happens\n         * when the context's next offset is less than the log start offset.\n         */\n        private void fireHandleSnapshot(SnapshotReader<T> reader) {\n            synchronized (this) {\n                nextOffset = reader.snapshotId().offset();\n                lastSent = null;\n            }\n\n            logger.debug(\"Notifying listener {} of snapshot {}\", listenerName(), reader.snapshotId());\n            listener.handleLoadSnapshot(reader);\n        }\n\n        /**\n         * This API is used for committed records that have been received through\n         * replication. In general, followers will write new data to disk before they\n         * know whether it has been committed. Rather than retaining the uncommitted\n         * data in memory, we let the state machine read the records from disk.\n         */\n        private void fireHandleCommit(long baseOffset, Records records) {\n            fireHandleCommit(\n                RecordsBatchReader.of(\n                    baseOffset,\n                    records,\n                    serde,\n                    BufferSupplier.create(),\n                    MAX_BATCH_SIZE_BYTES,\n                    this,\n                    true /* Validate batch CRC*/\n                )\n            );\n        }\n\n        /**\n         * This API is used for committed records originating from {@link #scheduleAppend(int, List)}\n         * or {@link #scheduleAtomicAppend(int, OptionalLong, List)} on this instance. In this case,\n         * we are able to save the original record objects, which saves the need to read them back\n         * from disk. This is a nice optimization for the leader which is typically doing more work\n         * than all of the * followers.\n         */\n        private void fireHandleCommit(\n            long baseOffset,\n            int epoch,\n            long appendTimestamp,\n            int sizeInBytes,\n            List<T> records\n        ) {\n            Batch<T> batch = Batch.data(baseOffset, epoch, appendTimestamp, sizeInBytes, records);\n            MemoryBatchReader<T> reader = MemoryBatchReader.of(Collections.singletonList(batch), this);\n            fireHandleCommit(reader);\n        }\n\n        private String listenerName() {\n            return KafkaRaftClient.listenerName(listener);\n        }\n\n        private void fireHandleCommit(BatchReader<T> reader) {\n            synchronized (this) {\n                this.lastSent = reader;\n            }\n            logger.debug(\n                \"Notifying listener {} of batch for baseOffset {} and lastOffset {}\",\n                listenerName(),\n                reader.baseOffset(),\n                reader.lastOffset()\n            );\n            listener.handleCommit(reader);\n        }\n\n        private void maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            if (shouldFireLeaderChange(leaderAndEpoch)) {\n                lastFiredLeaderChange = leaderAndEpoch;\n                logger.debug(\"Notifying listener {} of leader change {}\", listenerName(), leaderAndEpoch);\n                listener.handleLeaderChange(leaderAndEpoch);\n            }\n        }\n\n        private boolean shouldFireLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            if (leaderAndEpoch.equals(lastFiredLeaderChange)) {\n                return false;\n            } else if (leaderAndEpoch.epoch() > lastFiredLeaderChange.epoch()) {\n                return true;\n            } else {\n                return leaderAndEpoch.leaderId().isPresent() &&\n                    !lastFiredLeaderChange.leaderId().isPresent();\n            }\n        }\n\n        private void maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch, long epochStartOffset) {\n            // If this node is becoming the leader, then we can fire `handleLeaderChange` as soon\n            // as the listener has caught up to the start of the leader epoch. This guarantees\n            // that the state machine has seen the full committed state before it becomes\n            // leader and begins writing to the log.\n            //\n            // Note that the raft client doesn't need to compare nextOffset against the high-watermark\n            // to guarantee that the listener has caught up to the high-watermark. This is true because\n            // the only way nextOffset can be greater than epochStartOffset is for the leader to have\n            // established the new high-watermark (of at least epochStartOffset + 1) and for the listener\n            // to have consumed up to that new high-watermark.\n            if (shouldFireLeaderChange(leaderAndEpoch) && nextOffset() > epochStartOffset) {\n                lastFiredLeaderChange = leaderAndEpoch;\n                listener.handleLeaderChange(leaderAndEpoch);\n            }\n        }\n\n        public synchronized void onClose(BatchReader<T> reader) {\n            OptionalLong lastOffset = reader.lastOffset();\n\n            if (lastOffset.isPresent()) {\n                nextOffset = lastOffset.getAsLong() + 1;\n            }\n\n            if (lastSent == reader) {\n                lastSent = null;\n                wakeup();\n            }\n        }\n    }\n}",
                "methodCount": 136
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "updateFollowerHighWatermark",
                            "method_signature": "private updateFollowerHighWatermark(\n        FollowerState state,\n        OptionalLong highWatermarkOpt\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateLeaderEndOffsetAndTimestamp",
                            "method_signature": "private updateLeaderEndOffsetAndTimestamp(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUpdateLeaderHighWatermark",
                            "method_signature": "private onUpdateLeaderHighWatermark(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateListenersProgress",
                            "method_signature": "private updateListenersProgress(long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "latestSnapshot",
                            "method_signature": "private latestSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireHandleCommit",
                            "method_signature": "private maybeFireHandleCommit(long baseOffset, int epoch, long appendTimestamp, int sizeInBytes, List<T> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderState<T> state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": "public initialize(\n        Map<Integer, InetSocketAddress> voterAddresses,\n        String listenerName,\n        QuorumStateStore quorumStateStore,\n        Metrics metrics\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endOffset",
                            "method_signature": "private endOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetConnections",
                            "method_signature": "private resetConnections()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeLeader",
                            "method_signature": "private onBecomeLeader(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flushLeaderLog",
                            "method_signature": "private flushLeaderLog(LeaderState<T> state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeTransitionToLeader",
                            "method_signature": "private maybeTransitionToLeader(CandidateState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeCandidate",
                            "method_signature": "private onBecomeCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToCandidate",
                            "method_signature": "private transitionToCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToUnattached",
                            "method_signature": "private transitionToUnattached(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToResigned",
                            "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToVoted",
                            "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeFollower",
                            "method_signature": "private onBecomeFollower(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToFollower",
                            "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleVoteRequest",
                            "method_signature": "private handleVoteRequest(\n        RaftRequest.Inbound requestMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleVoteResponse",
                            "method_signature": "private handleVoteResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "binaryExponentialElectionBackoffMs",
                            "method_signature": "private binaryExponentialElectionBackoffMs(int retries)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "strictExponentialElectionBackoffMs",
                            "method_signature": "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBeginQuorumEpochResponse",
                            "method_signature": "private buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleBeginQuorumEpochRequest",
                            "method_signature": "private handleBeginQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleBeginQuorumEpochResponse",
                            "method_signature": "private handleBeginQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochResponse",
                            "method_signature": "private buildEndQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleEndQuorumEpochRequest",
                            "method_signature": "private handleEndQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endEpochElectionBackoff",
                            "method_signature": "private endEpochElectionBackoff(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleEndQuorumEpochResponse",
                            "method_signature": "private handleEndQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEmptyFetchResponse",
                            "method_signature": "private buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasValidClusterId",
                            "method_signature": "private hasValidClusterId(String requestClusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchRequest",
                            "method_signature": "private handleFetchRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tryCompleteFetchRequest",
                            "method_signature": "private tryCompleteFetchRequest(\n        int replicaId,\n        FetchRequestData.FetchPartition request,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isPartitionDiverged",
                            "method_signature": "private static isPartitionDiverged(FetchResponseData.PartitionData partitionResponseData)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isPartitionSnapshotted",
                            "method_signature": "private static isPartitionSnapshotted(FetchResponseData.PartitionData partitionResponseData)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "optionalLeaderId",
                            "method_signature": "private static optionalLeaderId(int leaderIdOrNil)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "listenerName",
                            "method_signature": "private static listenerName(Listener<?> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchResponse",
                            "method_signature": "private handleFetchResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsFollower",
                            "method_signature": "private appendAsFollower(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsLeader",
                            "method_signature": "private appendAsLeader(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleDescribeQuorumRequest",
                            "method_signature": "private handleDescribeQuorumRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchSnapshotRequest",
                            "method_signature": "private handleFetchSnapshotRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchSnapshotResponse",
                            "method_signature": "private handleFetchSnapshotResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasConsistentLeader",
                            "method_signature": "private hasConsistentLeader(int epoch, OptionalInt leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeHandleCommonResponse",
                            "method_signature": "private maybeHandleCommonResponse(\n        Errors error,\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeTransition",
                            "method_signature": "private maybeTransition(\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleTopLevelError",
                            "method_signature": "private handleTopLevelError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleResponse",
                            "method_signature": "private handleResponse(RaftResponse.Inbound response, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateVoterOnlyRequest",
                            "method_signature": "private validateVoterOnlyRequest(int remoteNodeId, int requestEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateLeaderOnlyRequest",
                            "method_signature": "private validateLeaderOnlyRequest(int requestEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleRequest",
                            "method_signature": "private handleRequest(RaftRequest.Inbound request, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleInboundMessage",
                            "method_signature": "private handleInboundMessage(RaftMessage message, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendRequest",
                            "method_signature": "private maybeSendRequest(\n        long currentTimeMs,\n        int destinationId,\n        Supplier<ApiMessage> requestSupplier\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochRequest",
                            "method_signature": "private buildEndQuorumEpochRequest(\n        ResignedState state\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendRequests",
                            "method_signature": "private maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBeginQuorumEpochRequest",
                            "method_signature": "private buildBeginQuorumEpochRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteRequest",
                            "method_signature": "private buildVoteRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchRequest",
                            "method_signature": "private buildFetchRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendAnyVoterFetch",
                            "method_signature": "private maybeSendAnyVoterFetch(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchSnapshotRequest",
                            "method_signature": "private buildFetchSnapshotRequest(OffsetAndEpoch snapshotId, long snapshotSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addQuorumLeader",
                            "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunning",
                            "method_signature": "public isRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendBatch",
                            "method_signature": "private appendBatch(\n        LeaderState<T> state,\n        BatchAccumulator.CompletedBatch<T> batch,\n        long appendTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAppendBatches",
                            "method_signature": "private maybeAppendBatches(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollResigned",
                            "method_signature": "private pollResigned(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollLeader",
                            "method_signature": "private pollLeader(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendVoteRequests",
                            "method_signature": "private maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollCandidate",
                            "method_signature": "private pollCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollower",
                            "method_signature": "private pollFollower(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollowerAsVoter",
                            "method_signature": "private pollFollowerAsVoter(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollowerAsObserver",
                            "method_signature": "private pollFollowerAsObserver(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasAnyInflightRequest",
                            "method_signature": "private hasAnyInflightRequest(FollowerState state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendFetchOrFetchSnapshot",
                            "method_signature": "private maybeSendFetchOrFetchSnapshot(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollVoted",
                            "method_signature": "private pollVoted(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattached",
                            "method_signature": "private pollUnattached(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattachedAsVoter",
                            "method_signature": "private pollUnattachedAsVoter(UnattachedState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattachedAsObserver",
                            "method_signature": "private pollUnattachedAsObserver(UnattachedState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollCurrentState",
                            "method_signature": "private pollCurrentState(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollListeners",
                            "method_signature": "private pollListeners()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processRegistration",
                            "method_signature": "private processRegistration(Registration<T> registration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCompleteShutdown",
                            "method_signature": "private maybeCompleteShutdown(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeClean",
                            "method_signature": "public maybeClean(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "wakeup",
                            "method_signature": "private wakeup()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handle",
                            "method_signature": "public handle(RaftRequest.Inbound request)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "poll",
                            "method_signature": "public poll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "append",
                            "method_signature": "private append(int epoch, List<T> records, OptionalLong requiredBaseOffset, boolean isAtomic)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int id, String listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isInitialized",
                            "method_signature": "private isInitialized()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "update",
                            "method_signature": "public update(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasTimedOut",
                            "method_signature": "public hasTimedOut()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isFinished",
                            "method_signature": "public isFinished()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "remainingTimeMs",
                            "method_signature": "public remainingTimeMs()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "complete",
                            "method_signature": "public complete()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "register",
                            "method_signature": "private static register(Listener<T> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unregister",
                            "method_signature": "private static unregister(Listener<T> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "nextExpectedOffset",
                            "method_signature": "private synchronized nextExpectedOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleSnapshot",
                            "method_signature": "private fireHandleSnapshot(SnapshotReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(long baseOffset, Records records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(\n            long baseOffset,\n            int epoch,\n            long appendTimestamp,\n            int sizeInBytes,\n            List<T> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "listenerName",
                            "method_signature": "private listenerName()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shouldFireLeaderChange",
                            "method_signature": "private shouldFireLeaderChange(LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch, long epochStartOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onClose",
                            "method_signature": "public synchronized onClose(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "resetConnections",
                            "method_signature": "private resetConnections()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "listenerName",
                            "method_signature": "private static listenerName(Listener<?> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "update",
                            "method_signature": "public update(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasTimedOut",
                            "method_signature": "public hasTimedOut()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unregister",
                            "method_signature": "private static unregister(Listener<T> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "register",
                            "method_signature": "private static register(Listener<T> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processRegistration",
                            "method_signature": "private processRegistration(Registration<T> registration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeClean",
                            "method_signature": "public maybeClean(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "remainingTimeMs",
                            "method_signature": "public remainingTimeMs()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendRequests",
                            "method_signature": "private maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "wakeup",
                            "method_signature": "private wakeup()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isFinished",
                            "method_signature": "public isFinished()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasValidClusterId",
                            "method_signature": "private hasValidClusterId(String requestClusterId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private resetConnections()": {
                    "first": {
                        "method_name": "resetConnections",
                        "method_signature": "private resetConnections()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.17540149800828203
                },
                "private static listenerName(Listener<?> listener)": {
                    "first": {
                        "method_name": "listenerName",
                        "method_signature": "private static listenerName(Listener<?> listener)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.18964640828058218
                },
                "public update(long currentTimeMs)": {
                    "first": {
                        "method_name": "update",
                        "method_signature": "public update(long currentTimeMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.20761149361012948
                },
                "public hasTimedOut()": {
                    "first": {
                        "method_name": "hasTimedOut",
                        "method_signature": "public hasTimedOut()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.22310116274419084
                },
                "private static unregister(Listener<T> listener)": {
                    "first": {
                        "method_name": "unregister",
                        "method_signature": "private static unregister(Listener<T> listener)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2271985135043723
                },
                "private static register(Listener<T> listener)": {
                    "first": {
                        "method_name": "register",
                        "method_signature": "private static register(Listener<T> listener)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2462544578530015
                },
                "private processRegistration(Registration<T> registration)": {
                    "first": {
                        "method_name": "processRegistration",
                        "method_signature": "private processRegistration(Registration<T> registration)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25672210762679315
                },
                "public maybeClean(long currentTimeMs)": {
                    "first": {
                        "method_name": "maybeClean",
                        "method_signature": "public maybeClean(long currentTimeMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.26162611641950567
                },
                "public remainingTimeMs()": {
                    "first": {
                        "method_name": "remainingTimeMs",
                        "method_signature": "public remainingTimeMs()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.27279216315095045
                },
                "private maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    )": {
                    "first": {
                        "method_name": "maybeSendRequests",
                        "method_signature": "private maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2751093503588426
                },
                "private wakeup()": {
                    "first": {
                        "method_name": "wakeup",
                        "method_signature": "private wakeup()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.28922855137625214
                },
                "public isFinished()": {
                    "first": {
                        "method_name": "isFinished",
                        "method_signature": "public isFinished()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2920123556144746
                },
                "public failWithTimeout()": {
                    "first": {
                        "method_name": "failWithTimeout",
                        "method_signature": "public failWithTimeout()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.29968437340175774
                },
                "public isShuttingDown()": {
                    "first": {
                        "method_name": "isShuttingDown",
                        "method_signature": "public isShuttingDown()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.303262594847254
                },
                "private hasValidClusterId(String requestClusterId)": {
                    "first": {
                        "method_name": "hasValidClusterId",
                        "method_signature": "private hasValidClusterId(String requestClusterId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.32716781791175525
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "459da4795a511f6933e940fcf105a824bd9e589c",
        "url": "https://github.com/apache/kafka/commit/459da4795a511f6933e940fcf105a824bd9e589c",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public onResponseResult(node Node, correlationId long, success boolean, timeMs long) : void extracted from private handleResponse(response RaftResponse.Inbound, currentTimeMs long) : void in class org.apache.kafka.raft.KafkaRaftClient & moved to class org.apache.kafka.raft.RequestManager",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1675,
                    "endLine": 1711,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private handleResponse(response RaftResponse.Inbound, currentTimeMs long) : void"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1709,
                    "endLine": 1709,
                    "startColumn": 13,
                    "endColumn": 79,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1706,
                    "endLine": 1710,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1706,
                    "endLine": 1708,
                    "startColumn": 34,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1708,
                    "endLine": 1710,
                    "startColumn": 16,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 234,
                    "endLine": 252,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public onResponseResult(node Node, correlationId long, success boolean, timeMs long) : void"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 249,
                    "endLine": 249,
                    "startColumn": 17,
                    "endColumn": 89,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 244,
                    "endLine": 250,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 244,
                    "endLine": 247,
                    "startColumn": 26,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 247,
                    "endLine": 250,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1736,
                    "endLine": 1772,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private handleResponse(response RaftResponse.Inbound, currentTimeMs long) : void"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 1766,
                    "endLine": 1771,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "requestManager.onResponseResult(response.source(),response.correlationId(),handledSuccessfully,currentTimeMs)"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 246,
                    "endLine": 246,
                    "startColumn": 17,
                    "endColumn": 29,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 243,
                    "endLine": 251,
                    "startColumn": 54,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/RequestManager.java",
                    "startLine": 243,
                    "endLine": 251,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 582,
        "extraction_results": {
            "success": true,
            "newCommitHash": "c1a446eb827fb6a57b6199f41beca07a2b229185",
            "newBranchName": "extract-onResponseResult-handleResponse-8a882a7"
        },
        "telemetry": {
            "id": "9535dfbf-2fa6-4028-8eba-e4861977548c",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2692,
                "lineStart": 109,
                "lineEnd": 2800,
                "bodyLineStart": 109,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                "sourceCode": "/**\n * This class implements a Kafkaesque version of the Raft protocol. Leader election\n * is more or less pure Raft, but replication is driven by replica fetching and we use Kafka's\n * log reconciliation protocol to truncate the log to a common point following each leader\n * election.\n *\n * Like Zookeeper, this protocol distinguishes between voters and observers. Voters are\n * the only ones who are eligible to handle protocol requests and they are the only ones\n * who take part in elections. The protocol does not yet support dynamic quorum changes.\n *\n * These are the APIs in this protocol:\n *\n * 1) {@link VoteRequestData}: Sent by valid voters when their election timeout expires and they\n *    become a candidate. This request includes the last offset in the log which electors use\n *    to tell whether or not to grant the vote.\n *\n * 2) {@link BeginQuorumEpochRequestData}: Sent by the leader of an epoch only to valid voters to\n *    assert its leadership of the new epoch. This request will be retried indefinitely for\n *    each voter until it acknowledges the request or a new election occurs.\n *\n *    This is not needed in usual Raft because the leader can use an empty data push\n *    to achieve the same purpose. The Kafka Raft implementation, however, is driven by\n *    fetch requests from followers, so there must be a way to find the new leader after\n *    an election has completed.\n *\n * 3) {@link EndQuorumEpochRequestData}: Sent by the leader of an epoch to valid voters in order to\n *    gracefully resign from the current epoch. This causes remaining voters to immediately\n *    begin a new election.\n *\n * 4) {@link FetchRequestData}: This is the same as the usual Fetch API in Kafka, but we add snapshot\n *    check before responding, and we also piggyback some additional metadata on responses (i.e. current\n *    leader and epoch). Unlike partition replication, we also piggyback truncation detection on this API\n *    rather than through a separate truncation state.\n *\n * 5) {@link FetchSnapshotRequestData}: Sent by the follower to the epoch leader in order to fetch a snapshot.\n *    This happens when a FetchResponse includes a snapshot ID due to the follower's log end offset being less\n *    than the leader's log start offset. This API is similar to the Fetch API since the snapshot is stored\n *    as FileRecords, but we use {@link UnalignedRecords} in FetchSnapshotResponse because the records\n *    are not necessarily offset-aligned.\n */\nfinal public class KafkaRaftClient<T> implements RaftClient<T> {\n    private static final int RETRY_BACKOFF_BASE_MS = 100;\n    public static final int MAX_FETCH_WAIT_MS = 500;\n    public static final int MAX_BATCH_SIZE_BYTES = 8 * 1024 * 1024;\n    public static final int MAX_FETCH_SIZE_BYTES = MAX_BATCH_SIZE_BYTES;\n\n    private final OptionalInt nodeId;\n    private final Uuid nodeDirectoryId;\n    private final AtomicReference<GracefulShutdown> shutdown = new AtomicReference<>();\n    private final LogContext logContext;\n    private final Logger logger;\n    private final Time time;\n    private final int fetchMaxWaitMs;\n    private final String clusterId;\n    private final NetworkChannel channel;\n    private final ReplicatedLog log;\n    private final Random random;\n    private final FuturePurgatory<Long> appendPurgatory;\n    private final FuturePurgatory<Long> fetchPurgatory;\n    private final RecordSerde<T> serde;\n    private final MemoryPool memoryPool;\n    private final RaftMessageQueue messageQueue;\n    private final QuorumConfig quorumConfig;\n    private final RaftMetadataLogCleanerManager snapshotCleaner;\n\n    private final Map<Listener<T>, ListenerContext> listenerContexts = new IdentityHashMap<>();\n    private final ConcurrentLinkedQueue<Registration<T>> pendingRegistrations = new ConcurrentLinkedQueue<>();\n\n    // These components need to be initialized by the method initialize() because they depend on\n    // the voter set\n    /*\n     * The key invariant for the kraft control record state machine is that it has always read to\n     * the LEO. This is achieved by:\n     *\n     * 1. reading the entire partition (snapshot and log) at start up,\n     * 2. updating the state when a snapshot is replaced, because of FETCH_SNAPSHOT, on the\n     *    followers\n     * 3. updating the state when the leader (call to append()) or follower (FETCH) appends to the\n     *    log\n     * 4. truncate new entries when a follower truncates their log\n     * 5. truncate old entries when a snapshot gets generated\n     */\n    private volatile KRaftControlRecordStateMachine partitionState;\n    private volatile KafkaRaftMetrics kafkaRaftMetrics;\n    private volatile QuorumState quorum;\n    private volatile RequestManager requestManager;\n\n    /**\n     * Create a new instance.\n     *\n     * Note that if the node ID is empty, then the client will behave as a\n     * non-participating observer.\n     */\n    public KafkaRaftClient(\n        OptionalInt nodeId,\n        Uuid nodeDirectoryId,\n        RecordSerde<T> serde,\n        NetworkChannel channel,\n        ReplicatedLog log,\n        Time time,\n        ExpirationService expirationService,\n        LogContext logContext,\n        String clusterId,\n        QuorumConfig quorumConfig\n    ) {\n        this(\n            nodeId,\n            nodeDirectoryId,\n            serde,\n            channel,\n            new BlockingMessageQueue(),\n            log,\n            new BatchMemoryPool(5, MAX_BATCH_SIZE_BYTES),\n            time,\n            expirationService,\n            MAX_FETCH_WAIT_MS,\n            clusterId,\n            logContext,\n            new Random(),\n            quorumConfig\n        );\n    }\n\n    KafkaRaftClient(\n        OptionalInt nodeId,\n        Uuid nodeDirectoryId,\n        RecordSerde<T> serde,\n        NetworkChannel channel,\n        RaftMessageQueue messageQueue,\n        ReplicatedLog log,\n        MemoryPool memoryPool,\n        Time time,\n        ExpirationService expirationService,\n        int fetchMaxWaitMs,\n        String clusterId,\n        LogContext logContext,\n        Random random,\n        QuorumConfig quorumConfig\n    ) {\n        this.nodeId = nodeId;\n        this.nodeDirectoryId = nodeDirectoryId;\n        this.logContext = logContext;\n        this.serde = serde;\n        this.channel = channel;\n        this.messageQueue = messageQueue;\n        this.log = log;\n        this.memoryPool = memoryPool;\n        this.fetchPurgatory = new ThresholdPurgatory<>(expirationService);\n        this.appendPurgatory = new ThresholdPurgatory<>(expirationService);\n        this.time = time;\n        this.clusterId = clusterId;\n        this.fetchMaxWaitMs = fetchMaxWaitMs;\n        this.logger = logContext.logger(KafkaRaftClient.class);\n        this.random = random;\n        this.quorumConfig = quorumConfig;\n        this.snapshotCleaner = new RaftMetadataLogCleanerManager(logger, time, 60000, log::maybeClean);\n    }\n\n    private void updateFollowerHighWatermark(\n        FollowerState state,\n        OptionalLong highWatermarkOpt\n    ) {\n        highWatermarkOpt.ifPresent(highWatermark -> {\n            long newHighWatermark = Math.min(endOffset().offset(), highWatermark);\n            if (state.updateHighWatermark(OptionalLong.of(newHighWatermark))) {\n                logger.debug(\"Follower high watermark updated to {}\", newHighWatermark);\n                log.updateHighWatermark(new LogOffsetMetadata(newHighWatermark));\n                updateListenersProgress(newHighWatermark);\n            }\n        });\n    }\n\n    private void updateLeaderEndOffsetAndTimestamp(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n\n        if (state.updateLocalState(endOffsetMetadata)) {\n            onUpdateLeaderHighWatermark(state, currentTimeMs);\n        }\n\n        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n    }\n\n    private void onUpdateLeaderHighWatermark(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        state.highWatermark().ifPresent(highWatermark -> {\n            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n            log.updateHighWatermark(highWatermark);\n\n            // After updating the high watermark, we first clear the append\n            // purgatory so that we have an opportunity to route the pending\n            // records still held in memory directly to the listener\n            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n\n            // It is also possible that the high watermark is being updated\n            // for the first time following the leader election, so we need\n            // to give lagging listeners an opportunity to catch up as well\n            updateListenersProgress(highWatermark.offset);\n        });\n    }\n\n    private void updateListenersProgress(long highWatermark) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.nextExpectedOffset().ifPresent(nextExpectedOffset -> {\n                // Send snapshot to the listener, if the listener is at the beginning of the log and there is a snapshot,\n                // or the listener is trying to read an offset for which there isn't a segment in the log.\n                if (nextExpectedOffset < highWatermark &&\n                    ((nextExpectedOffset == 0 && latestSnapshot().isPresent()) ||\n                     nextExpectedOffset < log.startOffset())\n                ) {\n                    SnapshotReader<T> snapshot = latestSnapshot().orElseThrow(() -> new IllegalStateException(\n                        String.format(\n                            \"Snapshot expected since next offset of %s is %d, log start offset is %d and high-watermark is %d\",\n                            listenerContext.listenerName(),\n                            nextExpectedOffset,\n                            log.startOffset(),\n                            highWatermark\n                        )\n                    ));\n                    listenerContext.fireHandleSnapshot(snapshot);\n                }\n            });\n\n            // Re-read the expected offset in case the snapshot had to be reloaded\n            listenerContext.nextExpectedOffset().ifPresent(nextExpectedOffset -> {\n                if (nextExpectedOffset < highWatermark) {\n                    LogFetchInfo readInfo = log.read(nextExpectedOffset, Isolation.COMMITTED);\n                    listenerContext.fireHandleCommit(nextExpectedOffset, readInfo.records);\n                }\n            });\n        }\n    }\n\n    private Optional<SnapshotReader<T>> latestSnapshot() {\n        return log.latestSnapshot().map(reader ->\n            RecordsSnapshotReader.of(reader,\n                serde,\n                BufferSupplier.create(),\n                MAX_BATCH_SIZE_BYTES,\n                true /* Validate batch CRC*/\n            )\n        );\n    }\n\n    private void maybeFireHandleCommit(long baseOffset, int epoch, long appendTimestamp, int sizeInBytes, List<T> records) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.nextExpectedOffset().ifPresent(nextOffset -> {\n                if (nextOffset == baseOffset) {\n                    listenerContext.fireHandleCommit(baseOffset, epoch, appendTimestamp, sizeInBytes, records);\n                }\n            });\n        }\n    }\n\n    private void maybeFireLeaderChange(LeaderState<T> state) {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.maybeFireLeaderChange(quorum.leaderAndEpoch(), state.epochStartOffset());\n        }\n    }\n\n    private void maybeFireLeaderChange() {\n        for (ListenerContext listenerContext : listenerContexts.values()) {\n            listenerContext.maybeFireLeaderChange(quorum.leaderAndEpoch());\n        }\n    }\n\n    public void initialize(\n        Map<Integer, InetSocketAddress> voterAddresses,\n        String listenerName,\n        QuorumStateStore quorumStateStore,\n        Metrics metrics\n    ) {\n        partitionState = new KRaftControlRecordStateMachine(\n            Optional.of(VoterSet.fromInetSocketAddresses(listenerName, voterAddresses)),\n            log,\n            serde,\n            BufferSupplier.create(),\n            MAX_BATCH_SIZE_BYTES,\n            logContext\n        );\n        // Read the entire log\n        logger.info(\"Reading KRaft snapshot and log as part of the initialization\");\n        partitionState.updateState();\n\n        VoterSet lastVoterSet = partitionState.lastVoterSet();\n        requestManager = new RequestManager(\n            lastVoterSet.voterIds(),\n            quorumConfig.retryBackoffMs(),\n            quorumConfig.requestTimeoutMs(),\n            random\n        );\n\n        quorum = new QuorumState(\n            nodeId,\n            nodeDirectoryId,\n            partitionState::lastVoterSet,\n            partitionState::lastKraftVersion,\n            quorumConfig.electionTimeoutMs(),\n            quorumConfig.fetchTimeoutMs(),\n            quorumStateStore,\n            time,\n            logContext,\n            random\n        );\n\n        kafkaRaftMetrics = new KafkaRaftMetrics(metrics, \"raft\", quorum);\n        // All Raft voters are statically configured and known at startup\n        // so there are no unknown voter connections. Report this metric as 0.\n        kafkaRaftMetrics.updateNumUnknownVoterConnections(0);\n\n        for (Integer voterId : lastVoterSet.voterIds()) {\n            channel.updateEndpoint(voterId, lastVoterSet.voterAddress(voterId, listenerName).get());\n        }\n\n        quorum.initialize(new OffsetAndEpoch(log.endOffset().offset, log.lastFetchedEpoch()));\n\n        long currentTimeMs = time.milliseconds();\n        if (quorum.isLeader()) {\n            throw new IllegalStateException(\"Voter cannot initialize as a Leader\");\n        } else if (quorum.isCandidate()) {\n            onBecomeCandidate(currentTimeMs);\n        } else if (quorum.isFollower()) {\n            onBecomeFollower(currentTimeMs);\n        }\n\n        // When there is only a single voter, become candidate immediately\n        if (quorum.isOnlyVoter() && !quorum.isCandidate()) {\n            transitionToCandidate(currentTimeMs);\n        }\n    }\n\n    @Override\n    public void register(Listener<T> listener) {\n        pendingRegistrations.add(Registration.register(listener));\n        wakeup();\n    }\n\n    @Override\n    public void unregister(Listener<T> listener) {\n        pendingRegistrations.add(Registration.unregister(listener));\n        // No need to wake up the polling thread. It is a removal so the updates can be\n        // delayed until the polling thread wakes up for other reasons.\n    }\n\n    @Override\n    public LeaderAndEpoch leaderAndEpoch() {\n        if (isInitialized()) {\n            return quorum.leaderAndEpoch();\n        } else {\n            return LeaderAndEpoch.UNKNOWN;\n        }\n    }\n\n    @Override\n    public OptionalInt nodeId() {\n        return nodeId;\n    }\n\n    private OffsetAndEpoch endOffset() {\n        return new OffsetAndEpoch(log.endOffset().offset, log.lastFetchedEpoch());\n    }\n\n    private void resetConnections() {\n        requestManager.resetAll();\n    }\n\n    private void onBecomeLeader(long currentTimeMs) {\n        long endOffset = log.endOffset().offset;\n\n        BatchAccumulator<T> accumulator = new BatchAccumulator<>(\n            quorum.epoch(),\n            endOffset,\n            quorumConfig.appendLingerMs(),\n            MAX_BATCH_SIZE_BYTES,\n            memoryPool,\n            time,\n            Compression.NONE,\n            serde\n        );\n\n        LeaderState<T> state = quorum.transitionToLeader(endOffset, accumulator);\n\n        log.initializeLeaderEpoch(quorum.epoch());\n\n        // The high watermark can only be advanced once we have written a record\n        // from the new leader's epoch. Hence we write a control message immediately\n        // to ensure there is no delay committing pending data.\n        state.appendLeaderChangeMessage(currentTimeMs);\n\n        resetConnections();\n        kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n    }\n\n    private void flushLeaderLog(LeaderState<T> state, long currentTimeMs) {\n        // We update the end offset before flushing so that parked fetches can return sooner.\n        updateLeaderEndOffsetAndTimestamp(state, currentTimeMs);\n        log.flush(false);\n    }\n\n    private boolean maybeTransitionToLeader(CandidateState state, long currentTimeMs) {\n        if (state.isVoteGranted()) {\n            onBecomeLeader(currentTimeMs);\n            return true;\n        } else {\n            return false;\n        }\n    }\n\n    private void onBecomeCandidate(long currentTimeMs) {\n        CandidateState state = quorum.candidateStateOrThrow();\n        if (!maybeTransitionToLeader(state, currentTimeMs)) {\n            resetConnections();\n            kafkaRaftMetrics.updateElectionStartMs(currentTimeMs);\n        }\n    }\n\n    private void transitionToCandidate(long currentTimeMs) {\n        quorum.transitionToCandidate();\n        maybeFireLeaderChange();\n        onBecomeCandidate(currentTimeMs);\n    }\n\n    private void transitionToUnattached(int epoch) {\n        quorum.transitionToUnattached(epoch);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void transitionToResigned(List<Integer> preferredSuccessors) {\n        fetchPurgatory.completeAllExceptionally(\n            Errors.NOT_LEADER_OR_FOLLOWER.exception(\"Not handling request since this node is resigning\"));\n        quorum.transitionToResigned(preferredSuccessors);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void transitionToVoted(ReplicaKey candidateKey, int epoch) {\n        quorum.transitionToVoted(epoch, candidateKey);\n        maybeFireLeaderChange();\n        resetConnections();\n    }\n\n    private void onBecomeFollower(long currentTimeMs) {\n        kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n\n        resetConnections();\n\n        // After becoming a follower, we need to complete all pending fetches so that\n        // they can be re-sent to the leader without waiting for their expirations\n        fetchPurgatory.completeAllExceptionally(new NotLeaderOrFollowerException(\n            \"Cannot process the fetch request because the node is no longer the leader.\"));\n\n        // Clearing the append purgatory should complete all futures exceptionally since this node is no longer the leader\n        appendPurgatory.completeAllExceptionally(new NotLeaderOrFollowerException(\n            \"Failed to receive sufficient acknowledgments for this append before leader change.\"));\n    }\n\n    private void transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    ) {\n        quorum.transitionToFollower(epoch, leaderId);\n        maybeFireLeaderChange();\n        onBecomeFollower(currentTimeMs);\n    }\n\n    private VoteResponseData buildVoteResponse(Errors partitionLevelError, boolean voteGranted) {\n        return VoteResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel(),\n            voteGranted);\n    }\n\n    /**\n     * Handle a Vote request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#INVALID_REQUEST} if the last epoch or offset are invalid\n     */\n    private VoteResponseData handleVoteRequest(\n        RaftRequest.Inbound requestMetadata\n    ) {\n        VoteRequestData request = (VoteRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new VoteResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat individual topic partition mismatches as invalid requests\n            return new VoteResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        VoteRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int candidateId = partitionRequest.candidateId();\n        int candidateEpoch = partitionRequest.candidateEpoch();\n\n        int lastEpoch = partitionRequest.lastOffsetEpoch();\n        long lastEpochEndOffset = partitionRequest.lastOffset();\n        if (lastEpochEndOffset < 0 || lastEpoch < 0 || lastEpoch >= candidateEpoch) {\n            return buildVoteResponse(Errors.INVALID_REQUEST, false);\n        }\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(candidateId, candidateEpoch);\n        if (errorOpt.isPresent()) {\n            return buildVoteResponse(errorOpt.get(), false);\n        }\n\n        if (candidateEpoch > quorum.epoch()) {\n            transitionToUnattached(candidateEpoch);\n        }\n\n        OffsetAndEpoch lastEpochEndOffsetAndEpoch = new OffsetAndEpoch(lastEpochEndOffset, lastEpoch);\n        ReplicaKey candidateKey = ReplicaKey.of(candidateId, Optional.empty());\n        boolean voteGranted = quorum.canGrantVote(\n            candidateKey,\n            lastEpochEndOffsetAndEpoch.compareTo(endOffset()) >= 0\n        );\n\n        if (voteGranted && quorum.isUnattached()) {\n            transitionToVoted(candidateKey, candidateEpoch);\n        }\n\n        logger.info(\"Vote request {} with epoch {} is {}\", request, candidateEpoch, voteGranted ? \"granted\" : \"rejected\");\n        return buildVoteResponse(Errors.NONE, voteGranted);\n    }\n\n    private boolean handleVoteResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        int remoteNodeId = responseMetadata.sourceId();\n        VoteResponseData response = (VoteResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        VoteResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors error = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (error == Errors.NONE) {\n            if (quorum.isLeader()) {\n                logger.debug(\"Ignoring vote response {} since we already became leader for epoch {}\",\n                    partitionResponse, quorum.epoch());\n            } else if (quorum.isCandidate()) {\n                CandidateState state = quorum.candidateStateOrThrow();\n                if (partitionResponse.voteGranted()) {\n                    state.recordGrantedVote(remoteNodeId);\n                    maybeTransitionToLeader(state, currentTimeMs);\n                } else {\n                    state.recordRejectedVote(remoteNodeId);\n\n                    // If our vote is rejected, we go immediately to the random backoff. This\n                    // ensures that we are not stuck waiting for the election timeout when the\n                    // vote has become gridlocked.\n                    if (state.isVoteRejected() && !state.isBackingOff()) {\n                        logger.info(\"Insufficient remaining votes to become leader (rejected by {}). \" +\n                            \"We will backoff before retrying election again\", state.rejectingVoters());\n\n                        state.startBackingOff(\n                            currentTimeMs,\n                            binaryExponentialElectionBackoffMs(state.retries())\n                        );\n                    }\n                }\n            } else {\n                logger.debug(\"Ignoring vote response {} since we are no longer a candidate in epoch {}\",\n                    partitionResponse, quorum.epoch());\n            }\n            return true;\n        } else {\n            return handleUnexpectedError(error, responseMetadata);\n        }\n    }\n\n    private int binaryExponentialElectionBackoffMs(int retries) {\n        if (retries <= 0) {\n            throw new IllegalArgumentException(\"Retries \" + retries + \" should be larger than zero\");\n        }\n        // upper limit exponential co-efficients at 20 to avoid overflow\n        return Math.min(RETRY_BACKOFF_BASE_MS * random.nextInt(2 << Math.min(20, retries - 1)),\n                quorumConfig.electionBackoffMaxMs());\n    }\n\n    private int strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors) {\n        if (positionInSuccessors <= 0 || positionInSuccessors >= totalNumSuccessors) {\n            throw new IllegalArgumentException(\"Position \" + positionInSuccessors + \" should be larger than zero\" +\n                    \" and smaller than total number of successors \" + totalNumSuccessors);\n        }\n\n        int retryBackOffBaseMs = quorumConfig.electionBackoffMaxMs() >> (totalNumSuccessors - 1);\n        return Math.min(quorumConfig.electionBackoffMaxMs(), retryBackOffBaseMs << (positionInSuccessors - 1));\n    }\n\n    private BeginQuorumEpochResponseData buildBeginQuorumEpochResponse(Errors partitionLevelError) {\n        return BeginQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel());\n    }\n\n    /**\n     * Handle a BeginEpoch request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     */\n    private BeginQuorumEpochResponseData handleBeginQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        BeginQuorumEpochRequestData request = (BeginQuorumEpochRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new BeginQuorumEpochResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return new BeginQuorumEpochResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        BeginQuorumEpochRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int requestLeaderId = partitionRequest.leaderId();\n        int requestEpoch = partitionRequest.leaderEpoch();\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(requestLeaderId, requestEpoch);\n        if (errorOpt.isPresent()) {\n            return buildBeginQuorumEpochResponse(errorOpt.get());\n        }\n\n        maybeTransition(OptionalInt.of(requestLeaderId), requestEpoch, currentTimeMs);\n        return buildBeginQuorumEpochResponse(Errors.NONE);\n    }\n\n    private boolean handleBeginQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        int remoteNodeId = responseMetadata.sourceId();\n        BeginQuorumEpochResponseData response = (BeginQuorumEpochResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        BeginQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors partitionError = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            partitionError, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (partitionError == Errors.NONE) {\n            if (quorum.isLeader()) {\n                LeaderState<T> state = quorum.leaderStateOrThrow();\n                state.addAcknowledgementFrom(remoteNodeId);\n            } else {\n                logger.debug(\"Ignoring BeginQuorumEpoch response {} since \" +\n                    \"this node is not the leader anymore\", response);\n            }\n            return true;\n        } else {\n            return handleUnexpectedError(partitionError, responseMetadata);\n        }\n    }\n\n    private EndQuorumEpochResponseData buildEndQuorumEpochResponse(Errors partitionLevelError) {\n        return EndQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            log.topicPartition(),\n            partitionLevelError,\n            quorum.epoch(),\n            quorum.leaderIdOrSentinel());\n    }\n\n    /**\n     * Handle an EndEpoch request. This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *      but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#INCONSISTENT_VOTER_SET} if the request suggests inconsistent voter membership (e.g.\n     *      if this node or the sender is not one of the current known voters)\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     */\n    private EndQuorumEpochResponseData handleEndQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        EndQuorumEpochRequestData request = (EndQuorumEpochRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return new EndQuorumEpochResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return new EndQuorumEpochResponseData().setErrorCode(Errors.INVALID_REQUEST.code());\n        }\n\n        EndQuorumEpochRequestData.PartitionData partitionRequest =\n            request.topics().get(0).partitions().get(0);\n\n        int requestEpoch = partitionRequest.leaderEpoch();\n        int requestLeaderId = partitionRequest.leaderId();\n\n        Optional<Errors> errorOpt = validateVoterOnlyRequest(requestLeaderId, requestEpoch);\n        if (errorOpt.isPresent()) {\n            return buildEndQuorumEpochResponse(errorOpt.get());\n        }\n        maybeTransition(OptionalInt.of(requestLeaderId), requestEpoch, currentTimeMs);\n\n        if (quorum.isFollower()) {\n            FollowerState state = quorum.followerStateOrThrow();\n            if (state.leaderId() == requestLeaderId) {\n                List<Integer> preferredSuccessors = partitionRequest.preferredSuccessors();\n                long electionBackoffMs = endEpochElectionBackoff(preferredSuccessors);\n                logger.debug(\"Overriding follower fetch timeout to {} after receiving \" +\n                    \"EndQuorumEpoch request from leader {} in epoch {}\", electionBackoffMs,\n                    requestLeaderId, requestEpoch);\n                state.overrideFetchTimeout(currentTimeMs, electionBackoffMs);\n            }\n        }\n        return buildEndQuorumEpochResponse(Errors.NONE);\n    }\n\n    private long endEpochElectionBackoff(List<Integer> preferredSuccessors) {\n        // Based on the priority inside the preferred successors, choose the corresponding delayed\n        // election backoff time based on strict exponential mechanism so that the most up-to-date\n        // voter has a higher chance to be elected. If the node's priority is highest, become\n        // candidate immediately instead of waiting for next poll.\n        int position = preferredSuccessors.indexOf(quorum.localIdOrThrow());\n        if (position <= 0) {\n            return 0;\n        } else {\n            return strictExponentialElectionBackoffMs(position, preferredSuccessors.size());\n        }\n    }\n\n    private boolean handleEndQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        EndQuorumEpochResponseData response = (EndQuorumEpochResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!hasValidTopicPartition(response, log.topicPartition())) {\n            return false;\n        }\n\n        EndQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        Errors partitionError = Errors.forCode(partitionResponse.errorCode());\n        OptionalInt responseLeaderId = optionalLeaderId(partitionResponse.leaderId());\n        int responseEpoch = partitionResponse.leaderEpoch();\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            partitionError, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        } else if (partitionError == Errors.NONE) {\n            ResignedState resignedState = quorum.resignedStateOrThrow();\n            resignedState.acknowledgeResignation(responseMetadata.sourceId());\n            return true;\n        } else {\n            return handleUnexpectedError(partitionError, responseMetadata);\n        }\n    }\n\n    private FetchResponseData buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    ) {\n        return RaftUtil.singletonFetchResponse(log.topicPartition(), log.topicId(), Errors.NONE, partitionData -> {\n            partitionData\n                .setRecords(records)\n                .setErrorCode(error.code())\n                .setLogStartOffset(log.startOffset())\n                .setHighWatermark(\n                    highWatermark.map(offsetMetadata -> offsetMetadata.offset).orElse(-1L)\n                );\n\n            partitionData.currentLeader()\n                .setLeaderEpoch(quorum.epoch())\n                .setLeaderId(quorum.leaderIdOrSentinel());\n\n            switch (validOffsetAndEpoch.kind()) {\n                case DIVERGING:\n                    partitionData.divergingEpoch()\n                        .setEpoch(validOffsetAndEpoch.offsetAndEpoch().epoch())\n                        .setEndOffset(validOffsetAndEpoch.offsetAndEpoch().offset());\n                    break;\n                case SNAPSHOT:\n                    partitionData.snapshotId()\n                        .setEpoch(validOffsetAndEpoch.offsetAndEpoch().epoch())\n                        .setEndOffset(validOffsetAndEpoch.offsetAndEpoch().offset());\n                    break;\n                default:\n            }\n        });\n    }\n\n    private FetchResponseData buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    ) {\n        return buildFetchResponse(\n            error,\n            MemoryRecords.EMPTY,\n            ValidOffsetAndEpoch.valid(),\n            highWatermark\n        );\n    }\n\n    private boolean hasValidClusterId(String requestClusterId) {\n        // We don't enforce the cluster id if it is not provided.\n        if (requestClusterId == null) {\n            return true;\n        }\n        return clusterId.equals(requestClusterId);\n    }\n\n    /**\n     * Handle a Fetch request. The fetch offset and last fetched epoch are always\n     * validated against the current log. In the case that they do not match, the response will\n     * indicate the diverging offset/epoch. A follower is expected to truncate its log in this\n     * case and resend the fetch.\n     *\n     * This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *     but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INVALID_REQUEST} if the request epoch is larger than the leader's current epoch\n     *     or if either the fetch offset or the last fetched epoch is invalid\n     */\n    private CompletableFuture<FetchResponseData> handleFetchRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        FetchRequestData request = (FetchRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(request.clusterId())) {\n            return completedFuture(new FetchResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code()));\n        }\n\n        if (!hasValidTopicPartition(request, log.topicPartition(), log.topicId())) {\n            // Until we support multi-raft, we treat topic partition mismatches as invalid requests\n            return completedFuture(new FetchResponseData().setErrorCode(Errors.INVALID_REQUEST.code()));\n        }\n        // If the ID is valid, we can set the topic name.\n        request.topics().get(0).setTopic(log.topicPartition().topic());\n\n        FetchRequestData.FetchPartition fetchPartition = request.topics().get(0).partitions().get(0);\n        if (request.maxWaitMs() < 0\n            || fetchPartition.fetchOffset() < 0\n            || fetchPartition.lastFetchedEpoch() < 0\n            || fetchPartition.lastFetchedEpoch() > fetchPartition.currentLeaderEpoch()) {\n            return completedFuture(\n                buildEmptyFetchResponse(Errors.INVALID_REQUEST, Optional.empty())\n            );\n        }\n\n        int replicaId = FetchRequest.replicaId(request);\n        FetchResponseData response = tryCompleteFetchRequest(replicaId, fetchPartition, currentTimeMs);\n        FetchResponseData.PartitionData partitionResponse =\n            response.responses().get(0).partitions().get(0);\n\n        if (partitionResponse.errorCode() != Errors.NONE.code()\n            || FetchResponse.recordsSize(partitionResponse) > 0\n            || request.maxWaitMs() == 0\n            || isPartitionDiverged(partitionResponse)\n            || isPartitionSnapshotted(partitionResponse)) {\n            // Reply immediately if any of the following is true\n            // 1. The response contains an error\n            // 2. There are records in the response\n            // 3. The fetching replica doesn't want to wait for the partition to contain new data\n            // 4. The fetching replica needs to truncate because the log diverged\n            // 5. The fetching replica needs to fetch a snapshot\n            return completedFuture(response);\n        }\n\n        CompletableFuture<Long> future = fetchPurgatory.await(\n            fetchPartition.fetchOffset(),\n            request.maxWaitMs());\n\n        return future.handle((completionTimeMs, exception) -> {\n            if (exception != null) {\n                Throwable cause = exception instanceof ExecutionException ?\n                    exception.getCause() : exception;\n\n                Errors error = Errors.forException(cause);\n                if (error == Errors.REQUEST_TIMED_OUT) {\n                    // Note that for this case the calling thread is the expiration service thread and not the\n                    // polling thread.\n                    //\n                    // If the fetch request timed out in purgatory, it means no new data is available,\n                    // just return the original fetch response.\n                    return response;\n                } else {\n                    // If there was any error other than REQUEST_TIMED_OUT, return it.\n                    logger.info(\"Failed to handle fetch from {} at {} due to {}\",\n                        replicaId, fetchPartition.fetchOffset(), error);\n                    return buildEmptyFetchResponse(error, Optional.empty());\n                }\n            }\n\n            // FIXME: `completionTimeMs`, which can be null\n            logger.trace(\"Completing delayed fetch from {} starting at offset {} at {}\",\n                replicaId, fetchPartition.fetchOffset(), completionTimeMs);\n\n            // It is safe to call tryCompleteFetchRequest because only the polling thread completes this\n            // future successfully. This is true because only the polling thread appends record batches to\n            // the log from maybeAppendBatches.\n            return tryCompleteFetchRequest(replicaId, fetchPartition, time.milliseconds());\n        });\n    }\n\n    private FetchResponseData tryCompleteFetchRequest(\n        int replicaId,\n        FetchRequestData.FetchPartition request,\n        long currentTimeMs\n    ) {\n        try {\n            Optional<Errors> errorOpt = validateLeaderOnlyRequest(request.currentLeaderEpoch());\n            if (errorOpt.isPresent()) {\n                return buildEmptyFetchResponse(errorOpt.get(), Optional.empty());\n            }\n\n            long fetchOffset = request.fetchOffset();\n            int lastFetchedEpoch = request.lastFetchedEpoch();\n            LeaderState<T> state = quorum.leaderStateOrThrow();\n\n            Optional<OffsetAndEpoch> latestSnapshotId = log.latestSnapshotId();\n            final ValidOffsetAndEpoch validOffsetAndEpoch;\n            if (fetchOffset == 0 && latestSnapshotId.isPresent()) {\n                // If the follower has an empty log and a snapshot exist, it is always more efficient\n                // to reply with a snapshot id (FETCH_SNAPSHOT) instead of fetching from the log segments.\n                validOffsetAndEpoch = ValidOffsetAndEpoch.snapshot(latestSnapshotId.get());\n            } else {\n                validOffsetAndEpoch = log.validateOffsetAndEpoch(fetchOffset, lastFetchedEpoch);\n            }\n\n            final Records records;\n            if (validOffsetAndEpoch.kind() == ValidOffsetAndEpoch.Kind.VALID) {\n                LogFetchInfo info = log.read(fetchOffset, Isolation.UNCOMMITTED);\n\n                if (state.updateReplicaState(replicaId, currentTimeMs, info.startOffsetMetadata)) {\n                    onUpdateLeaderHighWatermark(state, currentTimeMs);\n                }\n\n                records = info.records;\n            } else {\n                records = MemoryRecords.EMPTY;\n            }\n\n            return buildFetchResponse(Errors.NONE, records, validOffsetAndEpoch, state.highWatermark());\n        } catch (Exception e) {\n            logger.error(\"Caught unexpected error in fetch completion of request {}\", request, e);\n            return buildEmptyFetchResponse(Errors.UNKNOWN_SERVER_ERROR, Optional.empty());\n        }\n    }\n\n    private static boolean isPartitionDiverged(FetchResponseData.PartitionData partitionResponseData) {\n        FetchResponseData.EpochEndOffset divergingEpoch = partitionResponseData.divergingEpoch();\n\n        return divergingEpoch.epoch() != -1 || divergingEpoch.endOffset() != -1;\n    }\n\n    private static boolean isPartitionSnapshotted(FetchResponseData.PartitionData partitionResponseData) {\n        FetchResponseData.SnapshotId snapshotId = partitionResponseData.snapshotId();\n\n        return snapshotId.epoch() != -1 || snapshotId.endOffset() != -1;\n    }\n\n    private static OptionalInt optionalLeaderId(int leaderIdOrNil) {\n        if (leaderIdOrNil < 0)\n            return OptionalInt.empty();\n        return OptionalInt.of(leaderIdOrNil);\n    }\n\n    private static String listenerName(Listener<?> listener) {\n        return String.format(\"%s@%d\", listener.getClass().getTypeName(), System.identityHashCode(listener));\n    }\n\n    private boolean handleFetchResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        FetchResponseData response = (FetchResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(response.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (!RaftUtil.hasValidTopicPartition(response, log.topicPartition(), log.topicId())) {\n            return false;\n        }\n        // If the ID is valid, we can set the topic name.\n        response.responses().get(0).setTopic(log.topicPartition().topic());\n\n        FetchResponseData.PartitionData partitionResponse =\n            response.responses().get(0).partitions().get(0);\n\n        FetchResponseData.LeaderIdAndEpoch currentLeaderIdAndEpoch = partitionResponse.currentLeader();\n        OptionalInt responseLeaderId = optionalLeaderId(currentLeaderIdAndEpoch.leaderId());\n        int responseEpoch = currentLeaderIdAndEpoch.leaderEpoch();\n        Errors error = Errors.forCode(partitionResponse.errorCode());\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        }\n\n        FollowerState state = quorum.followerStateOrThrow();\n        if (error == Errors.NONE) {\n            FetchResponseData.EpochEndOffset divergingEpoch = partitionResponse.divergingEpoch();\n            if (divergingEpoch.epoch() >= 0) {\n                // The leader is asking us to truncate before continuing\n                final OffsetAndEpoch divergingOffsetAndEpoch = new OffsetAndEpoch(\n                    divergingEpoch.endOffset(), divergingEpoch.epoch());\n\n                state.highWatermark().ifPresent(highWatermark -> {\n                    if (divergingOffsetAndEpoch.offset() < highWatermark.offset) {\n                        throw new KafkaException(\"The leader requested truncation to offset \" +\n                            divergingOffsetAndEpoch.offset() + \", which is below the current high watermark\" +\n                            \" \" + highWatermark);\n                    }\n                });\n\n                long truncationOffset = log.truncateToEndOffset(divergingOffsetAndEpoch);\n                logger.info(\n                    \"Truncated to offset {} from Fetch response from leader {}\",\n                    truncationOffset,\n                    quorum.leaderIdOrSentinel()\n                );\n\n                // Update the internal listener to the new end offset\n                partitionState.truncateNewEntries(truncationOffset);\n            } else if (partitionResponse.snapshotId().epoch() >= 0 ||\n                       partitionResponse.snapshotId().endOffset() >= 0) {\n                // The leader is asking us to fetch a snapshot\n\n                if (partitionResponse.snapshotId().epoch() < 0) {\n                    logger.error(\n                        \"The leader sent a snapshot id with a valid end offset {} but with an invalid epoch {}\",\n                        partitionResponse.snapshotId().endOffset(),\n                        partitionResponse.snapshotId().epoch()\n                    );\n                    return false;\n                } else if (partitionResponse.snapshotId().endOffset() < 0) {\n                    logger.error(\n                        \"The leader sent a snapshot id with a valid epoch {} but with an invalid end offset {}\",\n                        partitionResponse.snapshotId().epoch(),\n                        partitionResponse.snapshotId().endOffset()\n                    );\n                    return false;\n                } else {\n                    final OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n                        partitionResponse.snapshotId().endOffset(),\n                        partitionResponse.snapshotId().epoch()\n                    );\n\n                    // Do not validate the snapshot id against the local replicated log since this\n                    // snapshot is expected to reference offsets and epochs greater than the log\n                    // end offset and high-watermark.\n                    state.setFetchingSnapshot(log.createNewSnapshotUnchecked(snapshotId));\n                    logger.info(\n                        \"Fetching snapshot {} from Fetch response from leader {}\",\n                        snapshotId,\n                        quorum.leaderIdOrSentinel()\n                    );\n                }\n            } else {\n                Records records = FetchResponse.recordsOrFail(partitionResponse);\n                if (records.sizeInBytes() > 0) {\n                    appendAsFollower(records);\n                }\n\n                OptionalLong highWatermark = partitionResponse.highWatermark() < 0 ?\n                    OptionalLong.empty() : OptionalLong.of(partitionResponse.highWatermark());\n                updateFollowerHighWatermark(state, highWatermark);\n            }\n\n            state.resetFetchTimeout(currentTimeMs);\n            return true;\n        } else {\n            return handleUnexpectedError(error, responseMetadata);\n        }\n    }\n\n    private void appendAsFollower(\n        Records records\n    ) {\n        LogAppendInfo info = log.appendAsFollower(records);\n        if (quorum.isVoter()) {\n            // the leader only requires that voters have flushed their log before sending\n            // a Fetch request\n            log.flush(false);\n        }\n\n        partitionState.updateState();\n\n        OffsetAndEpoch endOffset = endOffset();\n        kafkaRaftMetrics.updateFetchedRecords(info.lastOffset - info.firstOffset + 1);\n        kafkaRaftMetrics.updateLogEnd(endOffset);\n        logger.trace(\"Follower end offset updated to {} after append\", endOffset);\n    }\n\n    private LogAppendInfo appendAsLeader(\n        Records records\n    ) {\n        LogAppendInfo info = log.appendAsLeader(records, quorum.epoch());\n\n        partitionState.updateState();\n\n        OffsetAndEpoch endOffset = endOffset();\n        kafkaRaftMetrics.updateAppendRecords(info.lastOffset - info.firstOffset + 1);\n        kafkaRaftMetrics.updateLogEnd(endOffset);\n        logger.trace(\"Leader appended records at base offset {}, new end offset is {}\", info.firstOffset, endOffset);\n        return info;\n    }\n\n    private DescribeQuorumResponseData handleDescribeQuorumRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        DescribeQuorumRequestData describeQuorumRequestData = (DescribeQuorumRequestData) requestMetadata.data;\n        if (!hasValidTopicPartition(describeQuorumRequestData, log.topicPartition())) {\n            return DescribeQuorumRequest.getPartitionLevelErrorResponse(\n                describeQuorumRequestData, Errors.UNKNOWN_TOPIC_OR_PARTITION);\n        }\n\n        if (!quorum.isLeader()) {\n            return DescribeQuorumResponse.singletonErrorResponse(\n                log.topicPartition(),\n                Errors.NOT_LEADER_OR_FOLLOWER\n            );\n        }\n\n        LeaderState<T> leaderState = quorum.leaderStateOrThrow();\n        return DescribeQuorumResponse.singletonResponse(\n            log.topicPartition(),\n            leaderState.describeQuorum(currentTimeMs)\n        );\n    }\n\n    /**\n     * Handle a FetchSnapshot request, similar to the Fetch request but we use {@link UnalignedRecords}\n     * in response because the records are not necessarily offset-aligned.\n     *\n     * This API may return the following errors:\n     *\n     * - {@link Errors#INCONSISTENT_CLUSTER_ID} if the cluster id is presented in request\n     *     but different from this node\n     * - {@link Errors#BROKER_NOT_AVAILABLE} if this node is currently shutting down\n     * - {@link Errors#FENCED_LEADER_EPOCH} if the epoch is smaller than this node's epoch\n     * - {@link Errors#INVALID_REQUEST} if the request epoch is larger than the leader's current epoch\n     *     or if either the fetch offset or the last fetched epoch is invalid\n     * - {@link Errors#SNAPSHOT_NOT_FOUND} if the request snapshot id does not exists\n     * - {@link Errors#POSITION_OUT_OF_RANGE} if the request snapshot offset out of range\n     */\n    private FetchSnapshotResponseData handleFetchSnapshotRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    ) {\n        FetchSnapshotRequestData data = (FetchSnapshotRequestData) requestMetadata.data;\n\n        if (!hasValidClusterId(data.clusterId())) {\n            return new FetchSnapshotResponseData().setErrorCode(Errors.INCONSISTENT_CLUSTER_ID.code());\n        }\n\n        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n            return FetchSnapshotResponse.withTopLevelError(Errors.INVALID_REQUEST);\n        }\n\n        Optional<FetchSnapshotRequestData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotRequest\n            .forTopicPartition(data, log.topicPartition());\n        if (!partitionSnapshotOpt.isPresent()) {\n            // The Raft client assumes that there is only one topic partition.\n            TopicPartition unknownTopicPartition = new TopicPartition(\n                data.topics().get(0).name(),\n                data.topics().get(0).partitions().get(0).partition()\n            );\n\n            return FetchSnapshotResponse.singleton(\n                unknownTopicPartition,\n                responsePartitionSnapshot -> responsePartitionSnapshot\n                    .setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code())\n            );\n        }\n\n        FetchSnapshotRequestData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n        Optional<Errors> leaderValidation = validateLeaderOnlyRequest(\n                partitionSnapshot.currentLeaderEpoch()\n        );\n        if (leaderValidation.isPresent()) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(leaderValidation.get().code())\n            );\n        }\n\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n            partitionSnapshot.snapshotId().endOffset(),\n            partitionSnapshot.snapshotId().epoch()\n        );\n        Optional<RawSnapshotReader> snapshotOpt = log.readSnapshot(snapshotId);\n        if (!snapshotOpt.isPresent()) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code())\n            );\n        }\n\n        RawSnapshotReader snapshot = snapshotOpt.get();\n        long snapshotSize = snapshot.sizeInBytes();\n        if (partitionSnapshot.position() < 0 || partitionSnapshot.position() >= snapshotSize) {\n            return FetchSnapshotResponse.singleton(\n                log.topicPartition(),\n                responsePartitionSnapshot -> addQuorumLeader(responsePartitionSnapshot)\n                    .setErrorCode(Errors.POSITION_OUT_OF_RANGE.code())\n            );\n        }\n\n        if (partitionSnapshot.position() > Integer.MAX_VALUE) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Trying to fetch a snapshot with size (%d) and a position (%d) larger than %d\",\n                    snapshotSize,\n                    partitionSnapshot.position(),\n                    Integer.MAX_VALUE\n                )\n            );\n        }\n\n        int maxSnapshotSize;\n        try {\n            maxSnapshotSize = Math.toIntExact(snapshotSize);\n        } catch (ArithmeticException e) {\n            maxSnapshotSize = Integer.MAX_VALUE;\n        }\n\n        UnalignedRecords records = snapshot.slice(partitionSnapshot.position(), Math.min(data.maxBytes(), maxSnapshotSize));\n\n        LeaderState<T> state = quorum.leaderStateOrThrow();\n        state.updateCheckQuorumForFollowingVoter(data.replicaId(), currentTimeMs);\n\n        return FetchSnapshotResponse.singleton(\n            log.topicPartition(),\n            responsePartitionSnapshot -> {\n                addQuorumLeader(responsePartitionSnapshot)\n                    .snapshotId()\n                    .setEndOffset(snapshotId.offset())\n                    .setEpoch(snapshotId.epoch());\n\n                return responsePartitionSnapshot\n                    .setSize(snapshotSize)\n                    .setPosition(partitionSnapshot.position())\n                    .setUnalignedRecords(records);\n            }\n        );\n    }\n\n    private boolean handleFetchSnapshotResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    ) {\n        FetchSnapshotResponseData data = (FetchSnapshotResponseData) responseMetadata.data;\n        Errors topLevelError = Errors.forCode(data.errorCode());\n        if (topLevelError != Errors.NONE) {\n            return handleTopLevelError(topLevelError, responseMetadata);\n        }\n\n        if (data.topics().size() != 1 && data.topics().get(0).partitions().size() != 1) {\n            return false;\n        }\n\n        Optional<FetchSnapshotResponseData.PartitionSnapshot> partitionSnapshotOpt = FetchSnapshotResponse\n            .forTopicPartition(data, log.topicPartition());\n        if (!partitionSnapshotOpt.isPresent()) {\n            return false;\n        }\n\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot = partitionSnapshotOpt.get();\n\n        FetchSnapshotResponseData.LeaderIdAndEpoch currentLeaderIdAndEpoch = partitionSnapshot.currentLeader();\n        OptionalInt responseLeaderId = optionalLeaderId(currentLeaderIdAndEpoch.leaderId());\n        int responseEpoch = currentLeaderIdAndEpoch.leaderEpoch();\n        Errors error = Errors.forCode(partitionSnapshot.errorCode());\n\n        Optional<Boolean> handled = maybeHandleCommonResponse(\n            error, responseLeaderId, responseEpoch, currentTimeMs);\n        if (handled.isPresent()) {\n            return handled.get();\n        }\n\n        FollowerState state = quorum.followerStateOrThrow();\n\n        if (Errors.forCode(partitionSnapshot.errorCode()) == Errors.SNAPSHOT_NOT_FOUND ||\n            partitionSnapshot.snapshotId().endOffset() < 0 ||\n            partitionSnapshot.snapshotId().epoch() < 0) {\n\n            /* The leader deleted the snapshot before the follower could download it. Start over by\n             * resetting the fetching snapshot state and sending another fetch request.\n             */\n            logger.info(\n                \"Leader doesn't know about snapshot id {}, returned error {} and snapshot id {}\",\n                state.fetchingSnapshot(),\n                partitionSnapshot.errorCode(),\n                partitionSnapshot.snapshotId()\n            );\n            state.setFetchingSnapshot(Optional.empty());\n            state.resetFetchTimeout(currentTimeMs);\n            return true;\n        }\n\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(\n            partitionSnapshot.snapshotId().endOffset(),\n            partitionSnapshot.snapshotId().epoch()\n        );\n\n        RawSnapshotWriter snapshot;\n        if (state.fetchingSnapshot().isPresent()) {\n            snapshot = state.fetchingSnapshot().get();\n        } else {\n            throw new IllegalStateException(\n                String.format(\"Received unexpected fetch snapshot response: %s\", partitionSnapshot)\n            );\n        }\n\n        if (!snapshot.snapshotId().equals(snapshotId)) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Received fetch snapshot response with an invalid id. Expected %s; Received %s\",\n                    snapshot.snapshotId(),\n                    snapshotId\n                )\n            );\n        }\n        if (snapshot.sizeInBytes() != partitionSnapshot.position()) {\n            throw new IllegalStateException(\n                String.format(\n                    \"Received fetch snapshot response with an invalid position. Expected %d; Received %d\",\n                    snapshot.sizeInBytes(),\n                    partitionSnapshot.position()\n                )\n            );\n        }\n\n        final UnalignedMemoryRecords records;\n        if (partitionSnapshot.unalignedRecords() instanceof MemoryRecords) {\n            records = new UnalignedMemoryRecords(((MemoryRecords) partitionSnapshot.unalignedRecords()).buffer());\n        } else if (partitionSnapshot.unalignedRecords() instanceof UnalignedMemoryRecords) {\n            records = (UnalignedMemoryRecords) partitionSnapshot.unalignedRecords();\n        } else {\n            throw new IllegalStateException(String.format(\"Received unexpected fetch snapshot response: %s\", partitionSnapshot));\n        }\n        snapshot.append(records);\n\n        if (snapshot.sizeInBytes() == partitionSnapshot.size()) {\n            // Finished fetching the snapshot.\n            snapshot.freeze();\n            state.setFetchingSnapshot(Optional.empty());\n\n            if (log.truncateToLatestSnapshot()) {\n                logger.info(\n                    \"Fully truncated the log at ({}, {}) after downloading snapshot {} from leader {}\",\n                    log.endOffset(),\n                    log.lastFetchedEpoch(),\n                    snapshot.snapshotId(),\n                    quorum.leaderIdOrSentinel()\n                );\n\n                // This will always reload the snapshot because the internal next offset\n                // is always less than the snapshot id just downloaded.\n                partitionState.updateState();\n\n                updateFollowerHighWatermark(state, OptionalLong.of(log.highWatermark().offset));\n            } else {\n                throw new IllegalStateException(\n                    String.format(\n                        \"Full log truncation expected but didn't happen. Snapshot of %s, log end offset %s, last fetched %d\",\n                        snapshot.snapshotId(),\n                        log.endOffset(),\n                        log.lastFetchedEpoch()\n                    )\n                );\n            }\n        }\n\n        state.resetFetchTimeout(currentTimeMs);\n        return true;\n    }\n\n    private boolean hasConsistentLeader(int epoch, OptionalInt leaderId) {\n        // Only elected leaders are sent in the request/response header, so if we have an elected\n        // leaderId, it should be consistent with what is in the message.\n        if (leaderId.isPresent() && leaderId.getAsInt() == quorum.localIdOrSentinel()) {\n            // The response indicates that we should be the leader, so we verify that is the case\n            return quorum.isLeader();\n        } else {\n            return epoch != quorum.epoch()\n                || !leaderId.isPresent()\n                || !quorum.leaderId().isPresent()\n                || leaderId.equals(quorum.leaderId());\n        }\n    }\n\n    /**\n     * Handle response errors that are common across request types.\n     *\n     * @param error Error from the received response\n     * @param leaderId Optional leaderId from the response\n     * @param epoch Epoch received from the response\n     * @param currentTimeMs Current epoch time in milliseconds\n     * @return Optional value indicating whether the error was handled here and the outcome of\n     *    that handling. Specifically:\n     *\n     *    - Optional.empty means that the response was not handled here and the custom\n     *        API handler should be applied\n     *    - Optional.of(true) indicates that the response was successfully handled here and\n     *        the request does not need to be retried\n     *    - Optional.of(false) indicates that the response was handled here, but that the request\n     *        will need to be retried\n     */\n    private Optional<Boolean> maybeHandleCommonResponse(\n        Errors error,\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    ) {\n        if (epoch < quorum.epoch() || error == Errors.UNKNOWN_LEADER_EPOCH) {\n            // We have a larger epoch, so the response is no longer relevant\n            return Optional.of(true);\n        } else if (epoch > quorum.epoch()\n            || error == Errors.FENCED_LEADER_EPOCH\n            || error == Errors.NOT_LEADER_OR_FOLLOWER) {\n\n            // The response indicates that the request had a stale epoch, but we need\n            // to validate the epoch from the response against our current state.\n            maybeTransition(leaderId, epoch, currentTimeMs);\n            return Optional.of(true);\n        } else if (epoch == quorum.epoch()\n            && leaderId.isPresent()\n            && !quorum.hasLeader()) {\n\n            // Since we are transitioning to Follower, we will only forward the\n            // request to the handler if there is no error. Otherwise, we will let\n            // the request be retried immediately (if needed) after the transition.\n            // This handling allows an observer to discover the leader and append\n            // to the log in the same Fetch request.\n            transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n            if (error == Errors.NONE) {\n                return Optional.empty();\n            } else {\n                return Optional.of(true);\n            }\n        } else if (error == Errors.BROKER_NOT_AVAILABLE) {\n            return Optional.of(false);\n        } else if (error == Errors.INCONSISTENT_GROUP_PROTOCOL) {\n            // For now we treat this as a fatal error. Once we have support for quorum\n            // reassignment, this error could suggest that either we or the recipient of\n            // the request just has stale voter information, which means we can retry\n            // after backing off.\n            throw new IllegalStateException(\"Received error indicating inconsistent voter sets\");\n        } else if (error == Errors.INVALID_REQUEST) {\n            throw new IllegalStateException(\"Received unexpected invalid request error\");\n        }\n\n        return Optional.empty();\n    }\n\n    private void maybeTransition(\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    ) {\n        if (!hasConsistentLeader(epoch, leaderId)) {\n            throw new IllegalStateException(\"Received request or response with leader \" + leaderId +\n                \" and epoch \" + epoch + \" which is inconsistent with current leader \" +\n                quorum.leaderId() + \" and epoch \" + quorum.epoch());\n        } else if (epoch > quorum.epoch()) {\n            if (leaderId.isPresent()) {\n                transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n            } else {\n                transitionToUnattached(epoch);\n            }\n        } else if (leaderId.isPresent() && !quorum.hasLeader()) {\n            // The request or response indicates the leader of the current epoch,\n            // which is currently unknown\n            transitionToFollower(epoch, leaderId.getAsInt(), currentTimeMs);\n        }\n    }\n\n    private boolean handleTopLevelError(Errors error, RaftResponse.Inbound response) {\n        if (error == Errors.BROKER_NOT_AVAILABLE) {\n            return false;\n        } else if (error == Errors.CLUSTER_AUTHORIZATION_FAILED) {\n            throw new ClusterAuthorizationException(\"Received cluster authorization error in response \" + response);\n        } else {\n            return handleUnexpectedError(error, response);\n        }\n    }\n\n    private boolean handleUnexpectedError(Errors error, RaftResponse.Inbound response) {\n        logger.error(\"Unexpected error {} in {} response: {}\",\n            error, ApiKeys.forId(response.data.apiKey()), response);\n        return false;\n    }\n\n    private void handleResponse(RaftResponse.Inbound response, long currentTimeMs) {\n        // The response epoch matches the local epoch, so we can handle the response\n        ApiKeys apiKey = ApiKeys.forId(response.data.apiKey());\n        final boolean handledSuccessfully;\n\n        switch (apiKey) {\n            case FETCH:\n                handledSuccessfully = handleFetchResponse(response, currentTimeMs);\n                break;\n\n            case VOTE:\n                handledSuccessfully = handleVoteResponse(response, currentTimeMs);\n                break;\n\n            case BEGIN_QUORUM_EPOCH:\n                handledSuccessfully = handleBeginQuorumEpochResponse(response, currentTimeMs);\n                break;\n\n            case END_QUORUM_EPOCH:\n                handledSuccessfully = handleEndQuorumEpochResponse(response, currentTimeMs);\n                break;\n\n            case FETCH_SNAPSHOT:\n                handledSuccessfully = handleFetchSnapshotResponse(response, currentTimeMs);\n                break;\n\n            default:\n                throw new IllegalArgumentException(\"Received unexpected response type: \" + apiKey);\n        }\n\n        ConnectionState connection = requestManager.getOrCreate(response.sourceId());\n        onResponseResult(response, currentTimeMs, handledSuccessfully, connection);\n    }\n\n    private void onResponseResult(RaftResponse.Inbound response, long currentTimeMs, boolean handledSuccessfully, ConnectionState connection) {\n        if (handledSuccessfully) {\n            connection.onResponseReceived(response.correlationId);\n        } else {\n            connection.onResponseError(response.correlationId, currentTimeMs);\n        }\n    }\n\n    /**\n     * Validate common state for requests to establish leadership.\n     *\n     * These include the Vote, BeginQuorumEpoch and EndQuorumEpoch RPCs. If an error is present in\n     * the returned value, it should be returned in the response.\n     */\n    private Optional<Errors> validateVoterOnlyRequest(int remoteNodeId, int requestEpoch) {\n        if (requestEpoch < quorum.epoch()) {\n            return Optional.of(Errors.FENCED_LEADER_EPOCH);\n        } else if (remoteNodeId < 0) {\n            return Optional.of(Errors.INVALID_REQUEST);\n        } else {\n            return Optional.empty();\n        }\n    }\n\n    /**\n     * Validate a request which is intended for the current quorum leader.\n     * If an error is present in the returned value, it should be returned\n     * in the response.\n     */\n    private Optional<Errors> validateLeaderOnlyRequest(int requestEpoch) {\n        if (requestEpoch < quorum.epoch()) {\n            return Optional.of(Errors.FENCED_LEADER_EPOCH);\n        } else if (requestEpoch > quorum.epoch()) {\n            return Optional.of(Errors.UNKNOWN_LEADER_EPOCH);\n        } else if (!quorum.isLeader()) {\n            // In general, non-leaders do not expect to receive requests\n            // matching their own epoch, but it is possible when observers\n            // are using the Fetch API to find the result of an election.\n            return Optional.of(Errors.NOT_LEADER_OR_FOLLOWER);\n        } else if (shutdown.get() != null) {\n            return Optional.of(Errors.BROKER_NOT_AVAILABLE);\n        } else {\n            return Optional.empty();\n        }\n    }\n\n    private void handleRequest(RaftRequest.Inbound request, long currentTimeMs) {\n        ApiKeys apiKey = ApiKeys.forId(request.data.apiKey());\n        final CompletableFuture<? extends ApiMessage> responseFuture;\n\n        switch (apiKey) {\n            case FETCH:\n                responseFuture = handleFetchRequest(request, currentTimeMs);\n                break;\n\n            case VOTE:\n                responseFuture = completedFuture(handleVoteRequest(request));\n                break;\n\n            case BEGIN_QUORUM_EPOCH:\n                responseFuture = completedFuture(handleBeginQuorumEpochRequest(request, currentTimeMs));\n                break;\n\n            case END_QUORUM_EPOCH:\n                responseFuture = completedFuture(handleEndQuorumEpochRequest(request, currentTimeMs));\n                break;\n\n            case DESCRIBE_QUORUM:\n                responseFuture = completedFuture(handleDescribeQuorumRequest(request, currentTimeMs));\n                break;\n\n            case FETCH_SNAPSHOT:\n                responseFuture = completedFuture(handleFetchSnapshotRequest(request, currentTimeMs));\n                break;\n\n            default:\n                throw new IllegalArgumentException(\"Unexpected request type \" + apiKey);\n        }\n\n        responseFuture.whenComplete((response, exception) -> {\n            final ApiMessage message;\n            if (response != null) {\n                message = response;\n            } else {\n                message = RaftUtil.errorResponse(apiKey, Errors.forException(exception));\n            }\n\n            RaftResponse.Outbound responseMessage = new RaftResponse.Outbound(request.correlationId(), message);\n            request.completion.complete(responseMessage);\n            logger.trace(\"Sent response {} to inbound request {}\", responseMessage, request);\n        });\n    }\n\n    private void handleInboundMessage(RaftMessage message, long currentTimeMs) {\n        logger.trace(\"Received inbound message {}\", message);\n\n        if (message instanceof RaftRequest.Inbound) {\n            RaftRequest.Inbound request = (RaftRequest.Inbound) message;\n            handleRequest(request, currentTimeMs);\n        } else if (message instanceof RaftResponse.Inbound) {\n            RaftResponse.Inbound response = (RaftResponse.Inbound) message;\n            ConnectionState connection = requestManager.getOrCreate(response.sourceId());\n            if (connection.isResponseExpected(response.correlationId)) {\n                handleResponse(response, currentTimeMs);\n            } else {\n                logger.debug(\"Ignoring response {} since it is no longer needed\", response);\n            }\n        } else {\n            throw new IllegalArgumentException(\"Unexpected message \" + message);\n        }\n    }\n\n    /**\n     * Attempt to send a request. Return the time to wait before the request can be retried.\n     */\n    private long maybeSendRequest(\n        long currentTimeMs,\n        int destinationId,\n        Supplier<ApiMessage> requestSupplier\n    )  {\n        ConnectionState connection = requestManager.getOrCreate(destinationId);\n\n        if (connection.isBackingOff(currentTimeMs)) {\n            long remainingBackoffMs = connection.remainingBackoffMs(currentTimeMs);\n            logger.debug(\"Connection for {} is backing off for {} ms\", destinationId, remainingBackoffMs);\n            return remainingBackoffMs;\n        }\n\n        if (connection.isReady(currentTimeMs)) {\n            int correlationId = channel.newCorrelationId();\n            ApiMessage request = requestSupplier.get();\n\n            RaftRequest.Outbound requestMessage = new RaftRequest.Outbound(\n                correlationId,\n                request,\n                destinationId,\n                currentTimeMs\n            );\n\n            requestMessage.completion.whenComplete((response, exception) -> {\n                if (exception != null) {\n                    ApiKeys api = ApiKeys.forId(request.apiKey());\n                    Errors error = Errors.forException(exception);\n                    ApiMessage errorResponse = RaftUtil.errorResponse(api, error);\n\n                    response = new RaftResponse.Inbound(\n                        correlationId,\n                        errorResponse,\n                        destinationId\n                    );\n                }\n\n                messageQueue.add(response);\n            });\n\n            channel.send(requestMessage);\n            logger.trace(\"Sent outbound request: {}\", requestMessage);\n            connection.onRequestSent(correlationId, currentTimeMs);\n            return Long.MAX_VALUE;\n        }\n\n        return connection.remainingRequestTimeMs(currentTimeMs);\n    }\n\n    private EndQuorumEpochRequestData buildEndQuorumEpochRequest(\n        ResignedState state\n    ) {\n        return EndQuorumEpochRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow(),\n            state.preferredSuccessors()\n        );\n    }\n\n    private long maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    ) {\n        long minBackoffMs = Long.MAX_VALUE;\n        for (Integer destinationId : destinationIds) {\n            long backoffMs = maybeSendRequest(currentTimeMs, destinationId, requestSupplier);\n            if (backoffMs < minBackoffMs) {\n                minBackoffMs = backoffMs;\n            }\n        }\n        return minBackoffMs;\n    }\n\n    private BeginQuorumEpochRequestData buildBeginQuorumEpochRequest() {\n        return BeginQuorumEpochRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow()\n        );\n    }\n\n    private VoteRequestData buildVoteRequest() {\n        OffsetAndEpoch endOffset = endOffset();\n        return VoteRequest.singletonRequest(\n            log.topicPartition(),\n            clusterId,\n            quorum.epoch(),\n            quorum.localIdOrThrow(),\n            endOffset.epoch(),\n            endOffset.offset()\n        );\n    }\n\n    private FetchRequestData buildFetchRequest() {\n        FetchRequestData request = RaftUtil.singletonFetchRequest(log.topicPartition(), log.topicId(), fetchPartition -> {\n            fetchPartition\n                .setCurrentLeaderEpoch(quorum.epoch())\n                .setLastFetchedEpoch(log.lastFetchedEpoch())\n                .setFetchOffset(log.endOffset().offset);\n        });\n        return request\n            .setMaxBytes(MAX_FETCH_SIZE_BYTES)\n            .setMaxWaitMs(fetchMaxWaitMs)\n            .setClusterId(clusterId)\n            .setReplicaState(new FetchRequestData.ReplicaState().setReplicaId(quorum.localIdOrSentinel()));\n    }\n\n    private long maybeSendAnyVoterFetch(long currentTimeMs) {\n        OptionalInt readyVoterIdOpt = requestManager.findReadyVoter(currentTimeMs);\n        if (readyVoterIdOpt.isPresent()) {\n            return maybeSendRequest(\n                currentTimeMs,\n                readyVoterIdOpt.getAsInt(),\n                this::buildFetchRequest\n            );\n        } else {\n            return requestManager.backoffBeforeAvailableVoter(currentTimeMs);\n        }\n    }\n\n    private FetchSnapshotRequestData buildFetchSnapshotRequest(OffsetAndEpoch snapshotId, long snapshotSize) {\n        FetchSnapshotRequestData.SnapshotId requestSnapshotId = new FetchSnapshotRequestData.SnapshotId()\n            .setEpoch(snapshotId.epoch())\n            .setEndOffset(snapshotId.offset());\n\n        FetchSnapshotRequestData request = FetchSnapshotRequest.singleton(\n            clusterId,\n            quorum().localIdOrSentinel(),\n            log.topicPartition(),\n            snapshotPartition -> snapshotPartition\n                .setCurrentLeaderEpoch(quorum.epoch())\n                .setSnapshotId(requestSnapshotId)\n                .setPosition(snapshotSize)\n        );\n\n        return request.setReplicaId(quorum.localIdOrSentinel());\n    }\n\n    private FetchSnapshotResponseData.PartitionSnapshot addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    ) {\n        partitionSnapshot.currentLeader()\n            .setLeaderEpoch(quorum.epoch())\n            .setLeaderId(quorum.leaderIdOrSentinel());\n\n        return partitionSnapshot;\n    }\n\n    public boolean isRunning() {\n        GracefulShutdown gracefulShutdown = shutdown.get();\n        return gracefulShutdown == null || !gracefulShutdown.isFinished();\n    }\n\n    public boolean isShuttingDown() {\n        GracefulShutdown gracefulShutdown = shutdown.get();\n        return gracefulShutdown != null && !gracefulShutdown.isFinished();\n    }\n\n    private void appendBatch(\n        LeaderState<T> state,\n        BatchAccumulator.CompletedBatch<T> batch,\n        long appendTimeMs\n    ) {\n        try {\n            int epoch = state.epoch();\n            LogAppendInfo info = appendAsLeader(batch.data);\n            OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(info.lastOffset, epoch);\n            CompletableFuture<Long> future = appendPurgatory.await(\n                offsetAndEpoch.offset() + 1, Integer.MAX_VALUE);\n\n            future.whenComplete((commitTimeMs, exception) -> {\n                if (exception != null) {\n                    logger.debug(\"Failed to commit {} records up to last offset {}\", batch.numRecords, offsetAndEpoch, exception);\n                } else {\n                    long elapsedTime = Math.max(0, commitTimeMs - appendTimeMs);\n                    double elapsedTimePerRecord = (double) elapsedTime / batch.numRecords;\n                    kafkaRaftMetrics.updateCommitLatency(elapsedTimePerRecord, appendTimeMs);\n                    logger.debug(\"Completed commit of {} records up to last offset {}\", batch.numRecords, offsetAndEpoch);\n                    batch.records.ifPresent(records -> {\n                        maybeFireHandleCommit(batch.baseOffset, epoch, batch.appendTimestamp(), batch.sizeInBytes(), records);\n                    });\n                }\n            });\n        } finally {\n            batch.release();\n        }\n    }\n\n    private long maybeAppendBatches(\n        LeaderState<T> state,\n        long currentTimeMs\n    ) {\n        long timeUntilDrain = state.accumulator().timeUntilDrain(currentTimeMs);\n        if (timeUntilDrain <= 0) {\n            List<BatchAccumulator.CompletedBatch<T>> batches = state.accumulator().drain();\n            Iterator<BatchAccumulator.CompletedBatch<T>> iterator = batches.iterator();\n\n            try {\n                while (iterator.hasNext()) {\n                    BatchAccumulator.CompletedBatch<T> batch = iterator.next();\n                    appendBatch(state, batch, currentTimeMs);\n                }\n                flushLeaderLog(state, currentTimeMs);\n            } finally {\n                // Release and discard any batches which failed to be appended\n                while (iterator.hasNext()) {\n                    iterator.next().release();\n                }\n            }\n        }\n        return timeUntilDrain;\n    }\n\n    private long pollResigned(long currentTimeMs) {\n        ResignedState state = quorum.resignedStateOrThrow();\n        long endQuorumBackoffMs = maybeSendRequests(\n            currentTimeMs,\n            state.unackedVoters(),\n            () -> buildEndQuorumEpochRequest(state)\n        );\n\n        GracefulShutdown shutdown = this.shutdown.get();\n        final long stateTimeoutMs;\n        if (shutdown != null) {\n            // If we are shutting down, then we will remain in the resigned state\n            // until either the shutdown expires or an election bumps the epoch\n            stateTimeoutMs = shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            stateTimeoutMs = 0L;\n        } else {\n            stateTimeoutMs = state.remainingElectionTimeMs(currentTimeMs);\n        }\n\n        return Math.min(stateTimeoutMs, endQuorumBackoffMs);\n    }\n\n    private long pollLeader(long currentTimeMs) {\n        LeaderState<T> state = quorum.leaderStateOrThrow();\n        maybeFireLeaderChange(state);\n\n        long timeUntilCheckQuorumExpires = state.timeUntilCheckQuorumExpires(currentTimeMs);\n        if (shutdown.get() != null || state.isResignRequested() || timeUntilCheckQuorumExpires == 0) {\n            transitionToResigned(state.nonLeaderVotersByDescendingFetchOffset());\n            return 0L;\n        }\n\n        long timeUntilFlush = maybeAppendBatches(\n            state,\n            currentTimeMs\n        );\n\n        long timeUntilSend = maybeSendRequests(\n            currentTimeMs,\n            state.nonAcknowledgingVoters(),\n            this::buildBeginQuorumEpochRequest\n        );\n\n        return Math.min(timeUntilFlush, Math.min(timeUntilSend, timeUntilCheckQuorumExpires));\n    }\n\n    private long maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    ) {\n        // Continue sending Vote requests as long as we still have a chance to win the election\n        if (!state.isVoteRejected()) {\n            return maybeSendRequests(\n                currentTimeMs,\n                state.unrecordedVoters(),\n                this::buildVoteRequest\n            );\n        }\n        return Long.MAX_VALUE;\n    }\n\n    private long pollCandidate(long currentTimeMs) {\n        CandidateState state = quorum.candidateStateOrThrow();\n        GracefulShutdown shutdown = this.shutdown.get();\n\n        if (shutdown != null) {\n            // If we happen to shutdown while we are a candidate, we will continue\n            // with the current election until one of the following conditions is met:\n            //  1) we are elected as leader (which allows us to resign)\n            //  2) another leader is elected\n            //  3) the shutdown timer expires\n            long minRequestBackoffMs = maybeSendVoteRequests(state, currentTimeMs);\n            return Math.min(shutdown.remainingTimeMs(), minRequestBackoffMs);\n        } else if (state.isBackingOff()) {\n            if (state.isBackoffComplete(currentTimeMs)) {\n                logger.info(\"Re-elect as candidate after election backoff has completed\");\n                transitionToCandidate(currentTimeMs);\n                return 0L;\n            }\n            return state.remainingBackoffMs(currentTimeMs);\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            long backoffDurationMs = binaryExponentialElectionBackoffMs(state.retries());\n            logger.info(\"Election has timed out, backing off for {}ms before becoming a candidate again\",\n                backoffDurationMs);\n            state.startBackingOff(currentTimeMs, backoffDurationMs);\n            return backoffDurationMs;\n        } else {\n            long minRequestBackoffMs = maybeSendVoteRequests(state, currentTimeMs);\n            return Math.min(minRequestBackoffMs, state.remainingElectionTimeMs(currentTimeMs));\n        }\n    }\n\n    private long pollFollower(long currentTimeMs) {\n        FollowerState state = quorum.followerStateOrThrow();\n        if (quorum.isVoter()) {\n            return pollFollowerAsVoter(state, currentTimeMs);\n        } else {\n            return pollFollowerAsObserver(state, currentTimeMs);\n        }\n    }\n\n    private long pollFollowerAsVoter(FollowerState state, long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown != null) {\n            // If we are a follower, then we can shutdown immediately. We want to\n            // skip the transition to candidate in any case.\n            return 0;\n        } else if (state.hasFetchTimeoutExpired(currentTimeMs)) {\n            logger.info(\"Become candidate due to fetch timeout\");\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            long backoffMs = maybeSendFetchOrFetchSnapshot(state, currentTimeMs);\n\n            return Math.min(backoffMs, state.remainingFetchTimeMs(currentTimeMs));\n        }\n    }\n\n    private long pollFollowerAsObserver(FollowerState state, long currentTimeMs) {\n        if (state.hasFetchTimeoutExpired(currentTimeMs)) {\n            return maybeSendAnyVoterFetch(currentTimeMs);\n        } else {\n            final long backoffMs;\n\n            // If the current leader is backing off due to some failure or if the\n            // request has timed out, then we attempt to send the Fetch to another\n            // voter in order to discover if there has been a leader change.\n            ConnectionState connection = requestManager.getOrCreate(state.leaderId());\n            if (connection.hasRequestTimedOut(currentTimeMs)) {\n                backoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n                connection.reset();\n            } else if (connection.isBackingOff(currentTimeMs)) {\n                backoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n            } else {\n                backoffMs = maybeSendFetchOrFetchSnapshot(state, currentTimeMs);\n            }\n\n            return Math.min(backoffMs, state.remainingFetchTimeMs(currentTimeMs));\n        }\n    }\n\n    private long maybeSendFetchOrFetchSnapshot(FollowerState state, long currentTimeMs) {\n        final Supplier<ApiMessage> requestSupplier;\n\n        if (state.fetchingSnapshot().isPresent()) {\n            RawSnapshotWriter snapshot = state.fetchingSnapshot().get();\n            long snapshotSize = snapshot.sizeInBytes();\n\n            requestSupplier = () -> buildFetchSnapshotRequest(snapshot.snapshotId(), snapshotSize);\n        } else {\n            requestSupplier = this::buildFetchRequest;\n        }\n\n        return maybeSendRequest(currentTimeMs, state.leaderId(), requestSupplier);\n    }\n\n    private long pollVoted(long currentTimeMs) {\n        VotedState state = quorum.votedStateOrThrow();\n        GracefulShutdown shutdown = this.shutdown.get();\n\n        if (shutdown != null) {\n            // If shutting down, then remain in this state until either the\n            // shutdown completes or an epoch bump forces another state transition\n            return shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            return state.remainingElectionTimeMs(currentTimeMs);\n        }\n    }\n\n    private long pollUnattached(long currentTimeMs) {\n        UnattachedState state = quorum.unattachedStateOrThrow();\n        if (quorum.isVoter()) {\n            return pollUnattachedAsVoter(state, currentTimeMs);\n        } else {\n            return pollUnattachedAsObserver(state, currentTimeMs);\n        }\n    }\n\n    private long pollUnattachedAsVoter(UnattachedState state, long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown != null) {\n            // If shutting down, then remain in this state until either the\n            // shutdown completes or an epoch bump forces another state transition\n            return shutdown.remainingTimeMs();\n        } else if (state.hasElectionTimeoutExpired(currentTimeMs)) {\n            transitionToCandidate(currentTimeMs);\n            return 0L;\n        } else {\n            return state.remainingElectionTimeMs(currentTimeMs);\n        }\n    }\n\n    private long pollUnattachedAsObserver(UnattachedState state, long currentTimeMs) {\n        long fetchBackoffMs = maybeSendAnyVoterFetch(currentTimeMs);\n        return Math.min(fetchBackoffMs, state.remainingElectionTimeMs(currentTimeMs));\n    }\n\n    private long pollCurrentState(long currentTimeMs) {\n        if (quorum.isLeader()) {\n            return pollLeader(currentTimeMs);\n        } else if (quorum.isCandidate()) {\n            return pollCandidate(currentTimeMs);\n        } else if (quorum.isFollower()) {\n            return pollFollower(currentTimeMs);\n        } else if (quorum.isVoted()) {\n            return pollVoted(currentTimeMs);\n        } else if (quorum.isUnattached()) {\n            return pollUnattached(currentTimeMs);\n        } else if (quorum.isResigned()) {\n            return pollResigned(currentTimeMs);\n        } else {\n            throw new IllegalStateException(\"Unexpected quorum state \" + quorum);\n        }\n    }\n\n    private void pollListeners() {\n        // Apply all of the pending registration\n        while (true) {\n            Registration<T> registration = pendingRegistrations.poll();\n            if (registration == null) {\n                break;\n            }\n\n            processRegistration(registration);\n        }\n\n        // Check listener progress to see if reads are expected\n        quorum.highWatermark().ifPresent(highWatermarkMetadata -> {\n            updateListenersProgress(highWatermarkMetadata.offset);\n        });\n\n        // Notify the new listeners of the latest leader and epoch\n        Optional<LeaderState<T>> leaderState = quorum.maybeLeaderState();\n        if (leaderState.isPresent()) {\n            maybeFireLeaderChange(leaderState.get());\n        } else {\n            maybeFireLeaderChange();\n        }\n    }\n\n    private void processRegistration(Registration<T> registration) {\n        Listener<T> listener = registration.listener();\n        Registration.Ops ops = registration.ops();\n\n        if (ops == Registration.Ops.REGISTER) {\n            if (listenerContexts.putIfAbsent(listener, new ListenerContext(listener)) != null) {\n                logger.error(\"Attempting to add a listener that already exists: {}\", listenerName(listener));\n            } else {\n                logger.info(\"Registered the listener {}\", listenerName(listener));\n            }\n        } else {\n            if (listenerContexts.remove(listener) == null) {\n                logger.error(\"Attempting to remove a listener that doesn't exists: {}\", listenerName(listener));\n            } else {\n                logger.info(\"Unregistered the listener {}\", listenerName(listener));\n            }\n        }\n    }\n\n    private boolean maybeCompleteShutdown(long currentTimeMs) {\n        GracefulShutdown shutdown = this.shutdown.get();\n        if (shutdown == null) {\n            return false;\n        }\n\n        shutdown.update(currentTimeMs);\n        if (shutdown.hasTimedOut()) {\n            shutdown.failWithTimeout();\n            return true;\n        }\n\n        if (quorum.isObserver()\n            || quorum.isOnlyVoter()\n            || quorum.hasRemoteLeader()\n        ) {\n            shutdown.complete();\n            return true;\n        }\n\n        return false;\n    }\n\n    /**\n     * A simple timer based log cleaner\n     */\n    private static class RaftMetadataLogCleanerManager {\n        private final Logger logger;\n        private final Timer timer;\n        private final long delayMs;\n        private final Runnable cleaner;\n\n        RaftMetadataLogCleanerManager(Logger logger, Time time, long delayMs, Runnable cleaner) {\n            this.logger = logger;\n            this.timer = time.timer(delayMs);\n            this.delayMs = delayMs;\n            this.cleaner = cleaner;\n        }\n\n        public long maybeClean(long currentTimeMs) {\n            timer.update(currentTimeMs);\n            if (timer.isExpired()) {\n                try {\n                    cleaner.run();\n                } catch (Throwable t) {\n                    logger.error(\"Had an error during log cleaning\", t);\n                }\n                timer.reset(delayMs);\n            }\n            return timer.remainingMs();\n        }\n    }\n\n    private void wakeup() {\n        messageQueue.wakeup();\n    }\n\n    /**\n     * Handle an inbound request. The response will be returned through\n     * {@link RaftRequest.Inbound#completion}.\n     *\n     * @param request The inbound request\n     */\n    public void handle(RaftRequest.Inbound request) {\n        messageQueue.add(Objects.requireNonNull(request));\n    }\n\n    /**\n     * Poll for new events. This allows the client to handle inbound\n     * requests and send any needed outbound requests.\n     */\n    public void poll() {\n        if (!isInitialized()) {\n            throw new IllegalStateException(\"Replica needs to be initialized before polling\");\n        }\n\n        long startPollTimeMs = time.milliseconds();\n        if (maybeCompleteShutdown(startPollTimeMs)) {\n            return;\n        }\n\n        long pollStateTimeoutMs = pollCurrentState(startPollTimeMs);\n        long cleaningTimeoutMs = snapshotCleaner.maybeClean(startPollTimeMs);\n        long pollTimeoutMs = Math.min(pollStateTimeoutMs, cleaningTimeoutMs);\n\n        long startWaitTimeMs = time.milliseconds();\n        kafkaRaftMetrics.updatePollStart(startWaitTimeMs);\n\n        RaftMessage message = messageQueue.poll(pollTimeoutMs);\n\n        long endWaitTimeMs = time.milliseconds();\n        kafkaRaftMetrics.updatePollEnd(endWaitTimeMs);\n\n        if (message != null) {\n            handleInboundMessage(message, endWaitTimeMs);\n        }\n\n        pollListeners();\n    }\n\n    @Override\n    public long scheduleAppend(int epoch, List<T> records) {\n        return append(epoch, records, OptionalLong.empty(), false);\n    }\n\n    @Override\n    public long scheduleAtomicAppend(int epoch, OptionalLong requiredBaseOffset, List<T> records) {\n        return append(epoch, records, requiredBaseOffset, true);\n    }\n\n    private long append(int epoch, List<T> records, OptionalLong requiredBaseOffset, boolean isAtomic) {\n        if (!isInitialized()) {\n            throw new NotLeaderException(\"Append failed because the replica is not the current leader\");\n        }\n\n        LeaderState<T> leaderState = quorum.<T>maybeLeaderState().orElseThrow(\n            () -> new NotLeaderException(\"Append failed because the replica is not the current leader\")\n        );\n\n        BatchAccumulator<T> accumulator = leaderState.accumulator();\n        boolean isFirstAppend = accumulator.isEmpty();\n        final long offset = accumulator.append(epoch, records, requiredBaseOffset, isAtomic);\n\n        // Wakeup the network channel if either this is the first append\n        // or the accumulator is ready to drain now. Checking for the first\n        // append ensures that we give the IO thread a chance to observe\n        // the linger timeout so that it can schedule its own wakeup in case\n        // there are no additional appends.\n        if (isFirstAppend || accumulator.needsDrain(time.milliseconds())) {\n            wakeup();\n        }\n        return offset;\n    }\n\n    @Override\n    public CompletableFuture<Void> shutdown(int timeoutMs) {\n        logger.info(\"Beginning graceful shutdown\");\n        CompletableFuture<Void> shutdownComplete = new CompletableFuture<>();\n        shutdown.set(new GracefulShutdown(timeoutMs, shutdownComplete));\n        wakeup();\n        return shutdownComplete;\n    }\n\n    @Override\n    public void resign(int epoch) {\n        if (epoch < 0) {\n            throw new IllegalArgumentException(\"Attempt to resign from an invalid negative epoch \" + epoch);\n        } else if (!isInitialized()) {\n            throw new IllegalStateException(\"Replica needs to be initialized before resigning\");\n        } else if (!quorum.isVoter()) {\n            throw new IllegalStateException(\"Attempt to resign by a non-voter\");\n        }\n\n        LeaderAndEpoch leaderAndEpoch = leaderAndEpoch();\n        int currentEpoch = leaderAndEpoch.epoch();\n\n        if (epoch > currentEpoch) {\n            throw new IllegalArgumentException(\"Attempt to resign from epoch \" + epoch +\n                \" which is larger than the current epoch \" + currentEpoch);\n        } else if (epoch < currentEpoch) {\n            // If the passed epoch is smaller than the current epoch, then it might mean\n            // that the listener has not been notified about a leader change that already\n            // took place. In this case, we consider the call as already fulfilled and\n            // take no further action.\n            logger.debug(\"Ignoring call to resign from epoch {} since it is smaller than the \" +\n                \"current epoch {}\", epoch, currentEpoch);\n        } else if (!leaderAndEpoch.isLeader(quorum.localIdOrThrow())) {\n            throw new IllegalArgumentException(\"Cannot resign from epoch \" + epoch +\n                \" since we are not the leader\");\n        } else {\n            // Note that if we transition to another state before we have a chance to\n            // request resignation, then we consider the call fulfilled.\n            Optional<LeaderState<Object>> leaderStateOpt = quorum.maybeLeaderState();\n            if (!leaderStateOpt.isPresent()) {\n                logger.debug(\"Ignoring call to resign from epoch {} since this node is \" +\n                    \"no longer the leader\", epoch);\n                return;\n            }\n\n            LeaderState<Object> leaderState = leaderStateOpt.get();\n            if (leaderState.epoch() != epoch) {\n                logger.debug(\"Ignoring call to resign from epoch {} since it is smaller than the \" +\n                    \"current epoch {}\", epoch, leaderState.epoch());\n            } else {\n                logger.info(\"Received user request to resign from the current epoch {}\", currentEpoch);\n                leaderState.requestResign();\n                wakeup();\n            }\n        }\n    }\n\n    @Override\n    public Optional<SnapshotWriter<T>> createSnapshot(\n        OffsetAndEpoch snapshotId,\n        long lastContainedLogTimestamp\n    ) {\n        if (!isInitialized()) {\n            throw new IllegalStateException(\"Cannot create snapshot before the replica has been initialized\");\n        }\n\n        return log.createNewSnapshot(snapshotId).map(writer -> {\n            long lastContainedLogOffset = snapshotId.offset() - 1;\n\n            RawSnapshotWriter wrappedWriter = new NotifyingRawSnapshotWriter(writer, offsetAndEpoch -> {\n                // Trim the state in the internal listener up to the new snapshot\n                partitionState.truncateOldEntries(offsetAndEpoch.offset());\n            });\n\n            return new RecordsSnapshotWriter.Builder()\n                .setLastContainedLogTimestamp(lastContainedLogTimestamp)\n                .setTime(time)\n                .setMaxBatchSize(MAX_BATCH_SIZE_BYTES)\n                .setMemoryPool(memoryPool)\n                .setRawSnapshotWriter(wrappedWriter)\n                .setKraftVersion(partitionState.kraftVersionAtOffset(lastContainedLogOffset))\n                .setVoterSet(partitionState.voterSetAtOffset(lastContainedLogOffset))\n                .build(serde);\n        });\n    }\n\n    @Override\n    public Optional<OffsetAndEpoch> latestSnapshotId() {\n        return log.latestSnapshotId();\n    }\n\n    @Override\n    public long logEndOffset() {\n        return log.endOffset().offset;\n    }\n\n    @Override\n    public void close() {\n        log.flush(true);\n        if (kafkaRaftMetrics != null) {\n            kafkaRaftMetrics.close();\n        }\n        if (memoryPool instanceof BatchMemoryPool) {\n            BatchMemoryPool batchMemoryPool = (BatchMemoryPool) memoryPool;\n            batchMemoryPool.releaseRetained();\n        }\n    }\n\n    @Override\n    public OptionalLong highWatermark() {\n        if (isInitialized() && quorum.highWatermark().isPresent()) {\n            return OptionalLong.of(quorum.highWatermark().get().offset);\n        } else {\n            return OptionalLong.empty();\n        }\n    }\n\n    public Optional<Node> voterNode(int id, String listener) {\n        return partitionState.lastVoterSet().voterNode(id, listener);\n    }\n\n    // Visible only for test\n    QuorumState quorum() {\n        // It's okay to return null since this method is only called by tests\n        return quorum;\n    }\n\n    private boolean isInitialized() {\n        return partitionState != null && quorum != null && requestManager != null && kafkaRaftMetrics != null;\n    }\n\n    private class GracefulShutdown {\n        final Timer finishTimer;\n        final CompletableFuture<Void> completeFuture;\n\n        public GracefulShutdown(long shutdownTimeoutMs,\n                                CompletableFuture<Void> completeFuture) {\n            this.finishTimer = time.timer(shutdownTimeoutMs);\n            this.completeFuture = completeFuture;\n        }\n\n        public void update(long currentTimeMs) {\n            finishTimer.update(currentTimeMs);\n        }\n\n        public boolean hasTimedOut() {\n            return finishTimer.isExpired();\n        }\n\n        public boolean isFinished() {\n            return completeFuture.isDone();\n        }\n\n        public long remainingTimeMs() {\n            return finishTimer.remainingMs();\n        }\n\n        public void failWithTimeout() {\n            logger.warn(\"Graceful shutdown timed out after {}ms\", finishTimer.timeoutMs());\n            completeFuture.completeExceptionally(\n                new TimeoutException(\"Timeout expired before graceful shutdown completed\"));\n        }\n\n        public void complete() {\n            logger.info(\"Graceful shutdown completed\");\n            completeFuture.complete(null);\n        }\n    }\n\n    private static final class Registration<T> {\n        private final Ops ops;\n        private final Listener<T> listener;\n\n        private Registration(Ops ops, Listener<T> listener) {\n            this.ops = ops;\n            this.listener = listener;\n        }\n\n        private Ops ops() {\n            return ops;\n        }\n\n        private Listener<T> listener() {\n            return listener;\n        }\n\n        private enum Ops {\n            REGISTER, UNREGISTER\n        }\n\n        private static <T> Registration<T> register(Listener<T> listener) {\n            return new Registration<>(Ops.REGISTER, listener);\n        }\n\n        private static <T> Registration<T> unregister(Listener<T> listener) {\n            return new Registration<>(Ops.UNREGISTER, listener);\n        }\n    }\n\n    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n        private final RaftClient.Listener<T> listener;\n        // This field is used only by the Raft IO thread\n        private LeaderAndEpoch lastFiredLeaderChange = LeaderAndEpoch.UNKNOWN;\n\n        // These fields are visible to both the Raft IO thread and the listener\n        // and are protected through synchronization on this ListenerContext instance\n        private BatchReader<T> lastSent = null;\n        private long nextOffset = 0;\n\n        private ListenerContext(Listener<T> listener) {\n            this.listener = listener;\n        }\n\n        /**\n         * Get the last acked offset, which is one greater than the offset of the\n         * last record which was acked by the state machine.\n         */\n        private synchronized long nextOffset() {\n            return nextOffset;\n        }\n\n        /**\n         * Get the next expected offset, which might be larger than the last acked\n         * offset if there are inflight batches which have not been acked yet.\n         * Note that when fetching from disk, we may not know the last offset of\n         * inflight data until it has been processed by the state machine. In this case,\n         * we delay sending additional data until the state machine has read to the\n         * end and the last offset is determined.\n         */\n        private synchronized OptionalLong nextExpectedOffset() {\n            if (lastSent != null) {\n                OptionalLong lastSentOffset = lastSent.lastOffset();\n                if (lastSentOffset.isPresent()) {\n                    return OptionalLong.of(lastSentOffset.getAsLong() + 1);\n                } else {\n                    return OptionalLong.empty();\n                }\n            } else {\n                return OptionalLong.of(nextOffset);\n            }\n        }\n\n        /**\n         * This API is used when the Listener needs to be notified of a new snapshot. This happens\n         * when the context's next offset is less than the log start offset.\n         */\n        private void fireHandleSnapshot(SnapshotReader<T> reader) {\n            synchronized (this) {\n                nextOffset = reader.snapshotId().offset();\n                lastSent = null;\n            }\n\n            logger.debug(\"Notifying listener {} of snapshot {}\", listenerName(), reader.snapshotId());\n            listener.handleLoadSnapshot(reader);\n        }\n\n        /**\n         * This API is used for committed records that have been received through\n         * replication. In general, followers will write new data to disk before they\n         * know whether it has been committed. Rather than retaining the uncommitted\n         * data in memory, we let the state machine read the records from disk.\n         */\n        private void fireHandleCommit(long baseOffset, Records records) {\n            fireHandleCommit(\n                RecordsBatchReader.of(\n                    baseOffset,\n                    records,\n                    serde,\n                    BufferSupplier.create(),\n                    MAX_BATCH_SIZE_BYTES,\n                    this,\n                    true /* Validate batch CRC*/\n                )\n            );\n        }\n\n        /**\n         * This API is used for committed records originating from {@link #scheduleAppend(int, List)}\n         * or {@link #scheduleAtomicAppend(int, OptionalLong, List)} on this instance. In this case,\n         * we are able to save the original record objects, which saves the need to read them back\n         * from disk. This is a nice optimization for the leader which is typically doing more work\n         * than all of the * followers.\n         */\n        private void fireHandleCommit(\n            long baseOffset,\n            int epoch,\n            long appendTimestamp,\n            int sizeInBytes,\n            List<T> records\n        ) {\n            Batch<T> batch = Batch.data(baseOffset, epoch, appendTimestamp, sizeInBytes, records);\n            MemoryBatchReader<T> reader = MemoryBatchReader.of(Collections.singletonList(batch), this);\n            fireHandleCommit(reader);\n        }\n\n        private String listenerName() {\n            return KafkaRaftClient.listenerName(listener);\n        }\n\n        private void fireHandleCommit(BatchReader<T> reader) {\n            synchronized (this) {\n                this.lastSent = reader;\n            }\n            logger.debug(\n                \"Notifying listener {} of batch for baseOffset {} and lastOffset {}\",\n                listenerName(),\n                reader.baseOffset(),\n                reader.lastOffset()\n            );\n            listener.handleCommit(reader);\n        }\n\n        private void maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            if (shouldFireLeaderChange(leaderAndEpoch)) {\n                lastFiredLeaderChange = leaderAndEpoch;\n                logger.debug(\"Notifying listener {} of leader change {}\", listenerName(), leaderAndEpoch);\n                listener.handleLeaderChange(leaderAndEpoch);\n            }\n        }\n\n        private boolean shouldFireLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            if (leaderAndEpoch.equals(lastFiredLeaderChange)) {\n                return false;\n            } else if (leaderAndEpoch.epoch() > lastFiredLeaderChange.epoch()) {\n                return true;\n            } else {\n                return leaderAndEpoch.leaderId().isPresent() &&\n                    !lastFiredLeaderChange.leaderId().isPresent();\n            }\n        }\n\n        private void maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch, long epochStartOffset) {\n            // If this node is becoming the leader, then we can fire `handleLeaderChange` as soon\n            // as the listener has caught up to the start of the leader epoch. This guarantees\n            // that the state machine has seen the full committed state before it becomes\n            // leader and begins writing to the log.\n            //\n            // Note that the raft client doesn't need to compare nextOffset against the high-watermark\n            // to guarantee that the listener has caught up to the high-watermark. This is true because\n            // the only way nextOffset can be greater than epochStartOffset is for the leader to have\n            // established the new high-watermark (of at least epochStartOffset + 1) and for the listener\n            // to have consumed up to that new high-watermark.\n            if (shouldFireLeaderChange(leaderAndEpoch) && nextOffset() > epochStartOffset) {\n                lastFiredLeaderChange = leaderAndEpoch;\n                listener.handleLeaderChange(leaderAndEpoch);\n            }\n        }\n\n        public synchronized void onClose(BatchReader<T> reader) {\n            OptionalLong lastOffset = reader.lastOffset();\n\n            if (lastOffset.isPresent()) {\n                nextOffset = lastOffset.getAsLong() + 1;\n            }\n\n            if (lastSent == reader) {\n                lastSent = null;\n                wakeup();\n            }\n        }\n    }\n}",
                "methodCount": 136
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "updateFollowerHighWatermark",
                            "method_signature": "private updateFollowerHighWatermark(\n        FollowerState state,\n        OptionalLong highWatermarkOpt\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateLeaderEndOffsetAndTimestamp",
                            "method_signature": "private updateLeaderEndOffsetAndTimestamp(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUpdateLeaderHighWatermark",
                            "method_signature": "private onUpdateLeaderHighWatermark(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateListenersProgress",
                            "method_signature": "private updateListenersProgress(long highWatermark)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "latestSnapshot",
                            "method_signature": "private latestSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireHandleCommit",
                            "method_signature": "private maybeFireHandleCommit(long baseOffset, int epoch, long appendTimestamp, int sizeInBytes, List<T> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderState<T> state)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initialize",
                            "method_signature": "public initialize(\n        Map<Integer, InetSocketAddress> voterAddresses,\n        String listenerName,\n        QuorumStateStore quorumStateStore,\n        Metrics metrics\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endOffset",
                            "method_signature": "private endOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetConnections",
                            "method_signature": "private resetConnections()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeLeader",
                            "method_signature": "private onBecomeLeader(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flushLeaderLog",
                            "method_signature": "private flushLeaderLog(LeaderState<T> state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeTransitionToLeader",
                            "method_signature": "private maybeTransitionToLeader(CandidateState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeCandidate",
                            "method_signature": "private onBecomeCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToCandidate",
                            "method_signature": "private transitionToCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToUnattached",
                            "method_signature": "private transitionToUnattached(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToResigned",
                            "method_signature": "private transitionToResigned(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToVoted",
                            "method_signature": "private transitionToVoted(ReplicaKey candidateKey, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onBecomeFollower",
                            "method_signature": "private onBecomeFollower(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitionToFollower",
                            "method_signature": "private transitionToFollower(\n        int epoch,\n        int leaderId,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteResponse",
                            "method_signature": "private buildVoteResponse(Errors partitionLevelError, boolean voteGranted)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleVoteRequest",
                            "method_signature": "private handleVoteRequest(\n        RaftRequest.Inbound requestMetadata\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleVoteResponse",
                            "method_signature": "private handleVoteResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "binaryExponentialElectionBackoffMs",
                            "method_signature": "private binaryExponentialElectionBackoffMs(int retries)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "strictExponentialElectionBackoffMs",
                            "method_signature": "private strictExponentialElectionBackoffMs(int positionInSuccessors, int totalNumSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBeginQuorumEpochResponse",
                            "method_signature": "private buildBeginQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleBeginQuorumEpochRequest",
                            "method_signature": "private handleBeginQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleBeginQuorumEpochResponse",
                            "method_signature": "private handleBeginQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochResponse",
                            "method_signature": "private buildEndQuorumEpochResponse(Errors partitionLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleEndQuorumEpochRequest",
                            "method_signature": "private handleEndQuorumEpochRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endEpochElectionBackoff",
                            "method_signature": "private endEpochElectionBackoff(List<Integer> preferredSuccessors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleEndQuorumEpochResponse",
                            "method_signature": "private handleEndQuorumEpochResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchResponse",
                            "method_signature": "private buildFetchResponse(\n        Errors error,\n        Records records,\n        ValidOffsetAndEpoch validOffsetAndEpoch,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEmptyFetchResponse",
                            "method_signature": "private buildEmptyFetchResponse(\n        Errors error,\n        Optional<LogOffsetMetadata> highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasValidClusterId",
                            "method_signature": "private hasValidClusterId(String requestClusterId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchRequest",
                            "method_signature": "private handleFetchRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "tryCompleteFetchRequest",
                            "method_signature": "private tryCompleteFetchRequest(\n        int replicaId,\n        FetchRequestData.FetchPartition request,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isPartitionDiverged",
                            "method_signature": "private static isPartitionDiverged(FetchResponseData.PartitionData partitionResponseData)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isPartitionSnapshotted",
                            "method_signature": "private static isPartitionSnapshotted(FetchResponseData.PartitionData partitionResponseData)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "optionalLeaderId",
                            "method_signature": "private static optionalLeaderId(int leaderIdOrNil)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "listenerName",
                            "method_signature": "private static listenerName(Listener<?> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchResponse",
                            "method_signature": "private handleFetchResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsFollower",
                            "method_signature": "private appendAsFollower(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendAsLeader",
                            "method_signature": "private appendAsLeader(\n        Records records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleDescribeQuorumRequest",
                            "method_signature": "private handleDescribeQuorumRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchSnapshotRequest",
                            "method_signature": "private handleFetchSnapshotRequest(\n        RaftRequest.Inbound requestMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleFetchSnapshotResponse",
                            "method_signature": "private handleFetchSnapshotResponse(\n        RaftResponse.Inbound responseMetadata,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasConsistentLeader",
                            "method_signature": "private hasConsistentLeader(int epoch, OptionalInt leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeHandleCommonResponse",
                            "method_signature": "private maybeHandleCommonResponse(\n        Errors error,\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeTransition",
                            "method_signature": "private maybeTransition(\n        OptionalInt leaderId,\n        int epoch,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleTopLevelError",
                            "method_signature": "private handleTopLevelError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleUnexpectedError",
                            "method_signature": "private handleUnexpectedError(Errors error, RaftResponse.Inbound response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleResponse",
                            "method_signature": "private handleResponse(RaftResponse.Inbound response, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onResponseResult",
                            "method_signature": "private onResponseResult(RaftResponse.Inbound response, long currentTimeMs, boolean handledSuccessfully, ConnectionState connection)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateVoterOnlyRequest",
                            "method_signature": "private validateVoterOnlyRequest(int remoteNodeId, int requestEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateLeaderOnlyRequest",
                            "method_signature": "private validateLeaderOnlyRequest(int requestEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleRequest",
                            "method_signature": "private handleRequest(RaftRequest.Inbound request, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleInboundMessage",
                            "method_signature": "private handleInboundMessage(RaftMessage message, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendRequest",
                            "method_signature": "private maybeSendRequest(\n        long currentTimeMs,\n        int destinationId,\n        Supplier<ApiMessage> requestSupplier\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildEndQuorumEpochRequest",
                            "method_signature": "private buildEndQuorumEpochRequest(\n        ResignedState state\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendRequests",
                            "method_signature": "private maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBeginQuorumEpochRequest",
                            "method_signature": "private buildBeginQuorumEpochRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildVoteRequest",
                            "method_signature": "private buildVoteRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchRequest",
                            "method_signature": "private buildFetchRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendAnyVoterFetch",
                            "method_signature": "private maybeSendAnyVoterFetch(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildFetchSnapshotRequest",
                            "method_signature": "private buildFetchSnapshotRequest(OffsetAndEpoch snapshotId, long snapshotSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addQuorumLeader",
                            "method_signature": "private addQuorumLeader(\n        FetchSnapshotResponseData.PartitionSnapshot partitionSnapshot\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunning",
                            "method_signature": "public isRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendBatch",
                            "method_signature": "private appendBatch(\n        LeaderState<T> state,\n        BatchAccumulator.CompletedBatch<T> batch,\n        long appendTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAppendBatches",
                            "method_signature": "private maybeAppendBatches(\n        LeaderState<T> state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollResigned",
                            "method_signature": "private pollResigned(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollLeader",
                            "method_signature": "private pollLeader(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendVoteRequests",
                            "method_signature": "private maybeSendVoteRequests(\n        CandidateState state,\n        long currentTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollCandidate",
                            "method_signature": "private pollCandidate(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollower",
                            "method_signature": "private pollFollower(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollowerAsVoter",
                            "method_signature": "private pollFollowerAsVoter(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollFollowerAsObserver",
                            "method_signature": "private pollFollowerAsObserver(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendFetchOrFetchSnapshot",
                            "method_signature": "private maybeSendFetchOrFetchSnapshot(FollowerState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollVoted",
                            "method_signature": "private pollVoted(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattached",
                            "method_signature": "private pollUnattached(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattachedAsVoter",
                            "method_signature": "private pollUnattachedAsVoter(UnattachedState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUnattachedAsObserver",
                            "method_signature": "private pollUnattachedAsObserver(UnattachedState state, long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollCurrentState",
                            "method_signature": "private pollCurrentState(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollListeners",
                            "method_signature": "private pollListeners()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processRegistration",
                            "method_signature": "private processRegistration(Registration<T> registration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCompleteShutdown",
                            "method_signature": "private maybeCompleteShutdown(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeClean",
                            "method_signature": "public maybeClean(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "wakeup",
                            "method_signature": "private wakeup()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handle",
                            "method_signature": "public handle(RaftRequest.Inbound request)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "poll",
                            "method_signature": "public poll()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "append",
                            "method_signature": "private append(int epoch, List<T> records, OptionalLong requiredBaseOffset, boolean isAtomic)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int id, String listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isInitialized",
                            "method_signature": "private isInitialized()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "update",
                            "method_signature": "public update(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasTimedOut",
                            "method_signature": "public hasTimedOut()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isFinished",
                            "method_signature": "public isFinished()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "remainingTimeMs",
                            "method_signature": "public remainingTimeMs()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "complete",
                            "method_signature": "public complete()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "register",
                            "method_signature": "private static register(Listener<T> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unregister",
                            "method_signature": "private static unregister(Listener<T> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "nextExpectedOffset",
                            "method_signature": "private synchronized nextExpectedOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleSnapshot",
                            "method_signature": "private fireHandleSnapshot(SnapshotReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(long baseOffset, Records records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(\n            long baseOffset,\n            int epoch,\n            long appendTimestamp,\n            int sizeInBytes,\n            List<T> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "listenerName",
                            "method_signature": "private listenerName()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fireHandleCommit",
                            "method_signature": "private fireHandleCommit(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shouldFireLeaderChange",
                            "method_signature": "private shouldFireLeaderChange(LeaderAndEpoch leaderAndEpoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeFireLeaderChange",
                            "method_signature": "private maybeFireLeaderChange(LeaderAndEpoch leaderAndEpoch, long epochStartOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onClose",
                            "method_signature": "public synchronized onClose(BatchReader<T> reader)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "resetConnections",
                            "method_signature": "private resetConnections()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "listenerName",
                            "method_signature": "private static listenerName(Listener<?> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "update",
                            "method_signature": "public update(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasTimedOut",
                            "method_signature": "public hasTimedOut()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unregister",
                            "method_signature": "private static unregister(Listener<T> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "register",
                            "method_signature": "private static register(Listener<T> listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processRegistration",
                            "method_signature": "private processRegistration(Registration<T> registration)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeClean",
                            "method_signature": "public maybeClean(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "remainingTimeMs",
                            "method_signature": "public remainingTimeMs()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSendRequests",
                            "method_signature": "private maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "wakeup",
                            "method_signature": "private wakeup()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isFinished",
                            "method_signature": "public isFinished()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "failWithTimeout",
                            "method_signature": "public failWithTimeout()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasValidClusterId",
                            "method_signature": "private hasValidClusterId(String requestClusterId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private resetConnections()": {
                    "first": {
                        "method_name": "resetConnections",
                        "method_signature": "private resetConnections()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.17377593182990644
                },
                "private static listenerName(Listener<?> listener)": {
                    "first": {
                        "method_name": "listenerName",
                        "method_signature": "private static listenerName(Listener<?> listener)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.19035020981131778
                },
                "public update(long currentTimeMs)": {
                    "first": {
                        "method_name": "update",
                        "method_signature": "public update(long currentTimeMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.20462006414933237
                },
                "public hasTimedOut()": {
                    "first": {
                        "method_name": "hasTimedOut",
                        "method_signature": "public hasTimedOut()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.22183578846651397
                },
                "private static unregister(Listener<T> listener)": {
                    "first": {
                        "method_name": "unregister",
                        "method_signature": "private static unregister(Listener<T> listener)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.22748590706220378
                },
                "private static register(Listener<T> listener)": {
                    "first": {
                        "method_name": "register",
                        "method_signature": "private static register(Listener<T> listener)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.24641642272014283
                },
                "private processRegistration(Registration<T> registration)": {
                    "first": {
                        "method_name": "processRegistration",
                        "method_signature": "private processRegistration(Registration<T> registration)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25642377535248373
                },
                "public maybeClean(long currentTimeMs)": {
                    "first": {
                        "method_name": "maybeClean",
                        "method_signature": "public maybeClean(long currentTimeMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2604874486729821
                },
                "public remainingTimeMs()": {
                    "first": {
                        "method_name": "remainingTimeMs",
                        "method_signature": "public remainingTimeMs()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.27250738409945147
                },
                "private maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    )": {
                    "first": {
                        "method_name": "maybeSendRequests",
                        "method_signature": "private maybeSendRequests(\n        long currentTimeMs,\n        Set<Integer> destinationIds,\n        Supplier<ApiMessage> requestSupplier\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.27473061777428365
                },
                "private wakeup()": {
                    "first": {
                        "method_name": "wakeup",
                        "method_signature": "private wakeup()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2892190156348646
                },
                "public isFinished()": {
                    "first": {
                        "method_name": "isFinished",
                        "method_signature": "public isFinished()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.29121683962211625
                },
                "public failWithTimeout()": {
                    "first": {
                        "method_name": "failWithTimeout",
                        "method_signature": "public failWithTimeout()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2990096310222643
                },
                "public isShuttingDown()": {
                    "first": {
                        "method_name": "isShuttingDown",
                        "method_signature": "public isShuttingDown()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.30130508336580714
                },
                "private hasValidClusterId(String requestClusterId)": {
                    "first": {
                        "method_name": "hasValidClusterId",
                        "method_signature": "private hasValidClusterId(String requestClusterId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3264488112183739
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "638844f833b165d6f9ca52c173858d26b7254fac",
        "url": "https://github.com/apache/kafka/commit/638844f833b165d6f9ca52c173858d26b7254fac",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method private append(producerId long, producerEpoch short, verificationGuard VerificationGuard, records List<U>, replay boolean, event DeferredEvent) : void extracted from public run() : void in class org.apache.kafka.coordinator.group.runtime.CoordinatorRuntime.CoordinatorWriteEvent & moved to class org.apache.kafka.coordinator.group.runtime.CoordinatorRuntime.CoordinatorContext",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 773,
                    "endLine": 883,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public run() : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 788,
                    "endLine": 788,
                    "startColumn": 25,
                    "endColumn": 104,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 790,
                    "endLine": 790,
                    "startColumn": 29,
                    "endColumn": 93,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 792,
                    "endLine": 792,
                    "startColumn": 29,
                    "endColumn": 44,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 802,
                    "endLine": 802,
                    "startColumn": 25,
                    "endColumn": 66,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 849,
                    "endLine": 851,
                    "startColumn": 37,
                    "endColumn": 85,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 830,
                    "endLine": 835,
                    "startColumn": 37,
                    "endColumn": 39,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 789,
                    "endLine": 793,
                    "startColumn": 25,
                    "endColumn": 26,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 826,
                    "endLine": 836,
                    "startColumn": 33,
                    "endColumn": 34,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 823,
                    "endLine": 853,
                    "startColumn": 29,
                    "endColumn": 30,
                    "codeElementType": "FOR_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 805,
                    "endLine": 877,
                    "startColumn": 25,
                    "endColumn": 26,
                    "codeElementType": "TRY_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 872,
                    "endLine": 875,
                    "startColumn": 27,
                    "endColumn": 26,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 784,
                    "endLine": 878,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 789,
                    "endLine": 791,
                    "startColumn": 56,
                    "endColumn": 26,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 791,
                    "endLine": 793,
                    "startColumn": 32,
                    "endColumn": 26,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 826,
                    "endLine": 836,
                    "startColumn": 61,
                    "endColumn": 34,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 823,
                    "endLine": 853,
                    "startColumn": 79,
                    "endColumn": 30,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 784,
                    "endLine": 794,
                    "startColumn": 53,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 794,
                    "endLine": 878,
                    "startColumn": 28,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 848,
                    "endLine": 852,
                    "startColumn": 40,
                    "endColumn": 34,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 869,
                    "endLine": 871,
                    "startColumn": 36,
                    "endColumn": 30,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 871,
                    "endLine": 1008,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "private append(producerId long, producerEpoch short, verificationGuard VerificationGuard, records List<U>, replay boolean, event DeferredEvent) : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 902,
                    "endLine": 902,
                    "startColumn": 21,
                    "endColumn": 92,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 904,
                    "endLine": 904,
                    "startColumn": 25,
                    "endColumn": 82,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 906,
                    "endLine": 906,
                    "startColumn": 25,
                    "endColumn": 46,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 912,
                    "endLine": 912,
                    "startColumn": 17,
                    "endColumn": 58,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 951,
                    "endLine": 953,
                    "startColumn": 21,
                    "endColumn": 82,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 980,
                    "endLine": 985,
                    "startColumn": 29,
                    "endColumn": 31,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 903,
                    "endLine": 907,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 975,
                    "endLine": 987,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 976,
                    "endLine": 986,
                    "startColumn": 25,
                    "endColumn": 26,
                    "codeElementType": "FOR_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 973,
                    "endLine": 1000,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "TRY_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 994,
                    "endLine": 1000,
                    "startColumn": 19,
                    "endColumn": 18,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 895,
                    "endLine": 1007,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 903,
                    "endLine": 905,
                    "startColumn": 52,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 905,
                    "endLine": 907,
                    "startColumn": 28,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 975,
                    "endLine": 987,
                    "startColumn": 33,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 976,
                    "endLine": 986,
                    "startColumn": 66,
                    "endColumn": 26,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 895,
                    "endLine": 909,
                    "startColumn": 36,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 909,
                    "endLine": 1007,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 950,
                    "endLine": 954,
                    "startColumn": 77,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 956,
                    "endLine": 968,
                    "startColumn": 70,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1246,
                    "endLine": 1276,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public run() : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1258,
                    "endLine": 1265,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "context.append(producerId,producerEpoch,verificationGuard,result.records(),result.replayRecords(),this)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 892,
                    "endLine": 892,
                    "startColumn": 17,
                    "endColumn": 97,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 919,
                    "endLine": 919,
                    "startColumn": 21,
                    "endColumn": 41,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 923,
                    "endLine": 928,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 931,
                    "endLine": 931,
                    "startColumn": 17,
                    "endColumn": 86,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 933,
                    "endLine": 937,
                    "startColumn": 21,
                    "endColumn": 24,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 941,
                    "endLine": 945,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 961,
                    "endLine": 961,
                    "startColumn": 21,
                    "endColumn": 41,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 962,
                    "endLine": 967,
                    "startColumn": 21,
                    "endColumn": 23,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 971,
                    "endLine": 971,
                    "startColumn": 17,
                    "endColumn": 56,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 991,
                    "endLine": 991,
                    "startColumn": 25,
                    "endColumn": 61,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 992,
                    "endLine": 992,
                    "startColumn": 25,
                    "endColumn": 51,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 995,
                    "endLine": 995,
                    "startColumn": 21,
                    "endColumn": 97,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 999,
                    "endLine": 999,
                    "startColumn": 21,
                    "endColumn": 41,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1006,
                    "endLine": 1006,
                    "startColumn": 17,
                    "endColumn": 55,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 900,
                    "endLine": 900,
                    "startColumn": 21,
                    "endColumn": 60,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 891,
                    "endLine": 893,
                    "startColumn": 51,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 891,
                    "endLine": 893,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 899,
                    "endLine": 908,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 916,
                    "endLine": 920,
                    "startColumn": 63,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 916,
                    "endLine": 920,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 932,
                    "endLine": 938,
                    "startColumn": 42,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 932,
                    "endLine": 938,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "ENHANCED_FOR_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 950,
                    "endLine": 954,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 990,
                    "endLine": 993,
                    "startColumn": 65,
                    "endColumn": 22,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 990,
                    "endLine": 993,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "ENHANCED_FOR_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 956,
                    "endLine": 968,
                    "startColumn": 17,
                    "endColumn": 18,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 899,
                    "endLine": 901,
                    "startColumn": 43,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 901,
                    "endLine": 908,
                    "startColumn": 24,
                    "endColumn": 18,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 583,
        "extraction_results": {
            "success": true,
            "newCommitHash": "009fda06cd29dc3f6fa3a6a31a7b34eccf3695a2",
            "newBranchName": "extract-append-run-39ffdea"
        },
        "telemetry": {
            "id": "837f7e11-7cd8-49d4-8eda-8c9911a5adab",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 287,
                "lineStart": 636,
                "lineEnd": 922,
                "bodyLineStart": 636,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                "sourceCode": "/**\n     * A coordinator event that modifies the coordinator state.\n     *\n     * @param <T> The type of the response.\n     */\n    class CoordinatorWriteEvent<T> implements CoordinatorEvent, DeferredEvent {\n        /**\n         * The topic partition that this write event is applied to.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The operation name.\n         */\n        final String name;\n\n        /**\n         * The transactional id.\n         */\n        final String transactionalId;\n\n        /**\n         * The producer id.\n         */\n        final long producerId;\n\n        /**\n         * The producer epoch.\n         */\n        final short producerEpoch;\n\n        /**\n         * The verification guard.\n         */\n        final VerificationGuard verificationGuard;\n\n        /**\n         * The write operation to execute.\n         */\n        final CoordinatorWriteOperation<S, T, U> op;\n\n        /**\n         * The future that will be completed with the response\n         * generated by the write operation or an error.\n         */\n        final CompletableFuture<T> future;\n\n        /**\n         * Timeout value for the write operation.\n         */\n        final Duration writeTimeout;\n\n        /**\n         * The operation timeout.\n         */\n        private OperationTimeout operationTimeout = null;\n\n        /**\n         * The result of the write operation. It could be null\n         * if an exception is thrown before it is assigned.\n         */\n        CoordinatorResult<T, U> result;\n\n        /**\n         * The time this event was created.\n         */\n        private final long createdTimeMs;\n\n        /**\n         * Constructor.\n         *\n         * @param name                  The operation name.\n         * @param tp                    The topic partition that the operation is applied to.\n         * @param writeTimeout          The write operation timeout\n         * @param op                    The write operation.\n         */\n        CoordinatorWriteEvent(\n            String name,\n            TopicPartition tp,\n            Duration writeTimeout,\n            CoordinatorWriteOperation<S, T, U> op\n        ) {\n            this(\n                name,\n                tp,\n                null,\n                RecordBatch.NO_PRODUCER_ID,\n                RecordBatch.NO_PRODUCER_EPOCH,\n                VerificationGuard.SENTINEL,\n                writeTimeout,\n                op\n            );\n        }\n\n        /**\n         * Constructor.\n         *\n         * @param name                      The operation name.\n         * @param tp                        The topic partition that the operation is applied to.\n         * @param transactionalId           The transactional id.\n         * @param producerId                The producer id.\n         * @param producerEpoch             The producer epoch.\n         * @param verificationGuard         The verification guard.\n         * @param writeTimeout              The write operation timeout\n         * @param op                        The write operation.\n         */\n        CoordinatorWriteEvent(\n            String name,\n            TopicPartition tp,\n            String transactionalId,\n            long producerId,\n            short producerEpoch,\n            VerificationGuard verificationGuard,\n            Duration writeTimeout,\n            CoordinatorWriteOperation<S, T, U> op\n        ) {\n            this.tp = tp;\n            this.name = name;\n            this.op = op;\n            this.transactionalId = transactionalId;\n            this.producerId = producerId;\n            this.producerEpoch = producerEpoch;\n            this.verificationGuard = verificationGuard;\n            this.future = new CompletableFuture<>();\n            this.createdTimeMs = time.milliseconds();\n            this.writeTimeout = writeTimeout;\n        }\n\n        /**\n         * @return The key used by the CoordinatorEventProcessor to ensure\n         * that events with the same key are not processed concurrently.\n         */\n        @Override\n        public TopicPartition key() {\n            return tp;\n        }\n\n        /**\n         * Called by the CoordinatorEventProcessor when the event is executed.\n         */\n        @Override\n        public void run() {\n            try {\n                // Get the context of the coordinator or fail if the coordinator is not in active state.\n                withActiveContextOrThrow(tp, context -> {\n                    // Execute the operation.\n                    result = op.generateRecordsAndResult(context.coordinator.coordinator());\n\n                    append(context);\n                });\n            } catch (Throwable t) {\n                complete(t);\n            }\n        }\n\n        private void append(CoordinatorContext context) {\n            if (result.records().isEmpty()) {\n                // If the records are empty, it was a read operation after all. In this case,\n                // the response can be returned directly iff there are no pending write operations;\n                // otherwise, the read needs to wait on the last write operation to be completed.\n                OptionalLong pendingOffset = context.deferredEventQueue.highestPendingOffset();\n                if (pendingOffset.isPresent()) {\n                    context.deferredEventQueue.add(pendingOffset.getAsLong(), this);\n                } else {\n                    complete(null);\n                }\n            } else {\n                // If the records are not empty, first, they are applied to the state machine,\n                // second, then are written to the partition/log, and finally, the response\n                // is put into the deferred event queue.\n                long prevLastWrittenOffset = context.coordinator.lastWrittenOffset();\n                LogConfig logConfig = partitionWriter.config(tp);\n                byte magic = logConfig.recordVersion().value;\n                int maxBatchSize = logConfig.maxMessageSize();\n                long currentTimeMs = time.milliseconds();\n                ByteBuffer buffer = context.bufferSupplier.get(Math.min(MIN_BUFFER_SIZE, maxBatchSize));\n\n                try {\n                    MemoryRecordsBuilder builder = new MemoryRecordsBuilder(\n                        buffer,\n                        magic,\n                        compression,\n                        TimestampType.CREATE_TIME,\n                        0L,\n                        currentTimeMs,\n                        producerId,\n                        producerEpoch,\n                        0,\n                        producerId != RecordBatch.NO_PRODUCER_ID,\n                        false,\n                        RecordBatch.NO_PARTITION_LEADER_EPOCH,\n                        maxBatchSize\n                    );\n\n                    // Apply the records to the state machine and add them to the batch.\n                    for (int i = 0; i < result.records().size(); i++) {\n                        U record = result.records().get(i);\n\n                        if (result.replayRecords()) {\n                            // We compute the offset of the record based on the last written offset. The\n                            // coordinator is the single writer to the underlying partition so we can\n                            // deduce it like this.\n                            context.coordinator.replay(\n                                prevLastWrittenOffset + i,\n                                producerId,\n                                producerEpoch,\n                                record\n                            );\n                        }\n\n                        byte[] keyBytes = serializer.serializeKey(record);\n                        byte[] valBytes = serializer.serializeValue(record);\n\n                        if (builder.hasRoomFor(currentTimeMs, keyBytes, valBytes, EMPTY_HEADERS)) {\n                            builder.append(\n                                currentTimeMs,\n                                keyBytes,\n                                valBytes,\n                                EMPTY_HEADERS\n                            );\n                        } else {\n                            throw new RecordTooLargeException(\"Message batch size is \" + builder.estimatedSizeInBytes() +\n                                \" bytes in append to partition \" + tp + \" which exceeds the maximum \" +\n                                \"configured size of \" + maxBatchSize + \".\");\n                        }\n                    }\n\n                    // Write the records to the log and update the last written\n                    // offset.\n                    long offset = partitionWriter.append(\n                        tp,\n                        verificationGuard,\n                        builder.build()\n                    );\n                    context.coordinator.updateLastWrittenOffset(offset);\n\n                    // Add the response to the deferred queue.\n                    if (!future.isDone()) {\n                        context.deferredEventQueue.add(offset, this);\n                        operationTimeout = new OperationTimeout(tp, this, writeTimeout.toMillis());\n                        timer.add(operationTimeout);\n                    } else {\n                        complete(null);\n                    }\n                } catch (Throwable t) {\n                    context.coordinator.revertLastWrittenOffset(prevLastWrittenOffset);\n                    complete(t);\n                } finally {\n                    context.bufferSupplier.release(buffer);\n                }\n            }\n        }\n\n        /**\n         * Completes the future with either the result of the write operation\n         * or the provided exception.\n         *\n         * @param exception The exception to complete the future with.\n         */\n        @Override\n        public void complete(Throwable exception) {\n            CompletableFuture<Void> appendFuture = result != null ? result.appendFuture() : null;\n\n            if (exception == null) {\n                if (appendFuture != null) result.appendFuture().complete(null);\n                future.complete(result.response());\n            } else {\n                if (appendFuture != null) result.appendFuture().completeExceptionally(exception);\n                future.completeExceptionally(exception);\n            }\n\n            if (operationTimeout != null) {\n                operationTimeout.cancel();\n                operationTimeout = null;\n            }\n        }\n\n        @Override\n        public long createdTimeMs() {\n            return this.createdTimeMs;\n        }\n\n        @Override\n        public String toString() {\n            return \"CoordinatorWriteEvent(name=\" + name + \")\";\n        }\n    }",
                "methodCount": 8
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "append",
                            "method_signature": "private append(CoordinatorContext context)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "append",
                            "method_signature": "private append(CoordinatorContext context)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private append(CoordinatorContext context)": {
                    "first": {
                        "method_name": "append",
                        "method_signature": "private append(CoordinatorContext context)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5719484665620506
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "638844f833b165d6f9ca52c173858d26b7254fac",
        "url": "https://github.com/apache/kafka/commit/638844f833b165d6f9ca52c173858d26b7254fac",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method private completeTransaction(producerId long, producerEpoch short, coordinatorEpoch int, result TransactionResult, event DeferredEvent) : void extracted from public run() : void in class org.apache.kafka.coordinator.group.runtime.CoordinatorRuntime.CoordinatorCompleteTransactionEvent & moved to class org.apache.kafka.coordinator.group.runtime.CoordinatorRuntime.CoordinatorContext",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1138,
                    "endLine": 1184,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public run() : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1145,
                    "endLine": 1145,
                    "startColumn": 21,
                    "endColumn": 90,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1148,
                    "endLine": 1152,
                    "startColumn": 25,
                    "endColumn": 27,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1154,
                    "endLine": 1166,
                    "startColumn": 25,
                    "endColumn": 27,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1167,
                    "endLine": 1167,
                    "startColumn": 25,
                    "endColumn": 77,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1170,
                    "endLine": 1170,
                    "startColumn": 29,
                    "endColumn": 74,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1177,
                    "endLine": 1177,
                    "startColumn": 25,
                    "endColumn": 92,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1178,
                    "endLine": 1178,
                    "startColumn": 25,
                    "endColumn": 37,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1147,
                    "endLine": 1179,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "TRY_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1176,
                    "endLine": 1179,
                    "startColumn": 23,
                    "endColumn": 22,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1010,
                    "endLine": 1063,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "private completeTransaction(producerId long, producerEpoch short, coordinatorEpoch int, result TransactionResult, event DeferredEvent) : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1035,
                    "endLine": 1035,
                    "startColumn": 13,
                    "endColumn": 74,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1037,
                    "endLine": 1041,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1043,
                    "endLine": 1055,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1056,
                    "endLine": 1056,
                    "startColumn": 17,
                    "endColumn": 61,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1058,
                    "endLine": 1058,
                    "startColumn": 17,
                    "endColumn": 55,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1060,
                    "endLine": 1060,
                    "startColumn": 17,
                    "endColumn": 76,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1061,
                    "endLine": 1061,
                    "startColumn": 17,
                    "endColumn": 35,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1036,
                    "endLine": 1062,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "TRY_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1059,
                    "endLine": 1062,
                    "startColumn": 15,
                    "endColumn": 14,
                    "codeElementType": "CATCH_CLAUSE",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1531,
                    "endLine": 1554,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public run() : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1538,
                    "endLine": 1544,
                    "startColumn": 21,
                    "endColumn": 22,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "context.completeTransaction(producerId,producerEpoch,coordinatorEpoch,result,this)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1028,
                    "endLine": 1028,
                    "startColumn": 17,
                    "endColumn": 105,
                    "codeElementType": "THROW_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1033,
                    "endLine": 1033,
                    "startColumn": 13,
                    "endColumn": 33,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1027,
                    "endLine": 1029,
                    "startColumn": 51,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                    "startLine": 1027,
                    "endLine": 1029,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 584,
        "extraction_results": {
            "success": true,
            "newCommitHash": "fe9c6822775e39d22c74b77d5df030ccc5b570b0",
            "newBranchName": "extract-completeTransaction-run-39ffdea"
        },
        "telemetry": {
            "id": "c604a668-b119-4c72-8d9f-f94a8839601f",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 166,
                "lineStart": 1054,
                "lineEnd": 1219,
                "bodyLineStart": 1054,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/CoordinatorRuntime.java",
                "sourceCode": "/**\n     * A coordinator event that applies and writes a transaction end marker.\n     */\n    class CoordinatorCompleteTransactionEvent implements CoordinatorEvent, DeferredEvent {\n        /**\n         * The topic partition that this write event is applied to.\n         */\n        final TopicPartition tp;\n\n        /**\n         * The operation name.\n         */\n        final String name;\n\n        /**\n         * The producer id.\n         */\n        final long producerId;\n\n        /**\n         * The producer epoch.\n         */\n        final short producerEpoch;\n\n        /**\n         * The coordinator epoch of the transaction coordinator.\n         */\n        final int coordinatorEpoch;\n\n        /**\n         * The transaction result.\n         */\n        final TransactionResult result;\n\n        /**\n         * Timeout value for the write operation.\n         */\n        final Duration writeTimeout;\n\n        /**\n         * The operation timeout.\n         */\n        private OperationTimeout operationTimeout = null;\n\n        /**\n         * The future that will be completed with the response\n         * generated by the write operation or an error.\n         */\n        final CompletableFuture<Void> future;\n\n        /**\n         * The time this event was created.\n         */\n        private final long createdTimeMs;\n\n        CoordinatorCompleteTransactionEvent(\n            String name,\n            TopicPartition tp,\n            long producerId,\n            short producerEpoch,\n            int coordinatorEpoch,\n            TransactionResult result,\n            Duration writeTimeout\n        ) {\n            this.name = name;\n            this.tp = tp;\n            this.producerId = producerId;\n            this.producerEpoch = producerEpoch;\n            this.coordinatorEpoch = coordinatorEpoch;\n            this.result = result;\n            this.writeTimeout = writeTimeout;\n            this.future = new CompletableFuture<>();\n            this.createdTimeMs = time.milliseconds();\n        }\n\n        /**\n         * @return The key used by the CoordinatorEventProcessor to ensure\n         * that events with the same key are not processed concurrently.\n         */\n        @Override\n        public TopicPartition key() {\n            return tp;\n        }\n\n        /**\n         * Called by the CoordinatorEventProcessor when the event is executed.\n         */\n        @Override\n        public void run() {\n            try {\n                withActiveContextOrThrow(tp, context -> {\n                    long prevLastWrittenOffset = context.coordinator.lastWrittenOffset();\n\n                    completeTransaction(context, prevLastWrittenOffset);\n                });\n            } catch (Throwable t) {\n                complete(t);\n            }\n        }\n\n        private void completeTransaction(CoordinatorContext context, long prevLastWrittenOffset) {\n            try {\n                context.coordinator.replayEndTransactionMarker(\n                    producerId,\n                    producerEpoch,\n                    result\n                );\n\n                long offset = partitionWriter.append(\n                    tp,\n                    VerificationGuard.SENTINEL,\n                    MemoryRecords.withEndTransactionMarker(\n                        time.milliseconds(),\n                        producerId,\n                        producerEpoch,\n                        new EndTransactionMarker(\n                            result == TransactionResult.COMMIT ? ControlRecordType.COMMIT : ControlRecordType.ABORT,\n                            coordinatorEpoch\n                        )\n                    )\n                );\n                context.coordinator.updateLastWrittenOffset(offset);\n\n                if (!future.isDone()) {\n                    context.deferredEventQueue.add(offset, this);\n                    operationTimeout = new OperationTimeout(tp, this, writeTimeout.toMillis());\n                    timer.add(operationTimeout);\n                } else {\n                    complete(null);\n                }\n            } catch (Throwable t) {\n                context.coordinator.revertLastWrittenOffset(prevLastWrittenOffset);\n                complete(t);\n            }\n        }\n\n        /**\n         * Completes the future with either the result of the write operation\n         * or the provided exception.\n         *\n         * @param exception The exception to complete the future with.\n         */\n        @Override\n        public void complete(Throwable exception) {\n            if (exception == null) {\n                future.complete(null);\n            } else {\n                future.completeExceptionally(exception);\n            }\n\n            if (operationTimeout != null) {\n                operationTimeout.cancel();\n                operationTimeout = null;\n            }\n        }\n\n        @Override\n        public long createdTimeMs() {\n            return createdTimeMs;\n        }\n\n        @Override\n        public String toString() {\n            return \"CoordinatorCompleteTransactionEvent(name=\" + name + \")\";\n        }\n    }",
                "methodCount": 7
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "completeTransaction",
                            "method_signature": "private completeTransaction(CoordinatorContext context, long prevLastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "completeTransaction",
                            "method_signature": "private completeTransaction(CoordinatorContext context, long prevLastWrittenOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private completeTransaction(CoordinatorContext context, long prevLastWrittenOffset)": {
                    "first": {
                        "method_name": "completeTransaction",
                        "method_signature": "private completeTransaction(CoordinatorContext context, long prevLastWrittenOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7607440403617549
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "9a239c6142a8f2eb36f1600d1012224c31e58e71",
        "url": "https://github.com/apache/kafka/commit/9a239c6142a8f2eb36f1600d1012224c31e58e71",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method private registerStreamThread(streamThread StreamThread) : void extracted from private createAndAddStreamThread(cacheSizePerThread long, threadIdx int) : StreamThread in class org.apache.kafka.streams.KafkaStreams & moved to class org.apache.kafka.streams.KafkaStreams.StreamStateListener",
            "leftSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 1068,
                    "endLine": 1092,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private createAndAddStreamThread(cacheSizePerThread long, threadIdx int) : StreamThread"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 1089,
                    "endLine": 1089,
                    "startColumn": 9,
                    "endColumn": 69,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 699,
                    "endLine": 701,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "private registerStreamThread(streamThread StreamThread) : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 700,
                    "endLine": 700,
                    "startColumn": 13,
                    "endColumn": 73,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 1064,
                    "endLine": 1088,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private createAndAddStreamThread(cacheSizePerThread long, threadIdx int) : StreamThread"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                    "startLine": 1083,
                    "endLine": 1083,
                    "startColumn": 9,
                    "endColumn": 63,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "streamStateListener.registerStreamThread(streamThread)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 585,
        "extraction_results": {
            "success": true,
            "newCommitHash": "d737539bfd2a4db119995ea4d2d5e7126c5d9c02",
            "newBranchName": "extract-registerStreamThread-createAndAddStreamThread-fc6f8b6"
        },
        "telemetry": {
            "id": "d8107995-f91d-44eb-af72-c900a76b48e6",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 2141,
                "lineStart": 118,
                "lineEnd": 2258,
                "bodyLineStart": 118,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
                "sourceCode": "/**\n * A Kafka client that allows for performing continuous computation on input coming from one or more input topics and\n * sends output to zero, one, or more output topics.\n * <p>\n * The computational logic can be specified either by using the {@link Topology} to define a DAG topology of\n * {@link org.apache.kafka.streams.processor.api.Processor}s or by using the {@link StreamsBuilder} which provides the high-level DSL to define\n * transformations.\n * <p>\n * One {@code KafkaStreams} instance can contain one or more threads specified in the configs for the processing work.\n * <p>\n * A {@code KafkaStreams} instance can co-ordinate with any other instances with the same\n * {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} (whether in the same process, on other processes on this\n * machine, or on remote machines) as a single (possibly distributed) stream processing application.\n * These instances will divide up the work based on the assignment of the input topic partitions so that all partitions\n * are being consumed.\n * If instances are added or fail, all (remaining) instances will rebalance the partition assignment among themselves\n * to balance processing load and ensure that all input topic partitions are processed.\n * <p>\n * Internally a {@code KafkaStreams} instance contains a normal {@link KafkaProducer} and {@link KafkaConsumer} instance\n * that is used for reading input and writing output.\n * <p>\n * A simple example might look like this:\n * <pre>{@code\n * Properties props = new Properties();\n * props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"my-stream-processing-application\");\n * props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n * props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n * props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n *\n * StreamsBuilder builder = new StreamsBuilder();\n * builder.<String, String>stream(\"my-input-topic\").mapValues(value -> String.valueOf(value.length())).to(\"my-output-topic\");\n *\n * KafkaStreams streams = new KafkaStreams(builder.build(), props);\n * streams.start();\n * }</pre>\n *\n * @see org.apache.kafka.streams.StreamsBuilder\n * @see org.apache.kafka.streams.Topology\n */\npublic class KafkaStreams implements AutoCloseable {\n\n    private static final String JMX_PREFIX = \"kafka.streams\";\n\n    // processId is expected to be unique across JVMs and to be used\n    // in userData of the subscription request to allow assignor be aware\n    // of the co-location of stream thread's consumers. It is for internal\n    // usage only and should not be exposed to users at all.\n    private final Time time;\n    private final Logger log;\n    protected final String clientId;\n    private final Metrics metrics;\n    protected final StreamsConfig applicationConfigs;\n    protected final List<StreamThread> threads;\n    protected final StreamsMetadataState streamsMetadataState;\n    private final ScheduledExecutorService stateDirCleaner;\n    private final ScheduledExecutorService rocksDBMetricsRecordingService;\n    protected final Admin adminClient;\n    private final StreamsMetricsImpl streamsMetrics;\n    private final long totalCacheSize;\n    private final StreamStateListener streamStateListener;\n    private final DelegatingStateRestoreListener delegatingStateRestoreListener;\n    private final Map<Long, StreamThread.State> threadState;\n    private final UUID processId;\n    private final KafkaClientSupplier clientSupplier;\n    protected final TopologyMetadata topologyMetadata;\n    private final QueryableStoreProvider queryableStoreProvider;\n    private final DelegatingStandbyUpdateListener delegatingStandbyUpdateListener;\n\n    GlobalStreamThread globalStreamThread;\n    protected StateDirectory stateDirectory = null;\n    private KafkaStreams.StateListener stateListener;\n    private boolean oldHandler;\n    private BiConsumer<Throwable, Boolean> streamsUncaughtExceptionHandler;\n    private final Object changeThreadCount = new Object();\n\n    // container states\n    /**\n     * Kafka Streams states are the possible state that a Kafka Streams instance can be in.\n     * An instance must only be in one state at a time.\n     * The expected state transition with the following defined states is:\n     *\n     * <pre>\n     *                 +--------------+\n     *         +&lt;----- | Created (0)  |\n     *         |       +-----+--------+\n     *         |             |\n     *         |             v\n     *         |       +----+--+------+\n     *         |       | Re-          |\n     *         +&lt;----- | Balancing (1)| --------&gt;+\n     *         |       +-----+-+------+          |\n     *         |             | ^                 |\n     *         |             v |                 |\n     *         |       +--------------+          v\n     *         |       | Running (2)  | --------&gt;+\n     *         |       +------+-------+          |\n     *         |              |                  |\n     *         |              v                  |\n     *         |       +------+-------+     +----+-------+\n     *         +-----&gt; | Pending      |     | Pending    |\n     *                 | Shutdown (3) |     | Error (5)  |\n     *                 +------+-------+     +-----+------+\n     *                        |                   |\n     *                        v                   v\n     *                 +------+-------+     +-----+--------+\n     *                 | Not          |     | Error (6)    |\n     *                 | Running (4)  |     +--------------+\n     *                 +--------------+\n     *\n     *\n     * </pre>\n     * Note the following:\n     * - RUNNING state will transit to REBALANCING if any of its threads is in PARTITION_REVOKED or PARTITIONS_ASSIGNED state\n     * - REBALANCING state will transit to RUNNING if all of its threads are in RUNNING state\n     * - Any state except NOT_RUNNING, PENDING_ERROR or ERROR can go to PENDING_SHUTDOWN (whenever close is called)\n     * - Of special importance: If the global stream thread dies, or all stream threads die (or both) then\n     *   the instance will be in the ERROR state. The user will not need to close it.\n     */\n    public enum State {\n        // Note: if you add a new state, check the below methods and how they are used within Streams to see if\n        // any of them should be updated to include the new state. For example a new shutdown path or terminal\n        // state would likely need to be included in methods like isShuttingDown(), hasCompletedShutdown(), etc.\n        CREATED(1, 3),          // 0\n        REBALANCING(2, 3, 5),   // 1\n        RUNNING(1, 2, 3, 5),    // 2\n        PENDING_SHUTDOWN(4),    // 3\n        NOT_RUNNING,            // 4\n        PENDING_ERROR(6),       // 5\n        ERROR;                  // 6\n\n        private final Set<Integer> validTransitions = new HashSet<>();\n\n        State(final Integer... validTransitions) {\n            this.validTransitions.addAll(Arrays.asList(validTransitions));\n        }\n\n        public boolean hasNotStarted() {\n            return equals(CREATED);\n        }\n\n        public boolean isRunningOrRebalancing() {\n            return equals(RUNNING) || equals(REBALANCING);\n        }\n\n        public boolean isShuttingDown() {\n            return equals(PENDING_SHUTDOWN) || equals(PENDING_ERROR);\n        }\n\n        public boolean hasCompletedShutdown() {\n            return equals(NOT_RUNNING) || equals(ERROR);\n        }\n\n        public boolean hasStartedOrFinishedShuttingDown() {\n            return isShuttingDown() || hasCompletedShutdown();\n        }\n\n        public boolean isValidTransition(final State newState) {\n            return validTransitions.contains(newState.ordinal());\n        }\n    }\n\n    private final Object stateLock = new Object();\n    protected volatile State state = State.CREATED;\n\n    private boolean waitOnState(final State targetState, final long waitMs) {\n        final long begin = time.milliseconds();\n        synchronized (stateLock) {\n            boolean interrupted = false;\n            long elapsedMs = 0L;\n            try {\n                while (state != targetState) {\n                    if (waitMs > elapsedMs) {\n                        final long remainingMs = waitMs - elapsedMs;\n                        try {\n                            stateLock.wait(remainingMs);\n                        } catch (final InterruptedException e) {\n                            interrupted = true;\n                        }\n                    } else {\n                        log.debug(\"Cannot transit to {} within {}ms\", targetState, waitMs);\n                        return false;\n                    }\n                    elapsedMs = time.milliseconds() - begin;\n                }\n            } finally {\n                // Make sure to restore the interruption status before returning.\n                // We do not always own the current thread that executes this method, i.e., we do not know the\n                // interruption policy of the thread. The least we can do is restore the interruption status before\n                // the current thread exits this method.\n                if (interrupted) {\n                    Thread.currentThread().interrupt();\n                }\n            }\n            return true;\n        }\n    }\n\n    /**\n     * Sets the state\n     * @param newState New state\n     */\n    private boolean setState(final State newState) {\n        final State oldState;\n\n        synchronized (stateLock) {\n            oldState = state;\n\n            if (state == State.PENDING_SHUTDOWN && newState != State.NOT_RUNNING) {\n                // when the state is already in PENDING_SHUTDOWN, all other transitions than NOT_RUNNING (due to thread dying) will be\n                // refused but we do not throw exception here, to allow appropriate error handling\n                return false;\n            } else if (state == State.NOT_RUNNING && (newState == State.PENDING_SHUTDOWN || newState == State.NOT_RUNNING)) {\n                // when the state is already in NOT_RUNNING, its transition to PENDING_SHUTDOWN or NOT_RUNNING (due to consecutive close calls)\n                // will be refused but we do not throw exception here, to allow idempotent close calls\n                return false;\n            } else if (state == State.REBALANCING && newState == State.REBALANCING) {\n                // when the state is already in REBALANCING, it should not transit to REBALANCING again\n                return false;\n            } else if (state == State.ERROR && (newState == State.PENDING_ERROR || newState == State.ERROR)) {\n                // when the state is already in ERROR, its transition to PENDING_ERROR or ERROR (due to consecutive close calls)\n                return false;\n            } else if (state == State.PENDING_ERROR && newState != State.ERROR) {\n                // when the state is already in PENDING_ERROR, all other transitions than ERROR (due to thread dying) will be\n                // refused but we do not throw exception here, to allow appropriate error handling\n                return false;\n            } else if (!state.isValidTransition(newState)) {\n                throw new IllegalStateException(\"Stream-client \" + clientId + \": Unexpected state transition from \" + oldState + \" to \" + newState);\n            } else {\n                log.info(\"State transition from {} to {}\", oldState, newState);\n            }\n            state = newState;\n            stateLock.notifyAll();\n        }\n\n        // we need to call the user customized state listener outside the state lock to avoid potential deadlocks\n        if (stateListener != null) {\n            stateListener.onChange(newState, oldState);\n        }\n\n        return true;\n    }\n\n    /**\n     * Return the current {@link State} of this {@code KafkaStreams} instance.\n     *\n     * @return the current state of this Kafka Streams instance\n     */\n    public State state() {\n        return state;\n    }\n\n    protected boolean isRunningOrRebalancing() {\n        synchronized (stateLock) {\n            return state.isRunningOrRebalancing();\n        }\n    }\n\n    protected boolean hasStartedOrFinishedShuttingDown() {\n        synchronized (stateLock) {\n            return state.hasStartedOrFinishedShuttingDown();\n        }\n    }\n\n    protected void validateIsRunningOrRebalancing() {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                throw new StreamsNotStartedException(\"KafkaStreams has not been started, you can retry after calling start()\");\n            }\n            if (!state.isRunningOrRebalancing()) {\n                throw new IllegalStateException(\"KafkaStreams is not running. State is \" + state + \".\");\n            }\n        }\n    }\n\n    /**\n     * Listen to {@link State} change events.\n     */\n    public interface StateListener {\n\n        /**\n         * Called when state changes.\n         *\n         * @param newState new state\n         * @param oldState previous state\n         */\n        void onChange(final State newState, final State oldState);\n    }\n\n    /**\n     * An app can set a single {@link KafkaStreams.StateListener} so that the app is notified when state changes.\n     *\n     * @param listener a new state listener\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     */\n    public void setStateListener(final KafkaStreams.StateListener listener) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                stateListener = listener;\n            } else {\n                throw new IllegalStateException(\"Can only set StateListener before calling start(). Current state is: \" + state);\n            }\n        }\n    }\n\n    /**\n     * Set the handler invoked when an internal {@link StreamsConfig#NUM_STREAM_THREADS_CONFIG stream thread} abruptly\n     * terminates due to an uncaught exception.\n     *\n     * @param uncaughtExceptionHandler the uncaught exception handler for all internal threads; {@code null} deletes the current handler\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     *\n     * @deprecated Since 2.8.0. Use {@link KafkaStreams#setUncaughtExceptionHandler(StreamsUncaughtExceptionHandler)} instead.\n     *\n     */\n    @Deprecated\n    public void setUncaughtExceptionHandler(final Thread.UncaughtExceptionHandler uncaughtExceptionHandler) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                oldHandler = true;\n                processStreamThread(thread -> thread.setUncaughtExceptionHandler(uncaughtExceptionHandler));\n\n                if (globalStreamThread != null) {\n                    globalStreamThread.setUncaughtExceptionHandler(uncaughtExceptionHandler);\n                }\n            } else {\n                throw new IllegalStateException(\"Can only set UncaughtExceptionHandler before calling start(). \" +\n                    \"Current state is: \" + state);\n            }\n        }\n    }\n\n    /**\n     * Set the handler invoked when an internal {@link StreamsConfig#NUM_STREAM_THREADS_CONFIG stream thread}\n     * throws an unexpected exception.\n     * These might be exceptions indicating rare bugs in Kafka Streams, or they\n     * might be exceptions thrown by your code, for example a NullPointerException thrown from your processor logic.\n     * The handler will execute on the thread that produced the exception.\n     * In order to get the thread that threw the exception, use {@code Thread.currentThread()}.\n     * <p>\n     * Note, this handler must be threadsafe, since it will be shared among all threads, and invoked from any\n     * thread that encounters such an exception.\n     *\n     * @param userStreamsUncaughtExceptionHandler the uncaught exception handler of type {@link StreamsUncaughtExceptionHandler} for all internal threads\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     * @throws NullPointerException if userStreamsUncaughtExceptionHandler is null.\n     */\n    public void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler userStreamsUncaughtExceptionHandler) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                Objects.requireNonNull(userStreamsUncaughtExceptionHandler);\n                streamsUncaughtExceptionHandler =\n                    (exception, skipThreadReplacement) ->\n                        handleStreamsUncaughtException(exception, userStreamsUncaughtExceptionHandler, skipThreadReplacement);\n                processStreamThread(thread -> thread.setStreamsUncaughtExceptionHandler(streamsUncaughtExceptionHandler));\n                if (globalStreamThread != null) {\n                    globalStreamThread.setUncaughtExceptionHandler(\n                        exception -> handleStreamsUncaughtException(exception, userStreamsUncaughtExceptionHandler, false)\n                    );\n                }\n                processStreamThread(thread -> thread.setUncaughtExceptionHandler((t, e) -> { }\n                ));\n\n                if (globalStreamThread != null) {\n                    globalStreamThread.setUncaughtExceptionHandler((t, e) -> { }\n                    );\n                }\n            } else {\n                throw new IllegalStateException(\"Can only set UncaughtExceptionHandler before calling start(). \" +\n                    \"Current state is: \" + state);\n            }\n        }\n    }\n\n    private void defaultStreamsUncaughtExceptionHandler(final Throwable throwable, final boolean skipThreadReplacement) {\n        if (oldHandler) {\n            threads.remove(Thread.currentThread());\n            if (throwable instanceof RuntimeException) {\n                throw (RuntimeException) throwable;\n            } else if (throwable instanceof Error) {\n                throw (Error) throwable;\n            } else {\n                throw new RuntimeException(\"Unexpected checked exception caught in the uncaught exception handler\", throwable);\n            }\n        } else {\n            handleStreamsUncaughtException(throwable, t -> SHUTDOWN_CLIENT, skipThreadReplacement);\n        }\n    }\n\n    private void replaceStreamThread(final Throwable throwable) {\n        if (globalStreamThread != null && Thread.currentThread().getName().equals(globalStreamThread.getName())) {\n            log.warn(\"The global thread cannot be replaced. Reverting to shutting down the client.\");\n            log.error(\"Encountered the following exception during processing \" +\n                    \" The streams client is going to shut down now. \", throwable);\n            closeToError();\n        }\n        final StreamThread deadThread = (StreamThread) Thread.currentThread();\n        deadThread.shutdown();\n        addStreamThread();\n        if (throwable instanceof RuntimeException) {\n            throw (RuntimeException) throwable;\n        } else if (throwable instanceof Error) {\n            throw (Error) throwable;\n        } else {\n            throw new RuntimeException(\"Unexpected checked exception caught in the uncaught exception handler\", throwable);\n        }\n    }\n\n    private void handleStreamsUncaughtException(final Throwable throwable,\n                                                final StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler,\n                                                final boolean skipThreadReplacement) {\n        final StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse action = streamsUncaughtExceptionHandler.handle(throwable);\n        if (oldHandler) {\n            log.warn(\"Stream's new uncaught exception handler is set as well as the deprecated old handler.\" +\n                    \"The old handler will be ignored as long as a new handler is set.\");\n        }\n        switch (action) {\n            case REPLACE_THREAD:\n                if (!skipThreadReplacement) {\n                    log.error(\"Replacing thread in the streams uncaught exception handler\", throwable);\n                    replaceStreamThread(throwable);\n                } else {\n                    log.debug(\"Skipping thread replacement for recoverable error\");\n                }\n                break;\n            case SHUTDOWN_CLIENT:\n                log.error(\"Encountered the following exception during processing \" +\n                        \"and the registered exception handler opted to \" + action + \".\" +\n                        \" The streams client is going to shut down now. \", throwable);\n                closeToError();\n                break;\n            case SHUTDOWN_APPLICATION:\n                if (getNumLiveStreamThreads() == 1) {\n                    log.warn(\"Attempt to shut down the application requires adding a thread to communicate the shutdown. No processing will be done on this thread\");\n                    addStreamThread();\n                }\n                if (throwable instanceof Error) {\n                    log.error(\"This option requires running threads to shut down the application.\" +\n                            \"but the uncaught exception was an Error, which means this runtime is no \" +\n                            \"longer in a well-defined state. Attempting to send the shutdown command anyway.\", throwable);\n                }\n                if (Thread.currentThread().equals(globalStreamThread) && getNumLiveStreamThreads() == 0) {\n                    log.error(\"Exception in global thread caused the application to attempt to shutdown.\" +\n                            \" This action will succeed only if there is at least one StreamThread running on this client.\" +\n                            \" Currently there are no running threads so will now close the client.\");\n                    closeToError();\n                    break;\n                }\n                processStreamThread(thread -> thread.sendShutdownRequest(AssignorError.SHUTDOWN_REQUESTED));\n                log.error(\"Encountered the following exception during processing \" +\n                        \"and sent shutdown request for the entire application.\", throwable);\n                break;\n        }\n    }\n\n    /**\n     * Set the listener which is triggered whenever a {@link StateStore} is being restored in order to resume\n     * processing.\n     *\n     * @param globalStateRestoreListener The listener triggered when {@link StateStore} is being restored.\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     */\n    public void setGlobalStateRestoreListener(final StateRestoreListener globalStateRestoreListener) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                delegatingStateRestoreListener.setUserStateRestoreListener(globalStateRestoreListener);\n            } else {\n                throw new IllegalStateException(\"Can only set GlobalStateRestoreListener before calling start(). \" +\n                    \"Current state is: \" + state);\n            }\n        }\n    }\n\n    /**\n     * Set the listener which is triggered whenever a standby task is updated\n     *\n     * @param standbyListener The listener triggered when a standby task is updated.\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has already been started.\n     */\n    public void setStandbyUpdateListener(final StandbyUpdateListener standbyListener) {\n        synchronized (stateLock) {\n            if (state.hasNotStarted()) {\n                this.delegatingStandbyUpdateListener.setUserStandbyListener(standbyListener);\n            } else {\n                throw new IllegalStateException(\"Can only set StandbyUpdateListener before calling start(). \" +\n                        \"Current state is: \" + state);\n            }\n        }\n    }\n\n    /**\n     * Get read-only handle on global metrics registry, including streams client's own metrics plus\n     * its embedded producer, consumer and admin clients' metrics.\n     *\n     * @return Map of all metrics.\n     */\n    public Map<MetricName, ? extends Metric> metrics() {\n        final Map<MetricName, Metric> result = new LinkedHashMap<>();\n        // producer and consumer clients are per-thread\n        processStreamThread(thread -> {\n            result.putAll(thread.producerMetrics());\n            result.putAll(thread.consumerMetrics());\n            // admin client is shared, so we can actually move it\n            // to result.putAll(adminClient.metrics()).\n            // we did it intentionally just for flexibility.\n            result.putAll(thread.adminClientMetrics());\n        });\n        // global thread's consumer client\n        if (globalStreamThread != null) {\n            result.putAll(globalStreamThread.consumerMetrics());\n        }\n        // self streams metrics\n        result.putAll(metrics.metrics());\n        return Collections.unmodifiableMap(result);\n    }\n\n    /**\n     * Class that handles stream thread transitions\n     */\n    final class StreamStateListener implements StreamThread.StateListener {\n        private final Map<Long, StreamThread.State> threadState;\n        private GlobalStreamThread.State globalThreadState;\n        // this lock should always be held before the state lock\n        private final Object threadStatesLock;\n\n        StreamStateListener(final Map<Long, StreamThread.State> threadState,\n                            final GlobalStreamThread.State globalThreadState) {\n            this.threadState = threadState;\n            this.globalThreadState = globalThreadState;\n            this.threadStatesLock = new Object();\n        }\n\n        /**\n         * If all threads are up, including the global thread, set to RUNNING\n         */\n        private void maybeSetRunning() {\n            // state can be transferred to RUNNING if\n            // 1) all threads are either RUNNING or DEAD\n            // 2) thread is pending-shutdown and there are still other threads running\n            final boolean hasRunningThread = threadState.values().stream().anyMatch(s -> s == StreamThread.State.RUNNING);\n            for (final StreamThread.State state : threadState.values()) {\n                if (state == StreamThread.State.PENDING_SHUTDOWN && hasRunningThread) continue;\n                if (state != StreamThread.State.RUNNING && state != StreamThread.State.DEAD) {\n                    return;\n                }\n            }\n\n            // the global state thread is relevant only if it is started. There are cases\n            // when we don't have a global state thread at all, e.g., when we don't have global KTables\n            if (globalThreadState != null && globalThreadState != GlobalStreamThread.State.RUNNING) {\n                return;\n            }\n\n            setState(State.RUNNING);\n        }\n\n\n        @Override\n        public synchronized void onChange(final Thread thread,\n                                          final ThreadStateTransitionValidator abstractNewState,\n                                          final ThreadStateTransitionValidator abstractOldState) {\n            synchronized (threadStatesLock) {\n                // StreamThreads first\n                if (thread instanceof StreamThread) {\n                    final StreamThread.State newState = (StreamThread.State) abstractNewState;\n                    threadState.put(thread.getId(), newState);\n\n                    if (newState == StreamThread.State.PARTITIONS_REVOKED || newState == StreamThread.State.PARTITIONS_ASSIGNED) {\n                        setState(State.REBALANCING);\n                    } else if (newState == StreamThread.State.RUNNING) {\n                        maybeSetRunning();\n                    }\n                } else if (thread instanceof GlobalStreamThread) {\n                    // global stream thread has different invariants\n                    final GlobalStreamThread.State newState = (GlobalStreamThread.State) abstractNewState;\n                    globalThreadState = newState;\n\n                    if (newState == GlobalStreamThread.State.RUNNING) {\n                        maybeSetRunning();\n                    } else if (newState == GlobalStreamThread.State.DEAD) {\n                        if (state != State.PENDING_SHUTDOWN) {\n                            log.error(\"Global thread has died. The streams application or client will now close to ERROR.\");\n                            closeToError();\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    static final class DelegatingStateRestoreListener implements StateRestoreListener {\n        private StateRestoreListener userStateRestoreListener;\n\n        private void throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName) {\n            throw new StreamsException(\n                    String.format(\"Fatal user code error in store restore listener for store %s, partition %s.\",\n                            storeName,\n                            topicPartition),\n                    fatalUserException);\n        }\n\n        void setUserStateRestoreListener(final StateRestoreListener userStateRestoreListener) {\n            this.userStateRestoreListener = userStateRestoreListener;\n        }\n\n        @Override\n        public void onRestoreStart(final TopicPartition topicPartition,\n                                   final String storeName,\n                                   final long startingOffset,\n                                   final long endingOffset) {\n            if (userStateRestoreListener != null) {\n                try {\n                    userStateRestoreListener.onRestoreStart(topicPartition, storeName, startingOffset, endingOffset);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onBatchRestored(final TopicPartition topicPartition,\n                                    final String storeName,\n                                    final long batchEndOffset,\n                                    final long numRestored) {\n            if (userStateRestoreListener != null) {\n                try {\n                    userStateRestoreListener.onBatchRestored(topicPartition, storeName, batchEndOffset, numRestored);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onRestoreEnd(final TopicPartition topicPartition, final String storeName, final long totalRestored) {\n            if (userStateRestoreListener != null) {\n                try {\n                    userStateRestoreListener.onRestoreEnd(topicPartition, storeName, totalRestored);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onRestoreSuspended(final TopicPartition topicPartition, final String storeName, final long totalRestored) {\n            if (userStateRestoreListener != null) {\n                try {\n                    userStateRestoreListener.onRestoreSuspended(topicPartition, storeName, totalRestored);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n    }\n\n    static final class DelegatingStandbyUpdateListener implements StandbyUpdateListener {\n        private StandbyUpdateListener userStandbyListener;\n\n        private void throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName) {\n            throw new StreamsException(\n                    String.format(\"Fatal user code error in standby update listener for store %s, partition %s.\",\n                            storeName,\n                            topicPartition),\n                    fatalUserException);\n        }\n\n        @Override\n        public void onUpdateStart(final TopicPartition topicPartition,\n                          final String storeName, \n                          final long startingOffset) {\n            if (userStandbyListener != null) {\n                try {\n                    userStandbyListener.onUpdateStart(topicPartition, storeName, startingOffset);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onBatchLoaded(final TopicPartition topicPartition, final String storeName, final TaskId taskId, final long batchEndOffset, final long batchSize, final long currentEndOffset) {\n            if (userStandbyListener != null) {\n                try {\n                    userStandbyListener.onBatchLoaded(topicPartition, storeName, taskId, batchEndOffset, batchSize, currentEndOffset);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        @Override\n        public void onUpdateSuspended(final TopicPartition topicPartition, final String storeName, final long storeOffset, final long currentEndOffset, final SuspendReason reason) {\n            if (userStandbyListener != null) {\n                try {\n                    userStandbyListener.onUpdateSuspended(topicPartition, storeName, storeOffset, currentEndOffset, reason);\n                } catch (final Exception fatalUserException) {\n                    throwOnFatalException(fatalUserException, topicPartition, storeName);\n                }\n            }\n        }\n\n        void setUserStandbyListener(final StandbyUpdateListener userStandbyListener) {\n            this.userStandbyListener = userStandbyListener;\n        }\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology the topology specifying the computational logic\n     * @param props    properties for {@link StreamsConfig}\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final Properties props) {\n        this(topology, new StreamsConfig(props));\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param props          properties for {@link StreamsConfig}\n     * @param clientSupplier the Kafka clients supplier which provides underlying producer and consumer clients\n     *                       for the new {@code KafkaStreams} instance\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final Properties props,\n                        final KafkaClientSupplier clientSupplier) {\n        this(topology, new StreamsConfig(props), clientSupplier, Time.SYSTEM);\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param props          properties for {@link StreamsConfig}\n     * @param time           {@code Time} implementation; cannot be null\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final Properties props,\n                        final Time time) {\n        this(topology, new StreamsConfig(props), time);\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param props          properties for {@link StreamsConfig}\n     * @param clientSupplier the Kafka clients supplier which provides underlying producer and consumer clients\n     *                       for the new {@code KafkaStreams} instance\n     * @param time           {@code Time} implementation; cannot be null\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final Properties props,\n                        final KafkaClientSupplier clientSupplier,\n                        final Time time) {\n        this(topology, new StreamsConfig(props), clientSupplier, time);\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology  the topology specifying the computational logic\n     * @param applicationConfigs    configs for Kafka Streams\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final StreamsConfig applicationConfigs) {\n        this(topology, applicationConfigs, applicationConfigs.getKafkaClientSupplier());\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param applicationConfigs         configs for Kafka Streams\n     * @param clientSupplier the Kafka clients supplier which provides underlying producer and consumer clients\n     *                       for the new {@code KafkaStreams} instance\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final StreamsConfig applicationConfigs,\n                        final KafkaClientSupplier clientSupplier) {\n        this(new TopologyMetadata(topology.internalTopologyBuilder, applicationConfigs), applicationConfigs, clientSupplier);\n    }\n\n    /**\n     * Create a {@code KafkaStreams} instance.\n     * <p>\n     * Note: even if you never call {@link #start()} on a {@code KafkaStreams} instance,\n     * you still must {@link #close()} it to avoid resource leaks.\n     *\n     * @param topology       the topology specifying the computational logic\n     * @param applicationConfigs         configs for Kafka Streams\n     * @param time           {@code Time} implementation; cannot be null\n     * @throws StreamsException if any fatal error occurs\n     */\n    public KafkaStreams(final Topology topology,\n                        final StreamsConfig applicationConfigs,\n                        final Time time) {\n        this(new TopologyMetadata(topology.internalTopologyBuilder, applicationConfigs), applicationConfigs, applicationConfigs.getKafkaClientSupplier(), time);\n    }\n\n    private KafkaStreams(final Topology topology,\n                         final StreamsConfig applicationConfigs,\n                         final KafkaClientSupplier clientSupplier,\n                         final Time time) throws StreamsException {\n        this(new TopologyMetadata(topology.internalTopologyBuilder, applicationConfigs), applicationConfigs, clientSupplier, time);\n    }\n\n    protected KafkaStreams(final TopologyMetadata topologyMetadata,\n                           final StreamsConfig applicationConfigs,\n                           final KafkaClientSupplier clientSupplier) throws StreamsException {\n        this(topologyMetadata, applicationConfigs, clientSupplier, Time.SYSTEM);\n    }\n\n    @SuppressWarnings(\"this-escape\")\n    private KafkaStreams(final TopologyMetadata topologyMetadata,\n                         final StreamsConfig applicationConfigs,\n                         final KafkaClientSupplier clientSupplier,\n                         final Time time) throws StreamsException {\n        this.applicationConfigs = applicationConfigs;\n        this.time = time;\n\n        this.topologyMetadata = topologyMetadata;\n        this.topologyMetadata.buildAndRewriteTopology();\n\n        final boolean hasGlobalTopology = topologyMetadata.hasGlobalTopology();\n\n        try {\n            stateDirectory = new StateDirectory(applicationConfigs, time, topologyMetadata.hasPersistentStores(), topologyMetadata.hasNamedTopologies());\n            processId = stateDirectory.initializeProcessId();\n        } catch (final ProcessorStateException fatal) {\n            Utils.closeQuietly(stateDirectory, \"streams state directory\");\n            throw new StreamsException(fatal);\n        }\n\n        // The application ID is a required config and hence should always have value\n        final String userClientId = applicationConfigs.getString(StreamsConfig.CLIENT_ID_CONFIG);\n        final String applicationId = applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n        if (userClientId.length() <= 0) {\n            clientId = applicationId + \"-\" + processId;\n        } else {\n            clientId = userClientId;\n        }\n        final LogContext logContext = new LogContext(String.format(\"stream-client [%s] \", clientId));\n        this.log = logContext.logger(getClass());\n        topologyMetadata.setLog(logContext);\n\n        // use client id instead of thread client id since this admin client may be shared among threads\n        this.clientSupplier = clientSupplier;\n        adminClient = clientSupplier.getAdmin(applicationConfigs.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId)));\n\n        log.info(\"Kafka Streams version: {}\", ClientMetrics.version());\n        log.info(\"Kafka Streams commit ID: {}\", ClientMetrics.commitId());\n\n        metrics = getMetrics(applicationConfigs, time, clientId);\n        streamsMetrics = new StreamsMetricsImpl(\n            metrics,\n            clientId,\n            applicationConfigs.getString(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG),\n            time\n        );\n\n        ClientMetrics.addVersionMetric(streamsMetrics);\n        ClientMetrics.addCommitIdMetric(streamsMetrics);\n        ClientMetrics.addApplicationIdMetric(streamsMetrics, applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG));\n        ClientMetrics.addTopologyDescriptionMetric(streamsMetrics, (metricsConfig, now) -> this.topologyMetadata.topologyDescriptionString());\n        ClientMetrics.addStateMetric(streamsMetrics, (metricsConfig, now) -> state);\n        threads = Collections.synchronizedList(new LinkedList<>());\n        ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) -> getNumLiveStreamThreads());\n\n        streamsMetadataState = new StreamsMetadataState(\n            this.topologyMetadata,\n            parseHostInfo(applicationConfigs.getString(StreamsConfig.APPLICATION_SERVER_CONFIG)),\n            logContext\n        );\n\n        oldHandler = false;\n        streamsUncaughtExceptionHandler = this::defaultStreamsUncaughtExceptionHandler;\n        delegatingStateRestoreListener = new DelegatingStateRestoreListener();\n        delegatingStandbyUpdateListener = new DelegatingStandbyUpdateListener();\n\n        totalCacheSize = getTotalCacheSize(applicationConfigs);\n        final int numStreamThreads = topologyMetadata.getNumStreamThreads(applicationConfigs);\n        final long cacheSizePerThread = getCacheSizePerThread(numStreamThreads);\n\n        GlobalStreamThread.State globalThreadState = null;\n        if (hasGlobalTopology) {\n            final String globalThreadId = clientId + \"-GlobalStreamThread\";\n            globalStreamThread = new GlobalStreamThread(\n                topologyMetadata.globalTaskTopology(),\n                applicationConfigs,\n                clientSupplier.getGlobalConsumer(applicationConfigs.getGlobalConsumerConfigs(clientId)),\n                stateDirectory,\n                cacheSizePerThread,\n                streamsMetrics,\n                time,\n                globalThreadId,\n                delegatingStateRestoreListener,\n                exception -> defaultStreamsUncaughtExceptionHandler(exception, false)\n            );\n            globalThreadState = globalStreamThread.state();\n        }\n\n        threadState = new HashMap<>(numStreamThreads);\n        streamStateListener = new StreamStateListener(threadState, globalThreadState);\n\n        final GlobalStateStoreProvider globalStateStoreProvider = new GlobalStateStoreProvider(this.topologyMetadata.globalStateStores());\n\n        if (hasGlobalTopology) {\n            globalStreamThread.setStateListener(streamStateListener);\n        }\n\n        queryableStoreProvider = new QueryableStoreProvider(globalStateStoreProvider);\n        for (int i = 1; i <= numStreamThreads; i++) {\n            createAndAddStreamThread(cacheSizePerThread, i);\n        }\n\n        stateDirCleaner = setupStateDirCleaner();\n        rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, applicationConfigs);\n    }\n\n    private StreamThread createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx) {\n        final StreamThread streamThread = StreamThread.create(\n            topologyMetadata,\n            applicationConfigs,\n            clientSupplier,\n            adminClient,\n            processId,\n            clientId,\n            streamsMetrics,\n            time,\n            streamsMetadataState,\n            cacheSizePerThread,\n            stateDirectory,\n            delegatingStateRestoreListener,\n            delegatingStandbyUpdateListener,\n            threadIdx,\n            KafkaStreams.this::closeToError,\n            streamsUncaughtExceptionHandler\n        );\n        streamThread.setStateListener(streamStateListener);\n        threads.add(streamThread);\n        registerStreamThread(streamThread);\n        queryableStoreProvider.addStoreProviderForThread(streamThread.getName(), new StreamThreadStateStoreProvider(streamThread));\n        return streamThread;\n    }\n\n    private void registerStreamThread(StreamThread streamThread) {\n        threadState.put(streamThread.getId(), streamThread.state());\n    }\n\n    static Metrics getMetrics(final StreamsConfig config, final Time time, final String clientId) {\n        final MetricConfig metricConfig = new MetricConfig()\n            .samples(config.getInt(StreamsConfig.METRICS_NUM_SAMPLES_CONFIG))\n            .recordLevel(Sensor.RecordingLevel.forName(config.getString(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG)))\n            .timeWindow(config.getLong(StreamsConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG), TimeUnit.MILLISECONDS);\n        final List<MetricsReporter> reporters = CommonClientConfigs.metricsReporters(clientId, config);\n\n        final MetricsContext metricsContext = new KafkaMetricsContext(JMX_PREFIX,\n                                                                      config.originalsWithPrefix(CommonClientConfigs.METRICS_CONTEXT_PREFIX));\n        return new Metrics(metricConfig, reporters, time, metricsContext);\n    }\n\n    /**\n     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n     * Kafka Streams client.\n     * <p>\n     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n     * threads does not exceed the total cache size specified in configuration\n     * {@link StreamsConfig#STATESTORE_CACHE_MAX_BYTES_CONFIG}.\n     * <p>\n     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n     *\n     * @return name of the added stream thread or empty if a new stream thread could not be added\n     */\n    public Optional<String> addStreamThread() {\n        if (isRunningOrRebalancing()) {\n            final StreamThread streamThread;\n            synchronized (changeThreadCount) {\n                final int threadIdx = getNextThreadIndex();\n                final int numLiveThreads = getNumLiveStreamThreads();\n                final long cacheSizePerThread = getCacheSizePerThread(numLiveThreads + 1);\n                log.info(\"Adding StreamThread-{}, there will now be {} live threads and the new cache size per thread is {}\",\n                         threadIdx, numLiveThreads + 1, cacheSizePerThread);\n                resizeThreadCache(cacheSizePerThread);\n                // Creating thread should hold the lock in order to avoid duplicate thread index.\n                // If the duplicate index happen, the metadata of thread may be duplicate too.\n                streamThread = createAndAddStreamThread(cacheSizePerThread, threadIdx);\n            }\n\n            synchronized (stateLock) {\n                if (isRunningOrRebalancing()) {\n                    streamThread.start();\n                    return Optional.of(streamThread.getName());\n                } else {\n                    log.warn(\"Terminating the new thread because the Kafka Streams client is in state {}\", state);\n                    streamThread.shutdown();\n                    threads.remove(streamThread);\n                    final long cacheSizePerThread = getCacheSizePerThread(getNumLiveStreamThreads());\n                    log.info(\"Resizing thread cache due to terminating added thread, new cache size per thread is {}\", cacheSizePerThread);\n                    resizeThreadCache(cacheSizePerThread);\n                    return Optional.empty();\n                }\n            }\n        } else {\n            log.warn(\"Cannot add a stream thread when Kafka Streams client is in state {}\", state);\n            return Optional.empty();\n        }\n    }\n\n    /**\n     * Removes one stream thread out of the running stream threads from this Kafka Streams client.\n     * <p>\n     * The removed stream thread is gracefully shut down. This method does not specify which stream\n     * thread is shut down.\n     * <p>\n     * Since the number of stream threads decreases, the sizes of the caches in the remaining stream\n     * threads are adapted so that the sum of the cache sizes over all stream threads equals the total\n     * cache size specified in configuration {@link StreamsConfig#STATESTORE_CACHE_MAX_BYTES_CONFIG}.\n     *\n     * @return name of the removed stream thread or empty if a stream thread could not be removed because\n     *         no stream threads are alive\n     */\n    public Optional<String> removeStreamThread() {\n        return removeStreamThread(Long.MAX_VALUE);\n    }\n\n    /**\n     * Removes one stream thread out of the running stream threads from this Kafka Streams client.\n     * <p>\n     * The removed stream thread is gracefully shut down. This method does not specify which stream\n     * thread is shut down.\n     * <p>\n     * Since the number of stream threads decreases, the sizes of the caches in the remaining stream\n     * threads are adapted so that the sum of the cache sizes over all stream threads equals the total\n     * cache size specified in configuration {@link StreamsConfig#STATESTORE_CACHE_MAX_BYTES_CONFIG}.\n     *\n     * @param timeout The length of time to wait for the thread to shut down\n     * @throws org.apache.kafka.common.errors.TimeoutException if the thread does not stop in time\n     * @return name of the removed stream thread or empty if a stream thread could not be removed because\n     *         no stream threads are alive\n     */\n    public Optional<String> removeStreamThread(final Duration timeout) {\n        final String msgPrefix = prepareMillisCheckFailMsgPrefix(timeout, \"timeout\");\n        final long timeoutMs = validateMillisecondDuration(timeout, msgPrefix);\n        return removeStreamThread(timeoutMs);\n    }\n\n    private Optional<String> removeStreamThread(final long timeoutMs) throws TimeoutException {\n        final long startMs = time.milliseconds();\n\n        if (isRunningOrRebalancing()) {\n            synchronized (changeThreadCount) {\n                // make a copy of threads to avoid holding lock\n                for (final StreamThread streamThread : new ArrayList<>(threads)) {\n                    final boolean callingThreadIsNotCurrentStreamThread = !streamThread.getName().equals(Thread.currentThread().getName());\n                    if (streamThread.isThreadAlive() && (callingThreadIsNotCurrentStreamThread || getNumLiveStreamThreads() == 1)) {\n                        log.info(\"Removing StreamThread \" + streamThread.getName());\n                        final Optional<String> groupInstanceID = streamThread.getGroupInstanceID();\n                        streamThread.requestLeaveGroupDuringShutdown();\n                        streamThread.shutdown();\n                        if (!streamThread.getName().equals(Thread.currentThread().getName())) {\n                            final long remainingTimeMs = timeoutMs - (time.milliseconds() - startMs);\n                            if (remainingTimeMs <= 0 || !streamThread.waitOnThreadState(StreamThread.State.DEAD, remainingTimeMs)) {\n                                log.warn(\"{} did not shutdown in the allotted time.\", streamThread.getName());\n                                // Don't remove from threads until shutdown is complete. We will trim it from the\n                                // list once it reaches DEAD, and if for some reason it's hanging indefinitely in the\n                                // shutdown then we should just consider this thread.id to be burned\n                            } else {\n                                log.info(\"Successfully removed {} in {}ms\", streamThread.getName(), time.milliseconds() - startMs);\n                                threads.remove(streamThread);\n                                queryableStoreProvider.removeStoreProviderForThread(streamThread.getName());\n                            }\n                        } else {\n                            log.info(\"{} is the last remaining thread and must remove itself, therefore we cannot wait \"\n                                + \"for it to complete shutdown as this will result in deadlock.\", streamThread.getName());\n                        }\n\n                        final long cacheSizePerThread = getCacheSizePerThread(getNumLiveStreamThreads());\n                        log.info(\"Resizing thread cache due to thread removal, new cache size per thread is {}\", cacheSizePerThread);\n                        resizeThreadCache(cacheSizePerThread);\n                        if (groupInstanceID.isPresent() && callingThreadIsNotCurrentStreamThread) {\n                            final MemberToRemove memberToRemove = new MemberToRemove(groupInstanceID.get());\n                            final Collection<MemberToRemove> membersToRemove = Collections.singletonList(memberToRemove);\n                            final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = \n                                adminClient.removeMembersFromConsumerGroup(\n                                    applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG),\n                                    new RemoveMembersFromConsumerGroupOptions(membersToRemove)\n                                );\n                            try {\n                                final long remainingTimeMs = timeoutMs - (time.milliseconds() - startMs);\n                                removeMembersFromConsumerGroupResult.memberResult(memberToRemove).get(remainingTimeMs, TimeUnit.MILLISECONDS);\n                            } catch (final java.util.concurrent.TimeoutException exception) {\n                                log.error(\n                                    String.format(\n                                        \"Could not remove static member %s from consumer group %s due to a timeout:\",\n                                        groupInstanceID.get(),\n                                        applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG)\n                                    ),\n                                        exception\n                                );\n                                throw new TimeoutException(exception.getMessage(), exception);\n                            } catch (final InterruptedException e) {\n                                Thread.currentThread().interrupt();\n                            } catch (final ExecutionException exception) {\n                                log.error(\n                                    String.format(\n                                        \"Could not remove static member %s from consumer group %s due to:\",\n                                        groupInstanceID.get(),\n                                        applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG)\n                                    ),\n                                        exception\n                                );\n                                throw new StreamsException(\n                                        \"Could not remove static member \" + groupInstanceID.get()\n                                            + \" from consumer group \" + applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG)\n                                            + \" for the following reason: \",\n                                        exception.getCause()\n                                );\n                            }\n                        }\n                        final long remainingTimeMs = timeoutMs - (time.milliseconds() - startMs);\n                        if (remainingTimeMs <= 0) {\n                            throw new TimeoutException(\"Thread \" + streamThread.getName() + \" did not stop in the allotted time\");\n                        }\n                        return Optional.of(streamThread.getName());\n                    }\n                }\n            }\n            log.warn(\"There are no threads eligible for removal\");\n        } else {\n            log.warn(\"Cannot remove a stream thread when Kafka Streams client is in state  \" + state());\n        }\n        return Optional.empty();\n    }\n\n    /*\n     * Takes a snapshot and counts the number of stream threads which are not in PENDING_SHUTDOWN or DEAD\n     *\n     * note: iteration over SynchronizedList is not thread safe so it must be manually synchronized. However, we may\n     * require other locks when looping threads and it could cause deadlock. Hence, we create a copy to avoid holding\n     * threads lock when looping threads.\n     * @return number of alive stream threads\n     */\n    private int getNumLiveStreamThreads() {\n        final AtomicInteger numLiveThreads = new AtomicInteger(0);\n\n        synchronized (threads) {\n            processStreamThread(thread -> {\n                if (thread.state() == StreamThread.State.DEAD) {\n                    log.debug(\"Trimming thread {} from the threads list since it's state is {}\", thread.getName(), StreamThread.State.DEAD);\n                    threads.remove(thread);\n                } else if (thread.state() == StreamThread.State.PENDING_SHUTDOWN) {\n                    log.debug(\"Skipping thread {} from num live threads computation since it's state is {}\",\n                              thread.getName(), StreamThread.State.PENDING_SHUTDOWN);\n                } else {\n                    numLiveThreads.incrementAndGet();\n                }\n            });\n            return numLiveThreads.get();\n        }\n    }\n\n    private int getNextThreadIndex() {\n        final HashSet<String> allLiveThreadNames = new HashSet<>();\n        final AtomicInteger maxThreadId = new AtomicInteger(1);\n        synchronized (threads) {\n            processStreamThread(thread -> {\n                // trim any DEAD threads from the list so we can reuse the thread.id\n                // this is only safe to do once the thread has fully completed shutdown\n                if (thread.state() == StreamThread.State.DEAD) {\n                    threads.remove(thread);\n                } else {\n                    allLiveThreadNames.add(thread.getName());\n                    // Assume threads are always named with the \"-StreamThread-<threadId>\" suffix\n                    final int threadId = Integer.parseInt(thread.getName().substring(thread.getName().lastIndexOf(\"-\") + 1));\n                    if (threadId > maxThreadId.get()) {\n                        maxThreadId.set(threadId);\n                    }\n                }\n            });\n\n            final String baseName = clientId + \"-StreamThread-\";\n            for (int i = 1; i <= maxThreadId.get(); i++) {\n                final String name = baseName + i;\n                if (!allLiveThreadNames.contains(name)) {\n                    return i;\n                }\n            }\n            // It's safe to use threads.size() rather than getNumLiveStreamThreads() to infer the number of threads\n            // here since we trimmed any DEAD threads earlier in this method while holding the lock\n            return threads.size() + 1;\n        }\n    }\n\n    private long getCacheSizePerThread(final int numStreamThreads) {\n        if (numStreamThreads == 0) {\n            return totalCacheSize;\n        }\n        return totalCacheSize / (numStreamThreads + (topologyMetadata.hasGlobalTopology() ? 1 : 0));\n    }\n\n    private void resizeThreadCache(final long cacheSizePerThread) {\n        processStreamThread(thread -> thread.resizeCache(cacheSizePerThread));\n        if (globalStreamThread != null) {\n            globalStreamThread.resize(cacheSizePerThread);\n        }\n    }\n\n    private ScheduledExecutorService setupStateDirCleaner() {\n        return Executors.newSingleThreadScheduledExecutor(r -> {\n            final Thread thread = new Thread(r, clientId + \"-CleanupThread\");\n            thread.setDaemon(true);\n            return thread;\n        });\n    }\n\n    private static ScheduledExecutorService maybeCreateRocksDBMetricsRecordingService(final String clientId,\n                                                                                      final StreamsConfig config) {\n        if (RecordingLevel.forName(config.getString(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG)) == RecordingLevel.DEBUG) {\n            return Executors.newSingleThreadScheduledExecutor(r -> {\n                final Thread thread = new Thread(r, clientId + \"-RocksDBMetricsRecordingTrigger\");\n                thread.setDaemon(true);\n                return thread;\n            });\n        }\n        return null;\n    }\n\n    private static HostInfo parseHostInfo(final String endPoint) {\n        final HostInfo hostInfo = HostInfo.buildFromEndpoint(endPoint);\n        if (hostInfo == null) {\n            return StreamsMetadataState.UNKNOWN_HOST;\n        } else {\n            return hostInfo;\n        }\n    }\n\n    /**\n     * Start the {@code KafkaStreams} instance by starting all its threads.\n     * This function is expected to be called only once during the life cycle of the client.\n     * <p>\n     * Because threads are started in the background, this method does not block.\n     * However, if you have global stores in your topology, this method blocks until all global stores are restored.\n     * As a consequence, any fatal exception that happens during processing is by default only logged.\n     * If you want to be notified about dying threads, you can\n     * {@link #setUncaughtExceptionHandler(Thread.UncaughtExceptionHandler) register an uncaught exception handler}\n     * before starting the {@code KafkaStreams} instance.\n     * <p>\n     * Note, for brokers with version {@code 0.9.x} or lower, the broker version cannot be checked.\n     * There will be no error and the client will hang and retry to verify the broker version until it\n     * {@link StreamsConfig#REQUEST_TIMEOUT_MS_CONFIG times out}.\n\n     * @throws IllegalStateException if process was already started\n     * @throws StreamsException if the Kafka brokers have version 0.10.0.x or\n     *                          if {@link StreamsConfig#PROCESSING_GUARANTEE_CONFIG exactly-once} is enabled for pre 0.11.0.x brokers\n     */\n    public synchronized void start() throws IllegalStateException, StreamsException {\n        if (setState(State.REBALANCING)) {\n            log.debug(\"Starting Streams client\");\n\n            if (globalStreamThread != null) {\n                globalStreamThread.start();\n            }\n\n            final int numThreads = processStreamThread(StreamThread::start);\n\n            log.info(\"Started {} stream threads\", numThreads);\n\n            final Long cleanupDelay = applicationConfigs.getLong(StreamsConfig.STATE_CLEANUP_DELAY_MS_CONFIG);\n            stateDirCleaner.scheduleAtFixedRate(() -> {\n                // we do not use lock here since we only read on the value and act on it\n                if (state == State.RUNNING) {\n                    stateDirectory.cleanRemovedTasks(cleanupDelay);\n                }\n            }, cleanupDelay, cleanupDelay, TimeUnit.MILLISECONDS);\n\n            final long recordingDelay = 0;\n            final long recordingInterval = 1;\n            if (rocksDBMetricsRecordingService != null) {\n                rocksDBMetricsRecordingService.scheduleAtFixedRate(\n                    streamsMetrics.rocksDBMetricsRecordingTrigger(),\n                    recordingDelay,\n                    recordingInterval,\n                    TimeUnit.MINUTES\n                );\n            }\n        } else {\n            throw new IllegalStateException(\"The client is either already started or already stopped, cannot re-start\");\n        }\n    }\n\n    /**\n     * Class that handles options passed in case of {@code KafkaStreams} instance scale down\n     */\n    public static class CloseOptions {\n        private Duration timeout = Duration.ofMillis(Long.MAX_VALUE);\n        private boolean leaveGroup = false;\n\n        public CloseOptions timeout(final Duration timeout) {\n            this.timeout = timeout;\n            return this;\n        }\n\n        public CloseOptions leaveGroup(final boolean leaveGroup) {\n            this.leaveGroup = leaveGroup;\n            return this;\n        }\n    }\n\n    /**\n     * Shutdown this {@code KafkaStreams} instance by signaling all the threads to stop, and then wait for them to join.\n     * This will block until all threads have stopped.\n     */\n    public void close() {\n        close(Long.MAX_VALUE, false);\n    }\n\n    private Thread shutdownHelper(final boolean error, final long timeoutMs, final boolean leaveGroup) {\n        stateDirCleaner.shutdownNow();\n        if (rocksDBMetricsRecordingService != null) {\n            rocksDBMetricsRecordingService.shutdownNow();\n        }\n\n        // wait for all threads to join in a separate thread;\n        // save the current thread so that if it is a stream thread\n        // we don't attempt to join it and cause a deadlock\n        return new Thread(() -> {\n            // notify all the threads to stop; avoid deadlocks by stopping any\n            // further state reports from the thread since we're shutting down\n            int numStreamThreads = processStreamThread(StreamThread::shutdown);\n\n            log.info(\"Shutting down {} stream threads\", numStreamThreads);\n\n            topologyMetadata.wakeupThreads();\n\n            numStreamThreads = processStreamThread(thread -> {\n                try {\n                    if (!thread.isRunning()) {\n                        log.debug(\"Shutdown {} complete\", thread.getName());\n\n                        thread.join();\n                    }\n                } catch (final InterruptedException ex) {\n                    log.warn(\"Shutdown {} interrupted\", thread.getName());\n\n                    Thread.currentThread().interrupt();\n                }\n            });\n\n            if (leaveGroup) {\n                processStreamThread(streamThreadLeaveConsumerGroup(timeoutMs));\n            }\n\n            log.info(\"Shutdown {} stream threads complete\", numStreamThreads);\n\n            if (globalStreamThread != null) {\n                log.info(\"Shutting down the global stream threads\");\n\n                globalStreamThread.shutdown();\n            }\n\n            if (globalStreamThread != null && !globalStreamThread.stillRunning()) {\n                try {\n                    globalStreamThread.join();\n                } catch (final InterruptedException e) {\n                    log.warn(\"Shutdown the global stream thread interrupted\");\n\n                    Thread.currentThread().interrupt();\n                }\n                globalStreamThread = null;\n\n                log.info(\"Shutdown global stream threads complete\");\n            }\n\n            stateDirectory.close();\n            adminClient.close();\n\n            streamsMetrics.removeAllClientLevelSensorsAndMetrics();\n            metrics.close();\n            if (!error) {\n                setState(State.NOT_RUNNING);\n            } else {\n                setState(State.ERROR);\n            }\n        }, clientId + \"-CloseThread\");\n    }\n\n    private boolean close(final long timeoutMs, final boolean leaveGroup) {\n        if (state.hasCompletedShutdown()) {\n            log.info(\"Streams client is already in the terminal {} state, all resources are closed and the client has stopped.\", state);\n            return true;\n        }\n        if (state.isShuttingDown()) {\n            log.info(\"Streams client is in {}, all resources are being closed and the client will be stopped.\", state);\n            if (state == State.PENDING_ERROR && waitOnState(State.ERROR, timeoutMs)) {\n                log.info(\"Streams client stopped to ERROR completely\");\n                return true;\n            } else if (state == State.PENDING_SHUTDOWN && waitOnState(State.NOT_RUNNING, timeoutMs)) {\n                log.info(\"Streams client stopped to NOT_RUNNING completely\");\n                return true;\n            } else {\n                log.warn(\"Streams client cannot transition to {} completely within the timeout\",\n                         state == State.PENDING_SHUTDOWN ? State.NOT_RUNNING : State.ERROR);\n                return false;\n            }\n        }\n\n        if (!setState(State.PENDING_SHUTDOWN)) {\n            // if we can't transition to PENDING_SHUTDOWN but not because we're already shutting down, then it must be fatal\n            log.error(\"Failed to transition to PENDING_SHUTDOWN, current state is {}\", state);\n            throw new StreamsException(\"Failed to shut down while in state \" + state);\n        } else {\n\n            final Thread shutdownThread = shutdownHelper(false, timeoutMs, leaveGroup);\n\n            shutdownThread.setDaemon(true);\n            shutdownThread.start();\n        }\n\n        if (waitOnState(State.NOT_RUNNING, timeoutMs)) {\n            log.info(\"Streams client stopped completely\");\n            return true;\n        } else {\n            log.info(\"Streams client cannot stop completely within the {}ms timeout\", timeoutMs);\n            return false;\n        }\n    }\n\n    private void closeToError() {\n        if (!setState(State.PENDING_ERROR)) {\n            log.info(\"Skipping shutdown since we are already in \" + state());\n        } else {\n            final Thread shutdownThread = shutdownHelper(true, -1, false);\n\n            shutdownThread.setDaemon(true);\n            shutdownThread.start();\n        }\n    }\n\n    /**\n     * Shutdown this {@code KafkaStreams} by signaling all the threads to stop, and then wait up to the timeout for the\n     * threads to join.\n     * A {@code timeout} of Duration.ZERO (or any other zero duration) makes the close operation asynchronous.\n     * Negative-duration timeouts are rejected.\n     *\n     * @param timeout  how long to wait for the threads to shutdown\n     * @return {@code true} if all threads were successfully stopped&mdash;{@code false} if the timeout was reached\n     * before all threads stopped\n     * Note that this method must not be called in the {@link StateListener#onChange(KafkaStreams.State, KafkaStreams.State)} callback of {@link StateListener}.\n     * @throws IllegalArgumentException if {@code timeout} can't be represented as {@code long milliseconds}\n     */\n    public synchronized boolean close(final Duration timeout) throws IllegalArgumentException {\n        final String msgPrefix = prepareMillisCheckFailMsgPrefix(timeout, \"timeout\");\n        final long timeoutMs = validateMillisecondDuration(timeout, msgPrefix);\n        if (timeoutMs < 0) {\n            throw new IllegalArgumentException(\"Timeout can't be negative.\");\n        }\n\n        log.debug(\"Stopping Streams client with timeoutMillis = {} ms.\", timeoutMs);\n\n        return close(timeoutMs, false);\n    }\n\n    /**\n     * Shutdown this {@code KafkaStreams} by signaling all the threads to stop, and then wait up to the timeout for the\n     * threads to join.\n     * @param options  contains timeout to specify how long to wait for the threads to shutdown, and a flag leaveGroup to\n     *                 trigger consumer leave call\n     * @return {@code true} if all threads were successfully stopped&mdash;{@code false} if the timeout was reached\n     * before all threads stopped\n     * Note that this method must not be called in the {@link StateListener#onChange(KafkaStreams.State, KafkaStreams.State)} callback of {@link StateListener}.\n     * @throws IllegalArgumentException if {@code timeout} can't be represented as {@code long milliseconds}\n     */\n    public synchronized boolean close(final CloseOptions options) throws IllegalArgumentException {\n        Objects.requireNonNull(options, \"options cannot be null\");\n        final String msgPrefix = prepareMillisCheckFailMsgPrefix(options.timeout, \"timeout\");\n        final long timeoutMs = validateMillisecondDuration(options.timeout, msgPrefix);\n        if (timeoutMs < 0) {\n            throw new IllegalArgumentException(\"Timeout can't be negative.\");\n        }\n        log.debug(\"Stopping Streams client with timeoutMillis = {} ms.\", timeoutMs);\n        return close(timeoutMs, options.leaveGroup);\n    }\n\n    private Consumer<StreamThread> streamThreadLeaveConsumerGroup(final long remainingTimeMs) {\n        return thread -> {\n            final Optional<String> groupInstanceId = thread.getGroupInstanceID();\n            if (groupInstanceId.isPresent()) {\n                log.debug(\"Sending leave group trigger to removing instance from consumer group: {}.\",\n                    groupInstanceId.get());\n                final MemberToRemove memberToRemove = new MemberToRemove(groupInstanceId.get());\n                final Collection<MemberToRemove> membersToRemove = Collections.singletonList(memberToRemove);\n\n                final RemoveMembersFromConsumerGroupResult removeMembersFromConsumerGroupResult = adminClient\n                    .removeMembersFromConsumerGroup(\n                        applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG),\n                        new RemoveMembersFromConsumerGroupOptions(membersToRemove)\n                    );\n\n                try {\n                    removeMembersFromConsumerGroupResult.memberResult(memberToRemove)\n                        .get(remainingTimeMs, TimeUnit.MILLISECONDS);\n                } catch (final Exception e) {\n                    final String msg = String.format(\"Could not remove static member %s from consumer group %s.\",\n                                                     groupInstanceId.get(), applicationConfigs.getString(StreamsConfig.APPLICATION_ID_CONFIG));\n                    log.error(msg, e);\n                }\n            }\n        };\n    }\n\n    /**\n     * Do a clean up of the local {@link StateStore} directory ({@link StreamsConfig#STATE_DIR_CONFIG}) by deleting all\n     * data with regard to the {@link StreamsConfig#APPLICATION_ID_CONFIG application ID}.\n     * <p>\n     * May only be called either before this {@code KafkaStreams} instance is {@link #start() started} or after the\n     * instance is {@link #close() closed}.\n     * <p>\n     * Calling this method triggers a restore of local {@link StateStore}s on the next {@link #start() application start}.\n     *\n     * @throws IllegalStateException if this {@code KafkaStreams} instance has been started and hasn't fully shut down\n     * @throws StreamsException if cleanup failed\n     */\n    public void cleanUp() {\n        if (!(state.hasNotStarted() || state.hasCompletedShutdown())) {\n            throw new IllegalStateException(\"Cannot clean up while running.\");\n        }\n        stateDirectory.clean();\n    }\n\n    /**\n     * Find all currently running {@code KafkaStreams} instances (potentially remotely) that use the same\n     * {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} as this instance (i.e., all instances that belong to\n     * the same Kafka Streams application) and return {@link StreamsMetadata} for each discovered instance.\n     * <p>\n     * Note: this is a point in time view and it may change due to partition reassignment.\n     *\n     * @return {@link StreamsMetadata} for each {@code KafkaStreams} instances of this application\n     * @deprecated since 3.0.0 use {@link KafkaStreams#metadataForAllStreamsClients}\n     */\n    @Deprecated\n    public Collection<org.apache.kafka.streams.state.StreamsMetadata> allMetadata() {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getAllMetadata().stream().map(streamsMetadata ->\n                new org.apache.kafka.streams.state.StreamsMetadata(streamsMetadata.hostInfo(),\n                        streamsMetadata.stateStoreNames(),\n                        streamsMetadata.topicPartitions(),\n                        streamsMetadata.standbyStateStoreNames(),\n                        streamsMetadata.standbyTopicPartitions()))\n                .collect(Collectors.toSet());\n    }\n\n    /**\n     * Find all currently running {@code KafkaStreams} instances (potentially remotely) that use the same\n     * {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} as this instance (i.e., all instances that belong to\n     * the same Kafka Streams application) and return {@link StreamsMetadata} for each discovered instance.\n     * <p>\n     * Note: this is a point in time view and it may change due to partition reassignment.\n     *\n     * @return {@link StreamsMetadata} for each {@code KafkaStreams} instances of this application\n     */\n    public Collection<StreamsMetadata> metadataForAllStreamsClients() {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getAllMetadata();\n    }\n\n    /**\n     * Find all currently running {@code KafkaStreams} instances (potentially remotely) that\n     * <ul>\n     *   <li>use the same {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} as this instance (i.e., all\n     *       instances that belong to the same Kafka Streams application)</li>\n     *   <li>and that contain a {@link StateStore} with the given {@code storeName}</li>\n     * </ul>\n     * and return {@link StreamsMetadata} for each discovered instance.\n     * <p>\n     * Note: this is a point in time view and it may change due to partition reassignment.\n     *\n     * @param storeName the {@code storeName} to find metadata for\n     * @return {@link StreamsMetadata} for each {@code KafkaStreams} instances with the provide {@code storeName} of\n     * this application\n     * @deprecated since 3.0.0 use {@link KafkaStreams#streamsMetadataForStore} instead\n     */\n    @Deprecated\n    public Collection<org.apache.kafka.streams.state.StreamsMetadata> allMetadataForStore(final String storeName) {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getAllMetadataForStore(storeName).stream().map(streamsMetadata ->\n                new org.apache.kafka.streams.state.StreamsMetadata(streamsMetadata.hostInfo(),\n                        streamsMetadata.stateStoreNames(),\n                        streamsMetadata.topicPartitions(),\n                        streamsMetadata.standbyStateStoreNames(),\n                        streamsMetadata.standbyTopicPartitions()))\n                .collect(Collectors.toSet());\n    }\n\n    /**\n     * Find all currently running {@code KafkaStreams} instances (potentially remotely) that\n     * <ul>\n     *   <li>use the same {@link StreamsConfig#APPLICATION_ID_CONFIG application ID} as this instance (i.e., all\n     *       instances that belong to the same Kafka Streams application)</li>\n     *   <li>and that contain a {@link StateStore} with the given {@code storeName}</li>\n     * </ul>\n     * and return {@link StreamsMetadata} for each discovered instance.\n     * <p>\n     * Note: this is a point in time view and it may change due to partition reassignment.\n     *\n     * @param storeName the {@code storeName} to find metadata for\n     * @return {@link StreamsMetadata} for each {@code KafkaStreams} instances with the provide {@code storeName} of\n     * this application\n     */\n    public Collection<StreamsMetadata> streamsMetadataForStore(final String storeName) {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getAllMetadataForStore(storeName);\n    }\n\n    /**\n     * Finds the metadata containing the active hosts and standby hosts where the key being queried would reside.\n     *\n     * @param storeName     the {@code storeName} to find metadata for\n     * @param key           the key to find metadata for\n     * @param keySerializer serializer for the key\n     * @param <K>           key type\n     * Returns {@link KeyQueryMetadata} containing all metadata about hosting the given key for the given store,\n     * or {@code null} if no matching metadata could be found.\n     */\n    public <K> KeyQueryMetadata queryMetadataForKey(final String storeName,\n                                                    final K key,\n                                                    final Serializer<K> keySerializer) {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getKeyQueryMetadataForKey(storeName, key, keySerializer);\n    }\n\n    /**\n     * Finds the metadata containing the active hosts and standby hosts where the key being queried would reside.\n     *\n     * @param storeName     the {@code storeName} to find metadata for\n     * @param key           the key to find metadata for\n     * @param partitioner the partitioner to be use to locate the host for the key\n     * @param <K>           key type\n     * Returns {@link KeyQueryMetadata} containing all metadata about hosting the given key for the given store, using the\n     * the supplied partitioner, or {@code null} if no matching metadata could be found.\n     */\n    public <K> KeyQueryMetadata queryMetadataForKey(final String storeName,\n                                                    final K key,\n                                                    final StreamPartitioner<? super K, ?> partitioner) {\n        validateIsRunningOrRebalancing();\n        return streamsMetadataState.getKeyQueryMetadataForKey(storeName, key, partitioner);\n    }\n\n    /**\n     * Get a facade wrapping the local {@link StateStore} instances with the provided {@link StoreQueryParameters}.\n     * The returned object can be used to query the {@link StateStore} instances.\n     *\n     * @param storeQueryParameters   the parameters used to fetch a queryable store\n     * @return A facade wrapping the local {@link StateStore} instances\n     * @throws StreamsNotStartedException If Streams has not yet been started. Just call {@link KafkaStreams#start()}\n     *                                    and then retry this call.\n     * @throws UnknownStateStoreException If the specified store name does not exist in the topology.\n     * @throws InvalidStateStorePartitionException If the specified partition does not exist.\n     * @throws InvalidStateStoreException If the Streams instance isn't in a queryable state.\n     *                                    If the store's type does not match the QueryableStoreType,\n     *                                    the Streams instance is not in a queryable state with respect\n     *                                    to the parameters, or if the store is not available locally, then\n     *                                    an InvalidStateStoreException is thrown upon store access.\n     */\n    public <T> T store(final StoreQueryParameters<T> storeQueryParameters) {\n        validateIsRunningOrRebalancing();\n        final String storeName = storeQueryParameters.storeName();\n        if (!topologyMetadata.hasStore(storeName)) {\n            throw new UnknownStateStoreException(\n                \"Cannot get state store \" + storeName + \" because no such store is registered in the topology.\"\n            );\n        }\n        return queryableStoreProvider.getStore(storeQueryParameters);\n    }\n\n    /**\n     *  This method pauses processing for the KafkaStreams instance.\n     *\n     *  <p>Paused topologies will only skip over a) processing, b) punctuation, and c) standby tasks.\n     *  Notably, paused topologies will still poll Kafka consumers, and commit offsets.\n     *  This method sets transient state that is not maintained or managed among instances.\n     *  Note that pause() can be called before start() in order to start a KafkaStreams instance\n     *  in a manner where the processing is paused as described, but the consumers are started up.\n     */\n    public void pause() {\n        if (topologyMetadata.hasNamedTopologies()) {\n            for (final NamedTopology namedTopology : topologyMetadata.getAllNamedTopologies()) {\n                topologyMetadata.pauseTopology(namedTopology.name());\n            }\n        } else {\n            topologyMetadata.pauseTopology(UNNAMED_TOPOLOGY);\n        }\n    }\n\n    /**\n     * @return true when the KafkaStreams instance has its processing paused.\n     */\n    public boolean isPaused() {\n        if (topologyMetadata.hasNamedTopologies()) {\n            return topologyMetadata.getAllNamedTopologies().stream()\n                .map(NamedTopology::name)\n                .allMatch(topologyMetadata::isPaused);\n        } else {\n            return topologyMetadata.isPaused(UNNAMED_TOPOLOGY);\n        }\n    }\n\n    /**\n     * This method resumes processing for the KafkaStreams instance.\n     */\n    public void resume() {\n        if (topologyMetadata.hasNamedTopologies()) {\n            for (final NamedTopology namedTopology : topologyMetadata.getAllNamedTopologies()) {\n                topologyMetadata.resumeTopology(namedTopology.name());\n            }\n        } else {\n            topologyMetadata.resumeTopology(UNNAMED_TOPOLOGY);\n        }\n        threads.forEach(StreamThread::signalResume);\n    }\n\n    /**\n     * handle each stream thread in a snapshot of threads.\n     * noted: iteration over SynchronizedList is not thread safe so it must be manually synchronized. However, we may\n     * require other locks when looping threads and it could cause deadlock. Hence, we create a copy to avoid holding\n     * threads lock when looping threads.\n     * @param consumer handler\n     */\n    protected int processStreamThread(final Consumer<StreamThread> consumer) {\n        final List<StreamThread> copy = new ArrayList<>(threads);\n        for (final StreamThread thread : copy) consumer.accept(thread);\n\n        return copy.size();\n    }\n\n    /**\n     * Returns the internal clients' assigned {@code client instance ids}.\n     *\n     * @return The internal clients' assigned instance ids used for metrics collection.\n     *\n     * @throws IllegalArgumentException If {@code timeout} is negative.\n     * @throws IllegalStateException If {@code KafkaStreams} is not running.\n     * @throws TimeoutException Indicates that a request timed out.\n     * @throws StreamsException For any other error that might occur.\n     */\n    public synchronized ClientInstanceIds clientInstanceIds(final Duration timeout) {\n        if (timeout.isNegative()) {\n            throw new IllegalArgumentException(\"The timeout cannot be negative.\");\n        }\n        if (state().hasNotStarted()) {\n            throw new IllegalStateException(\"KafkaStreams has not been started, you can retry after calling start().\");\n        }\n        if (state().isShuttingDown() || state.hasCompletedShutdown()) {\n            throw new IllegalStateException(\"KafkaStreams has been stopped (\" + state + \").\");\n        }\n\n        final Timer remainingTime = time.timer(timeout.toMillis());\n        final ClientInstanceIdsImpl clientInstanceIds = new ClientInstanceIdsImpl();\n\n        // (1) fan-out calls to threads\n\n        // StreamThread for main/restore consumers and producer(s)\n        final Map<String, KafkaFuture<Uuid>> consumerFutures = new HashMap<>();\n        final Map<String, KafkaFuture<Map<String, KafkaFuture<Uuid>>>> producerFutures = new HashMap<>();\n        synchronized (changeThreadCount) {\n            for (final StreamThread streamThread : threads) {\n                consumerFutures.putAll(streamThread.consumerClientInstanceIds(timeout));\n                producerFutures.put(streamThread.getName(), streamThread.producersClientInstanceIds(timeout));\n            }\n        }\n\n        // GlobalThread\n        KafkaFuture<Uuid> globalThreadFuture = null;\n        if (globalStreamThread != null) {\n            globalThreadFuture = globalStreamThread.globalConsumerInstanceId(timeout);\n        }\n\n        // (2) get admin client instance id in a blocking fashion, while Stream/GlobalThreads work in parallel\n        try {\n            clientInstanceIds.setAdminInstanceId(adminClient.clientInstanceId(timeout));\n            remainingTime.update(time.milliseconds());\n        } catch (final IllegalStateException telemetryDisabledError) {\n            // swallow\n            log.debug(\"Telemetry is disabled on the admin client.\");\n        } catch (final TimeoutException timeoutException) {\n            throw timeoutException;\n        } catch (final Exception error) {\n            throw new StreamsException(\"Could not retrieve admin client instance id.\", error);\n        }\n\n        // (3) collect client instance ids from threads\n\n        // (3a) collect consumers from StreamsThread\n        for (final Map.Entry<String, KafkaFuture<Uuid>> consumerFuture : consumerFutures.entrySet()) {\n            final Uuid instanceId = getOrThrowException(\n                consumerFuture.getValue(),\n                remainingTime.remainingMs(),\n                () -> String.format(\n                    \"Could not retrieve consumer instance id for %s.\",\n                    consumerFuture.getKey()\n                )\n            );\n            remainingTime.update(time.milliseconds());\n\n            // could be `null` if telemetry is disabled on the consumer itself\n            if (instanceId != null) {\n                clientInstanceIds.addConsumerInstanceId(\n                    consumerFuture.getKey(),\n                    instanceId\n                );\n            } else {\n                log.debug(String.format(\"Telemetry is disabled for %s.\", consumerFuture.getKey()));\n            }\n        }\n\n        // (3b) collect producers from StreamsThread\n        for (final Map.Entry<String, KafkaFuture<Map<String, KafkaFuture<Uuid>>>> threadProducerFuture : producerFutures.entrySet()) {\n            final Map<String, KafkaFuture<Uuid>> streamThreadProducerFutures = getOrThrowException(\n                threadProducerFuture.getValue(),\n                remainingTime.remainingMs(),\n                () -> String.format(\n                    \"Could not retrieve producer instance id for %s.\",\n                    threadProducerFuture.getKey()\n                )\n            );\n            remainingTime.update(time.milliseconds());\n\n            for (final Map.Entry<String, KafkaFuture<Uuid>> producerFuture : streamThreadProducerFutures.entrySet()) {\n                final Uuid instanceId = getOrThrowException(\n                    producerFuture.getValue(),\n                    remainingTime.remainingMs(),\n                    () -> String.format(\n                        \"Could not retrieve producer instance id for %s.\",\n                        producerFuture.getKey()\n                    )\n                );\n                remainingTime.update(time.milliseconds());\n\n                // could be `null` if telemetry is disabled on the producer itself\n                if (instanceId != null) {\n                    clientInstanceIds.addProducerInstanceId(\n                        producerFuture.getKey(),\n                        instanceId\n                    );\n                } else {\n                    log.debug(String.format(\"Telemetry is disabled for %s.\", producerFuture.getKey()));\n                }\n            }\n        }\n\n        // (3c) collect from GlobalThread\n        if (globalThreadFuture != null) {\n            final Uuid instanceId = getOrThrowException(\n                globalThreadFuture,\n                remainingTime.remainingMs(),\n                () -> \"Could not retrieve global consumer client instance id.\"\n            );\n            remainingTime.update(time.milliseconds());\n\n            // could be `null` if telemetry is disabled on the client itself\n            if (instanceId != null) {\n                clientInstanceIds.addConsumerInstanceId(\n                    globalStreamThread.getName(),\n                    instanceId\n                );\n            } else {\n                log.debug(\"Telemetry is disabled for the global consumer.\");\n            }\n        }\n\n        return clientInstanceIds;\n    }\n\n    private <T> T getOrThrowException(\n        final KafkaFuture<T> future,\n        final long timeoutMs,\n        final Supplier<String> errorMessage) {\n        final Throwable cause;\n\n        try {\n            return future.get(timeoutMs, TimeUnit.MILLISECONDS);\n        } catch (final java.util.concurrent.TimeoutException timeout) {\n            throw new TimeoutException(errorMessage.get(), timeout);\n        } catch (final ExecutionException exception) {\n            cause = exception.getCause();\n            if (cause instanceof TimeoutException) {\n                throw (TimeoutException) cause;\n            }\n        } catch (final InterruptedException error) {\n            cause = error;\n        }\n\n        throw new StreamsException(errorMessage.get(), cause);\n    }\n\n    /**\n     * Returns runtime information about the local threads of this {@link KafkaStreams} instance.\n     *\n     * @return the set of {@link org.apache.kafka.streams.processor.ThreadMetadata}.\n     * @deprecated since 3.0 use {@link #metadataForLocalThreads()}\n     */\n    @Deprecated\n    public Set<org.apache.kafka.streams.processor.ThreadMetadata> localThreadsMetadata() {\n        return metadataForLocalThreads().stream().map(threadMetadata -> new org.apache.kafka.streams.processor.ThreadMetadata(\n                threadMetadata.threadName(),\n                threadMetadata.threadState(),\n                threadMetadata.consumerClientId(),\n                threadMetadata.restoreConsumerClientId(),\n                threadMetadata.producerClientIds(),\n                threadMetadata.adminClientId(),\n                threadMetadata.activeTasks().stream().map(taskMetadata -> new org.apache.kafka.streams.processor.TaskMetadata(\n                        taskMetadata.taskId().toString(),\n                        taskMetadata.topicPartitions(),\n                        taskMetadata.committedOffsets(),\n                        taskMetadata.endOffsets(),\n                        taskMetadata.timeCurrentIdlingStarted())\n                ).collect(Collectors.toSet()),\n                threadMetadata.standbyTasks().stream().map(taskMetadata -> new org.apache.kafka.streams.processor.TaskMetadata(\n                        taskMetadata.taskId().toString(),\n                        taskMetadata.topicPartitions(),\n                        taskMetadata.committedOffsets(),\n                        taskMetadata.endOffsets(),\n                        taskMetadata.timeCurrentIdlingStarted())\n                ).collect(Collectors.toSet())))\n                .collect(Collectors.toSet());\n    }\n\n    /**\n     * Returns runtime information about the local threads of this {@link KafkaStreams} instance.\n     *\n     * @return the set of {@link ThreadMetadata}.\n     */\n    public Set<ThreadMetadata> metadataForLocalThreads() {\n        final Set<ThreadMetadata> threadMetadata = new HashSet<>();\n        processStreamThread(thread -> {\n            synchronized (thread.getStateLock()) {\n                if (thread.state() != StreamThread.State.DEAD) {\n                    threadMetadata.add(thread.threadMetadata());\n                }\n            }\n        });\n        return threadMetadata;\n    }\n\n    /**\n     * Returns {@link LagInfo}, for all store partitions (active or standby) local to this Streams instance. Note that the\n     * values returned are just estimates and meant to be used for making soft decisions on whether the data in the store\n     * partition is fresh enough for querying.\n     *\n     * <p>Note: Each invocation of this method issues a call to the Kafka brokers. Thus, it's advisable to limit the frequency\n     * of invocation to once every few seconds.\n     *\n     * @return map of store names to another map of partition to {@link LagInfo}s\n     * @throws StreamsException if the admin client request throws exception\n     */\n    public Map<String, Map<Integer, LagInfo>> allLocalStorePartitionLags() {\n        final List<Task> allTasks = new ArrayList<>();\n        processStreamThread(thread -> allTasks.addAll(thread.readyOnlyAllTasks()));\n        return allLocalStorePartitionLags(allTasks);\n    }\n\n    protected Map<String, Map<Integer, LagInfo>> allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor) {\n        final Map<String, Map<Integer, LagInfo>> localStorePartitionLags = new TreeMap<>();\n        final Collection<TopicPartition> allPartitions = new LinkedList<>();\n        final Map<TopicPartition, Long> allChangelogPositions = new HashMap<>();\n\n        // Obtain the current positions, of all the active-restoring and standby tasks\n        for (final Task task : tasksToCollectLagFor) {\n            allPartitions.addAll(task.changelogPartitions());\n            // Note that not all changelog partitions, will have positions; since some may not have started\n            allChangelogPositions.putAll(task.changelogOffsets());\n        }\n\n        log.debug(\"Current changelog positions: {}\", allChangelogPositions);\n        final Map<TopicPartition, ListOffsetsResultInfo> allEndOffsets;\n        allEndOffsets = fetchEndOffsets(allPartitions, adminClient);\n        log.debug(\"Current end offsets :{}\", allEndOffsets);\n\n        for (final Map.Entry<TopicPartition, ListOffsetsResultInfo> entry : allEndOffsets.entrySet()) {\n            // Avoiding an extra admin API lookup by computing lags for not-yet-started restorations\n            // from zero instead of the real \"earliest offset\" for the changelog.\n            // This will yield the correct relative order of lagginess for the tasks in the cluster,\n            // but it is an over-estimate of how much work remains to restore the task from scratch.\n            final long earliestOffset = 0L;\n            final long changelogPosition = allChangelogPositions.getOrDefault(entry.getKey(), earliestOffset);\n            final long latestOffset = entry.getValue().offset();\n            final LagInfo lagInfo = new LagInfo(changelogPosition == Task.LATEST_OFFSET ? latestOffset : changelogPosition, latestOffset);\n            final String storeName = streamsMetadataState.getStoreForChangelogTopic(entry.getKey().topic());\n            localStorePartitionLags.computeIfAbsent(storeName, ignored -> new TreeMap<>())\n                .put(entry.getKey().partition(), lagInfo);\n        }\n\n        return Collections.unmodifiableMap(localStorePartitionLags);\n    }\n\n    /**\n     * Run an interactive query against a state store.\n     * <p>\n     * This method allows callers outside of the Streams runtime to access the internal state of\n     * stateful processors. See <a href=\"https://kafka.apache.org/documentation/streams/developer-guide/interactive-queries.html\">IQ docs</a>\n     * for more information.\n     * <p>\n     * NOTICE: This functionality is {@link Evolving} and subject to change in minor versions.\n     * Once it is stabilized, this notice and the evolving annotation will be removed.\n     *\n     * @param <R> The result type specified by the query.\n     * @throws StreamsNotStartedException If Streams has not yet been started. Just call {@link\n     *                                    KafkaStreams#start()} and then retry this call.\n     * @throws StreamsStoppedException    If Streams is in a terminal state like PENDING_SHUTDOWN,\n     *                                    NOT_RUNNING, PENDING_ERROR, or ERROR. The caller should\n     *                                    discover a new instance to query.\n     * @throws UnknownStateStoreException If the specified store name does not exist in the\n     *                                    topology.\n     */\n    @Evolving\n    public <R> StateQueryResult<R> query(final StateQueryRequest<R> request) {\n        final String storeName = request.getStoreName();\n        if (!topologyMetadata.hasStore(storeName)) {\n            throw new UnknownStateStoreException(\n                \"Cannot get state store \"\n                    + storeName\n                    + \" because no such store is registered in the topology.\"\n            );\n        }\n        if (state().hasNotStarted()) {\n            throw new StreamsNotStartedException(\n                \"KafkaStreams has not been started, you can retry after calling start().\"\n            );\n        }\n        if (state().isShuttingDown() || state.hasCompletedShutdown()) {\n            throw new StreamsStoppedException(\n                \"KafkaStreams has been stopped (\" + state + \").\"\n                    + \" This instance can no longer serve queries.\"\n            );\n        }\n        final StateQueryResult<R> result = new StateQueryResult<>();\n\n        final Map<String, StateStore> globalStateStores = topologyMetadata.globalStateStores();\n        if (globalStateStores.containsKey(storeName)) {\n            // See KAFKA-13523\n            result.setGlobalResult(\n                QueryResult.forFailure(\n                    FailureReason.UNKNOWN_QUERY_TYPE,\n                    \"Global stores do not yet support the KafkaStreams#query API. Use KafkaStreams#store instead.\"\n                )\n            );\n        } else {\n            for (final StreamThread thread : threads) {\n                final Set<Task> tasks = thread.readyOnlyAllTasks();\n                for (final Task task : tasks) {\n\n                    final TaskId taskId = task.id();\n                    final int partition = taskId.partition();\n                    if (request.isAllPartitions() || request.getPartitions().contains(partition)) {\n                        final StateStore store = task.getStore(storeName);\n                        if (store != null) {\n                            final StreamThread.State state = thread.state();\n                            final boolean active = task.isActive();\n                            if (request.isRequireActive()\n                                && (state != StreamThread.State.RUNNING || !active)) {\n\n                                result.addResult(\n                                    partition,\n                                    QueryResult.forFailure(\n                                        FailureReason.NOT_ACTIVE,\n                                        \"Query requires a running active task,\"\n                                            + \" but partition was in state \"\n                                            + state + \" and was \"\n                                            + (active ? \"active\" : \"not active\") + \".\"\n                                    )\n                                );\n                            } else {\n                                final QueryResult<R> r = store.query(\n                                    request.getQuery(),\n                                    request.isRequireActive()\n                                        ? PositionBound.unbounded()\n                                        : request.getPositionBound(),\n                                    new QueryConfig(request.executionInfoEnabled())\n                                );\n                                result.addResult(partition, r);\n                            }\n\n\n                            // optimization: if we have handled all the requested partitions,\n                            // we can return right away.\n                            if (!request.isAllPartitions()\n                                && result.getPartitionResults().keySet().containsAll(request.getPartitions())) {\n                                return result;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        if (!request.isAllPartitions()) {\n            for (final Integer partition : request.getPartitions()) {\n                if (!result.getPartitionResults().containsKey(partition)) {\n                    result.addResult(partition, QueryResult.forFailure(\n                        FailureReason.NOT_PRESENT,\n                        \"The requested partition was not present at the time of the query.\"\n                    ));\n                }\n            }\n        }\n\n        return result;\n    }\n\n}",
                "methodCount": 90
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "hasNotStarted",
                            "method_signature": "public hasNotStarted()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunningOrRebalancing",
                            "method_signature": "public isRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasCompletedShutdown",
                            "method_signature": "public hasCompletedShutdown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasStartedOrFinishedShuttingDown",
                            "method_signature": "public hasStartedOrFinishedShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isValidTransition",
                            "method_signature": "public isValidTransition(final State newState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "waitOnState",
                            "method_signature": "private waitOnState(final State targetState, final long waitMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunningOrRebalancing",
                            "method_signature": "protected isRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasStartedOrFinishedShuttingDown",
                            "method_signature": "protected hasStartedOrFinishedShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateIsRunningOrRebalancing",
                            "method_signature": "protected validateIsRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "defaultStreamsUncaughtExceptionHandler",
                            "method_signature": "private defaultStreamsUncaughtExceptionHandler(final Throwable throwable, final boolean skipThreadReplacement)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replaceStreamThread",
                            "method_signature": "private replaceStreamThread(final Throwable throwable)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleStreamsUncaughtException",
                            "method_signature": "private handleStreamsUncaughtException(final Throwable throwable,\n                                                final StreamsUncaughtExceptionHandler streamsUncaughtExceptionHandler,\n                                                final boolean skipThreadReplacement)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metrics",
                            "method_signature": "public metrics()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeSetRunning",
                            "method_signature": "private maybeSetRunning()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "throwOnFatalException",
                            "method_signature": "private throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "throwOnFatalException",
                            "method_signature": "private throwOnFatalException(final Exception fatalUserException,\n                                           final TopicPartition topicPartition,\n                                           final String storeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createAndAddStreamThread",
                            "method_signature": "private createAndAddStreamThread(final long cacheSizePerThread, final int threadIdx)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "registerStreamThread",
                            "method_signature": "private registerStreamThread(StreamThread streamThread)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getMetrics",
                            "method_signature": "static getMetrics(final StreamsConfig config, final Time time, final String clientId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addStreamThread",
                            "method_signature": "public addStreamThread()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeStreamThread",
                            "method_signature": "public removeStreamThread()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeStreamThread",
                            "method_signature": "public removeStreamThread(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeStreamThread",
                            "method_signature": "private removeStreamThread(final long timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getCacheSizePerThread",
                            "method_signature": "private getCacheSizePerThread(final int numStreamThreads)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resizeThreadCache",
                            "method_signature": "private resizeThreadCache(final long cacheSizePerThread)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "setupStateDirCleaner",
                            "method_signature": "private setupStateDirCleaner()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeCreateRocksDBMetricsRecordingService",
                            "method_signature": "private static maybeCreateRocksDBMetricsRecordingService(final String clientId,\n                                                                                      final StreamsConfig config)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "parseHostInfo",
                            "method_signature": "private static parseHostInfo(final String endPoint)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "start",
                            "method_signature": "public synchronized start()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "timeout",
                            "method_signature": "public timeout(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "leaveGroup",
                            "method_signature": "public leaveGroup(final boolean leaveGroup)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shutdownHelper",
                            "method_signature": "private shutdownHelper(final boolean error, final long timeoutMs, final boolean leaveGroup)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "private close(final long timeoutMs, final boolean leaveGroup)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeToError",
                            "method_signature": "private closeToError()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public synchronized close(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public synchronized close(final CloseOptions options)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "streamThreadLeaveConsumerGroup",
                            "method_signature": "private streamThreadLeaveConsumerGroup(final long remainingTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanUp",
                            "method_signature": "public cleanUp()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allMetadata",
                            "method_signature": "@Deprecated\n    public allMetadata()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metadataForAllStreamsClients",
                            "method_signature": "public metadataForAllStreamsClients()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allMetadataForStore",
                            "method_signature": "@Deprecated\n    public allMetadataForStore(final String storeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "streamsMetadataForStore",
                            "method_signature": "public streamsMetadataForStore(final String storeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "queryMetadataForKey",
                            "method_signature": "public queryMetadataForKey(final String storeName,\n                                                    final K key,\n                                                    final Serializer<K> keySerializer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "queryMetadataForKey",
                            "method_signature": "public queryMetadataForKey(final String storeName,\n                                                    final K key,\n                                                    final StreamPartitioner<? super K, ?> partitioner)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "store",
                            "method_signature": "public store(final StoreQueryParameters<T> storeQueryParameters)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pause",
                            "method_signature": "public pause()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isPaused",
                            "method_signature": "public isPaused()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resume",
                            "method_signature": "public resume()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processStreamThread",
                            "method_signature": "protected processStreamThread(final Consumer<StreamThread> consumer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clientInstanceIds",
                            "method_signature": "public synchronized clientInstanceIds(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrThrowException",
                            "method_signature": "private getOrThrowException(\n        final KafkaFuture<T> future,\n        final long timeoutMs,\n        final Supplier<String> errorMessage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "localThreadsMetadata",
                            "method_signature": "@Deprecated\n    public localThreadsMetadata()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "metadataForLocalThreads",
                            "method_signature": "public metadataForLocalThreads()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allLocalStorePartitionLags",
                            "method_signature": "public allLocalStorePartitionLags()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allLocalStorePartitionLags",
                            "method_signature": "protected allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "query",
                            "method_signature": "@Evolving\n    public query(final StateQueryRequest<R> request)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "timeout",
                            "method_signature": "public timeout(final Duration timeout)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "leaveGroup",
                            "method_signature": "public leaveGroup(final boolean leaveGroup)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isValidTransition",
                            "method_signature": "public isValidTransition(final State newState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasNotStarted",
                            "method_signature": "public hasNotStarted()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasCompletedShutdown",
                            "method_signature": "public hasCompletedShutdown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "waitOnState",
                            "method_signature": "private waitOnState(final State targetState, final long waitMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasStartedOrFinishedShuttingDown",
                            "method_signature": "public hasStartedOrFinishedShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasStartedOrFinishedShuttingDown",
                            "method_signature": "protected hasStartedOrFinishedShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isShuttingDown",
                            "method_signature": "public isShuttingDown()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "parseHostInfo",
                            "method_signature": "private static parseHostInfo(final String endPoint)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunningOrRebalancing",
                            "method_signature": "public isRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "setupStateDirCleaner",
                            "method_signature": "private setupStateDirCleaner()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRunningOrRebalancing",
                            "method_signature": "protected isRunningOrRebalancing()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "allLocalStorePartitionLags",
                            "method_signature": "protected allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeToError",
                            "method_signature": "private closeToError()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "public timeout(final Duration timeout)": {
                    "first": {
                        "method_name": "timeout",
                        "method_signature": "public timeout(final Duration timeout)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.15784254719489232
                },
                "public leaveGroup(final boolean leaveGroup)": {
                    "first": {
                        "method_name": "leaveGroup",
                        "method_signature": "public leaveGroup(final boolean leaveGroup)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.18531209491970282
                },
                "public isValidTransition(final State newState)": {
                    "first": {
                        "method_name": "isValidTransition",
                        "method_signature": "public isValidTransition(final State newState)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.20274440278223785
                },
                "public hasNotStarted()": {
                    "first": {
                        "method_name": "hasNotStarted",
                        "method_signature": "public hasNotStarted()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.22006714945895314
                },
                "public hasCompletedShutdown()": {
                    "first": {
                        "method_name": "hasCompletedShutdown",
                        "method_signature": "public hasCompletedShutdown()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2887360825734333
                },
                "private waitOnState(final State targetState, final long waitMs)": {
                    "first": {
                        "method_name": "waitOnState",
                        "method_signature": "private waitOnState(final State targetState, final long waitMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2946398635519916
                },
                "public hasStartedOrFinishedShuttingDown()": {
                    "first": {
                        "method_name": "hasStartedOrFinishedShuttingDown",
                        "method_signature": "public hasStartedOrFinishedShuttingDown()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3374418177840132
                },
                "protected hasStartedOrFinishedShuttingDown()": {
                    "first": {
                        "method_name": "hasStartedOrFinishedShuttingDown",
                        "method_signature": "protected hasStartedOrFinishedShuttingDown()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.34226308204984146
                },
                "public isShuttingDown()": {
                    "first": {
                        "method_name": "isShuttingDown",
                        "method_signature": "public isShuttingDown()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3422641070375944
                },
                "private static parseHostInfo(final String endPoint)": {
                    "first": {
                        "method_name": "parseHostInfo",
                        "method_signature": "private static parseHostInfo(final String endPoint)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.34904441990827306
                },
                "public isRunningOrRebalancing()": {
                    "first": {
                        "method_name": "isRunningOrRebalancing",
                        "method_signature": "public isRunningOrRebalancing()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3622707443391717
                },
                "private setupStateDirCleaner()": {
                    "first": {
                        "method_name": "setupStateDirCleaner",
                        "method_signature": "private setupStateDirCleaner()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.37517496910693743
                },
                "protected isRunningOrRebalancing()": {
                    "first": {
                        "method_name": "isRunningOrRebalancing",
                        "method_signature": "protected isRunningOrRebalancing()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.380752242662194
                },
                "protected allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor)": {
                    "first": {
                        "method_name": "allLocalStorePartitionLags",
                        "method_signature": "protected allLocalStorePartitionLags(final List<Task> tasksToCollectLagFor)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3839319081348691
                },
                "private closeToError()": {
                    "first": {
                        "method_name": "closeToError",
                        "method_signature": "private closeToError()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.39495931157977715
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from private handleVoteRequest(requestMetadata RaftRequest.Inbound) : VoteResponseData in class org.apache.kafka.raft.KafkaRaftClient & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 634,
                    "endLine": 693,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private handleVoteRequest(requestMetadata RaftRequest.Inbound) : VoteResponseData"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 681,
                    "endLine": 681,
                    "startColumn": 9,
                    "endColumn": 80,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 681,
                    "endLine": 681,
                    "startColumn": 62,
                    "endColumn": 78,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 646,
                    "endLine": 740,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private handleVoteRequest(requestMetadata RaftRequest.Inbound) : VoteResponseData"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java",
                    "startLine": 720,
                    "endLine": 723,
                    "startColumn": 35,
                    "endColumn": 10,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(candidateId,partitionRequest.candidateDirectoryId())"
                }
            ],
            "isStatic": true
        },
        "ref_id": 586,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a1a79576cfdda1fadb6e45c03ef22dde5ee09052",
            "newBranchName": "extract-of-handleVoteRequest-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from public fromVotersRecord(voters VotersRecord) : VoterSet in class org.apache.kafka.raft.internals.VoterSet & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 338,
                    "endLine": 376,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public fromVotersRecord(voters VotersRecord) : VoterSet"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 351,
                    "endLine": 351,
                    "startColumn": 17,
                    "endColumn": 48,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 351,
                    "endLine": 351,
                    "startColumn": 31,
                    "endColumn": 47,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 368,
                    "endLine": 391,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public fromVotersRecord(voters VotersRecord) : VoterSet"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 380,
                    "endLine": 380,
                    "startColumn": 21,
                    "endColumn": 77,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(voter.voterId(),voter.voterDirectoryId())"
                }
            ],
            "isStatic": true
        },
        "ref_id": 587,
        "extraction_results": {
            "success": true,
            "newCommitHash": "07604225fbb2b54ae0e2274780d4869c6135d8c4",
            "newBranchName": "extract-of-fromVotersRecord-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from package testVotedCandidateWithoutVotedDirectoryId() : void in class org.apache.kafka.raft.ElectionStateTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/ElectionStateTest.java",
                    "startLine": 44,
                    "endLine": 55,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package testVotedCandidateWithoutVotedDirectoryId() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/ElectionStateTest.java",
                    "startLine": 46,
                    "endLine": 50,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/ElectionStateTest.java",
                    "startLine": 48,
                    "endLine": 48,
                    "startColumn": 30,
                    "endColumn": 46,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/ElectionStateTest.java",
                    "startLine": 43,
                    "endLine": 54,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package testVotedCandidateWithoutVotedDirectoryId() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/ElectionStateTest.java",
                    "startLine": 47,
                    "endLine": 47,
                    "startColumn": 13,
                    "endColumn": 57,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(1,ReplicaKey.NO_DIRECTORY_ID)"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/ElectionStateTest.java",
                    "startLine": 50,
                    "endLine": 50,
                    "startColumn": 51,
                    "endColumn": 95,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(1,ReplicaKey.NO_DIRECTORY_ID)"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/ElectionStateTest.java",
                    "startLine": 52,
                    "endLine": 52,
                    "startColumn": 44,
                    "endColumn": 79,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(1,Uuid.randomUuid())"
                }
            ],
            "isStatic": true
        },
        "ref_id": 588,
        "extraction_results": {
            "success": true,
            "newCommitHash": "1213ba4ab5900b98825d3181ce4aaaade032185b",
            "newBranchName": "extract-of-testVotedCandidateWithoutVotedDirectoryId-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from public testRejectVotesFromSameEpochAfterResigningCandidacy() : void in class org.apache.kafka.raft.KafkaRaftClientTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 133,
                    "endLine": 154,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testRejectVotesFromSameEpochAfterResigningCandidacy() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 140,
                    "endLine": 143,
                    "startColumn": 9,
                    "endColumn": 22,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 142,
                    "endLine": 142,
                    "startColumn": 63,
                    "endColumn": 79,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 140,
                    "endLine": 169,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testRejectVotesFromSameEpochAfterResigningCandidacy(withKip853Rpc boolean) : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 150,
                    "endLine": 150,
                    "startColumn": 40,
                    "endColumn": 90,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(localId,ReplicaKey.NO_DIRECTORY_ID)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 589,
        "extraction_results": {
            "success": true,
            "newCommitHash": "c09364c40b313e17967ae8aecd005823c6716904",
            "newBranchName": "extract-of-testRejectVotesFromSameEpochAfterResigningCandidacy-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from public testGrantVotesFromHigherEpochAfterResigningCandidacy() : void in class org.apache.kafka.raft.KafkaRaftClientTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 184,
                    "endLine": 210,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testGrantVotesFromHigherEpochAfterResigningCandidacy() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 191,
                    "endLine": 194,
                    "startColumn": 9,
                    "endColumn": 22,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 193,
                    "endLine": 193,
                    "startColumn": 63,
                    "endColumn": 79,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 207,
                    "endLine": 241,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testGrantVotesFromHigherEpochAfterResigningCandidacy(withKip853Rpc boolean) : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 40,
                    "endColumn": 90,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(localId,ReplicaKey.NO_DIRECTORY_ID)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 590,
        "extraction_results": {
            "success": true,
            "newCommitHash": "4a35ba7fa81b0d225add0b085934b0ac100d809c",
            "newBranchName": "extract-of-testGrantVotesFromHigherEpochAfterResigningCandidacy-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from public testCandidateIgnoreVoteRequestOnSameEpoch() : void in class org.apache.kafka.raft.KafkaRaftClientTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 1333,
                    "endLine": 1350,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testCandidateIgnoreVoteRequestOnSameEpoch() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 1340,
                    "endLine": 1342,
                    "startColumn": 9,
                    "endColumn": 22,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 1341,
                    "endLine": 1341,
                    "startColumn": 69,
                    "endColumn": 85,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 1465,
                    "endLine": 1484,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testCandidateIgnoreVoteRequestOnSameEpoch(withKip853Rpc boolean) : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java",
                    "startLine": 1474,
                    "endLine": 1474,
                    "startColumn": 46,
                    "endColumn": 96,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(localId,ReplicaKey.NO_DIRECTORY_ID)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 591,
        "extraction_results": {
            "success": true,
            "newCommitHash": "69922171830e9631f9ae21792260660ce528b49b",
            "newBranchName": "extract-of-testCandidateIgnoreVoteRequestOnSameEpoch-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from private voterNode(id int, directoryId Uuid) : VoterSet.VoterNode in class org.apache.kafka.raft.LeaderStateTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/LeaderStateTest.java",
                    "startLine": 924,
                    "endLine": 926,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private voterNode(id int, directoryId Uuid) : VoterSet.VoterNode"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/LeaderStateTest.java",
                    "startLine": 925,
                    "endLine": 925,
                    "startColumn": 9,
                    "endColumn": 96,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/LeaderStateTest.java",
                    "startLine": 925,
                    "endLine": 925,
                    "startColumn": 57,
                    "endColumn": 81,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 70,
                    "endColumn": 94,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/LeaderStateTest.java",
                    "startLine": 1152,
                    "endLine": 1155,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private replicaKey(id int, withDirectoryId boolean) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/LeaderStateTest.java",
                    "startLine": 1154,
                    "endLine": 1154,
                    "startColumn": 16,
                    "endColumn": 46,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(id,directoryId)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 592,
        "extraction_results": {
            "success": true,
            "newCommitHash": "921989f3bae571a01a8168533f4a097f8463c251",
            "newBranchName": "extract-of-voterNode-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from package assertVotedCandidate(epoch int, candidateId int) : void in class org.apache.kafka.raft.RaftClientTestContext & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 471,
                    "endLine": 480,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package assertVotedCandidate(epoch int, candidateId int) : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 472,
                    "endLine": 479,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 475,
                    "endLine": 475,
                    "startColumn": 44,
                    "endColumn": 60,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 504,
                    "endLine": 513,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package assertVotedCandidate(epoch int, candidateId int) : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 508,
                    "endLine": 508,
                    "startColumn": 17,
                    "endColumn": 71,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(candidateId,ReplicaKey.NO_DIRECTORY_ID)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 593,
        "extraction_results": {
            "success": true,
            "newCommitHash": "1010b38457f76212ac13126d380b383dd5a07816",
            "newBranchName": "extract-of-assertVotedCandidate-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from public testCanGrantVoteWithoutDirectoryId(isLogUpToDate boolean) : void in class org.apache.kafka.raft.VotedStateTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 81,
                    "endLine": 99,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testCanGrantVoteWithoutDirectoryId(isLogUpToDate boolean) : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 86,
                    "endLine": 88,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 87,
                    "endLine": 87,
                    "startColumn": 55,
                    "endColumn": 71,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 80,
                    "endLine": 98,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testCanGrantVoteWithoutDirectoryId(isLogUpToDate boolean) : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 86,
                    "endLine": 86,
                    "startColumn": 32,
                    "endColumn": 82,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(votedId,ReplicaKey.NO_DIRECTORY_ID)"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 90,
                    "endLine": 90,
                    "startColumn": 17,
                    "endColumn": 58,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(votedId,Uuid.randomUuid())"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 96,
                    "endLine": 96,
                    "startColumn": 32,
                    "endColumn": 86,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(votedId + 1,ReplicaKey.NO_DIRECTORY_ID)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 594,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a3dcbc8e1113d76e79c62cdfefc1178927822949",
            "newBranchName": "extract-of-testCanGrantVoteWithoutDirectoryId-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from package testCanGrantVoteWithDirectoryId() : void in class org.apache.kafka.raft.VotedStateTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 101,
                    "endLine": 115,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package testCanGrantVoteWithDirectoryId() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 111,
                    "endLine": 111,
                    "startColumn": 9,
                    "endColumn": 90,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 111,
                    "endLine": 111,
                    "startColumn": 63,
                    "endColumn": 79,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 100,
                    "endLine": 114,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package testCanGrantVoteWithDirectoryId() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 108,
                    "endLine": 108,
                    "startColumn": 32,
                    "endColumn": 73,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(votedId,Uuid.randomUuid())"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 110,
                    "endLine": 110,
                    "startColumn": 40,
                    "endColumn": 90,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(votedId,ReplicaKey.NO_DIRECTORY_ID)"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 113,
                    "endLine": 113,
                    "startColumn": 40,
                    "endColumn": 94,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(votedId + 1,ReplicaKey.NO_DIRECTORY_ID)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 595,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a37b52583af45c77835ce95b947b845eb8ca5b72",
            "newBranchName": "extract-of-testCanGrantVoteWithDirectoryId-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from package testRemoveVoter() : void in class org.apache.kafka.raft.internals.VoterSetTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 99,
                    "endLine": 112,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package testRemoveVoter() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 104,
                    "endLine": 104,
                    "startColumn": 9,
                    "endColumn": 98,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 103,
                    "endLine": 116,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package testRemoveVoter() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 108,
                    "endLine": 108,
                    "startColumn": 61,
                    "endColumn": 105,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(4,ReplicaKey.NO_DIRECTORY_ID)"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 109,
                    "endLine": 109,
                    "startColumn": 61,
                    "endColumn": 96,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(4,Uuid.randomUuid())"
                }
            ],
            "isStatic": true
        },
        "ref_id": 596,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a067ca529f03eb9895605805a5db023875822a95",
            "newBranchName": "extract-of-testRemoveVoter-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public of(id int, directoryId Uuid) : ReplicaKey extracted from package testIsVoterWithoutDirectoryId() : void in class org.apache.kafka.raft.internals.VoterSetTest & moved to class org.apache.kafka.raft.internals.ReplicaKey",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 131,
                    "endLine": 140,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package testIsVoterWithoutDirectoryId() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 136,
                    "endLine": 136,
                    "startColumn": 9,
                    "endColumn": 74,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 136,
                    "endLine": 136,
                    "startColumn": 54,
                    "endColumn": 70,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 76,
                    "endLine": 81,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public of(id int, directoryId Uuid) : ReplicaKey"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 77,
                    "endLine": 80,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/ReplicaKey.java",
                    "startLine": 79,
                    "endLine": 79,
                    "startColumn": 51,
                    "endColumn": 67,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 135,
                    "endLine": 144,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package testIsVoterWithoutDirectoryId() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 140,
                    "endLine": 140,
                    "startColumn": 37,
                    "endColumn": 81,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(1,ReplicaKey.NO_DIRECTORY_ID)"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 141,
                    "endLine": 141,
                    "startColumn": 37,
                    "endColumn": 72,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(1,Uuid.randomUuid())"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 142,
                    "endLine": 142,
                    "startColumn": 38,
                    "endColumn": 73,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(4,Uuid.randomUuid())"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/internals/VoterSetTest.java",
                    "startLine": 143,
                    "endLine": 143,
                    "startColumn": 38,
                    "endColumn": 82,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ReplicaKey.of(4,ReplicaKey.NO_DIRECTORY_ID)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 597,
        "extraction_results": {
            "success": true,
            "newCommitHash": "2a5f8c1975fbb531ec70cf01da080908a94bf7ce",
            "newBranchName": "extract-of-testIsVoterWithoutDirectoryId-5b0e96d"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public isVoter(replicaKey ReplicaKey) : boolean extracted from public isVoter(nodeKey ReplicaKey) : boolean in class org.apache.kafka.raft.internals.VoterSet & moved to class org.apache.kafka.raft.internals.VoterSet.VoterNode",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 97,
                    "endLine": 122,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public isVoter(nodeKey ReplicaKey) : boolean"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 117,
                    "endLine": 117,
                    "startColumn": 17,
                    "endColumn": 29,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 113,
                    "endLine": 113,
                    "startColumn": 17,
                    "endColumn": 84,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 112,
                    "endLine": 118,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 112,
                    "endLine": 114,
                    "startColumn": 60,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 114,
                    "endLine": 118,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 303,
                    "endLine": 325,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public isVoter(replicaKey ReplicaKey) : boolean"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 323,
                    "endLine": 323,
                    "startColumn": 17,
                    "endColumn": 29,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 319,
                    "endLine": 319,
                    "startColumn": 17,
                    "endColumn": 80,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 318,
                    "endLine": 324,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 318,
                    "endLine": 320,
                    "startColumn": 53,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 320,
                    "endLine": 324,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 98,
                    "endLine": 114,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public isVoter(replicaKey ReplicaKey) : boolean"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 112,
                    "endLine": 112,
                    "startColumn": 26,
                    "endColumn": 50,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "node.isVoter(replicaKey)"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 316,
                    "endLine": 316,
                    "startColumn": 51,
                    "endColumn": 64,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 316,
                    "endLine": 316,
                    "startColumn": 13,
                    "endColumn": 64,
                    "codeElementType": "IF_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 598,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a59dce86eeab4914e3eae243b556d5c0ee07379a",
            "newBranchName": "extract-isVoter-isVoter-5b0e96d"
        },
        "telemetry": {
            "id": "f38b226c-7093-422b-ac40-0b9f40a0cfe6",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 367,
                "lineStart": 40,
                "lineEnd": 406,
                "bodyLineStart": 40,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                "sourceCode": "/**\n * A type for representing the set of voters for a topic partition.\n *\n * It encapsulates static information like a voter's endpoint and their supported kraft.version.\n *\n * It provides functionality for converting to and from {@code VotersRecord} and for converting\n * from the static configuration.\n */\npublic final class VoterSet {\n    private final Map<Integer, VoterNode> voters;\n\n    VoterSet(Map<Integer, VoterNode> voters) {\n        if (voters.isEmpty()) {\n            throw new IllegalArgumentException(\"Voters cannot be empty\");\n        }\n\n        this.voters = voters;\n    }\n\n    /**\n     * Returns the node information for all the given voter ids and listener.\n     *\n     * @param voterIds the ids of the voters\n     * @param listenerName the name of the listener\n     * @return the node information for all of the voter ids\n     * @throws IllegalArgumentException if there are missing endpoints\n     */\n    public Set<Node> voterNodes(Stream<Integer> voterIds, ListenerName listenerName) {\n        return voterIds\n            .map(voterId ->\n                voterNode(voterId, listenerName).orElseThrow(() ->\n                    new IllegalArgumentException(\n                        String.format(\n                            \"Unable to find endpoint for voter %d and listener %s in %s\",\n                            voterId,\n                            listenerName,\n                            voters\n                        )\n                    )\n                )\n            )\n            .collect(Collectors.toSet());\n    }\n\n    /**\n     * Returns the node information for a given voter id and listener.\n     *\n     * @param voterId the id of the voter\n     * @param listenerName the name of the listener\n     * @return the node information if it exists, otherwise {@code Optional.empty()}\n     */\n    public Optional<Node> voterNode(int voterId, ListenerName listenerName) {\n        return Optional.ofNullable(voters.get(voterId))\n            .flatMap(voterNode -> voterNode.address(listenerName))\n            .map(address -> new Node(voterId, address.getHostString(), address.getPort()));\n    }\n\n    /**\n     * Returns if the node is a voter in the set of voters.\n     *\n     * If the voter set includes the directory id, the {@code nodeKey} directory id must match the\n     * directory id specified by the voter set.\n     *\n     * If the voter set doesn't include the directory id ({@code Optional.empty()}), a node is in\n     * the voter set as long as the node id matches. The directory id is not checked.\n     *\n     * @param nodeKey the node's id and directory id\n     * @return true if the node is a voter in the voter set, otherwise false\n     */\n    public boolean isVoter(ReplicaKey nodeKey) {\n        VoterNode node = voters.get(nodeKey.id());\n        if (node != null) {\n            return isVoter(nodeKey, node);\n        } else {\n            return false;\n        }\n    }\n\n    private boolean isVoter(ReplicaKey nodeKey, VoterNode node) {\n        if (node.voterKey().directoryId().isPresent()) {\n            return node.voterKey().directoryId().equals(nodeKey.directoryId());\n        } else {\n            // configured voter set doesn't include a directory id so it is a voter as long as the node id\n            // matches\n            return true;\n        }\n    }\n\n    /**\n     * Returns if the node is the only voter in the set of voters.\n     *\n     * @param nodeKey the node's id and directory id\n     * @return true if the node is the only voter in the voter set, otherwise false\n     */\n    public boolean isOnlyVoter(ReplicaKey nodeKey) {\n        return voters.size() == 1 && isVoter(nodeKey);\n    }\n\n    /**\n     * Returns all of the voter ids.\n     */\n    public Set<Integer> voterIds() {\n        return voters.keySet();\n    }\n\n    public Map<Integer, VoterNode> voters() {\n        return voters;\n    }\n\n    /**\n     * Adds a voter to the voter set.\n     *\n     * This object is immutable. A new voter set is returned if the voter was added.\n     *\n     * A new voter can be added to a voter set if its id doesn't already exist in the voter set.\n     *\n     * @param voter the new voter to add\n     * @return a new voter set if the voter was added, otherwise {@code Optional.empty()}\n     */\n    public Optional<VoterSet> addVoter(VoterNode voter) {\n        if (voters.containsKey(voter.voterKey().id())) {\n            return Optional.empty();\n        }\n\n        HashMap<Integer, VoterNode> newVoters = new HashMap<>(voters);\n        newVoters.put(voter.voterKey().id(), voter);\n\n        return Optional.of(new VoterSet(newVoters));\n    }\n\n    /**\n     * Remove a voter from the voter set.\n     *\n     * This object is immutable. A new voter set is returned if the voter was removed.\n     *\n     * A voter can be removed from the voter set if its id and directory id match.\n     *\n     * @param voterKey the voter key\n     * @return a new voter set if the voter was removed, otherwise {@code Optional.empty()}\n     */\n    public Optional<VoterSet> removeVoter(ReplicaKey voterKey) {\n        VoterNode oldVoter = voters.get(voterKey.id());\n        if (oldVoter != null && Objects.equals(oldVoter.voterKey(), voterKey)) {\n            HashMap<Integer, VoterNode> newVoters = new HashMap<>(voters);\n            newVoters.remove(voterKey.id());\n\n            return Optional.of(new VoterSet(newVoters));\n        }\n\n        return Optional.empty();\n    }\n\n    /**\n     * Converts a voter set to a voters record for a given version.\n     *\n     * @param version the version of the voters record\n     */\n    public VotersRecord toVotersRecord(short version) {\n        Function<VoterNode, VotersRecord.Voter> voterConvertor = voter -> {\n            Iterator<VotersRecord.Endpoint> endpoints = voter\n                .listeners()\n                .entrySet()\n                .stream()\n                .map(entry ->\n                    new VotersRecord.Endpoint()\n                        .setName(entry.getKey().value())\n                        .setHost(entry.getValue().getHostString())\n                        .setPort(entry.getValue().getPort())\n                )\n                .iterator();\n\n            VotersRecord.KRaftVersionFeature kraftVersionFeature = new VotersRecord.KRaftVersionFeature()\n                .setMinSupportedVersion(voter.supportedKRaftVersion().min())\n                .setMaxSupportedVersion(voter.supportedKRaftVersion().max());\n\n            return new VotersRecord.Voter()\n                .setVoterId(voter.voterKey().id())\n                .setVoterDirectoryId(voter.voterKey().directoryId().orElse(Uuid.ZERO_UUID))\n                .setEndpoints(new VotersRecord.EndpointCollection(endpoints))\n                .setKRaftVersionFeature(kraftVersionFeature);\n        };\n\n        List<VotersRecord.Voter> voterRecordVoters = voters\n            .values()\n            .stream()\n            .map(voterConvertor)\n            .collect(Collectors.toList());\n\n        return new VotersRecord()\n            .setVersion(version)\n            .setVoters(voterRecordVoters);\n    }\n\n    /**\n     * Determines if two sets of voters have an overlapping majority.\n     *\n     * An overlapping majority means that for all majorities in {@code this} set of voters and for\n     * all majority in {@code that} set of voters, they have at least one voter in common.\n     *\n     * If this function returns true, it means that if one of the set of voters commits an offset,\n     * the other set of voters cannot commit a conflicting offset.\n     *\n     * @param that the other voter set to compare\n     * @return true if they have an overlapping majority, false otherwise\n     */\n    public boolean hasOverlappingMajority(VoterSet that) {\n        Set<ReplicaKey> thisReplicaKeys = voters\n            .values()\n            .stream()\n            .map(VoterNode::voterKey)\n            .collect(Collectors.toSet());\n\n        Set<ReplicaKey> thatReplicaKeys = that.voters\n            .values()\n            .stream()\n            .map(VoterNode::voterKey)\n            .collect(Collectors.toSet());\n\n        if (Utils.diff(HashSet::new, thisReplicaKeys, thatReplicaKeys).size() > 1) return false;\n        return Utils.diff(HashSet::new, thatReplicaKeys, thisReplicaKeys).size() <= 1;\n    }\n\n    @Override\n    public boolean equals(Object o) {\n        if (this == o) return true;\n        if (o == null || getClass() != o.getClass()) return false;\n\n        VoterSet that = (VoterSet) o;\n\n        return voters.equals(that.voters);\n    }\n\n    @Override\n    public int hashCode() {\n        return Objects.hashCode(voters);\n    }\n\n    @Override\n    public String toString() {\n        return String.format(\"VoterSet(voters=%s)\", voters);\n    }\n\n    public static final class VoterNode {\n        private final ReplicaKey voterKey;\n        private final Map<ListenerName, InetSocketAddress> listeners;\n        private final SupportedVersionRange supportedKRaftVersion;\n\n        public VoterNode(\n            ReplicaKey voterKey,\n            Map<ListenerName, InetSocketAddress> listeners,\n            SupportedVersionRange supportedKRaftVersion\n        ) {\n            this.voterKey = voterKey;\n            this.listeners = listeners;\n            this.supportedKRaftVersion = supportedKRaftVersion;\n        }\n\n        public ReplicaKey voterKey() {\n            return voterKey;\n        }\n\n        Map<ListenerName, InetSocketAddress> listeners() {\n            return listeners;\n        }\n\n        SupportedVersionRange supportedKRaftVersion() {\n            return supportedKRaftVersion;\n        }\n\n\n        Optional<InetSocketAddress> address(ListenerName listener) {\n            return Optional.ofNullable(listeners.get(listener));\n        }\n\n        @Override\n        public boolean equals(Object o) {\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n\n            VoterNode that = (VoterNode) o;\n\n            if (!Objects.equals(voterKey, that.voterKey)) return false;\n            if (!Objects.equals(supportedKRaftVersion, that.supportedKRaftVersion)) return false;\n            return Objects.equals(listeners, that.listeners);\n        }\n\n        @Override\n        public int hashCode() {\n            return Objects.hash(voterKey, listeners, supportedKRaftVersion);\n        }\n\n        @Override\n        public String toString() {\n            return String.format(\n                \"VoterNode(voterKey=%s, listeners=%s, supportedKRaftVersion=%s)\",\n                voterKey,\n                listeners,\n                supportedKRaftVersion\n            );\n        }\n    }\n\n    /**\n     * Converts a {@code VotersRecord} to a {@code VoterSet}.\n     *\n     * @param voters the set of voters control record\n     * @return the voter set\n     */\n    public static VoterSet fromVotersRecord(VotersRecord voters) {\n        HashMap<Integer, VoterNode> voterNodes = new HashMap<>(voters.voters().size());\n        for (VotersRecord.Voter voter: voters.voters()) {\n            final Optional<Uuid> directoryId;\n            if (!voter.voterDirectoryId().equals(Uuid.ZERO_UUID)) {\n                directoryId = Optional.of(voter.voterDirectoryId());\n            } else {\n                directoryId = Optional.empty();\n            }\n\n            Map<ListenerName, InetSocketAddress> listeners = new HashMap<>(voter.endpoints().size());\n            for (VotersRecord.Endpoint endpoint : voter.endpoints()) {\n                listeners.put(\n                    ListenerName.normalised(endpoint.name()),\n                    InetSocketAddress.createUnresolved(endpoint.host(), endpoint.port())\n                );\n            }\n\n            voterNodes.put(\n                voter.voterId(),\n                new VoterNode(\n                    ReplicaKey.of(voter.voterId(), directoryId),\n                    listeners,\n                    new SupportedVersionRange(\n                        voter.kRaftVersionFeature().minSupportedVersion(),\n                        voter.kRaftVersionFeature().maxSupportedVersion()\n                    )\n                )\n            );\n        }\n\n        return new VoterSet(voterNodes);\n    }\n\n    /**\n     * Creates a voter set from a map of socket addresses.\n     *\n     * @param listener the listener name for all of the endpoints\n     * @param voters the socket addresses by voter id\n     * @return the voter set\n     */\n    public static VoterSet fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters) {\n        Map<Integer, VoterNode> voterNodes = voters\n            .entrySet()\n            .stream()\n            .collect(\n                Collectors.toMap(\n                    Map.Entry::getKey,\n                    entry -> new VoterNode(\n                        ReplicaKey.of(entry.getKey(), Optional.empty()),\n                        Collections.singletonMap(listener, entry.getValue()),\n                        new SupportedVersionRange((short) 0, (short) 0)\n                    )\n                )\n            );\n\n        return new VoterSet(voterNodes);\n    }\n}",
                "methodCount": 25
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "voterNodes",
                            "method_signature": "public voterNodes(Stream<Integer> voterIds, ListenerName listenerName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int voterId, ListenerName listenerName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isVoter",
                            "method_signature": "public isVoter(ReplicaKey nodeKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isVoter",
                            "method_signature": "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isOnlyVoter",
                            "method_signature": "public isOnlyVoter(ReplicaKey nodeKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterIds",
                            "method_signature": "public voterIds()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addVoter",
                            "method_signature": "public addVoter(VoterNode voter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeVoter",
                            "method_signature": "public removeVoter(ReplicaKey voterKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toVotersRecord",
                            "method_signature": "public toVotersRecord(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOverlappingMajority",
                            "method_signature": "public hasOverlappingMajority(VoterSet that)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "address",
                            "method_signature": " address(ListenerName listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fromVotersRecord",
                            "method_signature": "public static fromVotersRecord(VotersRecord voters)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fromInetSocketAddresses",
                            "method_signature": "public static fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "address",
                            "method_signature": " address(ListenerName listener)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOverlappingMajority",
                            "method_signature": "public hasOverlappingMajority(VoterSet that)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isOnlyVoter",
                            "method_signature": "public isOnlyVoter(ReplicaKey nodeKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterIds",
                            "method_signature": "public voterIds()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isVoter",
                            "method_signature": "public isVoter(ReplicaKey nodeKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNode",
                            "method_signature": "public voterNode(int voterId, ListenerName listenerName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isVoter",
                            "method_signature": "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterNodes",
                            "method_signature": "public voterNodes(Stream<Integer> voterIds, ListenerName listenerName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeVoter",
                            "method_signature": "public removeVoter(ReplicaKey voterKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addVoter",
                            "method_signature": "public addVoter(VoterNode voter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toVotersRecord",
                            "method_signature": "public toVotersRecord(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fromInetSocketAddresses",
                            "method_signature": "public static fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fromVotersRecord",
                            "method_signature": "public static fromVotersRecord(VotersRecord voters)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                " address(ListenerName listener)": {
                    "first": {
                        "method_name": "address",
                        "method_signature": " address(ListenerName listener)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.38133646675114885
                },
                "public hasOverlappingMajority(VoterSet that)": {
                    "first": {
                        "method_name": "hasOverlappingMajority",
                        "method_signature": "public hasOverlappingMajority(VoterSet that)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5133125854160105
                },
                "public isOnlyVoter(ReplicaKey nodeKey)": {
                    "first": {
                        "method_name": "isOnlyVoter",
                        "method_signature": "public isOnlyVoter(ReplicaKey nodeKey)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5940896907643518
                },
                "public voterIds()": {
                    "first": {
                        "method_name": "voterIds",
                        "method_signature": "public voterIds()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6007570955350559
                },
                "public isVoter(ReplicaKey nodeKey)": {
                    "first": {
                        "method_name": "isVoter",
                        "method_signature": "public isVoter(ReplicaKey nodeKey)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6344012744714012
                },
                "public voterNode(int voterId, ListenerName listenerName)": {
                    "first": {
                        "method_name": "voterNode",
                        "method_signature": "public voterNode(int voterId, ListenerName listenerName)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6460647093026198
                },
                "private isVoter(ReplicaKey nodeKey, VoterNode node)": {
                    "first": {
                        "method_name": "isVoter",
                        "method_signature": "private isVoter(ReplicaKey nodeKey, VoterNode node)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6541872841213975
                },
                "public voterNodes(Stream<Integer> voterIds, ListenerName listenerName)": {
                    "first": {
                        "method_name": "voterNodes",
                        "method_signature": "public voterNodes(Stream<Integer> voterIds, ListenerName listenerName)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6674553839812288
                },
                "public removeVoter(ReplicaKey voterKey)": {
                    "first": {
                        "method_name": "removeVoter",
                        "method_signature": "public removeVoter(ReplicaKey voterKey)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6697522585183326
                },
                "public addVoter(VoterNode voter)": {
                    "first": {
                        "method_name": "addVoter",
                        "method_signature": "public addVoter(VoterNode voter)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6774990165798196
                },
                "public toVotersRecord(short version)": {
                    "first": {
                        "method_name": "toVotersRecord",
                        "method_signature": "public toVotersRecord(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7263314158254318
                },
                "public static fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters)": {
                    "first": {
                        "method_name": "fromInetSocketAddresses",
                        "method_signature": "public static fromInetSocketAddresses(ListenerName listener, Map<Integer, InetSocketAddress> voters)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7456380831407743
                },
                "public static fromVotersRecord(VotersRecord voters)": {
                    "first": {
                        "method_name": "fromVotersRecord",
                        "method_signature": "public static fromVotersRecord(VotersRecord voters)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.8536823805328253
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public voterKeys() : Set<ReplicaKey> extracted from public advanceLocalLeaderHighWatermarkToLogEndOffset() : void in class org.apache.kafka.raft.RaftClientTestContext & moved to class org.apache.kafka.raft.internals.VoterSet",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 1156,
                    "endLine": 1171,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public advanceLocalLeaderHighWatermarkToLogEndOffset() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 1159,
                    "endLine": 1159,
                    "startColumn": 9,
                    "endColumn": 123,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 133,
                    "endLine": 142,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public voterKeys() : Set<ReplicaKey>"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/internals/VoterSet.java",
                    "startLine": 137,
                    "endLine": 141,
                    "startColumn": 9,
                    "endColumn": 42,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 1466,
                    "endLine": 1487,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public advanceLocalLeaderHighWatermarkToLogEndOffset() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 1470,
                    "endLine": 1471,
                    "startColumn": 48,
                    "endColumn": 25,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "voters.voterKeys()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 599,
        "extraction_results": {
            "success": true,
            "newCommitHash": "62284eacd3fc8ac2c2d1c96aec657cfc93a1434c",
            "newBranchName": "extract-voterKeys-advanceLocalLeaderHighWatermarkToLogEndOffset-5b0e96d"
        },
        "telemetry": {
            "id": "f9a50bfc-b3e8-4a40-ad9f-d04a0217d921",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1218,
                "lineStart": 94,
                "lineEnd": 1311,
                "bodyLineStart": 94,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                "sourceCode": "public final class RaftClientTestContext {\n    public final RecordSerde<String> serde = Builder.SERDE;\n    final TopicPartition metadataPartition = Builder.METADATA_PARTITION;\n    final Uuid metadataTopicId = Uuid.METADATA_TOPIC_ID;\n    final int electionBackoffMaxMs = Builder.ELECTION_BACKOFF_MAX_MS;\n    final int fetchMaxWaitMs = Builder.FETCH_MAX_WAIT_MS;\n    final int fetchTimeoutMs = Builder.FETCH_TIMEOUT_MS;\n    final int checkQuorumTimeoutMs = (int) (fetchTimeoutMs * CHECK_QUORUM_TIMEOUT_FACTOR);\n    final int retryBackoffMs = Builder.RETRY_BACKOFF_MS;\n\n    private int electionTimeoutMs;\n    private int requestTimeoutMs;\n    private int appendLingerMs;\n\n    private final QuorumStateStore quorumStateStore;\n    final Uuid clusterId;\n    private final OptionalInt localId;\n    public final KafkaRaftClient<String> client;\n    final Metrics metrics;\n    public final MockLog log;\n    final MockNetworkChannel channel;\n    final MockMessageQueue messageQueue;\n    final MockTime time;\n    final MockListener listener;\n    final Set<Integer> voters;\n    final Set<Integer> bootstrapIds;\n\n    private final List<RaftResponse.Outbound> sentResponses = new ArrayList<>();\n\n    public static final class Builder {\n        static final int DEFAULT_ELECTION_TIMEOUT_MS = 10000;\n\n        private static final RecordSerde<String> SERDE = new StringSerde();\n        private static final TopicPartition METADATA_PARTITION = new TopicPartition(\"metadata\", 0);\n        private static final int ELECTION_BACKOFF_MAX_MS = 100;\n        private static final int FETCH_MAX_WAIT_MS = 0;\n        // fetch timeout is usually larger than election timeout\n        private static final int FETCH_TIMEOUT_MS = 50000;\n        private static final int DEFAULT_REQUEST_TIMEOUT_MS = 5000;\n        private static final int RETRY_BACKOFF_MS = 50;\n        private static final int DEFAULT_APPEND_LINGER_MS = 0;\n\n        private final MockMessageQueue messageQueue = new MockMessageQueue();\n        private final MockTime time = new MockTime();\n        private final QuorumStateStore quorumStateStore = new MockQuorumStateStore();\n        private final MockableRandom random = new MockableRandom(1L);\n        private final LogContext logContext = new LogContext();\n        private final MockLog log = new MockLog(METADATA_PARTITION, Uuid.METADATA_TOPIC_ID, logContext);\n        private final Uuid clusterId = Uuid.randomUuid();\n        private final Set<Integer> voters;\n        private final OptionalInt localId;\n        private final Uuid localDirectoryId = Uuid.randomUuid();\n        private final short kraftVersion = 0;\n\n        private int requestTimeoutMs = DEFAULT_REQUEST_TIMEOUT_MS;\n        private int electionTimeoutMs = DEFAULT_ELECTION_TIMEOUT_MS;\n        private int appendLingerMs = DEFAULT_APPEND_LINGER_MS;\n        private MemoryPool memoryPool = MemoryPool.NONE;\n        private List<InetSocketAddress> bootstrapServers = Collections.emptyList();\n\n        public Builder(int localId, Set<Integer> voters) {\n            this(OptionalInt.of(localId), voters);\n        }\n\n        public Builder(OptionalInt localId, Set<Integer> voters) {\n            this.voters = voters;\n            this.localId = localId;\n        }\n\n        Builder withElectedLeader(int epoch, int leaderId) {\n            quorumStateStore.writeElectionState(\n                ElectionState.withElectedLeader(epoch, leaderId, voters),\n                kraftVersion\n            );\n            return this;\n        }\n\n        Builder withUnknownLeader(int epoch) {\n            quorumStateStore.writeElectionState(\n                ElectionState.withUnknownLeader(epoch, voters),\n                kraftVersion\n            );\n            return this;\n        }\n\n        Builder withVotedCandidate(int epoch, ReplicaKey votedKey) {\n            quorumStateStore.writeElectionState(\n                ElectionState.withVotedCandidate(epoch, votedKey, voters),\n                kraftVersion\n            );\n            return this;\n        }\n\n        Builder updateRandom(Consumer<MockableRandom> consumer) {\n            consumer.accept(random);\n            return this;\n        }\n\n        Builder withMemoryPool(MemoryPool pool) {\n            this.memoryPool = pool;\n            return this;\n        }\n\n        Builder withAppendLingerMs(int appendLingerMs) {\n            this.appendLingerMs = appendLingerMs;\n            return this;\n        }\n\n        public Builder appendToLog(int epoch, List<String> records) {\n            MemoryRecords batch = buildBatch(\n                time.milliseconds(),\n                log.endOffset().offset,\n                epoch,\n                records\n            );\n            log.appendAsLeader(batch, epoch);\n            // Need to flush the log to update the last flushed offset. This is always correct\n            // because append operation was done in the Builder which represent the state of the\n            // log before the replica starts.\n            log.flush(false);\n\n            // Reset the value of this method since \"flush\" before the replica start should not\n            // count when checking for flushes by the KRaft client.\n            log.flushedSinceLastChecked();\n            return this;\n        }\n\n        Builder withEmptySnapshot(OffsetAndEpoch snapshotId) {\n            try (RawSnapshotWriter snapshot = log.createNewSnapshotUnchecked(snapshotId).get()) {\n                snapshot.freeze();\n            }\n            return this;\n        }\n\n        Builder deleteBeforeSnapshot(OffsetAndEpoch snapshotId) {\n            if (snapshotId.offset() > log.highWatermark().offset) {\n                log.updateHighWatermark(new LogOffsetMetadata(snapshotId.offset()));\n            }\n            log.deleteBeforeSnapshot(snapshotId);\n\n            return this;\n        }\n\n        Builder withElectionTimeoutMs(int electionTimeoutMs) {\n            this.electionTimeoutMs = electionTimeoutMs;\n            return this;\n        }\n\n        Builder withRequestTimeoutMs(int requestTimeoutMs) {\n            this.requestTimeoutMs = requestTimeoutMs;\n            return this;\n        }\n\n        Builder withBootstrapServers(List<InetSocketAddress> bootstrapServers) {\n            this.bootstrapServers = bootstrapServers;\n            return this;\n        }\n\n        public RaftClientTestContext build() throws IOException {\n            Metrics metrics = new Metrics(time);\n            MockNetworkChannel channel = new MockNetworkChannel();\n            MockListener listener = new MockListener(localId);\n            Map<Integer, InetSocketAddress> voterAddressMap = voters\n                .stream()\n                .collect(Collectors.toMap(Function.identity(), RaftClientTestContext::mockAddress));\n\n            QuorumConfig quorumConfig = new QuorumConfig(\n                requestTimeoutMs,\n                RETRY_BACKOFF_MS,\n                electionTimeoutMs,\n                ELECTION_BACKOFF_MAX_MS,\n                FETCH_TIMEOUT_MS,\n                appendLingerMs\n            );\n\n            KafkaRaftClient<String> client = new KafkaRaftClient<>(\n                localId,\n                localDirectoryId,\n                SERDE,\n                channel,\n                messageQueue,\n                log,\n                memoryPool,\n                time,\n                new MockExpirationService(time),\n                FETCH_MAX_WAIT_MS,\n                clusterId.toString(),\n                bootstrapServers,\n                logContext,\n                random,\n                quorumConfig\n            );\n\n            client.register(listener);\n            client.initialize(\n                voterAddressMap,\n                quorumStateStore,\n                metrics\n            );\n\n            RaftClientTestContext context = new RaftClientTestContext(\n                clusterId,\n                localId,\n                client,\n                log,\n                channel,\n                messageQueue,\n                time,\n                quorumStateStore,\n                voters,\n                IntStream\n                    .iterate(-2, id -> id - 1)\n                    .limit(bootstrapServers.size())\n                    .boxed()\n                    .collect(Collectors.toSet()),\n                metrics,\n                listener\n            );\n\n            context.electionTimeoutMs = electionTimeoutMs;\n            context.requestTimeoutMs = requestTimeoutMs;\n            context.appendLingerMs = appendLingerMs;\n\n            return context;\n        }\n    }\n\n    private RaftClientTestContext(\n        Uuid clusterId,\n        OptionalInt localId,\n        KafkaRaftClient<String> client,\n        MockLog log,\n        MockNetworkChannel channel,\n        MockMessageQueue messageQueue,\n        MockTime time,\n        QuorumStateStore quorumStateStore,\n        Set<Integer> voters,\n        Set<Integer> bootstrapIds,\n        Metrics metrics,\n        MockListener listener\n    ) {\n        this.clusterId = clusterId;\n        this.localId = localId;\n        this.client = client;\n        this.log = log;\n        this.channel = channel;\n        this.messageQueue = messageQueue;\n        this.time = time;\n        this.quorumStateStore = quorumStateStore;\n        this.voters = voters;\n        this.bootstrapIds = bootstrapIds;\n        this.metrics = metrics;\n        this.listener = listener;\n    }\n\n    int electionTimeoutMs() {\n        return electionTimeoutMs;\n    }\n\n    int requestTimeoutMs() {\n        return requestTimeoutMs;\n    }\n\n    int appendLingerMs() {\n        return appendLingerMs;\n    }\n\n    MemoryRecords buildBatch(\n        long baseOffset,\n        int epoch,\n        List<String> records\n    ) {\n        return buildBatch(time.milliseconds(), baseOffset, epoch, records);\n    }\n\n    static MemoryRecords buildBatch(\n        long timestamp,\n        long baseOffset,\n        int epoch,\n        List<String> records\n    ) {\n        ByteBuffer buffer = ByteBuffer.allocate(512);\n        BatchBuilder<String> builder = new BatchBuilder<>(\n            buffer,\n            Builder.SERDE,\n            Compression.NONE,\n            baseOffset,\n            timestamp,\n            false,\n            epoch,\n            512\n        );\n\n        for (String record : records) {\n            builder.appendRecord(record, null);\n        }\n\n        return builder.build();\n    }\n\n    static RaftClientTestContext initializeAsLeader(int localId, Set<Integer> voters, int epoch) throws Exception {\n        if (epoch <= 0) {\n            throw new IllegalArgumentException(\"Cannot become leader in epoch \" + epoch);\n        }\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withUnknownLeader(epoch - 1)\n            .build();\n\n        context.assertUnknownLeader(epoch - 1);\n        context.becomeLeader();\n        return context;\n    }\n\n    public void becomeLeader() throws Exception {\n        int currentEpoch = currentEpoch();\n        time.sleep(electionTimeoutMs * 2L);\n        expectAndGrantVotes(currentEpoch + 1);\n        expectBeginEpoch(currentEpoch + 1);\n    }\n\n    public OptionalInt currentLeader() {\n        return currentLeaderAndEpoch().leaderId();\n    }\n\n    public int currentEpoch() {\n        return currentLeaderAndEpoch().epoch();\n    }\n\n    LeaderAndEpoch currentLeaderAndEpoch() {\n        ElectionState election = quorumStateStore.readElectionState().get();\n        return new LeaderAndEpoch(election.optionalLeaderId(), election.epoch());\n    }\n\n    void expectAndGrantVotes(int epoch) throws Exception {\n        pollUntilRequest();\n\n        List<RaftRequest.Outbound> voteRequests = collectVoteRequests(epoch,\n            log.lastFetchedEpoch(), log.endOffset().offset);\n\n        for (RaftRequest.Outbound request : voteRequests) {\n            VoteResponseData voteResponse = voteResponse(true, Optional.empty(), epoch);\n            deliverResponse(request.correlationId(), request.destination(), voteResponse);\n        }\n\n        client.poll();\n        assertElectedLeader(epoch, localIdOrThrow());\n    }\n\n    private int localIdOrThrow() {\n        return localId.orElseThrow(() -> new AssertionError(\"Required local id is not defined\"));\n    }\n\n    private void expectBeginEpoch(int epoch) throws Exception {\n        pollUntilRequest();\n        for (RaftRequest.Outbound request : collectBeginEpochRequests(epoch)) {\n            BeginQuorumEpochResponseData beginEpochResponse = beginEpochResponse(epoch, localIdOrThrow());\n            deliverResponse(request.correlationId(), request.destination(), beginEpochResponse);\n        }\n        client.poll();\n    }\n\n    public void pollUntil(TestCondition condition) throws InterruptedException {\n        TestUtils.waitForCondition(() -> {\n            client.poll();\n            return condition.conditionMet();\n        }, 5000, \"Condition failed to be satisfied before timeout\");\n    }\n\n    void pollUntilResponse() throws InterruptedException {\n        pollUntil(() -> !sentResponses.isEmpty());\n    }\n\n    void pollUntilRequest() throws InterruptedException {\n        pollUntil(channel::hasSentRequests);\n    }\n\n    void assertVotedCandidate(int epoch, int candidateId) {\n        assertEquals(\n            ElectionState.withVotedCandidate(\n                epoch,\n                ReplicaKey.of(candidateId, Optional.empty()),\n                voters\n            ),\n            quorumStateStore.readElectionState().get()\n        );\n    }\n\n    public void assertElectedLeader(int epoch, int leaderId) {\n        assertEquals(\n            ElectionState.withElectedLeader(epoch, leaderId, voters),\n            quorumStateStore.readElectionState().get()\n        );\n    }\n\n    void assertUnknownLeader(int epoch) {\n        assertEquals(\n            ElectionState.withUnknownLeader(epoch, voters),\n            quorumStateStore.readElectionState().get()\n        );\n    }\n\n    void assertResignedLeader(int epoch, int leaderId) {\n        assertTrue(client.quorum().isResigned());\n        assertEquals(\n            ElectionState.withElectedLeader(epoch, leaderId, voters),\n            quorumStateStore.readElectionState().get()\n        );\n    }\n\n    DescribeQuorumResponseData collectDescribeQuorumResponse() {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.DESCRIBE_QUORUM);\n        assertEquals(1, sentMessages.size());\n        RaftResponse.Outbound raftMessage = sentMessages.get(0);\n        assertInstanceOf(\n            DescribeQuorumResponseData.class,\n            raftMessage.data(),\n            \"Unexpected request type \" + raftMessage.data());\n        return (DescribeQuorumResponseData) raftMessage.data();\n    }\n\n    void assertSentDescribeQuorumResponse(\n        int leaderId,\n        int leaderEpoch,\n        long highWatermark,\n        List<ReplicaState> voterStates,\n        List<ReplicaState> observerStates\n    ) {\n        DescribeQuorumResponseData response = collectDescribeQuorumResponse();\n\n        DescribeQuorumResponseData.PartitionData partitionData = new DescribeQuorumResponseData.PartitionData()\n            .setErrorCode(Errors.NONE.code())\n            .setLeaderId(leaderId)\n            .setLeaderEpoch(leaderEpoch)\n            .setHighWatermark(highWatermark)\n            .setCurrentVoters(voterStates)\n            .setObservers(observerStates);\n\n        DescribeQuorumResponseData.NodeCollection nodes = new DescribeQuorumResponseData.NodeCollection();\n\n        Consumer<DescribeQuorumResponseData.ReplicaState> addToNodes = replicaState -> {\n            if (nodes.find(replicaState.replicaId()) != null)\n                return;\n\n            nodes.add(new DescribeQuorumResponseData.Node()\n                .setNodeId(replicaState.replicaId()));\n        };\n\n        voterStates.forEach(addToNodes);\n        observerStates.forEach(addToNodes);\n\n        DescribeQuorumResponseData expectedResponse = DescribeQuorumResponse.singletonResponse(\n            metadataPartition,\n            partitionData,\n            nodes\n        );\n        assertEquals(expectedResponse, response);\n    }\n\n    RaftRequest.Outbound assertSentVoteRequest(int epoch, int lastEpoch, long lastEpochOffset, int numVoteReceivers) {\n        List<RaftRequest.Outbound> voteRequests = collectVoteRequests(epoch, lastEpoch, lastEpochOffset);\n        assertEquals(numVoteReceivers, voteRequests.size());\n        return voteRequests.iterator().next();\n    }\n\n    void assertSentVoteResponse(Errors error) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.VOTE);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(VoteResponseData.class, raftMessage.data());\n        VoteResponseData response = (VoteResponseData) raftMessage.data();\n\n        assertEquals(error, Errors.forCode(response.errorCode()));\n    }\n\n    void assertSentVoteResponse(\n        Errors error,\n        int epoch,\n        OptionalInt leaderId,\n        boolean voteGranted\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.VOTE);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(VoteResponseData.class, raftMessage.data());\n        VoteResponseData response = (VoteResponseData) raftMessage.data();\n        assertTrue(hasValidTopicPartition(response, metadataPartition));\n\n        VoteResponseData.PartitionData partitionResponse = response.topics().get(0).partitions().get(0);\n\n        assertEquals(voteGranted, partitionResponse.voteGranted());\n        assertEquals(error, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.leaderEpoch());\n        assertEquals(leaderId.orElse(-1), partitionResponse.leaderId());\n    }\n\n    List<RaftRequest.Outbound> collectVoteRequests(\n        int epoch,\n        int lastEpoch,\n        long lastEpochOffset\n    ) {\n        List<RaftRequest.Outbound> voteRequests = new ArrayList<>();\n        for (RaftRequest.Outbound raftMessage : channel.drainSendQueue()) {\n            if (raftMessage.data() instanceof VoteRequestData) {\n                VoteRequestData request = (VoteRequestData) raftMessage.data();\n                VoteRequestData.PartitionData partitionRequest = unwrap(request);\n\n                assertEquals(epoch, partitionRequest.candidateEpoch());\n                assertEquals(localIdOrThrow(), partitionRequest.candidateId());\n                assertEquals(lastEpoch, partitionRequest.lastOffsetEpoch());\n                assertEquals(lastEpochOffset, partitionRequest.lastOffset());\n                voteRequests.add(raftMessage);\n            }\n        }\n        return voteRequests;\n    }\n\n    void deliverRequest(ApiMessage request) {\n        RaftRequest.Inbound inboundRequest = new RaftRequest.Inbound(\n            channel.newCorrelationId(), request.highestSupportedVersion(), request, time.milliseconds());\n        inboundRequest.completion.whenComplete((response, exception) -> {\n            if (exception != null) {\n                throw new RuntimeException(exception);\n            } else {\n                sentResponses.add(response);\n            }\n        });\n        client.handle(inboundRequest);\n    }\n\n    void deliverResponse(int correlationId, Node source, ApiMessage response) {\n        channel.mockReceive(new RaftResponse.Inbound(correlationId, response, source));\n    }\n\n    RaftRequest.Outbound assertSentBeginQuorumEpochRequest(int epoch, int numBeginEpochRequests) {\n        List<RaftRequest.Outbound> requests = collectBeginEpochRequests(epoch);\n        assertEquals(numBeginEpochRequests, requests.size());\n        return requests.get(0);\n    }\n\n    private List<RaftResponse.Outbound> drainSentResponses(\n        ApiKeys apiKey\n    ) {\n        List<RaftResponse.Outbound> res = new ArrayList<>();\n        Iterator<RaftResponse.Outbound> iterator = sentResponses.iterator();\n        while (iterator.hasNext()) {\n            RaftResponse.Outbound response = iterator.next();\n            if (response.data().apiKey() == apiKey.id) {\n                res.add(response);\n                iterator.remove();\n            }\n        }\n        return res;\n    }\n\n    void assertSentBeginQuorumEpochResponse(\n            Errors responseError\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.BEGIN_QUORUM_EPOCH);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(BeginQuorumEpochResponseData.class, raftMessage.data());\n        BeginQuorumEpochResponseData response = (BeginQuorumEpochResponseData) raftMessage.data();\n        assertEquals(responseError, Errors.forCode(response.errorCode()));\n    }\n\n    void assertSentBeginQuorumEpochResponse(\n        Errors partitionError,\n        int epoch,\n        OptionalInt leaderId\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.BEGIN_QUORUM_EPOCH);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(BeginQuorumEpochResponseData.class, raftMessage.data());\n        BeginQuorumEpochResponseData response = (BeginQuorumEpochResponseData) raftMessage.data();\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n\n        BeginQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        assertEquals(epoch, partitionResponse.leaderEpoch());\n        assertEquals(leaderId.orElse(-1), partitionResponse.leaderId());\n        assertEquals(partitionError, Errors.forCode(partitionResponse.errorCode()));\n    }\n\n    RaftRequest.Outbound assertSentEndQuorumEpochRequest(int epoch, int destinationId) {\n        List<RaftRequest.Outbound> endQuorumRequests = collectEndQuorumRequests(\n            epoch,\n            Collections.singleton(destinationId),\n            Optional.empty()\n        );\n        assertEquals(1, endQuorumRequests.size());\n        return endQuorumRequests.get(0);\n    }\n\n    void assertSentEndQuorumEpochResponse(\n        Errors responseError\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.END_QUORUM_EPOCH);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(EndQuorumEpochResponseData.class, raftMessage.data());\n        EndQuorumEpochResponseData response = (EndQuorumEpochResponseData) raftMessage.data();\n        assertEquals(responseError, Errors.forCode(response.errorCode()));\n    }\n\n    void assertSentEndQuorumEpochResponse(\n        Errors partitionError,\n        int epoch,\n        OptionalInt leaderId\n    ) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.END_QUORUM_EPOCH);\n        assertEquals(1, sentMessages.size());\n        RaftMessage raftMessage = sentMessages.get(0);\n        assertInstanceOf(EndQuorumEpochResponseData.class, raftMessage.data());\n        EndQuorumEpochResponseData response = (EndQuorumEpochResponseData) raftMessage.data();\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n\n        EndQuorumEpochResponseData.PartitionData partitionResponse =\n            response.topics().get(0).partitions().get(0);\n\n        assertEquals(epoch, partitionResponse.leaderEpoch());\n        assertEquals(leaderId.orElse(-1), partitionResponse.leaderId());\n        assertEquals(partitionError, Errors.forCode(partitionResponse.errorCode()));\n    }\n\n    RaftRequest.Outbound assertSentFetchRequest() {\n        List<RaftRequest.Outbound> sentRequests = channel.drainSentRequests(Optional.of(ApiKeys.FETCH));\n        assertEquals(1, sentRequests.size());\n        return sentRequests.get(0);\n    }\n\n    RaftRequest.Outbound assertSentFetchRequest(\n        int epoch,\n        long fetchOffset,\n        int lastFetchedEpoch\n    ) {\n        List<RaftRequest.Outbound> sentMessages = channel.drainSendQueue();\n        assertEquals(1, sentMessages.size());\n\n        RaftRequest.Outbound raftRequest = sentMessages.get(0);\n        assertFetchRequestData(raftRequest, epoch, fetchOffset, lastFetchedEpoch);\n        return raftRequest;\n    }\n\n    FetchResponseData.PartitionData assertSentFetchPartitionResponse() {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.FETCH);\n        assertEquals(\n            1, sentMessages.size(), \"Found unexpected sent messages \" + sentMessages);\n        RaftResponse.Outbound raftMessage = sentMessages.get(0);\n        assertEquals(ApiKeys.FETCH.id, raftMessage.data().apiKey());\n        FetchResponseData response = (FetchResponseData) raftMessage.data();\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n\n        assertEquals(1, response.responses().size());\n        assertEquals(metadataPartition.topic(), response.responses().get(0).topic());\n        assertEquals(1, response.responses().get(0).partitions().size());\n        return response.responses().get(0).partitions().get(0);\n    }\n\n    void assertSentFetchPartitionResponse(Errors topLevelError) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.FETCH);\n        assertEquals(\n            1, sentMessages.size(), \"Found unexpected sent messages \" + sentMessages);\n        RaftResponse.Outbound raftMessage = sentMessages.get(0);\n        assertEquals(ApiKeys.FETCH.id, raftMessage.data().apiKey());\n        FetchResponseData response = (FetchResponseData) raftMessage.data();\n        assertEquals(topLevelError, Errors.forCode(response.errorCode()));\n    }\n\n\n    MemoryRecords assertSentFetchPartitionResponse(\n        Errors error,\n        int epoch,\n        OptionalInt leaderId\n    ) {\n        FetchResponseData.PartitionData partitionResponse = assertSentFetchPartitionResponse();\n        assertEquals(error, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(leaderId.orElse(-1), partitionResponse.currentLeader().leaderId());\n        assertEquals(-1, partitionResponse.divergingEpoch().endOffset());\n        assertEquals(-1, partitionResponse.divergingEpoch().epoch());\n        assertEquals(-1, partitionResponse.snapshotId().endOffset());\n        assertEquals(-1, partitionResponse.snapshotId().epoch());\n        return (MemoryRecords) partitionResponse.records();\n    }\n\n    MemoryRecords assertSentFetchPartitionResponse(\n        long highWatermark,\n        int leaderEpoch\n    ) {\n        FetchResponseData.PartitionData partitionResponse = assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(leaderEpoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(highWatermark, partitionResponse.highWatermark());\n        assertEquals(-1, partitionResponse.divergingEpoch().endOffset());\n        assertEquals(-1, partitionResponse.divergingEpoch().epoch());\n        assertEquals(-1, partitionResponse.snapshotId().endOffset());\n        assertEquals(-1, partitionResponse.snapshotId().epoch());\n        return (MemoryRecords) partitionResponse.records();\n    }\n\n    RaftRequest.Outbound assertSentFetchSnapshotRequest() {\n        List<RaftRequest.Outbound> sentRequests = channel.drainSentRequests(Optional.of(ApiKeys.FETCH_SNAPSHOT));\n        assertEquals(1, sentRequests.size());\n\n        return sentRequests.get(0);\n    }\n\n    void assertSentFetchSnapshotResponse(Errors responseError) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.FETCH_SNAPSHOT);\n        assertEquals(1, sentMessages.size());\n\n        RaftMessage message = sentMessages.get(0);\n        assertInstanceOf(FetchSnapshotResponseData.class, message.data());\n\n        FetchSnapshotResponseData response = (FetchSnapshotResponseData) message.data();\n        assertEquals(responseError, Errors.forCode(response.errorCode()));\n    }\n\n    Optional<FetchSnapshotResponseData.PartitionSnapshot> assertSentFetchSnapshotResponse(TopicPartition topicPartition) {\n        List<RaftResponse.Outbound> sentMessages = drainSentResponses(ApiKeys.FETCH_SNAPSHOT);\n        assertEquals(1, sentMessages.size());\n\n        RaftMessage message = sentMessages.get(0);\n        assertInstanceOf(FetchSnapshotResponseData.class, message.data());\n\n        FetchSnapshotResponseData response = (FetchSnapshotResponseData) message.data();\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n\n        return FetchSnapshotResponse.forTopicPartition(response, topicPartition);\n    }\n\n    List<RaftRequest.Outbound> collectEndQuorumRequests(\n        int epoch,\n        Set<Integer> destinationIdSet,\n        Optional<List<Integer>> preferredSuccessorsOpt\n    ) {\n        List<RaftRequest.Outbound> endQuorumRequests = new ArrayList<>();\n        Set<Integer> collectedDestinationIdSet = new HashSet<>();\n        for (RaftRequest.Outbound raftMessage : channel.drainSendQueue()) {\n            if (raftMessage.data() instanceof EndQuorumEpochRequestData) {\n                EndQuorumEpochRequestData request = (EndQuorumEpochRequestData) raftMessage.data();\n\n                EndQuorumEpochRequestData.PartitionData partitionRequest =\n                    request.topics().get(0).partitions().get(0);\n\n                assertEquals(epoch, partitionRequest.leaderEpoch());\n                assertEquals(localIdOrThrow(), partitionRequest.leaderId());\n                preferredSuccessorsOpt.ifPresent(preferredSuccessors -> {\n                    assertEquals(preferredSuccessors, partitionRequest.preferredSuccessors());\n                });\n\n                collectedDestinationIdSet.add(raftMessage.destination().id());\n                endQuorumRequests.add(raftMessage);\n            }\n        }\n        assertEquals(destinationIdSet, collectedDestinationIdSet);\n        return endQuorumRequests;\n    }\n\n    void discoverLeaderAsObserver(\n        int leaderId,\n        int epoch\n    ) throws Exception {\n        pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = assertSentFetchRequest();\n        int destinationId = fetchRequest.destination().id();\n        assertTrue(\n            voters.contains(destinationId) || bootstrapIds.contains(destinationId),\n            String.format(\"id %d is not in sets %s or %s\", destinationId, voters, bootstrapIds)\n        );\n        assertFetchRequestData(fetchRequest, 0, 0L, 0);\n\n        deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            fetchResponse(epoch, leaderId, MemoryRecords.EMPTY, 0L, Errors.NONE)\n        );\n        client.poll();\n        assertElectedLeader(epoch, leaderId);\n    }\n\n    private List<RaftRequest.Outbound> collectBeginEpochRequests(int epoch) {\n        List<RaftRequest.Outbound> requests = new ArrayList<>();\n        for (RaftRequest.Outbound raftRequest : channel.drainSentRequests(Optional.of(ApiKeys.BEGIN_QUORUM_EPOCH))) {\n            assertInstanceOf(BeginQuorumEpochRequestData.class, raftRequest.data());\n            BeginQuorumEpochRequestData request = (BeginQuorumEpochRequestData) raftRequest.data();\n\n            BeginQuorumEpochRequestData.PartitionData partitionRequest =\n                request.topics().get(0).partitions().get(0);\n\n            assertEquals(epoch, partitionRequest.leaderEpoch());\n            assertEquals(localIdOrThrow(), partitionRequest.leaderId());\n            requests.add(raftRequest);\n        }\n        return requests;\n    }\n\n    public static InetSocketAddress mockAddress(int id) {\n        return new InetSocketAddress(\"localhost\", 9990 + id);\n    }\n\n    EndQuorumEpochResponseData endEpochResponse(\n        int epoch,\n        OptionalInt leaderId\n    ) {\n        return EndQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            metadataPartition,\n            Errors.NONE,\n            epoch,\n            leaderId.orElse(-1)\n        );\n    }\n\n    EndQuorumEpochRequestData endEpochRequest(\n        int epoch,\n        int leaderId,\n        List<Integer> preferredSuccessors\n    ) {\n        return EndQuorumEpochRequest.singletonRequest(\n            metadataPartition,\n            epoch,\n            leaderId,\n            preferredSuccessors\n        );\n    }\n\n    EndQuorumEpochRequestData endEpochRequest(\n        String clusterId,\n        int epoch,\n        int leaderId,\n        List<Integer> preferredSuccessors\n    ) {\n        return EndQuorumEpochRequest.singletonRequest(\n            metadataPartition,\n            clusterId,\n            epoch,\n            leaderId,\n            preferredSuccessors\n        );\n    }\n\n    BeginQuorumEpochRequestData beginEpochRequest(String clusterId, int epoch, int leaderId) {\n        return BeginQuorumEpochRequest.singletonRequest(\n            metadataPartition,\n            clusterId,\n            epoch,\n            leaderId\n        );\n    }\n\n    BeginQuorumEpochRequestData beginEpochRequest(int epoch, int leaderId) {\n        return BeginQuorumEpochRequest.singletonRequest(\n            metadataPartition,\n            epoch,\n            leaderId\n        );\n    }\n\n    private BeginQuorumEpochResponseData beginEpochResponse(int epoch, int leaderId) {\n        return BeginQuorumEpochResponse.singletonResponse(\n            Errors.NONE,\n            metadataPartition,\n            Errors.NONE,\n            epoch,\n            leaderId\n        );\n    }\n\n    VoteRequestData voteRequest(int epoch, int candidateId, int lastEpoch, long lastEpochOffset) {\n        return VoteRequest.singletonRequest(\n            metadataPartition,\n            clusterId.toString(),\n            epoch,\n            candidateId,\n            lastEpoch,\n            lastEpochOffset\n        );\n    }\n\n    VoteRequestData voteRequest(\n        String clusterId,\n        int epoch,\n        int candidateId,\n        int lastEpoch,\n        long lastEpochOffset\n    ) {\n        return VoteRequest.singletonRequest(\n                metadataPartition,\n                clusterId,\n                epoch,\n                candidateId,\n                lastEpoch,\n                lastEpochOffset\n        );\n    }\n\n    VoteResponseData voteResponse(boolean voteGranted, Optional<Integer> leaderId, int epoch) {\n        return VoteResponse.singletonResponse(\n            Errors.NONE,\n            metadataPartition,\n            Errors.NONE,\n            epoch,\n            leaderId.orElse(-1),\n            voteGranted\n        );\n    }\n\n    private VoteRequestData.PartitionData unwrap(VoteRequestData voteRequest) {\n        assertTrue(RaftUtil.hasValidTopicPartition(voteRequest, metadataPartition));\n        return voteRequest.topics().get(0).partitions().get(0);\n    }\n\n    static void assertMatchingRecords(\n        String[] expected,\n        Records actual\n    ) {\n        List<Record> recordList = Utils.toList(actual.records());\n        assertEquals(expected.length, recordList.size());\n        for (int i = 0; i < expected.length; i++) {\n            Record record = recordList.get(i);\n            assertEquals(expected[i], Utils.utf8(record.value()),\n                \"Record at offset \" + record.offset() + \" does not match expected\");\n        }\n    }\n\n    static void verifyLeaderChangeMessage(\n        int leaderId,\n        List<Integer> voters,\n        List<Integer> grantingVoters,\n        ByteBuffer recordKey,\n        ByteBuffer recordValue\n    ) {\n        assertEquals(ControlRecordType.LEADER_CHANGE, ControlRecordType.parse(recordKey));\n\n        LeaderChangeMessage leaderChangeMessage = ControlRecordUtils.deserializeLeaderChangeMessage(recordValue);\n        assertEquals(leaderId, leaderChangeMessage.leaderId());\n        assertEquals(voters.stream().map(voterId -> new Voter().setVoterId(voterId)).collect(Collectors.toList()),\n            leaderChangeMessage.voters());\n        assertEquals(grantingVoters.stream().map(voterId -> new Voter().setVoterId(voterId)).collect(Collectors.toSet()),\n            new HashSet<>(leaderChangeMessage.grantingVoters()));\n    }\n\n    void assertFetchRequestData(\n        RaftRequest.Outbound message,\n        int epoch,\n        long fetchOffset,\n        int lastFetchedEpoch\n    ) {\n        assertInstanceOf(\n            FetchRequestData.class,\n            message.data(),\n            \"unexpected request type \" + message.data());\n        FetchRequestData request = (FetchRequestData) message.data();\n        assertEquals(KafkaRaftClient.MAX_FETCH_SIZE_BYTES, request.maxBytes());\n        assertEquals(fetchMaxWaitMs, request.maxWaitMs());\n\n        assertEquals(1, request.topics().size());\n        assertEquals(metadataPartition.topic(), request.topics().get(0).topic());\n        assertEquals(1, request.topics().get(0).partitions().size());\n\n        FetchRequestData.FetchPartition fetchPartition = request.topics().get(0).partitions().get(0);\n        assertEquals(epoch, fetchPartition.currentLeaderEpoch());\n        assertEquals(fetchOffset, fetchPartition.fetchOffset());\n        assertEquals(lastFetchedEpoch, fetchPartition.lastFetchedEpoch());\n        assertEquals(localId.orElse(-1), request.replicaState().replicaId());\n\n        // Assert that voters have flushed up to the fetch offset\n        if (localId.isPresent() && voters.contains(localId.getAsInt())) {\n            assertEquals(\n                log.firstUnflushedOffset(),\n                fetchOffset,\n                String.format(\n                    \"expected voters have the fetch offset (%s) be the same as the unflushed offset (%s)\",\n                    log.firstUnflushedOffset(),\n                    fetchOffset\n                )\n            );\n        } else {\n            assertFalse(log.flushedSinceLastChecked(), \"KRaft client should not explicitly flush when it is an observer\");\n        }\n    }\n\n    FetchRequestData fetchRequest(\n        int epoch,\n        int replicaId,\n        long fetchOffset,\n        int lastFetchedEpoch,\n        int maxWaitTimeMs\n    ) {\n        return fetchRequest(\n            epoch,\n            clusterId.toString(),\n            replicaId,\n            fetchOffset,\n            lastFetchedEpoch,\n            maxWaitTimeMs\n        );\n    }\n\n    FetchRequestData fetchRequest(\n        int epoch,\n        String clusterId,\n        int replicaId,\n        long fetchOffset,\n        int lastFetchedEpoch,\n        int maxWaitTimeMs\n    ) {\n        FetchRequestData request = RaftUtil.singletonFetchRequest(metadataPartition, metadataTopicId, fetchPartition -> {\n            fetchPartition\n                .setCurrentLeaderEpoch(epoch)\n                .setLastFetchedEpoch(lastFetchedEpoch)\n                .setFetchOffset(fetchOffset);\n        });\n        return request\n            .setMaxWaitMs(maxWaitTimeMs)\n            .setClusterId(clusterId)\n            .setReplicaState(new FetchRequestData.ReplicaState().setReplicaId(replicaId));\n    }\n\n    FetchResponseData fetchResponse(\n        int epoch,\n        int leaderId,\n        Records records,\n        long highWatermark,\n        Errors error\n    ) {\n        return RaftUtil.singletonFetchResponse(metadataPartition, metadataTopicId, Errors.NONE, partitionData -> {\n            partitionData\n                .setRecords(records)\n                .setErrorCode(error.code())\n                .setHighWatermark(highWatermark);\n\n            partitionData.currentLeader()\n                .setLeaderEpoch(epoch)\n                .setLeaderId(leaderId);\n        });\n    }\n\n    FetchResponseData divergingFetchResponse(\n        int epoch,\n        int leaderId,\n        long divergingEpochEndOffset,\n        int divergingEpoch,\n        long highWatermark\n    ) {\n        return RaftUtil.singletonFetchResponse(metadataPartition, metadataTopicId, Errors.NONE, partitionData -> {\n            partitionData.setHighWatermark(highWatermark);\n\n            partitionData.currentLeader()\n                .setLeaderEpoch(epoch)\n                .setLeaderId(leaderId);\n\n            partitionData.divergingEpoch()\n                .setEpoch(divergingEpoch)\n                .setEndOffset(divergingEpochEndOffset);\n        });\n    }\n\n    public void advanceLocalLeaderHighWatermarkToLogEndOffset() throws InterruptedException {\n        assertEquals(localId, currentLeader());\n        long localLogEndOffset = log.endOffset().offset;\n        Set<Integer> followers = voterKeys();\n\n        // Send a request from every follower\n        for (int follower : followers) {\n            deliverRequest(\n                fetchRequest(currentEpoch(), follower, localLogEndOffset, currentEpoch(), 0)\n            );\n            pollUntilResponse();\n            assertSentFetchPartitionResponse(Errors.NONE, currentEpoch(), localId);\n        }\n\n        pollUntil(() -> OptionalLong.of(localLogEndOffset).equals(client.highWatermark()));\n    }\n\n    private Set<Integer> voterKeys() {\n        Set<Integer> followers = voters.stream().filter(voter -> voter != localId.getAsInt()).collect(Collectors.toSet());\n        return followers;\n    }\n\n    static class MockListener implements RaftClient.Listener<String> {\n        private final List<Batch<String>> commits = new ArrayList<>();\n        private final List<BatchReader<String>> savedBatches = new ArrayList<>();\n        private final Map<Integer, Long> claimedEpochStartOffsets = new HashMap<>();\n        private LeaderAndEpoch currentLeaderAndEpoch = new LeaderAndEpoch(OptionalInt.empty(), 0);\n        private final OptionalInt localId;\n        private Optional<SnapshotReader<String>> snapshot = Optional.empty();\n        private boolean readCommit = true;\n\n        MockListener(OptionalInt localId) {\n            this.localId = localId;\n        }\n\n        int numCommittedBatches() {\n            return commits.size();\n        }\n\n        Long claimedEpochStartOffset(int epoch) {\n            return claimedEpochStartOffsets.get(epoch);\n        }\n\n        LeaderAndEpoch currentLeaderAndEpoch() {\n            return currentLeaderAndEpoch;\n        }\n\n        List<Batch<String>> committedBatches() {\n            return commits;\n        }\n\n        Batch<String> lastCommit() {\n            if (commits.isEmpty()) {\n                return null;\n            } else {\n                return commits.get(commits.size() - 1);\n            }\n        }\n\n        OptionalLong lastCommitOffset() {\n            if (commits.isEmpty()) {\n                return OptionalLong.empty();\n            } else {\n                return OptionalLong.of(commits.get(commits.size() - 1).lastOffset());\n            }\n        }\n\n        OptionalInt currentClaimedEpoch() {\n            if (localId.isPresent() && currentLeaderAndEpoch.isLeader(localId.getAsInt())) {\n                return OptionalInt.of(currentLeaderAndEpoch.epoch());\n            } else {\n                return OptionalInt.empty();\n            }\n        }\n\n        List<String> commitWithLastOffset(long lastOffset) {\n            return commits.stream()\n                .filter(batch -> batch.lastOffset() == lastOffset)\n                .findFirst()\n                .map(batch -> batch.records())\n                .orElse(null);\n        }\n\n        Optional<SnapshotReader<String>> drainHandledSnapshot() {\n            Optional<SnapshotReader<String>> temp = snapshot;\n            snapshot = Optional.empty();\n            return temp;\n        }\n\n        void updateReadCommit(boolean readCommit) {\n            this.readCommit = readCommit;\n\n            if (readCommit) {\n                for (BatchReader<String> batch : savedBatches) {\n                    readBatch(batch);\n                }\n\n                savedBatches.clear();\n            }\n        }\n\n        void readBatch(BatchReader<String> reader) {\n            try {\n                while (reader.hasNext()) {\n                    long nextOffset = lastCommitOffset().isPresent() ?\n                        lastCommitOffset().getAsLong() + 1 : 0L;\n                    Batch<String> batch = reader.next();\n                    // We expect monotonic offsets, but not necessarily sequential\n                    // offsets since control records will be filtered.\n                    assertTrue(batch.baseOffset() >= nextOffset,\n                        \"Received non-monotonic commit \" + batch +\n                            \". We expected an offset at least as large as \" + nextOffset);\n                    commits.add(batch);\n                }\n            } finally {\n                reader.close();\n            }\n        }\n\n        @Override\n        public void handleLeaderChange(LeaderAndEpoch leaderAndEpoch) {\n            // We record the current committed offset as the claimed epoch's start\n            // offset. This is useful to verify that the `handleLeaderChange` callback\n            // was not received early on the leader.\n            assertTrue(\n                leaderAndEpoch.epoch() >= currentLeaderAndEpoch.epoch(),\n                String.format(\"new epoch (%d) not >= than old epoch (%d)\", leaderAndEpoch.epoch(), currentLeaderAndEpoch.epoch())\n            );\n            assertNotEquals(currentLeaderAndEpoch, leaderAndEpoch);\n            this.currentLeaderAndEpoch = leaderAndEpoch;\n\n            currentClaimedEpoch().ifPresent(claimedEpoch -> {\n                long claimedEpochStartOffset = lastCommitOffset().isPresent() ?\n                    lastCommitOffset().getAsLong() : 0L;\n                this.claimedEpochStartOffsets.put(leaderAndEpoch.epoch(), claimedEpochStartOffset);\n            });\n        }\n\n        @Override\n        public void handleCommit(BatchReader<String> reader) {\n            if (readCommit) {\n                readBatch(reader);\n            } else {\n                savedBatches.add(reader);\n            }\n        }\n\n        @Override\n        public void handleLoadSnapshot(SnapshotReader<String> reader) {\n            snapshot.ifPresent(snapshot -> assertDoesNotThrow(snapshot::close));\n            commits.clear();\n            savedBatches.clear();\n            snapshot = Optional.of(reader);\n        }\n    }\n}",
                "methodCount": 98
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "withElectedLeader",
                            "method_signature": " withElectedLeader(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withUnknownLeader",
                            "method_signature": " withUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withVotedCandidate",
                            "method_signature": " withVotedCandidate(int epoch, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateRandom",
                            "method_signature": " updateRandom(Consumer<MockableRandom> consumer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withMemoryPool",
                            "method_signature": " withMemoryPool(MemoryPool pool)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withAppendLingerMs",
                            "method_signature": " withAppendLingerMs(int appendLingerMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "appendToLog",
                            "method_signature": "public appendToLog(int epoch, List<String> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withEmptySnapshot",
                            "method_signature": " withEmptySnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteBeforeSnapshot",
                            "method_signature": " deleteBeforeSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withElectionTimeoutMs",
                            "method_signature": " withElectionTimeoutMs(int electionTimeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withRequestTimeoutMs",
                            "method_signature": " withRequestTimeoutMs(int requestTimeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withBootstrapServers",
                            "method_signature": " withBootstrapServers(List<InetSocketAddress> bootstrapServers)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBatch",
                            "method_signature": " buildBatch(\n        long baseOffset,\n        int epoch,\n        List<String> records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildBatch",
                            "method_signature": "static buildBatch(\n        long timestamp,\n        long baseOffset,\n        int epoch,\n        List<String> records\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initializeAsLeader",
                            "method_signature": "static initializeAsLeader(int localId, Set<Integer> voters, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "becomeLeader",
                            "method_signature": "public becomeLeader()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "currentLeader",
                            "method_signature": "public currentLeader()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "currentEpoch",
                            "method_signature": "public currentEpoch()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "currentLeaderAndEpoch",
                            "method_signature": " currentLeaderAndEpoch()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "expectAndGrantVotes",
                            "method_signature": " expectAndGrantVotes(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "localIdOrThrow",
                            "method_signature": "private localIdOrThrow()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "expectBeginEpoch",
                            "method_signature": "private expectBeginEpoch(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntil",
                            "method_signature": "public pollUntil(TestCondition condition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntilResponse",
                            "method_signature": " pollUntilResponse()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntilRequest",
                            "method_signature": " pollUntilRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertVotedCandidate",
                            "method_signature": " assertVotedCandidate(int epoch, int candidateId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertElectedLeader",
                            "method_signature": "public assertElectedLeader(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertUnknownLeader",
                            "method_signature": " assertUnknownLeader(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertResignedLeader",
                            "method_signature": " assertResignedLeader(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "collectDescribeQuorumResponse",
                            "method_signature": " collectDescribeQuorumResponse()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentDescribeQuorumResponse",
                            "method_signature": " assertSentDescribeQuorumResponse(\n        int leaderId,\n        int leaderEpoch,\n        long highWatermark,\n        List<ReplicaState> voterStates,\n        List<ReplicaState> observerStates\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentVoteRequest",
                            "method_signature": " assertSentVoteRequest(int epoch, int lastEpoch, long lastEpochOffset, int numVoteReceivers)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentVoteResponse",
                            "method_signature": " assertSentVoteResponse(Errors error)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentVoteResponse",
                            "method_signature": " assertSentVoteResponse(\n        Errors error,\n        int epoch,\n        OptionalInt leaderId,\n        boolean voteGranted\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "collectVoteRequests",
                            "method_signature": " collectVoteRequests(\n        int epoch,\n        int lastEpoch,\n        long lastEpochOffset\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deliverRequest",
                            "method_signature": " deliverRequest(ApiMessage request)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deliverResponse",
                            "method_signature": " deliverResponse(int correlationId, Node source, ApiMessage response)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentBeginQuorumEpochRequest",
                            "method_signature": " assertSentBeginQuorumEpochRequest(int epoch, int numBeginEpochRequests)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "drainSentResponses",
                            "method_signature": "private drainSentResponses(\n        ApiKeys apiKey\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentBeginQuorumEpochResponse",
                            "method_signature": " assertSentBeginQuorumEpochResponse(\n            Errors responseError\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentBeginQuorumEpochResponse",
                            "method_signature": " assertSentBeginQuorumEpochResponse(\n        Errors partitionError,\n        int epoch,\n        OptionalInt leaderId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentEndQuorumEpochRequest",
                            "method_signature": " assertSentEndQuorumEpochRequest(int epoch, int destinationId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentEndQuorumEpochResponse",
                            "method_signature": " assertSentEndQuorumEpochResponse(\n        Errors responseError\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentEndQuorumEpochResponse",
                            "method_signature": " assertSentEndQuorumEpochResponse(\n        Errors partitionError,\n        int epoch,\n        OptionalInt leaderId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchRequest",
                            "method_signature": " assertSentFetchRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchRequest",
                            "method_signature": " assertSentFetchRequest(\n        int epoch,\n        long fetchOffset,\n        int lastFetchedEpoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchPartitionResponse",
                            "method_signature": " assertSentFetchPartitionResponse()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchPartitionResponse",
                            "method_signature": " assertSentFetchPartitionResponse(Errors topLevelError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchPartitionResponse",
                            "method_signature": " assertSentFetchPartitionResponse(\n        Errors error,\n        int epoch,\n        OptionalInt leaderId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchPartitionResponse",
                            "method_signature": " assertSentFetchPartitionResponse(\n        long highWatermark,\n        int leaderEpoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchSnapshotRequest",
                            "method_signature": " assertSentFetchSnapshotRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchSnapshotResponse",
                            "method_signature": " assertSentFetchSnapshotResponse(Errors responseError)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSentFetchSnapshotResponse",
                            "method_signature": " assertSentFetchSnapshotResponse(TopicPartition topicPartition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "collectEndQuorumRequests",
                            "method_signature": " collectEndQuorumRequests(\n        int epoch,\n        Set<Integer> destinationIdSet,\n        Optional<List<Integer>> preferredSuccessorsOpt\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "discoverLeaderAsObserver",
                            "method_signature": " discoverLeaderAsObserver(\n        int leaderId,\n        int epoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "collectBeginEpochRequests",
                            "method_signature": "private collectBeginEpochRequests(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mockAddress",
                            "method_signature": "public static mockAddress(int id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endEpochResponse",
                            "method_signature": " endEpochResponse(\n        int epoch,\n        OptionalInt leaderId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endEpochRequest",
                            "method_signature": " endEpochRequest(\n        int epoch,\n        int leaderId,\n        List<Integer> preferredSuccessors\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endEpochRequest",
                            "method_signature": " endEpochRequest(\n        String clusterId,\n        int epoch,\n        int leaderId,\n        List<Integer> preferredSuccessors\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginEpochRequest",
                            "method_signature": " beginEpochRequest(String clusterId, int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginEpochRequest",
                            "method_signature": " beginEpochRequest(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginEpochResponse",
                            "method_signature": "private beginEpochResponse(int epoch, int leaderId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voteRequest",
                            "method_signature": " voteRequest(int epoch, int candidateId, int lastEpoch, long lastEpochOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voteRequest",
                            "method_signature": " voteRequest(\n        String clusterId,\n        int epoch,\n        int candidateId,\n        int lastEpoch,\n        long lastEpochOffset\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voteResponse",
                            "method_signature": " voteResponse(boolean voteGranted, Optional<Integer> leaderId, int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "unwrap",
                            "method_signature": "private unwrap(VoteRequestData voteRequest)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertMatchingRecords",
                            "method_signature": "static assertMatchingRecords(\n        String[] expected,\n        Records actual\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyLeaderChangeMessage",
                            "method_signature": "static verifyLeaderChangeMessage(\n        int leaderId,\n        List<Integer> voters,\n        List<Integer> grantingVoters,\n        ByteBuffer recordKey,\n        ByteBuffer recordValue\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertFetchRequestData",
                            "method_signature": " assertFetchRequestData(\n        RaftRequest.Outbound message,\n        int epoch,\n        long fetchOffset,\n        int lastFetchedEpoch\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchRequest",
                            "method_signature": " fetchRequest(\n        int epoch,\n        int replicaId,\n        long fetchOffset,\n        int lastFetchedEpoch,\n        int maxWaitTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchRequest",
                            "method_signature": " fetchRequest(\n        int epoch,\n        String clusterId,\n        int replicaId,\n        long fetchOffset,\n        int lastFetchedEpoch,\n        int maxWaitTimeMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchResponse",
                            "method_signature": " fetchResponse(\n        int epoch,\n        int leaderId,\n        Records records,\n        long highWatermark,\n        Errors error\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "divergingFetchResponse",
                            "method_signature": " divergingFetchResponse(\n        int epoch,\n        int leaderId,\n        long divergingEpochEndOffset,\n        int divergingEpoch,\n        long highWatermark\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "advanceLocalLeaderHighWatermarkToLogEndOffset",
                            "method_signature": "public advanceLocalLeaderHighWatermarkToLogEndOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "voterKeys",
                            "method_signature": "private voterKeys()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "numCommittedBatches",
                            "method_signature": " numCommittedBatches()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "claimedEpochStartOffset",
                            "method_signature": " claimedEpochStartOffset(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "lastCommit",
                            "method_signature": " lastCommit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "lastCommitOffset",
                            "method_signature": " lastCommitOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "currentClaimedEpoch",
                            "method_signature": " currentClaimedEpoch()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitWithLastOffset",
                            "method_signature": " commitWithLastOffset(long lastOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "drainHandledSnapshot",
                            "method_signature": " drainHandledSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateReadCommit",
                            "method_signature": " updateReadCommit(boolean readCommit)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "readBatch",
                            "method_signature": " readBatch(BatchReader<String> reader)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "withRequestTimeoutMs",
                            "method_signature": " withRequestTimeoutMs(int requestTimeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withAppendLingerMs",
                            "method_signature": " withAppendLingerMs(int appendLingerMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withMemoryPool",
                            "method_signature": " withMemoryPool(MemoryPool pool)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "localIdOrThrow",
                            "method_signature": "private localIdOrThrow()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "numCommittedBatches",
                            "method_signature": " numCommittedBatches()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withElectionTimeoutMs",
                            "method_signature": " withElectionTimeoutMs(int electionTimeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withBootstrapServers",
                            "method_signature": " withBootstrapServers(List<InetSocketAddress> bootstrapServers)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateRandom",
                            "method_signature": " updateRandom(Consumer<MockableRandom> consumer)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "lastCommit",
                            "method_signature": " lastCommit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "drainHandledSnapshot",
                            "method_signature": " drainHandledSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "pollUntil",
                            "method_signature": "public pollUntil(TestCondition condition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "lastCommitOffset",
                            "method_signature": " lastCommitOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "drainSentResponses",
                            "method_signature": "private drainSentResponses(\n        ApiKeys apiKey\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "mockAddress",
                            "method_signature": "public static mockAddress(int id)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "claimedEpochStartOffset",
                            "method_signature": " claimedEpochStartOffset(int epoch)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                " withRequestTimeoutMs(int requestTimeoutMs)": {
                    "first": {
                        "method_name": "withRequestTimeoutMs",
                        "method_signature": " withRequestTimeoutMs(int requestTimeoutMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.16131484906057567
                },
                " withAppendLingerMs(int appendLingerMs)": {
                    "first": {
                        "method_name": "withAppendLingerMs",
                        "method_signature": " withAppendLingerMs(int appendLingerMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.21032813089277885
                },
                " withMemoryPool(MemoryPool pool)": {
                    "first": {
                        "method_name": "withMemoryPool",
                        "method_signature": " withMemoryPool(MemoryPool pool)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.22583090181305954
                },
                "private localIdOrThrow()": {
                    "first": {
                        "method_name": "localIdOrThrow",
                        "method_signature": "private localIdOrThrow()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.25766245743065563
                },
                " numCommittedBatches()": {
                    "first": {
                        "method_name": "numCommittedBatches",
                        "method_signature": " numCommittedBatches()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2825664176077091
                },
                " withElectionTimeoutMs(int electionTimeoutMs)": {
                    "first": {
                        "method_name": "withElectionTimeoutMs",
                        "method_signature": " withElectionTimeoutMs(int electionTimeoutMs)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2915589255568853
                },
                " withBootstrapServers(List<InetSocketAddress> bootstrapServers)": {
                    "first": {
                        "method_name": "withBootstrapServers",
                        "method_signature": " withBootstrapServers(List<InetSocketAddress> bootstrapServers)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.30671601904992124
                },
                " updateRandom(Consumer<MockableRandom> consumer)": {
                    "first": {
                        "method_name": "updateRandom",
                        "method_signature": " updateRandom(Consumer<MockableRandom> consumer)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.30706068611093107
                },
                " lastCommit()": {
                    "first": {
                        "method_name": "lastCommit",
                        "method_signature": " lastCommit()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.32658092439494
                },
                " drainHandledSnapshot()": {
                    "first": {
                        "method_name": "drainHandledSnapshot",
                        "method_signature": " drainHandledSnapshot()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3314512139816718
                },
                "public pollUntil(TestCondition condition)": {
                    "first": {
                        "method_name": "pollUntil",
                        "method_signature": "public pollUntil(TestCondition condition)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3379495019661108
                },
                " lastCommitOffset()": {
                    "first": {
                        "method_name": "lastCommitOffset",
                        "method_signature": " lastCommitOffset()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.342807251345877
                },
                "private drainSentResponses(\n        ApiKeys apiKey\n    )": {
                    "first": {
                        "method_name": "drainSentResponses",
                        "method_signature": "private drainSentResponses(\n        ApiKeys apiKey\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.346335171839786
                },
                "public static mockAddress(int id)": {
                    "first": {
                        "method_name": "mockAddress",
                        "method_signature": "public static mockAddress(int id)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.35225438383116214
                },
                " claimedEpochStartOffset(int epoch)": {
                    "first": {
                        "method_name": "claimedEpochStartOffset",
                        "method_signature": " claimedEpochStartOffset(int epoch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3643059352906588
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "9a78122fb0b1b37961139d5c62e49fa758dc08ac",
        "url": "https://github.com/apache/kafka/commit/9a78122fb0b1b37961139d5c62e49fa758dc08ac",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public requiresKnownMemberId(request JoinGroupRequestData, apiVersion short) : boolean extracted from private classicGroupJoinToConsumerGroup(group ConsumerGroup, context RequestContext, request JoinGroupRequestData, responseFuture CompletableFuture<JoinGroupResponseData>) : CoordinatorResult<Void,CoordinatorRecord> in class org.apache.kafka.coordinator.group.GroupMetadataManager & moved to class org.apache.kafka.common.requests.JoinGroupRequest",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 1528,
                    "endLine": 1729,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private classicGroupJoinToConsumerGroup(group ConsumerGroup, context RequestContext, request JoinGroupRequestData, responseFuture CompletableFuture<JoinGroupResponseData>) : CoordinatorResult<Void,CoordinatorRecord>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 1564,
                    "endLine": 1564,
                    "startColumn": 17,
                    "endColumn": 96,
                    "codeElementType": "IF_STATEMENT_CONDITION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/requests/JoinGroupRequest.java",
                    "startLine": 105,
                    "endLine": 126,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public requiresKnownMemberId(request JoinGroupRequestData, apiVersion short) : boolean"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/requests/JoinGroupRequest.java",
                    "startLine": 123,
                    "endLine": 125,
                    "startColumn": 9,
                    "endColumn": 50,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 1513,
                    "endLine": 1702,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private classicGroupJoinToConsumerGroup(group ConsumerGroup, context RequestContext, request JoinGroupRequestData, responseFuture CompletableFuture<JoinGroupResponseData>) : CoordinatorResult<Void,CoordinatorRecord>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 1543,
                    "endLine": 1543,
                    "startColumn": 13,
                    "endColumn": 82,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "JoinGroupRequest.requiresKnownMemberId(request,context.apiVersion())"
                }
            ],
            "isStatic": true
        },
        "ref_id": 600,
        "extraction_results": {
            "success": true,
            "newCommitHash": "63c8e0264cfb6509f0c869bbaa1b5fe682252576",
            "newBranchName": "extract-requiresKnownMemberId-classicGroupJoinToConsumerGroup-15a4501"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "27220d146c5d043da4adc3d636036bd6e7b112d2",
        "url": "https://github.com/apache/kafka/commit/27220d146c5d043da4adc3d636036bd6e7b112d2",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public summarize() : String extracted from public completeRequest(cb FutureCallback<T>) : T in class org.apache.kafka.connect.runtime.rest.HerderRequestHandler & moved to class org.apache.kafka.connect.util.Stage",
            "leftSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 54,
                    "endLine": 89,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public completeRequest(cb FutureCallback<T>) : T"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 70,
                    "endLine": 73,
                    "startColumn": 17,
                    "endColumn": 67,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 71,
                    "endLine": 71,
                    "startColumn": 49,
                    "endColumn": 68,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 72,
                    "endLine": 72,
                    "startColumn": 67,
                    "endColumn": 87,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 75,
                    "endLine": 77,
                    "startColumn": 17,
                    "endColumn": 65,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 76,
                    "endLine": 76,
                    "startColumn": 49,
                    "endColumn": 68,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 69,
                    "endLine": 78,
                    "startColumn": 13,
                    "endColumn": 14,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 69,
                    "endLine": 74,
                    "startColumn": 44,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 74,
                    "endLine": 78,
                    "startColumn": 20,
                    "endColumn": 14,
                    "codeElementType": "BLOCK",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 64,
                    "endLine": 76,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public summarize() : String"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 67,
                    "endLine": 70,
                    "startColumn": 13,
                    "endColumn": 63,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 68,
                    "endLine": 68,
                    "startColumn": 39,
                    "endColumn": 58,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 69,
                    "endLine": 69,
                    "startColumn": 57,
                    "endColumn": 77,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 72,
                    "endLine": 74,
                    "startColumn": 13,
                    "endColumn": 61,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 73,
                    "endLine": 73,
                    "startColumn": 39,
                    "endColumn": 58,
                    "codeElementType": "STRING_LITERAL",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 66,
                    "endLine": 75,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "IF_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 66,
                    "endLine": 71,
                    "startColumn": 32,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 71,
                    "endLine": 75,
                    "startColumn": 16,
                    "endColumn": 10,
                    "codeElementType": "BLOCK",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 53,
                    "endLine": 78,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public completeRequest(cb FutureCallback<T>) : T"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                    "startLine": 67,
                    "endLine": 67,
                    "startColumn": 54,
                    "endColumn": 71,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "stage.summarize()"
                },
                {
                    "filePath": "connect/runtime/src/main/java/org/apache/kafka/connect/util/Stage.java",
                    "startLine": 65,
                    "endLine": 65,
                    "startColumn": 9,
                    "endColumn": 41,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 601,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a0c65163f3c3b5006b942822829c4c7c4acf3987",
            "newBranchName": "extract-summarize-completeRequest-4550550"
        },
        "telemetry": {
            "id": "7204c34a-8912-4ab4-9b1d-805d8ff4b56c",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 133,
                "lineStart": 41,
                "lineEnd": 173,
                "bodyLineStart": 41,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/HerderRequestHandler.java",
                "sourceCode": "public class HerderRequestHandler {\n\n    private static final Logger log = LoggerFactory.getLogger(HerderRequestHandler.class);\n\n    private final RestClient restClient;\n\n    private final RestRequestTimeout requestTimeout;\n\n    public HerderRequestHandler(RestClient restClient, RestRequestTimeout requestTimeout) {\n        this.restClient = restClient;\n        this.requestTimeout = requestTimeout;\n    }\n\n    /**\n     * Wait for a {@link FutureCallback} to complete and return the result if successful.\n     * @param cb the future callback to wait for\n     * @return the future callback's result if successful\n     * @param <T> the future's result type\n     * @throws Throwable if the future callback isn't successful\n     */\n    public <T> T completeRequest(FutureCallback<T> cb) throws Throwable {\n        try {\n            return cb.get(requestTimeout.timeoutMs(), TimeUnit.MILLISECONDS);\n        } catch (ExecutionException e) {\n            throw e.getCause();\n        } catch (StagedTimeoutException e) {\n            String message;\n            Stage stage = e.stage();\n            message = summarize(stage);\n            // This timeout is for the operation itself. None of the timeout error codes are relevant, so internal server\n            // error is the best option\n            throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), message);\n        } catch (TimeoutException e) {\n            // This timeout is for the operation itself. None of the timeout error codes are relevant, so internal server\n            // error is the best option\n            throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), \"Request timed out\");\n        } catch (InterruptedException e) {\n            throw new ConnectRestException(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), \"Request interrupted\");\n        }\n    }\n\n    private String summarize(Stage stage) {\n        String message;\n        if (stage.completed() != null) {\n            message = \"Request timed out. The last operation the worker completed was \"\n                    + stage.description() + \", which began at \"\n                    + Instant.ofEpochMilli(stage.started()) + \" and completed at \"\n                    + Instant.ofEpochMilli(stage.completed());\n        } else {\n            message = \"Request timed out. The worker is currently \"\n                    + stage.description() + \", which began at \"\n                    + Instant.ofEpochMilli(stage.started());\n        }\n        return message;\n    }\n\n    /**\n     * Wait for a {@link FutureCallback} to complete. If it succeeds, return the parsed response. If it fails, try to forward the\n     * request to the indicated target.\n     */\n    public <T, U> T completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward) throws Throwable {\n        try {\n            return completeRequest(cb);\n        } catch (RequestTargetException e) {\n            if (forward == null || forward) {\n                // the only time we allow recursive forwarding is when no forward flag has\n                // been set, which should only be seen by the first worker to handle a user request.\n                // this gives two total hops to resolve the request before giving up.\n                boolean recursiveForward = forward == null;\n                String forwardedUrl = e.forwardUrl();\n                if (forwardedUrl == null) {\n                    // the target didn't know of the leader at this moment.\n                    throw new ConnectRestException(Response.Status.CONFLICT.getStatusCode(),\n                            \"Cannot complete request momentarily due to no known leader URL, \"\n                                    + \"likely because a rebalance was underway.\");\n                }\n                UriBuilder uriBuilder = UriBuilder.fromUri(forwardedUrl)\n                        .path(path)\n                        .queryParam(\"forward\", recursiveForward);\n                if (queryParameters != null) {\n                    queryParameters.forEach(uriBuilder::queryParam);\n                }\n                String forwardUrl = uriBuilder.build().toString();\n                log.debug(\"Forwarding request {} {} {}\", forwardUrl, method, body);\n                return translator.translate(restClient.httpRequest(forwardUrl, method, headers, body, resultType));\n            } else {\n                log.error(\"Request '{} {}' failed because it couldn't find the target Connect worker within two hops (between workers).\",\n                        method, path);\n                // we should find the right target for the query within two hops, so if\n                // we don't, it probably means that a rebalance has taken place.\n                throw new ConnectRestException(Response.Status.CONFLICT.getStatusCode(),\n                        \"Cannot complete request because of a conflicting operation (e.g. worker rebalance)\");\n            }\n        } catch (RebalanceNeededException e) {\n            throw new ConnectRestException(Response.Status.CONFLICT.getStatusCode(),\n                    \"Cannot complete request momentarily due to stale configuration (typically caused by a concurrent config change)\");\n        }\n    }\n\n    public <T, U> T completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                             TypeReference<U> resultType, Translator<T, U> translator, Boolean forward) throws Throwable {\n        return completeOrForwardRequest(cb, path, method, headers, null, body, resultType, translator, forward);\n    }\n\n    public <T> T completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                                 TypeReference<T> resultType, Boolean forward) throws Throwable {\n        return completeOrForwardRequest(cb, path, method, headers, body, resultType, new IdentityTranslator<>(), forward);\n    }\n\n    public void completeOrForwardRequest(FutureCallback<Void> cb, String path, String method, HttpHeaders headers, Object body,\n                                          Boolean forward) throws Throwable {\n        completeOrForwardRequest(cb, path, method, headers, body, new TypeReference<Void>() { }, new IdentityTranslator<>(), forward);\n    }\n\n    public interface Translator<T, U> {\n        T translate(RestClient.HttpResponse<U> response);\n    }\n\n    public static class IdentityTranslator<T> implements Translator<T, T> {\n        @Override\n        public T translate(RestClient.HttpResponse<T> response) {\n            return response.body();\n        }\n    }\n}",
                "methodCount": 9
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "completeRequest",
                            "method_signature": "public completeRequest(FutureCallback<T> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "summarize",
                            "method_signature": "private summarize(Stage stage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                             TypeReference<U> resultType, Translator<T, U> translator, Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                                 TypeReference<T> resultType, Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<Void> cb, String path, String method, HttpHeaders headers, Object body,\n                                          Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "summarize",
                            "method_signature": "private summarize(Stage stage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                             TypeReference<U> resultType, Translator<T, U> translator, Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<Void> cb, String path, String method, HttpHeaders headers, Object body,\n                                          Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                                 TypeReference<T> resultType, Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeRequest",
                            "method_signature": "public completeRequest(FutureCallback<T> cb)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeOrForwardRequest",
                            "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private summarize(Stage stage)": {
                    "first": {
                        "method_name": "summarize",
                        "method_signature": "private summarize(Stage stage)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.34085826567987454
                },
                "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                             TypeReference<U> resultType, Translator<T, U> translator, Boolean forward)": {
                    "first": {
                        "method_name": "completeOrForwardRequest",
                        "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                             TypeReference<U> resultType, Translator<T, U> translator, Boolean forward)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.491585831304996
                },
                "public completeOrForwardRequest(FutureCallback<Void> cb, String path, String method, HttpHeaders headers, Object body,\n                                          Boolean forward)": {
                    "first": {
                        "method_name": "completeOrForwardRequest",
                        "method_signature": "public completeOrForwardRequest(FutureCallback<Void> cb, String path, String method, HttpHeaders headers, Object body,\n                                          Boolean forward)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4938855711837237
                },
                "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                                 TypeReference<T> resultType, Boolean forward)": {
                    "first": {
                        "method_name": "completeOrForwardRequest",
                        "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb, String path, String method, HttpHeaders headers, Object body,\n                                                 TypeReference<T> resultType, Boolean forward)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5231409134583582
                },
                "public completeRequest(FutureCallback<T> cb)": {
                    "first": {
                        "method_name": "completeRequest",
                        "method_signature": "public completeRequest(FutureCallback<T> cb)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6347566863505427
                },
                "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)": {
                    "first": {
                        "method_name": "completeOrForwardRequest",
                        "method_signature": "public completeOrForwardRequest(FutureCallback<T> cb,\n                                             String path,\n                                             String method,\n                                             HttpHeaders headers,\n                                             Map<String, String> queryParameters,\n                                             Object body,\n                                             TypeReference<U> resultType,\n                                             Translator<T, U> translator,\n                                             Boolean forward)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6944410132030491
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4de83d38c926bd991ce3e2d2804537a392ae6691",
        "url": "https://github.com/apache/kafka/commit/4de83d38c926bd991ce3e2d2804537a392ae6691",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public init(context InternalProcessorContext<KOut,VOut>, processingExceptionHandler ProcessingExceptionHandler) : void extracted from private initializeTopology() : void in class org.apache.kafka.streams.processor.internals.StreamTask & moved to class org.apache.kafka.streams.processor.internals.ProcessorNode",
            "leftSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java",
                    "startLine": 1016,
                    "endLine": 1028,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private initializeTopology() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java",
                    "startLine": 1023,
                    "endLine": 1023,
                    "startColumn": 17,
                    "endColumn": 45,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorNode.java",
                    "startLine": 139,
                    "endLine": 142,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public init(context InternalProcessorContext<KOut,VOut>, processingExceptionHandler ProcessingExceptionHandler) : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorNode.java",
                    "startLine": 140,
                    "endLine": 140,
                    "startColumn": 9,
                    "endColumn": 23,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java",
                    "startLine": 1025,
                    "endLine": 1037,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private initializeTopology() : void"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java",
                    "startLine": 1032,
                    "endLine": 1032,
                    "startColumn": 17,
                    "endColumn": 72,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "node.init(processorContext,processingExceptionHandler)"
                },
                {
                    "filePath": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorNode.java",
                    "startLine": 141,
                    "endLine": 141,
                    "startColumn": 9,
                    "endColumn": 70,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": false
        },
        "ref_id": 602,
        "extraction_results": {
            "success": true,
            "newCommitHash": "fa2cb0ebdf44479f6bad42bdb0f0cb5bc1cc37a7",
            "newBranchName": "extract-init-initializeTopology-931bb62"
        },
        "telemetry": {
            "id": "d13a0c52-c267-48c1-8424-826c1832b903",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1290,
                "lineStart": 67,
                "lineEnd": 1356,
                "bodyLineStart": 67,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java",
                "sourceCode": "/**\n * A StreamTask is associated with a {@link AbstractPartitionGroup}, and is assigned to a StreamThread for processing.\n */\npublic class StreamTask extends AbstractTask implements ProcessorNodePunctuator, Task {\n\n    private final Time time;\n    private final Consumer<byte[], byte[]> mainConsumer;\n\n    // we want to abstract eos logic out of StreamTask, however\n    // there's still an optimization that requires this info to be\n    // leaked into this class, which is to checkpoint after committing if EOS is not enabled.\n    private final boolean eosEnabled;\n\n    private final int maxBufferedSize;\n    private final AbstractPartitionGroup partitionGroup;\n    private final RecordCollector recordCollector;\n    private final AbstractPartitionGroup.RecordInfo recordInfo;\n    private final Map<TopicPartition, Long> consumedOffsets;\n    private final Map<TopicPartition, Long> committedOffsets;\n    private final Map<TopicPartition, Long> highWatermark;\n    private final Set<TopicPartition> resetOffsetsForPartitions;\n    private final Set<TopicPartition> partitionsToResume;\n    private final PunctuationQueue streamTimePunctuationQueue;\n    private final PunctuationQueue systemTimePunctuationQueue;\n    private final StreamsMetricsImpl streamsMetrics;\n\n    private long processTimeMs = 0L;\n\n    private final Sensor closeTaskSensor;\n    private final Sensor processRatioSensor;\n    private final Sensor processLatencySensor;\n    private final Sensor restoreSensor;\n    private final Sensor restoreRemainingSensor;\n    private final Sensor punctuateLatencySensor;\n    private final Sensor bufferedRecordsSensor;\n    private final Map<String, Sensor> e2eLatencySensors = new HashMap<>();\n\n    private final RecordQueueCreator recordQueueCreator;\n\n    @SuppressWarnings(\"rawtypes\")\n    protected final InternalProcessorContext processorContext;\n\n    private StampedRecord record;\n    private boolean commitNeeded = false;\n    private boolean commitRequested = false;\n    private boolean hasPendingTxCommit = false;\n    private Optional<Long> timeCurrentIdlingStarted;\n\n    @SuppressWarnings({\"rawtypes\", \"this-escape\", \"checkstyle:ParameterNumber\"})\n    public StreamTask(final TaskId id,\n                      final Set<TopicPartition> inputPartitions,\n                      final ProcessorTopology topology,\n                      final Consumer<byte[], byte[]> mainConsumer,\n                      final TaskConfig config,\n                      final StreamsMetricsImpl streamsMetrics,\n                      final StateDirectory stateDirectory,\n                      final ThreadCache cache,\n                      final Time time,\n                      final ProcessorStateManager stateMgr,\n                      final RecordCollector recordCollector,\n                      final InternalProcessorContext processorContext,\n                      final LogContext logContext,\n                      final boolean processingThreadsEnabled\n                      ) {\n        super(\n            id,\n            topology,\n            stateDirectory,\n            stateMgr,\n            inputPartitions,\n            config,\n            \"task\",\n            StreamTask.class\n        );\n        this.mainConsumer = mainConsumer;\n\n        this.processorContext = processorContext;\n        processorContext.transitionToActive(this, recordCollector, cache);\n\n        this.time = time;\n        this.recordCollector = recordCollector;\n        this.eosEnabled = config.eosEnabled;\n\n        final String threadId = Thread.currentThread().getName();\n        this.streamsMetrics = streamsMetrics;\n        closeTaskSensor = ThreadMetrics.closeTaskSensor(threadId, streamsMetrics);\n        final String taskId = id.toString();\n        restoreSensor = TaskMetrics.restoreSensor(threadId, taskId, streamsMetrics);\n        restoreRemainingSensor = TaskMetrics.restoreRemainingRecordsSensor(threadId, taskId, streamsMetrics);\n        processRatioSensor = TaskMetrics.activeProcessRatioSensor(threadId, taskId, streamsMetrics);\n        processLatencySensor = TaskMetrics.processLatencySensor(threadId, taskId, streamsMetrics);\n        punctuateLatencySensor = TaskMetrics.punctuateSensor(threadId, taskId, streamsMetrics);\n        bufferedRecordsSensor = TaskMetrics.activeBufferedRecordsSensor(threadId, taskId, streamsMetrics);\n\n        for (final String terminalNodeName : topology.terminalNodes()) {\n            e2eLatencySensors.put(\n                terminalNodeName,\n                ProcessorNodeMetrics.e2ELatencySensor(threadId, taskId, terminalNodeName, streamsMetrics)\n            );\n        }\n\n        for (final ProcessorNode<?, ?, ?, ?> sourceNode : topology.sources()) {\n            final String sourceNodeName = sourceNode.name();\n            e2eLatencySensors.put(\n                sourceNodeName,\n                ProcessorNodeMetrics.e2ELatencySensor(threadId, taskId, sourceNodeName, streamsMetrics)\n            );\n        }\n\n        streamTimePunctuationQueue = new PunctuationQueue();\n        systemTimePunctuationQueue = new PunctuationQueue();\n        maxBufferedSize = config.maxBufferedSize;\n\n        // initialize the consumed and committed offset cache\n        consumedOffsets = new HashMap<>();\n        resetOffsetsForPartitions = new HashSet<>();\n        partitionsToResume = new HashSet<>();\n\n        recordQueueCreator = new RecordQueueCreator(this.logContext, config.timestampExtractor, config.deserializationExceptionHandler);\n\n        recordInfo = new RecordInfo();\n\n        final Sensor enforcedProcessingSensor;\n        enforcedProcessingSensor = TaskMetrics.enforcedProcessingSensor(threadId, taskId, streamsMetrics);\n        final long maxTaskIdleMs = config.maxTaskIdleMs;\n        if (processingThreadsEnabled) {\n            partitionGroup = new SynchronizedPartitionGroup(new PartitionGroup(\n                logContext,\n                createPartitionQueues(),\n                mainConsumer::currentLag,\n                TaskMetrics.recordLatenessSensor(threadId, taskId, streamsMetrics),\n                enforcedProcessingSensor,\n                maxTaskIdleMs\n            ));\n        } else {\n            partitionGroup = new PartitionGroup(\n                logContext,\n                createPartitionQueues(),\n                mainConsumer::currentLag,\n                TaskMetrics.recordLatenessSensor(threadId, taskId, streamsMetrics),\n                enforcedProcessingSensor,\n                maxTaskIdleMs\n            );\n        }\n\n        stateMgr.registerGlobalStateStores(topology.globalStateStores());\n        committedOffsets = new HashMap<>();\n        highWatermark = new HashMap<>();\n        for (final TopicPartition topicPartition: inputPartitions) {\n            committedOffsets.put(topicPartition, -1L);\n            highWatermark.put(topicPartition, -1L);\n        }\n        timeCurrentIdlingStarted = Optional.empty();\n    }\n\n    // create queues for each assigned partition and associate them\n    // to corresponding source nodes in the processor topology\n    private Map<TopicPartition, RecordQueue> createPartitionQueues() {\n        final Map<TopicPartition, RecordQueue> partitionQueues = new HashMap<>();\n        for (final TopicPartition partition : inputPartitions()) {\n            partitionQueues.put(partition, recordQueueCreator.createQueue(partition));\n        }\n        return partitionQueues;\n    }\n\n    @Override\n    public boolean isActive() {\n        return true;\n    }\n\n    @Override\n    public void recordRestoration(final Time time, final long numRecords, final boolean initRemaining) {\n        if (initRemaining) {\n            maybeRecordSensor(numRecords, time, restoreRemainingSensor);\n        } else {\n            maybeRecordSensor(numRecords, time, restoreSensor);\n            maybeRecordSensor(-1 * numRecords, time, restoreRemainingSensor);\n        }\n    }\n\n    /**\n     * @throws TaskCorruptedException if the state cannot be reused (with EOS) and needs to be reset\n     * @throws LockException    could happen when multi-threads within the single instance, could retry\n     * @throws TimeoutException if initializing record collector timed out\n     * @throws StreamsException fatal error, should close the thread\n     */\n    @Override\n    public void initializeIfNeeded() {\n        if (state() == State.CREATED) {\n            recordCollector.initialize();\n\n            StateManagerUtil.registerStateStores(log, logPrefix, topology, stateMgr, stateDirectory, processorContext);\n\n            // without EOS the checkpoint file would not be deleted after loading, and\n            // with EOS we would not checkpoint ever during running state anyways.\n            // therefore we can initialize the snapshot as empty so that we would checkpoint right after loading\n            offsetSnapshotSinceLastFlush = Collections.emptyMap();\n\n            transitionTo(State.RESTORING);\n\n            log.info(\"Initialized\");\n        }\n    }\n\n    public void addPartitionsForOffsetReset(final Set<TopicPartition> partitionsForOffsetReset) {\n        mainConsumer.pause(partitionsForOffsetReset);\n        resetOffsetsForPartitions.addAll(partitionsForOffsetReset);\n    }\n\n    /**\n     * @throws TimeoutException if fetching committed offsets timed out\n     */\n    @Override\n    public void completeRestoration(final java.util.function.Consumer<Set<TopicPartition>> offsetResetter) {\n        switch (state()) {\n            case RUNNING:\n                return;\n\n            case RESTORING:\n                resetOffsetsIfNeededAndInitializeMetadata(offsetResetter);\n                initializeTopology();\n                processorContext.initialize();\n                if (!eosEnabled) {\n                    maybeCheckpoint(true);\n                }\n\n                transitionTo(State.RUNNING);\n\n                log.info(\"Restored and ready to run\");\n\n                break;\n\n            case CREATED:\n            case SUSPENDED:\n            case CLOSED:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while completing restoration for active task \" + id);\n\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while completing restoration for active task \" + id);\n        }\n    }\n\n    @Override\n    public void suspend() {\n        switch (state()) {\n            case CREATED:\n                transitToSuspend();\n                break;\n\n            case RESTORING:\n                transitToSuspend();\n                break;\n\n            case RUNNING:\n                try {\n                    // use try-catch to ensure state transition to SUSPENDED even if user code throws in `Processor#close()`\n                    closeTopology();\n\n                    // we must clear the buffered records when suspending because upon resuming the consumer would\n                    // re-fetch those records starting from the committed position\n                    partitionGroup.clear();\n                } finally {\n                    transitToSuspend();\n                }\n\n                break;\n\n            case SUSPENDED:\n                log.info(\"Skip suspending since state is {}\", state());\n\n                break;\n\n            case CLOSED:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while suspending active task \" + id);\n\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while suspending active task \" + id);\n        }\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    private void closeTopology() {\n        log.trace(\"Closing processor topology\");\n\n        // close the processors\n        // make sure close() is called for each node even when there is a RuntimeException\n        RuntimeException exception = null;\n        for (final ProcessorNode<?, ?, ?, ?> node : topology.processors()) {\n            processorContext.setCurrentNode(node);\n            try {\n                node.close();\n            } catch (final RuntimeException e) {\n                exception = e;\n            } finally {\n                processorContext.setCurrentNode(null);\n            }\n        }\n\n        if (exception != null) {\n            throw exception;\n        }\n    }\n\n    /**\n     * <pre>\n     * - resume the task\n     * </pre>\n     */\n    @Override\n    public void resume() {\n        switch (state()) {\n            case CREATED:\n            case RUNNING:\n            case RESTORING:\n                // no need to do anything, just let them continue running / restoring / closing\n                log.trace(\"Skip resuming since state is {}\", state());\n                break;\n\n            case SUSPENDED:\n                // just transit the state without any logical changes: suspended and restoring states\n                // are not actually any different for inner modules\n\n                // Deleting checkpoint file before transition to RESTORING state (KAFKA-10362)\n                try {\n                    stateMgr.deleteCheckPointFileIfEOSEnabled();\n                    log.debug(\"Deleted check point file upon resuming with EOS enabled\");\n                } catch (final IOException ioe) {\n                    log.error(\"Encountered error while deleting the checkpoint file due to this exception\", ioe);\n                }\n\n                transitionTo(State.RESTORING);\n                log.info(\"Resumed to restoring state\");\n\n                break;\n\n            case CLOSED:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while resuming active task \" + id);\n\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while resuming active task \" + id);\n        }\n        timeCurrentIdlingStarted = Optional.empty();\n    }\n\n\n    public void flush() {\n        stateMgr.flushCache();\n        recordCollector.flush();\n    }\n\n    /**\n     * @throws StreamsException fatal error that should cause the thread to die\n     * @throws TaskMigratedException recoverable error that would cause the task to be removed\n     * @return offsets that should be committed for this task\n     */\n    @Override\n    public Map<TopicPartition, OffsetAndMetadata> prepareCommit() {\n        switch (state()) {\n            case CREATED:\n            case RESTORING:\n            case RUNNING:\n            case SUSPENDED:\n                // the commitNeeded flag just indicates whether we have reached RUNNING and processed any new data,\n                // so it only indicates whether the record collector should be flushed or not, whereas the state\n                // manager should always be flushed; either there is newly restored data or the flush will be a no-op\n                if (commitNeeded) {\n                    // we need to flush the store caches before flushing the record collector since it may cause some\n                    // cached records to be processed and hence generate more records to be sent out\n                    //\n                    // TODO: this should be removed after we decouple caching with emitting\n                    flush();\n                    hasPendingTxCommit = eosEnabled;\n\n                    log.debug(\"Prepared {} task for committing\", state());\n                    return committableOffsetsAndMetadata();\n                } else {\n                    log.debug(\"Skipped preparing {} task for commit since there is nothing to commit\", state());\n                    return Collections.emptyMap();\n                }\n\n            case CLOSED:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while preparing active task \" + id + \" for committing\");\n\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while preparing active task \" + id + \" for committing\");\n        }\n    }\n\n    private Long findOffset(final TopicPartition partition) {\n        Long offset = partitionGroup.headRecordOffset(partition);\n        if (offset == null) {\n            try {\n                offset = mainConsumer.position(partition);\n            } catch (final TimeoutException error) {\n                // the `consumer.position()` call should never block, because we know that we did process data\n                // for the requested partition and thus the consumer should have a valid local position\n                // that it can return immediately\n\n                // hence, a `TimeoutException` indicates a bug and thus we rethrow it as fatal `IllegalStateException`\n                throw new IllegalStateException(error);\n            } catch (final KafkaException fatal) {\n                throw new StreamsException(fatal);\n            }\n        }\n        return offset;\n    }\n\n    private Map<TopicPartition, OffsetAndMetadata> committableOffsetsAndMetadata() {\n        final Map<TopicPartition, OffsetAndMetadata> committableOffsets;\n\n        switch (state()) {\n            case CREATED:\n            case RESTORING:\n                committableOffsets = Collections.emptyMap();\n\n                break;\n\n            case RUNNING:\n            case SUSPENDED:\n                final Map<TopicPartition, Long> partitionTimes = extractPartitionTimes();\n\n                // If there's processor metadata to be committed. We need to commit them to all\n                // input partitions\n                final Set<TopicPartition> partitionsNeedCommit = processorContext.getProcessorMetadata().needsCommit() ?\n                    inputPartitions() : consumedOffsets.keySet();\n                committableOffsets = new HashMap<>(partitionsNeedCommit.size());\n\n                for (final TopicPartition partition : partitionsNeedCommit) {\n                    final Long offset = findOffset(partition);\n                    final long partitionTime = partitionTimes.get(partition);\n                    committableOffsets.put(partition, new OffsetAndMetadata(offset,\n                        new TopicPartitionMetadata(partitionTime, processorContext.getProcessorMetadata()).encode()));\n                }\n                break;\n\n            case CLOSED:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while getting committable offsets for active task \" + id);\n\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while post committing active task \" + id);\n        }\n\n        return committableOffsets;\n    }\n\n    @Override\n    public void postCommit(final boolean enforceCheckpoint) {\n        switch (state()) {\n            case CREATED:\n                // We should never write a checkpoint for a CREATED task as we may overwrite an existing checkpoint\n                // with empty uninitialized offsets\n                log.debug(\"Skipped writing checkpoint for {} task\", state());\n\n                break;\n\n            case RESTORING:\n            case SUSPENDED:\n                maybeCheckpoint(enforceCheckpoint);\n                log.debug(\"Finalized commit for {} task with enforce checkpoint {}\", state(), enforceCheckpoint);\n\n                break;\n\n            case RUNNING:\n                if (enforceCheckpoint || !eosEnabled) {\n                    maybeCheckpoint(enforceCheckpoint);\n                }\n                log.debug(\"Finalized commit for {} task with eos {} enforce checkpoint {}\", state(), eosEnabled, enforceCheckpoint);\n\n                break;\n\n            case CLOSED:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while post committing active task \" + id);\n\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while post committing active task \" + id);\n        }\n\n        clearCommitStatuses();\n    }\n\n    private void clearCommitStatuses() {\n        commitNeeded = false;\n        commitRequested = false;\n        hasPendingTxCommit = false;\n        processorContext.getProcessorMetadata().setNeedsCommit(false);\n    }\n\n    private Map<TopicPartition, Long> extractPartitionTimes() {\n        final Map<TopicPartition, Long> partitionTimes = new HashMap<>();\n        for (final TopicPartition partition : partitionGroup.partitions()) {\n            partitionTimes.put(partition, partitionGroup.partitionTimestamp(partition));\n        }\n        return partitionTimes;\n    }\n\n    @Override\n    public void closeClean() {\n        validateClean();\n        removeAllSensors();\n        clearCommitStatuses();\n        close(true);\n        log.info(\"Closed clean\");\n    }\n\n    @Override\n    public void closeDirty() {\n        removeAllSensors();\n        clearCommitStatuses();\n        close(false);\n        log.info(\"Closed dirty\");\n    }\n\n    @Override\n    public void updateInputPartitions(final Set<TopicPartition> topicPartitions, final Map<String, List<String>> allTopologyNodesToSourceTopics) {\n        super.updateInputPartitions(topicPartitions, allTopologyNodesToSourceTopics);\n        partitionGroup.updatePartitions(topicPartitions, recordQueueCreator::createQueue);\n        processorContext.getProcessorMetadata().setNeedsCommit(true);\n    }\n\n    @Override\n    public void prepareRecycle() {\n        validateClean();\n        removeAllSensors();\n        clearCommitStatuses();\n        switch (state()) {\n            case SUSPENDED:\n                stateMgr.recycle();\n                partitionGroup.close();\n                recordCollector.closeClean();\n\n                break;\n\n            case CREATED:\n            case RESTORING:\n            case RUNNING:\n            case CLOSED:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while recycling active task \" + id);\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while recycling active task \" + id);\n        }\n\n        closeTaskSensor.record();\n        transitionTo(State.CLOSED);\n\n        log.info(\"Closed and recycled state\");\n    }\n\n    @Override\n    public void resumePollingForPartitionsWithAvailableSpace() {\n        if (!partitionsToResume.isEmpty()) {\n            mainConsumer.resume(partitionsToResume);\n            partitionsToResume.clear();\n        }\n    }\n\n    @Override\n    public void updateLags() {\n        if (state() == State.RUNNING) {\n            partitionGroup.updateLags();\n        }\n    }\n\n    /**\n     * The following exceptions maybe thrown from the state manager flushing call\n     *\n     * @throws TaskMigratedException recoverable error sending changelog records that would cause the task to be removed\n     * @throws StreamsException fatal error when flushing the state store, for example sending changelog records failed\n     *                          or flushing state store get IO errors; such error should cause the thread to die\n     */\n    @Override\n    public void maybeCheckpoint(final boolean enforceCheckpoint) {\n        // commitNeeded indicates we may have processed some records since last commit\n        // and hence we need to refresh checkpointable offsets regardless whether we should checkpoint or not\n        if (commitNeeded || enforceCheckpoint) {\n            stateMgr.updateChangelogOffsets(checkpointableOffsets());\n        }\n\n        super.maybeCheckpoint(enforceCheckpoint);\n    }\n\n    private void validateClean() {\n        // It may be that we failed to commit a task during handleRevocation, but \"forgot\" this and tried to\n        // closeClean in handleAssignment. We should throw if we detect this to force the TaskManager to closeDirty\n        if (commitNeeded) {\n            log.debug(\"Tried to close clean but there was pending uncommitted data, this means we failed to\"\n                          + \" commit and should close as dirty instead\");\n            throw new TaskMigratedException(\"Tried to close dirty task as clean\");\n        }\n    }\n\n    private void removeAllSensors() {\n        streamsMetrics.removeAllTaskLevelSensors(Thread.currentThread().getName(), id.toString());\n        for (final String nodeName : e2eLatencySensors.keySet()) {\n            streamsMetrics.removeAllNodeLevelSensors(Thread.currentThread().getName(), id.toString(), nodeName);\n        }\n    }\n\n    /**\n     * You must commit a task and checkpoint the state manager before closing as this will release the state dir lock\n     */\n    private void close(final boolean clean) {\n        switch (state()) {\n            case SUSPENDED:\n                TaskManager.executeAndMaybeSwallow(\n                    clean,\n                    partitionGroup::close,\n                    \"partition group close\",\n                    log\n                );\n\n                // first close state manager (which is idempotent) then close the record collector\n                // if the latter throws and we re-close dirty which would close the state manager again.\n                TaskManager.executeAndMaybeSwallow(\n                    clean,\n                    () -> StateManagerUtil.closeStateManager(\n                        log,\n                        logPrefix,\n                        clean,\n                        eosEnabled,\n                        stateMgr,\n                        stateDirectory,\n                        TaskType.ACTIVE\n                    ),\n                    \"state manager close\",\n                    log);\n\n                TaskManager.executeAndMaybeSwallow(\n                    clean,\n                    clean ? recordCollector::closeClean : recordCollector::closeDirty,\n                    \"record collector close\",\n                    log\n                );\n\n                break;\n\n            case CLOSED:\n                log.trace(\"Skip closing since state is {}\", state());\n                return;\n\n            case CREATED:\n            case RESTORING:\n            case RUNNING:\n                throw new IllegalStateException(\"Illegal state \" + state() + \" while closing active task \" + id);\n\n            default:\n                throw new IllegalStateException(\"Unknown state \" + state() + \" while closing active task \" + id);\n        }\n\n        record = null;\n        closeTaskSensor.record();\n        partitionsToResume.clear();\n\n        transitionTo(State.CLOSED);\n    }\n\n    /**\n     * An active task is processable if its buffer contains data for all of its input\n     * source topic partitions, or if it is enforced to be processable.\n     */\n    public boolean isProcessable(final long wallClockTime) {\n        if (state() == State.CLOSED) {\n            // a task is only closing / closed when 1) task manager is closing, 2) a rebalance is undergoing;\n            // in either case we can just log it and move on without notifying the thread since the consumer\n            // would soon be updated to not return any records for this task anymore.\n            log.info(\"Stream task {} is already in {} state, skip processing it.\", id(), state());\n\n            return false;\n        }\n\n        if (hasPendingTxCommit) {\n            // if the task has a pending TX commit, we should just retry the commit but not process any records\n            // thus, the task is not processable, even if there is available data in the record queue\n            return false;\n        }\n        final boolean readyToProcess = partitionGroup.readyToProcess(wallClockTime);\n        if (!readyToProcess) {\n            if (!timeCurrentIdlingStarted.isPresent()) {\n                timeCurrentIdlingStarted = Optional.of(wallClockTime);\n            }\n        } else {\n            timeCurrentIdlingStarted = Optional.empty();\n        }\n        return readyToProcess;\n    }\n\n    /**\n     * Process one record.\n     *\n     * @return true if this method processes a record, false if it does not process a record.\n     * @throws TaskMigratedException if the task producer got fenced (EOS only)\n     */\n    @SuppressWarnings(\"unchecked\")\n    public boolean process(final long wallClockTime) {\n        if (record == null) {\n            if (!isProcessable(wallClockTime)) {\n                return false;\n            }\n\n            // get the next record to process\n            record = partitionGroup.nextRecord(recordInfo, wallClockTime);\n\n            // if there is no record to process, return immediately\n            if (record == null) {\n                return false;\n            }\n        }\n\n        try {\n            final TopicPartition partition = recordInfo.partition();\n\n            if (!(record instanceof CorruptedRecord)) {\n                doProcess(wallClockTime);\n            }\n\n            // update the consumed offset map after processing is done\n            consumedOffsets.put(partition, record.offset());\n            commitNeeded = true;\n\n            // after processing this record, if its partition queue's buffered size has been\n            // decreased to the threshold, we can then resume the consumption on this partition\n            if (recordInfo.queue().size() == maxBufferedSize) {\n                partitionsToResume.add(partition);\n            }\n\n            record = null;\n        } catch (final TimeoutException timeoutException) {\n            if (!eosEnabled) {\n                throw timeoutException;\n            } else {\n                record = null;\n                throw new TaskCorruptedException(Collections.singleton(id));\n            }\n        } catch (final StreamsException exception) {\n            record = null;\n            throw exception;\n        } catch (final RuntimeException e) {\n            final StreamsException error = new StreamsException(\n                String.format(\n                    \"Exception caught in process. taskId=%s, processor=%s, topic=%s, partition=%d, offset=%d, stacktrace=%s\",\n                    id(),\n                    processorContext.currentNode().name(),\n                    record.topic(),\n                    record.partition(),\n                    record.offset(),\n                    getStacktraceString(e)\n                ),\n                e\n            );\n            record = null;\n\n            throw error;\n        } finally {\n            processorContext.setCurrentNode(null);\n        }\n\n        return true;\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    private void doProcess(final long wallClockTime) {\n        // process the record by passing to the source node of the topology\n        final ProcessorNode<Object, Object, Object, Object> currNode = (ProcessorNode<Object, Object, Object, Object>) recordInfo.node();\n        log.trace(\"Start processing one record [{}]\", record);\n\n        final ProcessorRecordContext recordContext = new ProcessorRecordContext(\n            record.timestamp,\n            record.offset(),\n            record.partition(),\n            record.topic(),\n            record.headers()\n        );\n        updateProcessorContext(currNode, wallClockTime, recordContext);\n\n        maybeRecordE2ELatency(record.timestamp, wallClockTime, currNode.name());\n        final Record<Object, Object> toProcess = new Record<>(\n            record.key(),\n            record.value(),\n            processorContext.timestamp(),\n            processorContext.headers()\n        );\n        maybeMeasureLatency(() -> currNode.process(toProcess), time, processLatencySensor);\n\n        log.trace(\"Completed processing one record [{}]\", record);\n    }\n\n    @Override\n    public void recordProcessBatchTime(final long processBatchTime) {\n        processTimeMs += processBatchTime;\n    }\n\n    @Override\n    public void recordProcessTimeRatioAndBufferSize(final long allTaskProcessMs, final long now) {\n        bufferedRecordsSensor.record(partitionGroup.numBuffered());\n        processRatioSensor.record((double) processTimeMs / allTaskProcessMs, now);\n        processTimeMs = 0L;\n    }\n\n    private String getStacktraceString(final RuntimeException e) {\n        String stacktrace = null;\n        try (final StringWriter stringWriter = new StringWriter();\n             final PrintWriter printWriter = new PrintWriter(stringWriter)) {\n            e.printStackTrace(printWriter);\n            stacktrace = stringWriter.toString();\n        } catch (final IOException ioe) {\n            log.error(\"Encountered error extracting stacktrace from this exception\", ioe);\n        }\n        return stacktrace;\n    }\n\n    /**\n     * @throws IllegalStateException if the current node is not null\n     * @throws TaskMigratedException if the task producer got fenced (EOS only)\n     */\n    @SuppressWarnings(\"unchecked\")\n    @Override\n    public void punctuate(final ProcessorNode<?, ?, ?, ?> node,\n                          final long timestamp,\n                          final PunctuationType type,\n                          final Punctuator punctuator) {\n        if (processorContext.currentNode() != null) {\n            throw new IllegalStateException(String.format(\"%sCurrent node is not null\", logPrefix));\n        }\n\n        // when punctuating, we need to preserve the timestamp (this can be either system time or event time)\n        // while other record context are set as dummy: null topic, -1 partition, -1 offset and empty header\n        final ProcessorRecordContext recordContext = new ProcessorRecordContext(\n            timestamp,\n            -1L,\n            -1,\n            null,\n            new RecordHeaders()\n        );\n        updateProcessorContext(node, time.milliseconds(), recordContext);\n\n        if (log.isTraceEnabled()) {\n            log.trace(\"Punctuating processor {} with timestamp {} and punctuation type {}\", node.name(), timestamp, type);\n        }\n\n        try {\n            maybeMeasureLatency(() -> node.punctuate(timestamp, punctuator), time, punctuateLatencySensor);\n        } catch (final StreamsException e) {\n            throw e;\n        } catch (final RuntimeException e) {\n            throw new StreamsException(String.format(\"%sException caught while punctuating processor '%s'\", logPrefix, node.name()), e);\n        } finally {\n            processorContext.setCurrentNode(null);\n        }\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    private void updateProcessorContext(final ProcessorNode<?, ?, ?, ?> currNode,\n                                        final long wallClockTime,\n                                        final ProcessorRecordContext recordContext) {\n        processorContext.setRecordContext(recordContext);\n        processorContext.setCurrentNode(currNode);\n        processorContext.setSystemTimeMs(wallClockTime);\n    }\n\n    /**\n     * Return all the checkpointable offsets(written + consumed) to the state manager.\n     * Currently only changelog topic offsets need to be checkpointed.\n     */\n    private Map<TopicPartition, Long> checkpointableOffsets() {\n        final Map<TopicPartition, Long> checkpointableOffsets = new HashMap<>(recordCollector.offsets());\n        for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {\n            checkpointableOffsets.putIfAbsent(entry.getKey(), entry.getValue());\n        }\n\n        log.debug(\"Checkpointable offsets {}\", checkpointableOffsets);\n\n        return checkpointableOffsets;\n    }\n\n    private void resetOffsetsIfNeededAndInitializeMetadata(final java.util.function.Consumer<Set<TopicPartition>> offsetResetter) {\n        try {\n            final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata = mainConsumer.committed(inputPartitions());\n\n            for (final Map.Entry<TopicPartition, OffsetAndMetadata> committedEntry : offsetsAndMetadata.entrySet()) {\n                if (resetOffsetsForPartitions.contains(committedEntry.getKey())) {\n                    final OffsetAndMetadata offsetAndMetadata = committedEntry.getValue();\n                    if (offsetAndMetadata != null) {\n                        mainConsumer.seek(committedEntry.getKey(), offsetAndMetadata);\n                        resetOffsetsForPartitions.remove(committedEntry.getKey());\n                    }\n                }\n            }\n\n            offsetResetter.accept(resetOffsetsForPartitions);\n            resetOffsetsForPartitions.clear();\n\n            initializeTaskTimeAndProcessorMetadata(offsetsAndMetadata.entrySet().stream()\n                .filter(e -> e.getValue() != null)\n                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue))\n            );\n        } catch (final TimeoutException timeoutException) {\n            log.warn(\n                \"Encountered {} while trying to fetch committed offsets, will retry initializing the metadata in the next loop.\" +\n                    \"\\nConsider overwriting consumer config {} to a larger value to avoid timeout errors\",\n                time.toString(),\n                ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG);\n\n            // re-throw to trigger `task.timeout.ms`\n            throw timeoutException;\n        } catch (final KafkaException e) {\n            throw new StreamsException(String.format(\"task [%s] Failed to initialize offsets for %s\", id, inputPartitions()), e);\n        }\n    }\n\n    private void initializeTaskTimeAndProcessorMetadata(final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata) {\n        final ProcessorMetadata finalProcessMetadata = new ProcessorMetadata();\n        for (final Map.Entry<TopicPartition, OffsetAndMetadata> entry : offsetsAndMetadata.entrySet()) {\n            final TopicPartition partition = entry.getKey();\n            final OffsetAndMetadata metadata = entry.getValue();\n\n            if (metadata != null) {\n                final TopicPartitionMetadata committedTimestampAndMeta = TopicPartitionMetadata.decode(metadata.metadata());\n                final long committedTimestamp = committedTimestampAndMeta.partitionTime();\n                partitionGroup.setPartitionTime(partition, committedTimestamp);\n                log.debug(\"A committed timestamp was detected: setting the partition time of partition {}\"\n                    + \" to {} in stream task {}\", partition, committedTimestamp, id);\n\n                final ProcessorMetadata processorMetadata = committedTimestampAndMeta.processorMetadata();\n                finalProcessMetadata.update(processorMetadata);\n            } else {\n                log.debug(\"No committed timestamp was found in metadata for partition {}\", partition);\n            }\n        }\n        processorContext.setProcessorMetadata(finalProcessMetadata);\n\n        final Set<TopicPartition> nonCommitted = new HashSet<>(inputPartitions());\n        nonCommitted.removeAll(offsetsAndMetadata.keySet());\n        for (final TopicPartition partition : nonCommitted) {\n            log.debug(\"No committed offset for partition {}, therefore no timestamp can be found for this partition\", partition);\n        }\n    }\n\n    @Override\n    public Map<TopicPartition, Long> purgeableOffsets() {\n        final Map<TopicPartition, Long> purgeableConsumedOffsets = new HashMap<>();\n        for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {\n            final TopicPartition tp = entry.getKey();\n            if (topology.isRepartitionTopic(tp.topic())) {\n                purgeableConsumedOffsets.put(tp, entry.getValue() + 1);\n            }\n        }\n\n        return purgeableConsumedOffsets;\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    private void initializeTopology() {\n        // initialize the task by initializing all its processor nodes in the topology\n        log.trace(\"Initializing processor nodes of the topology\");\n        for (final ProcessorNode<?, ?, ?, ?> node : topology.processors()) {\n            processorContext.setCurrentNode(node);\n            try {\n                init(node);\n            } finally {\n                processorContext.setCurrentNode(null);\n            }\n        }\n    }\n\n    private void init(ProcessorNode<?, ?, ?, ?> node) {\n        node.init(processorContext);\n    }\n\n    /**\n     * Adds records to queues. If a record has an invalid (i.e., negative) timestamp, the record is skipped\n     * and not added to the queue for processing\n     *\n     * @param partition the partition\n     * @param records   the records\n     */\n    @Override\n    public void addRecords(final TopicPartition partition, final Iterable<ConsumerRecord<byte[], byte[]>> records) {\n        final int newQueueSize = partitionGroup.addRawRecords(partition, records);\n\n        if (log.isTraceEnabled()) {\n            log.trace(\"Added records into the buffered queue of partition {}, new queue size is {}\", partition, newQueueSize);\n        }\n\n        // if after adding these records, its partition queue's buffered size has been\n        // increased beyond the threshold, we can then pause the consumption for this partition\n        if (newQueueSize > maxBufferedSize) {\n            mainConsumer.pause(singleton(partition));\n        }\n    }\n\n    /**\n     * Schedules a punctuation for the processor\n     *\n     * @param interval the interval in milliseconds\n     * @param type     the punctuation type\n     * @throws IllegalStateException if the current node is not null\n     */\n    public Cancellable schedule(final long interval, final PunctuationType type, final Punctuator punctuator) {\n        switch (type) {\n            case STREAM_TIME:\n                // align punctuation to 0L, punctuate as soon as we have data\n                return schedule(0L, interval, type, punctuator);\n            case WALL_CLOCK_TIME:\n                // align punctuation to now, punctuate after interval has elapsed\n                return schedule(time.milliseconds() + interval, interval, type, punctuator);\n            default:\n                throw new IllegalArgumentException(\"Unrecognized PunctuationType: \" + type);\n        }\n    }\n\n    /**\n     * Schedules a punctuation for the processor\n     *\n     * @param startTime time of the first punctuation\n     * @param interval  the interval in milliseconds\n     * @param type      the punctuation type\n     * @throws IllegalStateException if the current node is not null\n     */\n    private Cancellable schedule(final long startTime, final long interval, final PunctuationType type, final Punctuator punctuator) {\n        if (processorContext.currentNode() == null) {\n            throw new IllegalStateException(String.format(\"%sCurrent node is null\", logPrefix));\n        }\n\n        final PunctuationSchedule schedule = new PunctuationSchedule(processorContext.currentNode(), startTime, interval, punctuator);\n\n        switch (type) {\n            case STREAM_TIME:\n                // STREAM_TIME punctuation is data driven, will first punctuate as soon as stream-time is known and >= time,\n                // stream-time is known when we have received at least one record from each input topic\n                return streamTimePunctuationQueue.schedule(schedule);\n            case WALL_CLOCK_TIME:\n                // WALL_CLOCK_TIME is driven by the wall clock time, will first punctuate when now >= time\n                return systemTimePunctuationQueue.schedule(schedule);\n            default:\n                throw new IllegalArgumentException(\"Unrecognized PunctuationType: \" + type);\n        }\n    }\n\n    /**\n     * Possibly trigger registered stream-time punctuation functions if\n     * current partition group timestamp has reached the defined stamp\n     * Note, this is only called in the presence of new records\n     *\n     * @throws TaskMigratedException if the task producer got fenced (EOS only)\n     */\n    public boolean maybePunctuateStreamTime() {\n        final long streamTime = partitionGroup.streamTime();\n\n        // if the timestamp is not known yet, meaning there is not enough data accumulated\n        // to reason stream partition time, then skip.\n        if (streamTime == RecordQueue.UNKNOWN) {\n            return false;\n        } else {\n            final boolean punctuated = streamTimePunctuationQueue.maybePunctuate(streamTime, PunctuationType.STREAM_TIME, this);\n\n            if (punctuated) {\n                commitNeeded = true;\n            }\n\n            return punctuated;\n        }\n    }\n\n    public boolean canPunctuateStreamTime() {\n        final long streamTime = partitionGroup.streamTime();\n        return streamTimePunctuationQueue.canPunctuate(streamTime);\n    }\n\n    /**\n     * Possibly trigger registered system-time punctuation functions if\n     * current system timestamp has reached the defined stamp\n     * Note, this is called irrespective of the presence of new records\n     *\n     * @throws TaskMigratedException if the task producer got fenced (EOS only)\n     */\n    public boolean maybePunctuateSystemTime() {\n        final long systemTime = time.milliseconds();\n\n        final boolean punctuated = systemTimePunctuationQueue.maybePunctuate(systemTime, PunctuationType.WALL_CLOCK_TIME, this);\n\n        if (punctuated) {\n            commitNeeded = true;\n        }\n\n        return punctuated;\n    }\n\n    public boolean canPunctuateSystemTime() {\n        final long systemTime = time.milliseconds();\n        return systemTimePunctuationQueue.canPunctuate(systemTime);\n    }\n\n    void maybeRecordE2ELatency(final long recordTimestamp, final long now, final String nodeName) {\n        final Sensor e2eLatencySensor = e2eLatencySensors.get(nodeName);\n        if (e2eLatencySensor == null) {\n            throw new IllegalStateException(\"Requested to record e2e latency but could not find sensor for node \" + nodeName);\n        } else if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {\n            e2eLatencySensor.record(now - recordTimestamp, now);\n        }\n    }\n\n    /**\n     * Request committing the current task's state\n     */\n    void requestCommit() {\n        commitRequested = true;\n    }\n\n    /**\n     * Whether or not a request has been made to commit the current state\n     */\n    @Override\n    public boolean commitRequested() {\n        return commitRequested;\n    }\n\n    @SuppressWarnings(\"rawtypes\")\n    public InternalProcessorContext processorContext() {\n        return processorContext;\n    }\n\n    /**\n     * Produces a string representation containing useful information about a Task.\n     * This is useful in debugging scenarios.\n     *\n     * @return A string representation of the StreamTask instance.\n     */\n    @Override\n    public String toString() {\n        return toString(\"\");\n    }\n\n    /**\n     * Produces a string representation containing useful information about a Task starting with the given indent.\n     * This is useful in debugging scenarios.\n     *\n     * @return A string representation of the Task instance.\n     */\n    public String toString(final String indent) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(indent);\n        sb.append(\"TaskId: \");\n        sb.append(id);\n        sb.append(\"\\n\");\n\n        // print topology\n        if (topology != null) {\n            sb.append(indent).append(topology.toString(indent + \"\\t\"));\n        }\n\n        // print assigned partitions\n        final Set<TopicPartition> partitions = inputPartitions();\n        if (partitions != null && !partitions.isEmpty()) {\n            sb.append(indent).append(\"Partitions [\");\n            for (final TopicPartition topicPartition : partitions) {\n                sb.append(topicPartition).append(\", \");\n            }\n            sb.setLength(sb.length() - 2);\n            sb.append(\"]\\n\");\n        }\n        return sb.toString();\n    }\n\n    @Override\n    public boolean commitNeeded() {\n        // we need to do an extra check if the flag was false, that\n        // if the consumer position has been updated; this is because\n        // there may be non data records such as control markers bypassed\n        if (commitNeeded) {\n            return true;\n        } else {\n            for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {\n                final TopicPartition partition = entry.getKey();\n                try {\n                    final long offset = mainConsumer.position(partition);\n\n                    // note the position in consumer is the \"next\" record to fetch,\n                    // so it should be larger than the consumed offset by 1; if it is\n                    // more than 1 it means there are control records, which the consumer skips over silently\n                    if (offset > entry.getValue() + 1) {\n                        commitNeeded = true;\n                        entry.setValue(offset - 1);\n                    }\n                } catch (final TimeoutException swallow) {\n                    log.debug(\n                        String.format(\"Could not get consumer position for partition %s\", partition),\n                        swallow\n                    );\n                } catch (final KafkaException fatal) {\n                    throw new StreamsException(fatal);\n                }\n            }\n\n            return commitNeeded;\n        }\n    }\n\n    @Override\n    public Map<TopicPartition, Long> changelogOffsets() {\n        if (state() == State.RUNNING) {\n            // if we are in running state, just return the latest offset sentinel indicating\n            // we should be at the end of the changelog\n            return changelogPartitions().stream()\n                                        .collect(Collectors.toMap(Function.identity(), tp -> Task.LATEST_OFFSET));\n        } else {\n            return Collections.unmodifiableMap(stateMgr.changelogOffsets());\n        }\n    }\n\n    @Override\n    public Map<TopicPartition, Long> committedOffsets() {\n        return Collections.unmodifiableMap(committedOffsets);\n    }\n\n    @Override\n    public Map<TopicPartition, Long> highWaterMark() {\n        return Collections.unmodifiableMap(highWatermark);\n    }\n\n    private void transitToSuspend() {\n        log.info(\"Suspended from {}\", state());\n        transitionTo(State.SUSPENDED);\n        timeCurrentIdlingStarted = Optional.of(System.currentTimeMillis());\n    }\n\n    @Override\n    public Optional<Long> timeCurrentIdlingStarted() {\n        return timeCurrentIdlingStarted;\n    }\n\n    public void updateCommittedOffsets(final TopicPartition topicPartition, final Long offset) {\n        committedOffsets.put(topicPartition, offset);\n    }\n\n    public void updateEndOffsets(final TopicPartition topicPartition, final Long offset) {\n        highWatermark.put(topicPartition, offset);\n    }\n\n    public boolean hasRecordsQueued() {\n        return numBuffered() > 0;\n    }\n\n    RecordCollector recordCollector() {\n        return recordCollector;\n    }\n\n    // below are visible for testing only\n    int numBuffered() {\n        return partitionGroup.numBuffered();\n    }\n\n    long streamTime() {\n        return partitionGroup.streamTime();\n    }\n\n    private class RecordQueueCreator {\n        private final LogContext logContext;\n        private final TimestampExtractor defaultTimestampExtractor;\n        private final DeserializationExceptionHandler defaultDeserializationExceptionHandler;\n\n        private RecordQueueCreator(final LogContext logContext,\n                                   final TimestampExtractor defaultTimestampExtractor,\n                                   final DeserializationExceptionHandler defaultDeserializationExceptionHandler) {\n            this.logContext = logContext;\n            this.defaultTimestampExtractor = defaultTimestampExtractor;\n            this.defaultDeserializationExceptionHandler = defaultDeserializationExceptionHandler;\n        }\n\n        public RecordQueue createQueue(final TopicPartition partition) {\n            final SourceNode<?, ?> source = topology.source(partition.topic());\n            if (source == null) {\n                throw new TopologyException(\n                        \"Topic \" + partition.topic() + \" is unknown to the topology. \" +\n                                \"This may happen if different KafkaStreams instances of the same application execute different Topologies. \" +\n                                \"Note that Topologies are only identical if all operators are added in the same order.\"\n                );\n            }\n\n            final TimestampExtractor sourceTimestampExtractor = source.getTimestampExtractor();\n            final TimestampExtractor timestampExtractor = sourceTimestampExtractor != null ? sourceTimestampExtractor : defaultTimestampExtractor;\n            return new RecordQueue(\n                    partition,\n                    source,\n                    timestampExtractor,\n                    defaultDeserializationExceptionHandler,\n                    processorContext,\n                    logContext\n            );\n        }\n    }\n}",
                "methodCount": 68
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createPartitionQueues",
                            "method_signature": "private createPartitionQueues()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "addPartitionsForOffsetReset",
                            "method_signature": "public addPartitionsForOffsetReset(final Set<TopicPartition> partitionsForOffsetReset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeTopology",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private closeTopology()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flush",
                            "method_signature": "public flush()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "findOffset",
                            "method_signature": "private findOffset(final TopicPartition partition)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "committableOffsetsAndMetadata",
                            "method_signature": "private committableOffsetsAndMetadata()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clearCommitStatuses",
                            "method_signature": "private clearCommitStatuses()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "extractPartitionTimes",
                            "method_signature": "private extractPartitionTimes()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateClean",
                            "method_signature": "private validateClean()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "removeAllSensors",
                            "method_signature": "private removeAllSensors()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "private close(final boolean clean)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isProcessable",
                            "method_signature": "public isProcessable(final long wallClockTime)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "process",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    public process(final long wallClockTime)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doProcess",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private doProcess(final long wallClockTime)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getStacktraceString",
                            "method_signature": "private getStacktraceString(final RuntimeException e)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateProcessorContext",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private updateProcessorContext(final ProcessorNode<?, ?, ?, ?> currNode,\n                                        final long wallClockTime,\n                                        final ProcessorRecordContext recordContext)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "checkpointableOffsets",
                            "method_signature": "private checkpointableOffsets()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "resetOffsetsIfNeededAndInitializeMetadata",
                            "method_signature": "private resetOffsetsIfNeededAndInitializeMetadata(final java.util.function.Consumer<Set<TopicPartition>> offsetResetter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initializeTaskTimeAndProcessorMetadata",
                            "method_signature": "private initializeTaskTimeAndProcessorMetadata(final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "initializeTopology",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private initializeTopology()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "init",
                            "method_signature": "private init(ProcessorNode<?, ?, ?, ?> node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "schedule",
                            "method_signature": "public schedule(final long interval, final PunctuationType type, final Punctuator punctuator)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "schedule",
                            "method_signature": "private schedule(final long startTime, final long interval, final PunctuationType type, final Punctuator punctuator)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybePunctuateStreamTime",
                            "method_signature": "public maybePunctuateStreamTime()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "canPunctuateStreamTime",
                            "method_signature": "public canPunctuateStreamTime()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybePunctuateSystemTime",
                            "method_signature": "public maybePunctuateSystemTime()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "canPunctuateSystemTime",
                            "method_signature": "public canPunctuateSystemTime()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeRecordE2ELatency",
                            "method_signature": " maybeRecordE2ELatency(final long recordTimestamp, final long now, final String nodeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toString",
                            "method_signature": "public toString(final String indent)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitToSuspend",
                            "method_signature": "private transitToSuspend()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateCommittedOffsets",
                            "method_signature": "public updateCommittedOffsets(final TopicPartition topicPartition, final Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateEndOffsets",
                            "method_signature": "public updateEndOffsets(final TopicPartition topicPartition, final Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasRecordsQueued",
                            "method_signature": "public hasRecordsQueued()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "numBuffered",
                            "method_signature": " numBuffered()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "streamTime",
                            "method_signature": " streamTime()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createQueue",
                            "method_signature": "public createQueue(final TopicPartition partition)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "getStacktraceString",
                            "method_signature": "private getStacktraceString(final RuntimeException e)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasRecordsQueued",
                            "method_signature": "public hasRecordsQueued()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "init",
                            "method_signature": "private init(ProcessorNode<?, ?, ?, ?> node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeRecordE2ELatency",
                            "method_signature": " maybeRecordE2ELatency(final long recordTimestamp, final long now, final String nodeName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "canPunctuateSystemTime",
                            "method_signature": "public canPunctuateSystemTime()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "flush",
                            "method_signature": "public flush()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "validateClean",
                            "method_signature": "private validateClean()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "transitToSuspend",
                            "method_signature": "private transitToSuspend()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "numBuffered",
                            "method_signature": " numBuffered()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateProcessorContext",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private updateProcessorContext(final ProcessorNode<?, ?, ?, ?> currNode,\n                                        final long wallClockTime,\n                                        final ProcessorRecordContext recordContext)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "clearCommitStatuses",
                            "method_signature": "private clearCommitStatuses()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeTopology",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private closeTopology()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "schedule",
                            "method_signature": "public schedule(final long interval, final PunctuationType type, final Punctuator punctuator)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "doProcess",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private doProcess(final long wallClockTime)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateEndOffsets",
                            "method_signature": "public updateEndOffsets(final TopicPartition topicPartition, final Long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private getStacktraceString(final RuntimeException e)": {
                    "first": {
                        "method_name": "getStacktraceString",
                        "method_signature": "private getStacktraceString(final RuntimeException e)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.23249828945363674
                },
                "public hasRecordsQueued()": {
                    "first": {
                        "method_name": "hasRecordsQueued",
                        "method_signature": "public hasRecordsQueued()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2971902817672385
                },
                "private init(ProcessorNode<?, ?, ?, ?> node)": {
                    "first": {
                        "method_name": "init",
                        "method_signature": "private init(ProcessorNode<?, ?, ?, ?> node)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.30250472412706836
                },
                " maybeRecordE2ELatency(final long recordTimestamp, final long now, final String nodeName)": {
                    "first": {
                        "method_name": "maybeRecordE2ELatency",
                        "method_signature": " maybeRecordE2ELatency(final long recordTimestamp, final long now, final String nodeName)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3086338653887924
                },
                "public canPunctuateSystemTime()": {
                    "first": {
                        "method_name": "canPunctuateSystemTime",
                        "method_signature": "public canPunctuateSystemTime()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3213380166349322
                },
                "public flush()": {
                    "first": {
                        "method_name": "flush",
                        "method_signature": "public flush()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.32267999990226515
                },
                "private validateClean()": {
                    "first": {
                        "method_name": "validateClean",
                        "method_signature": "private validateClean()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.38159796612885966
                },
                "private transitToSuspend()": {
                    "first": {
                        "method_name": "transitToSuspend",
                        "method_signature": "private transitToSuspend()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4023069077287732
                },
                " numBuffered()": {
                    "first": {
                        "method_name": "numBuffered",
                        "method_signature": " numBuffered()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41048296902039433
                },
                "@SuppressWarnings(\"unchecked\")\n    private updateProcessorContext(final ProcessorNode<?, ?, ?, ?> currNode,\n                                        final long wallClockTime,\n                                        final ProcessorRecordContext recordContext)": {
                    "first": {
                        "method_name": "updateProcessorContext",
                        "method_signature": "@SuppressWarnings(\"unchecked\")\n    private updateProcessorContext(final ProcessorNode<?, ?, ?, ?> currNode,\n                                        final long wallClockTime,\n                                        final ProcessorRecordContext recordContext)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4105923648025136
                },
                "private clearCommitStatuses()": {
                    "first": {
                        "method_name": "clearCommitStatuses",
                        "method_signature": "private clearCommitStatuses()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41294342856625993
                },
                "@SuppressWarnings(\"unchecked\")\n    private closeTopology()": {
                    "first": {
                        "method_name": "closeTopology",
                        "method_signature": "@SuppressWarnings(\"unchecked\")\n    private closeTopology()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41974246348002503
                },
                "public schedule(final long interval, final PunctuationType type, final Punctuator punctuator)": {
                    "first": {
                        "method_name": "schedule",
                        "method_signature": "public schedule(final long interval, final PunctuationType type, final Punctuator punctuator)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.42325096567719733
                },
                "@SuppressWarnings(\"unchecked\")\n    private doProcess(final long wallClockTime)": {
                    "first": {
                        "method_name": "doProcess",
                        "method_signature": "@SuppressWarnings(\"unchecked\")\n    private doProcess(final long wallClockTime)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.42365069555811935
                },
                "public updateEndOffsets(final TopicPartition topicPartition, final Long offset)": {
                    "first": {
                        "method_name": "updateEndOffsets",
                        "method_signature": "public updateEndOffsets(final TopicPartition topicPartition, final Long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.43294199340457507
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package SnapshottableCoordinator(logContext LogContext, snapshotRegistry SnapshotRegistry, coordinator S, tp TopicPartition) in class org.apache.kafka.coordinator.group.runtime.SnapshottableCoordinator & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 65,
                    "endLine": 78,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package SnapshottableCoordinator(logContext LogContext, snapshotRegistry SnapshotRegistry, coordinator S, tp TopicPartition)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 77,
                    "endLine": 77,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 65,
                    "endLine": 78,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package SnapshottableCoordinator(logContext LogContext, snapshotRegistry SnapshotRegistry, coordinator S, tp TopicPartition)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 77,
                    "endLine": 77,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 603,
        "extraction_results": {
            "success": false
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public updateLastWrittenOffset(offset Long) : void in class org.apache.kafka.coordinator.group.runtime.SnapshottableCoordinator & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 134,
                    "endLine": 150,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public updateLastWrittenOffset(offset Long) : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 148,
                    "endLine": 148,
                    "startColumn": 9,
                    "endColumn": 54,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 134,
                    "endLine": 150,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public updateLastWrittenOffset(offset Long) : void"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                    "startLine": 148,
                    "endLine": 148,
                    "startColumn": 9,
                    "endColumn": 58,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(offset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 604,
        "extraction_results": {
            "success": true,
            "newCommitHash": "c20cab42e1d9996007ecc6fa1defd949297eb8be",
            "newBranchName": "extract-idempotentCreateSnapshot-updateLastWrittenOffset-130af38"
        },
        "telemetry": {
            "id": "045c1f71-259e-4ec2-a0de-0b11d33a9d8a",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 213,
                "lineStart": 28,
                "lineEnd": 240,
                "bodyLineStart": 28,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/runtime/SnapshottableCoordinator.java",
                "sourceCode": "/**\n * SnapshottableCoordinator is a wrapper on top of the coordinator state machine. This object is not accessed concurrently\n * but multiple threads access it while loading the coordinator partition and therefore requires all methods to be\n * synchronized.\n */\nclass SnapshottableCoordinator<S extends CoordinatorShard<U>, U> implements CoordinatorPlayback<U> {\n    /**\n     * The logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry backing the coordinator.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The actual state machine.\n     */\n    private final S coordinator;\n\n    /**\n     * The topic partition.\n     */\n    private final TopicPartition tp;\n\n    /**\n     * The last offset written to the partition.\n     */\n    private long lastWrittenOffset;\n\n    /**\n     * The last offset committed. This represents the high\n     * watermark of the partition.\n     */\n    private long lastCommittedOffset;\n\n    SnapshottableCoordinator(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        S coordinator,\n        TopicPartition tp\n    ) {\n        this.log = logContext.logger(SnapshottableCoordinator.class);\n        this.coordinator = coordinator;\n        this.snapshotRegistry = snapshotRegistry;\n        this.tp = tp;\n        this.lastWrittenOffset = 0;\n        this.lastCommittedOffset = 0;\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    /**\n     * Reverts the last written offset. This also reverts the snapshot\n     * registry to this offset. All the changes applied after the offset\n     * are lost.\n     *\n     * @param offset The offset to revert to.\n     */\n    synchronized void revertLastWrittenOffset(\n        long offset\n    ) {\n        if (offset > lastWrittenOffset) {\n            throw new IllegalStateException(\"New offset \" + offset + \" of \" + tp +\n                \" must be smaller than \" + lastWrittenOffset + \".\");\n        }\n\n        log.debug(\"Revert last written offset of {} to {}.\", tp, offset);\n        lastWrittenOffset = offset;\n        snapshotRegistry.revertToSnapshot(offset);\n    }\n\n    /**\n     * Replays the record onto the state machine.\n     *\n     * @param offset        The offset of the record in the log.\n     * @param producerId    The producer id.\n     * @param producerEpoch The producer epoch.\n     * @param record        A record.\n     */\n    @Override\n    public synchronized void replay(\n        long offset,\n        long producerId,\n        short producerEpoch,\n        U record\n    ) {\n        coordinator.replay(offset, producerId, producerEpoch, record);\n    }\n\n    /**\n     * Applies the end transaction marker.\n     *\n     * @param producerId    The producer id.\n     * @param producerEpoch The producer epoch.\n     * @param result        The result of the transaction.\n     */\n    @Override\n    public synchronized void replayEndTransactionMarker(\n        long producerId,\n        short producerEpoch,\n        TransactionResult result\n    ) {\n        coordinator.replayEndTransactionMarker(producerId, producerEpoch, result);\n    }\n\n    /**\n     * Updates the last written offset. This also create a new snapshot\n     * in the snapshot registry.\n     *\n     * @param offset The new last written offset.\n     */\n    @Override\n    public synchronized void updateLastWrittenOffset(Long offset) {\n        if (offset <= lastWrittenOffset) {\n            throw new IllegalStateException(\"New last written offset \" + offset + \" of \" + tp +\n                \" must be greater than \" + lastWrittenOffset + \".\");\n        }\n\n        lastWrittenOffset = offset;\n        idempotentCreateSnapshot(offset);\n        log.debug(\"Updated last written offset of {} to {}.\", tp, offset);\n    }\n\n    private void idempotentCreateSnapshot(Long offset) {\n        snapshotRegistry.getOrCreateSnapshot(offset);\n    }\n\n    /**\n     * Updates the last committed offset. This completes all the deferred\n     * events waiting on this offset. This also cleanups all the snapshots\n     * prior to this offset.\n     *\n     * @param offset The new last committed offset.\n     */\n    @Override\n    public synchronized void updateLastCommittedOffset(Long offset) {\n        if (offset < lastCommittedOffset) {\n            throw new IllegalStateException(\"New committed offset \" + offset + \" of \" + tp +\n                \" must be greater than or equal to \" + lastCommittedOffset + \".\");\n        }\n\n        if (offset > lastWrittenOffset) {\n            throw new IllegalStateException(\"New committed offset \" + offset + \" of \" + tp +\n                \"must be less than or equal to \" + lastWrittenOffset + \".\");\n        }\n\n        lastCommittedOffset = offset;\n        snapshotRegistry.deleteSnapshotsUpTo(offset);\n        log.debug(\"Updated committed offset of {} to {}.\", tp, offset);\n    }\n\n    /**\n     * The coordinator has been loaded. This is used to apply any\n     * post loading operations.\n     *\n     * @param newImage  The metadata image.\n     */\n    synchronized void onLoaded(MetadataImage newImage) {\n        this.coordinator.onLoaded(newImage);\n    }\n\n    /**\n     * The coordinator has been unloaded. This is used to apply\n     * any post unloading operations.\n     */\n    synchronized void onUnloaded() {\n        if (this.coordinator != null) {\n            this.coordinator.onUnloaded();\n        }\n    }\n\n    /**\n     * @return The last written offset.\n     */\n    synchronized long lastWrittenOffset() {\n        return this.lastWrittenOffset;\n    }\n\n    /**\n     * A new metadata image is available. This is only called after {@link SnapshottableCoordinator#onLoaded(MetadataImage)}\n     * is called to signal that the coordinator has been fully loaded.\n     *\n     * @param newImage  The new metadata image.\n     * @param delta     The delta image.\n     */\n    synchronized void onNewMetadataImage(MetadataImage newImage, MetadataDelta delta) {\n        this.coordinator.onNewMetadataImage(newImage, delta);\n    }\n\n    /**\n     * @return The last committed offset.\n     */\n    synchronized long lastCommittedOffset() {\n        return this.lastCommittedOffset;\n    }\n\n    /**\n     * @return The coordinator.\n     */\n    synchronized S coordinator() {\n        return this.coordinator;\n    }\n\n    /**\n     * @return The snapshot registry.\n     *\n     * Only used for testing.\n     */\n    synchronized SnapshotRegistry snapshotRegistry() {\n        return this.snapshotRegistry;\n    }\n}",
                "methodCount": 14
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "revertLastWrittenOffset",
                            "method_signature": "synchronized revertLastWrittenOffset(\n        long offset\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onLoaded",
                            "method_signature": "synchronized onLoaded(MetadataImage newImage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUnloaded",
                            "method_signature": "synchronized onUnloaded()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onNewMetadataImage",
                            "method_signature": "synchronized onNewMetadataImage(MetadataImage newImage, MetadataDelta delta)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(Long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUnloaded",
                            "method_signature": "synchronized onUnloaded()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onLoaded",
                            "method_signature": "synchronized onLoaded(MetadataImage newImage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onNewMetadataImage",
                            "method_signature": "synchronized onNewMetadataImage(MetadataImage newImage, MetadataDelta delta)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "revertLastWrittenOffset",
                            "method_signature": "synchronized revertLastWrittenOffset(\n        long offset\n    )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(Long offset)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(Long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4829411782283535
                },
                "synchronized onUnloaded()": {
                    "first": {
                        "method_name": "onUnloaded",
                        "method_signature": "synchronized onUnloaded()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5104168733674731
                },
                "synchronized onLoaded(MetadataImage newImage)": {
                    "first": {
                        "method_name": "onLoaded",
                        "method_signature": "synchronized onLoaded(MetadataImage newImage)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5242682460614034
                },
                "synchronized onNewMetadataImage(MetadataImage newImage, MetadataDelta delta)": {
                    "first": {
                        "method_name": "onNewMetadataImage",
                        "method_signature": "synchronized onNewMetadataImage(MetadataImage newImage, MetadataDelta delta)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5658381360384384
                },
                "synchronized revertLastWrittenOffset(\n        long offset\n    )": {
                    "first": {
                        "method_name": "revertLastWrittenOffset",
                        "method_signature": "synchronized revertLastWrittenOffset(\n        long offset\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5829997779551179
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public GroupMetadataManagerTestContext(time MockTime, timer MockCoordinatorTimer<Void,CoordinatorRecord>, snapshotRegistry SnapshotRegistry, metrics GroupCoordinatorMetricsShard, groupMetadataManager GroupMetadataManager, classicGroupInitialRebalanceDelayMs int, classicGroupNewMemberJoinTimeoutMs int) in class org.apache.kafka.coordinator.group.GroupMetadataManagerTestContext & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 519,
                    "endLine": 536,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public GroupMetadataManagerTestContext(time MockTime, timer MockCoordinatorTimer<Void,CoordinatorRecord>, snapshotRegistry SnapshotRegistry, metrics GroupCoordinatorMetricsShard, groupMetadataManager GroupMetadataManager, classicGroupInitialRebalanceDelayMs int, classicGroupNewMemberJoinTimeoutMs int)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 535,
                    "endLine": 535,
                    "startColumn": 9,
                    "endColumn": 65,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 519,
                    "endLine": 536,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public GroupMetadataManagerTestContext(time MockTime, timer MockCoordinatorTimer<Void,CoordinatorRecord>, snapshotRegistry SnapshotRegistry, metrics GroupCoordinatorMetricsShard, groupMetadataManager GroupMetadataManager, classicGroupInitialRebalanceDelayMs int, classicGroupNewMemberJoinTimeoutMs int)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 535,
                    "endLine": 535,
                    "startColumn": 9,
                    "endColumn": 69,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastWrittenOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 605,
        "extraction_results": {
            "success": false
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public replay(record CoordinatorRecord, groupType GroupType) : void in class org.apache.kafka.coordinator.group.GroupMetadataManagerTestContext & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 1430,
                    "endLine": 1516,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public replay(record CoordinatorRecord, groupType GroupType) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 1515,
                    "endLine": 1515,
                    "startColumn": 9,
                    "endColumn": 65,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 1430,
                    "endLine": 1516,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public replay(record CoordinatorRecord, groupType GroupType) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                    "startLine": 1515,
                    "endLine": 1515,
                    "startColumn": 9,
                    "endColumn": 69,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastWrittenOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 606,
        "extraction_results": {
            "success": true,
            "newCommitHash": "43fa7a86608100f7e4de70d740725c4d4db8f008",
            "newBranchName": "extract-idempotentCreateSnapshot-replay-130af38"
        },
        "telemetry": {
            "id": "fcd0de29-f2bf-4590-9a25-7e5294f6bbea",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1406,
                "lineStart": 120,
                "lineEnd": 1525,
                "bodyLineStart": 120,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTestContext.java",
                "sourceCode": "public class GroupMetadataManagerTestContext {\n\n    public static void assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts) {\n        assertTrue(timeouts.size() <= 1);\n        timeouts.forEach(timeout -> assertEquals(EMPTY_RESULT, timeout.result));\n    }\n\n    public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toProtocols(String... protocolNames) {\n        JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols = new JoinGroupRequestData.JoinGroupRequestProtocolCollection(0);\n        List<String> topicNames = Arrays.asList(\"foo\", \"bar\", \"baz\");\n        for (int i = 0; i < protocolNames.length; i++) {\n            protocols.add(new JoinGroupRequestData.JoinGroupRequestProtocol()\n                .setName(protocolNames[i])\n                .setMetadata(ConsumerProtocol.serializeSubscription(new ConsumerPartitionAssignor.Subscription(\n                    Collections.singletonList(topicNames.get(i % topicNames.size())))).array())\n            );\n        }\n        return protocols;\n    }\n\n    public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toConsumerProtocol(\n        List<String> topicNames,\n        List<TopicPartition> ownedPartitions\n    ) {\n        return toConsumerProtocol(topicNames, ownedPartitions, ConsumerProtocolSubscription.HIGHEST_SUPPORTED_VERSION);\n    }\n\n    public static JoinGroupRequestData.JoinGroupRequestProtocolCollection toConsumerProtocol(\n        List<String> topicNames,\n        List<TopicPartition> ownedPartitions,\n        short version\n    ) {\n        JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols =\n            new JoinGroupRequestData.JoinGroupRequestProtocolCollection(0);\n        protocols.add(new JoinGroupRequestData.JoinGroupRequestProtocol()\n            .setName(\"range\")\n            .setMetadata(ConsumerProtocol.serializeSubscription(\n                new ConsumerPartitionAssignor.Subscription(\n                    topicNames,\n                    null,\n                    ownedPartitions\n                ),\n                version\n            ).array())\n        );\n        return protocols;\n    }\n\n    public static CoordinatorRecord newGroupMetadataRecord(\n        String groupId,\n        GroupMetadataValue value,\n        MetadataVersion metadataVersion\n    ) {\n        return new CoordinatorRecord(\n            new ApiMessageAndVersion(\n                new GroupMetadataKey()\n                    .setGroup(groupId),\n                (short) 2\n            ),\n            new ApiMessageAndVersion(\n                value,\n                metadataVersion.groupMetadataValueVersion()\n            )\n        );\n    }\n\n    public static class RebalanceResult {\n        int generationId;\n        String leaderId;\n        byte[] leaderAssignment;\n        String followerId;\n        byte[] followerAssignment;\n\n        RebalanceResult(\n            int generationId,\n            String leaderId,\n            byte[] leaderAssignment,\n            String followerId,\n            byte[] followerAssignment\n        ) {\n            this.generationId = generationId;\n            this.leaderId = leaderId;\n            this.leaderAssignment = leaderAssignment;\n            this.followerId = followerId;\n            this.followerAssignment = followerAssignment;\n        }\n    }\n\n    public static class PendingMemberGroupResult {\n        String leaderId;\n        String followerId;\n        JoinGroupResponseData pendingMemberResponse;\n\n        public PendingMemberGroupResult(\n            String leaderId,\n            String followerId,\n            JoinGroupResponseData pendingMemberResponse\n        ) {\n            this.leaderId = leaderId;\n            this.followerId = followerId;\n            this.pendingMemberResponse = pendingMemberResponse;\n        }\n    }\n\n    public static class JoinResult {\n        CompletableFuture<JoinGroupResponseData> joinFuture;\n        List<CoordinatorRecord> records;\n        CompletableFuture<Void> appendFuture;\n\n        public JoinResult(\n            CompletableFuture<JoinGroupResponseData> joinFuture,\n            CoordinatorResult<Void, CoordinatorRecord> coordinatorResult\n        ) {\n            this.joinFuture = joinFuture;\n            this.records = coordinatorResult.records();\n            this.appendFuture = coordinatorResult.appendFuture();\n        }\n    }\n\n    public static class SyncResult {\n        CompletableFuture<SyncGroupResponseData> syncFuture;\n        List<CoordinatorRecord> records;\n        CompletableFuture<Void> appendFuture;\n\n        public SyncResult(\n            CompletableFuture<SyncGroupResponseData> syncFuture,\n            CoordinatorResult<Void, CoordinatorRecord> coordinatorResult\n        ) {\n            this.syncFuture = syncFuture;\n            this.records = coordinatorResult.records();\n            this.appendFuture = coordinatorResult.appendFuture();\n        }\n    }\n\n    public static class JoinGroupRequestBuilder {\n        String groupId = null;\n        String groupInstanceId = null;\n        String memberId = null;\n        String protocolType = \"consumer\";\n        JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols = new JoinGroupRequestData.JoinGroupRequestProtocolCollection(0);\n        int sessionTimeoutMs = 500;\n        int rebalanceTimeoutMs = 500;\n        String reason = null;\n\n        JoinGroupRequestBuilder withGroupId(String groupId) {\n            this.groupId = groupId;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withGroupInstanceId(String groupInstanceId) {\n            this.groupInstanceId = groupInstanceId;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withMemberId(String memberId) {\n            this.memberId = memberId;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withDefaultProtocolTypeAndProtocols() {\n            this.protocols = toProtocols(\"range\");\n            return this;\n        }\n\n        JoinGroupRequestBuilder withProtocolSuperset() {\n            this.protocols = toProtocols(\"range\", \"roundrobin\");\n            return this;\n        }\n\n        JoinGroupRequestBuilder withProtocolType(String protocolType) {\n            this.protocolType = protocolType;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withProtocols(JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols) {\n            this.protocols = protocols;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withRebalanceTimeoutMs(int rebalanceTimeoutMs) {\n            this.rebalanceTimeoutMs = rebalanceTimeoutMs;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withSessionTimeoutMs(int sessionTimeoutMs) {\n            this.sessionTimeoutMs = sessionTimeoutMs;\n            return this;\n        }\n\n        JoinGroupRequestBuilder withReason(String reason) {\n            this.reason = reason;\n            return this;\n        }\n\n        JoinGroupRequestData build() {\n            return new JoinGroupRequestData()\n                .setGroupId(groupId)\n                .setGroupInstanceId(groupInstanceId)\n                .setMemberId(memberId)\n                .setProtocolType(protocolType)\n                .setProtocols(protocols)\n                .setRebalanceTimeoutMs(rebalanceTimeoutMs)\n                .setSessionTimeoutMs(sessionTimeoutMs)\n                .setReason(reason);\n        }\n    }\n\n    public static class SyncGroupRequestBuilder {\n        String groupId = null;\n        String groupInstanceId = null;\n        String memberId = null;\n        String protocolType = \"consumer\";\n        String protocolName = \"range\";\n        int generationId = 0;\n        List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment = Collections.emptyList();\n\n        SyncGroupRequestBuilder withGroupId(String groupId) {\n            this.groupId = groupId;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withGroupInstanceId(String groupInstanceId) {\n            this.groupInstanceId = groupInstanceId;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withMemberId(String memberId) {\n            this.memberId = memberId;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withGenerationId(int generationId) {\n            this.generationId = generationId;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withProtocolType(String protocolType) {\n            this.protocolType = protocolType;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withProtocolName(String protocolName) {\n            this.protocolName = protocolName;\n            return this;\n        }\n\n        SyncGroupRequestBuilder withAssignment(List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment) {\n            this.assignment = assignment;\n            return this;\n        }\n\n\n        SyncGroupRequestData build() {\n            return new SyncGroupRequestData()\n                .setGroupId(groupId)\n                .setGroupInstanceId(groupInstanceId)\n                .setMemberId(memberId)\n                .setGenerationId(generationId)\n                .setProtocolType(protocolType)\n                .setProtocolName(protocolName)\n                .setAssignments(assignment);\n        }\n    }\n\n    public static class Builder {\n        private final MockTime time = new MockTime();\n        private final MockCoordinatorTimer<Void, CoordinatorRecord> timer = new MockCoordinatorTimer<>(time);\n        private final LogContext logContext = new LogContext();\n        private final SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        private MetadataImage metadataImage;\n        private List<ConsumerGroupPartitionAssignor> consumerGroupAssignors = Collections.singletonList(new MockPartitionAssignor(\"range\"));\n        private final List<ConsumerGroupBuilder> consumerGroupBuilders = new ArrayList<>();\n        private int consumerGroupMaxSize = Integer.MAX_VALUE;\n        private int consumerGroupMetadataRefreshIntervalMs = Integer.MAX_VALUE;\n        private int classicGroupMaxSize = Integer.MAX_VALUE;\n        private int classicGroupInitialRebalanceDelayMs = 3000;\n        private final int classicGroupNewMemberJoinTimeoutMs = 5 * 60 * 1000;\n        private int classicGroupMinSessionTimeoutMs = 10;\n        private int classicGroupMaxSessionTimeoutMs = 10 * 60 * 1000;\n        private final GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        private ConsumerGroupMigrationPolicy consumerGroupMigrationPolicy = ConsumerGroupMigrationPolicy.DISABLED;\n        // Share group configs\n        private ShareGroupPartitionAssignor shareGroupAssignor = new MockPartitionAssignor(\"share\");\n        private int shareGroupMaxSize = Integer.MAX_VALUE;\n\n        public Builder withMetadataImage(MetadataImage metadataImage) {\n            this.metadataImage = metadataImage;\n            return this;\n        }\n\n        public Builder withConsumerGroupAssignors(List<ConsumerGroupPartitionAssignor> assignors) {\n            this.consumerGroupAssignors = assignors;\n            return this;\n        }\n\n        public Builder withConsumerGroup(ConsumerGroupBuilder builder) {\n            this.consumerGroupBuilders.add(builder);\n            return this;\n        }\n\n        public Builder withConsumerGroupMaxSize(int consumerGroupMaxSize) {\n            this.consumerGroupMaxSize = consumerGroupMaxSize;\n            return this;\n        }\n\n        public Builder withConsumerGroupMetadataRefreshIntervalMs(int consumerGroupMetadataRefreshIntervalMs) {\n            this.consumerGroupMetadataRefreshIntervalMs = consumerGroupMetadataRefreshIntervalMs;\n            return this;\n        }\n\n        public Builder withClassicGroupMaxSize(int classicGroupMaxSize) {\n            this.classicGroupMaxSize = classicGroupMaxSize;\n            return this;\n        }\n\n        public Builder withClassicGroupInitialRebalanceDelayMs(int classicGroupInitialRebalanceDelayMs) {\n            this.classicGroupInitialRebalanceDelayMs = classicGroupInitialRebalanceDelayMs;\n            return this;\n        }\n\n        public Builder withClassicGroupMinSessionTimeoutMs(int classicGroupMinSessionTimeoutMs) {\n            this.classicGroupMinSessionTimeoutMs = classicGroupMinSessionTimeoutMs;\n            return this;\n        }\n\n        public Builder withClassicGroupMaxSessionTimeoutMs(int classicGroupMaxSessionTimeoutMs) {\n            this.classicGroupMaxSessionTimeoutMs = classicGroupMaxSessionTimeoutMs;\n            return this;\n        }\n\n        public Builder withConsumerGroupMigrationPolicy(ConsumerGroupMigrationPolicy consumerGroupMigrationPolicy) {\n            this.consumerGroupMigrationPolicy = consumerGroupMigrationPolicy;\n            return this;\n        }\n\n        public Builder withShareGroupAssignor(ShareGroupPartitionAssignor shareGroupAssignor) {\n            this.shareGroupAssignor = shareGroupAssignor;\n            return this;\n        }\n\n        public Builder withShareGroupMaxSize(int shareGroupMaxSize) {\n            this.shareGroupMaxSize = shareGroupMaxSize;\n            return this;\n        }\n\n        public GroupMetadataManagerTestContext build() {\n            if (metadataImage == null) metadataImage = MetadataImage.EMPTY;\n            if (consumerGroupAssignors == null) consumerGroupAssignors = Collections.emptyList();\n\n            GroupMetadataManagerTestContext context = new GroupMetadataManagerTestContext(\n                time,\n                timer,\n                snapshotRegistry,\n                metrics,\n                new GroupMetadataManager.Builder()\n                    .withSnapshotRegistry(snapshotRegistry)\n                    .withLogContext(logContext)\n                    .withTime(time)\n                    .withTimer(timer)\n                    .withMetadataImage(metadataImage)\n                    .withConsumerGroupHeartbeatInterval(5000)\n                    .withConsumerGroupSessionTimeout(45000)\n                    .withConsumerGroupMaxSize(consumerGroupMaxSize)\n                    .withConsumerGroupAssignors(consumerGroupAssignors)\n                    .withConsumerGroupMetadataRefreshIntervalMs(consumerGroupMetadataRefreshIntervalMs)\n                    .withClassicGroupMaxSize(classicGroupMaxSize)\n                    .withClassicGroupMinSessionTimeoutMs(classicGroupMinSessionTimeoutMs)\n                    .withClassicGroupMaxSessionTimeoutMs(classicGroupMaxSessionTimeoutMs)\n                    .withClassicGroupInitialRebalanceDelayMs(classicGroupInitialRebalanceDelayMs)\n                    .withClassicGroupNewMemberJoinTimeoutMs(classicGroupNewMemberJoinTimeoutMs)\n                    .withGroupCoordinatorMetricsShard(metrics)\n                    .withConsumerGroupMigrationPolicy(consumerGroupMigrationPolicy)\n                    .withShareGroupAssignor(shareGroupAssignor)\n                    .withShareGroupMaxSize(shareGroupMaxSize)\n                    .build(),\n                classicGroupInitialRebalanceDelayMs,\n                classicGroupNewMemberJoinTimeoutMs\n            );\n\n            consumerGroupBuilders.forEach(builder -> builder.build(metadataImage.topics()).forEach(context::replay));\n            consumerGroupBuilders.forEach(builder -> builder.build(metadataImage.topics()).forEach(context::replay));\n\n            context.commit();\n\n            return context;\n        }\n    }\n\n    final MockTime time;\n    final MockCoordinatorTimer<Void, CoordinatorRecord> timer;\n    final SnapshotRegistry snapshotRegistry;\n    final GroupCoordinatorMetricsShard metrics;\n    final GroupMetadataManager groupMetadataManager;\n    final int classicGroupInitialRebalanceDelayMs;\n    final int classicGroupNewMemberJoinTimeoutMs;\n\n    long lastCommittedOffset = 0L;\n    long lastWrittenOffset = 0L;\n\n    public GroupMetadataManagerTestContext(\n        MockTime time,\n        MockCoordinatorTimer<Void, CoordinatorRecord> timer,\n        SnapshotRegistry snapshotRegistry,\n        GroupCoordinatorMetricsShard metrics,\n        GroupMetadataManager groupMetadataManager,\n        int classicGroupInitialRebalanceDelayMs,\n        int classicGroupNewMemberJoinTimeoutMs\n    ) {\n        this.time = time;\n        this.timer = timer;\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.groupMetadataManager = groupMetadataManager;\n        this.classicGroupInitialRebalanceDelayMs = classicGroupInitialRebalanceDelayMs;\n        this.classicGroupNewMemberJoinTimeoutMs = classicGroupNewMemberJoinTimeoutMs;\n        snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n    }\n\n    public void commit() {\n        long lastCommittedOffset = this.lastCommittedOffset;\n        this.lastCommittedOffset = lastWrittenOffset;\n        snapshotRegistry.deleteSnapshotsUpTo(lastCommittedOffset);\n    }\n\n    public void rollback() {\n        lastWrittenOffset = lastCommittedOffset;\n        snapshotRegistry.revertToSnapshot(lastCommittedOffset);\n    }\n\n    public ConsumerGroup.ConsumerGroupState consumerGroupState(\n        String groupId\n    ) {\n        return groupMetadataManager\n            .consumerGroup(groupId)\n            .state();\n    }\n\n    public ShareGroup.ShareGroupState shareGroupState(\n        String groupId\n    ) {\n        return groupMetadataManager\n            .shareGroup(groupId)\n            .state();\n    }\n\n    public MemberState consumerGroupMemberState(\n        String groupId,\n        String memberId\n    ) {\n        return groupMetadataManager\n            .consumerGroup(groupId)\n            .getOrMaybeCreateMember(memberId, false)\n            .state();\n    }\n\n    public CoordinatorResult<ConsumerGroupHeartbeatResponseData, CoordinatorRecord> consumerGroupHeartbeat(\n        ConsumerGroupHeartbeatRequestData request\n    ) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.CONSUMER_GROUP_HEARTBEAT,\n                ApiKeys.CONSUMER_GROUP_HEARTBEAT.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        CoordinatorResult<ConsumerGroupHeartbeatResponseData, CoordinatorRecord> result = groupMetadataManager.consumerGroupHeartbeat(\n            context,\n            request\n        );\n\n        if (result.replayRecords()) {\n            result.records().forEach(this::replay);\n        }\n        return result;\n    }\n\n    public CoordinatorResult<ShareGroupHeartbeatResponseData, CoordinatorRecord> shareGroupHeartbeat(\n        ShareGroupHeartbeatRequestData request\n    ) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.SHARE_GROUP_HEARTBEAT,\n                ApiKeys.SHARE_GROUP_HEARTBEAT.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        CoordinatorResult<ShareGroupHeartbeatResponseData, CoordinatorRecord> result = groupMetadataManager.shareGroupHeartbeat(\n            context,\n            request\n        );\n\n        result.records().forEach(this::replay);\n        return result;\n    }\n\n    public List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> sleep(long ms) {\n        time.sleep(ms);\n        List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = timer.poll();\n        timeouts.forEach(timeout -> {\n            if (timeout.result.replayRecords()) {\n                timeout.result.records().forEach(this::replay);\n            }\n        });\n        return timeouts;\n    }\n\n    public void assertSessionTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(groupSessionTimeoutKey(groupId, memberId));\n        assertNotNull(timeout);\n        assertEquals(time.milliseconds() + delayMs, timeout.deadlineMs);\n    }\n\n    public void assertNoSessionTimeout(\n        String groupId,\n        String memberId\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(groupSessionTimeoutKey(groupId, memberId));\n        assertNull(timeout);\n    }\n\n    public MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> assertRebalanceTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupRebalanceTimeoutKey(groupId, memberId));\n        assertNotNull(timeout);\n        assertEquals(time.milliseconds() + delayMs, timeout.deadlineMs);\n        return timeout;\n    }\n\n    public void assertNoRebalanceTimeout(\n        String groupId,\n        String memberId\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupRebalanceTimeoutKey(groupId, memberId));\n        assertNull(timeout);\n    }\n\n    public MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> assertJoinTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupJoinKey(groupId, memberId));\n        assertNotNull(timeout);\n        assertEquals(time.milliseconds() + delayMs, timeout.deadlineMs);\n        return timeout;\n    }\n\n    public void assertNoJoinTimeout(\n        String groupId,\n        String memberId\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupJoinKey(groupId, memberId));\n        assertNull(timeout);\n    }\n\n    public MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> assertSyncTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupSyncKey(groupId, memberId));\n        assertNotNull(timeout);\n        assertEquals(time.milliseconds() + delayMs, timeout.deadlineMs);\n        return timeout;\n    }\n\n    public void assertNoSyncTimeout(\n        String groupId,\n        String memberId\n    ) {\n        MockCoordinatorTimer.ScheduledTimeout<Void, CoordinatorRecord> timeout =\n            timer.timeout(consumerGroupSyncKey(groupId, memberId));\n        assertNull(timeout);\n    }\n\n    ClassicGroup createClassicGroup(String groupId) {\n        return groupMetadataManager.getOrMaybeCreateClassicGroup(groupId, true);\n    }\n\n    public JoinResult sendClassicGroupJoin(\n        JoinGroupRequestData request\n    ) {\n        return sendClassicGroupJoin(request, false);\n    }\n\n    public JoinResult sendClassicGroupJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId\n    ) {\n        return sendClassicGroupJoin(request, requireKnownMemberId, false);\n    }\n\n    public JoinResult sendClassicGroupJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    ) {\n        // requireKnownMemberId is true: version >= 4 (See JoinGroupRequest#requiresKnownMemberId())\n        // supportSkippingAssignment is true: version >= 9 (See JoinGroupRequest#supportsSkippingAssignment())\n        short joinGroupVersion = 3;\n\n        if (requireKnownMemberId) {\n            joinGroupVersion = 4;\n            if (supportSkippingAssignment) {\n                joinGroupVersion = ApiKeys.JOIN_GROUP.latestVersion();\n            }\n        }\n\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.JOIN_GROUP,\n                joinGroupVersion,\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        CompletableFuture<JoinGroupResponseData> responseFuture = new CompletableFuture<>();\n        CoordinatorResult<Void, CoordinatorRecord> coordinatorResult = groupMetadataManager.classicGroupJoin(\n            context,\n            request,\n            responseFuture\n        );\n\n        if (coordinatorResult.replayRecords()) {\n            coordinatorResult.records().forEach(this::replay);\n        }\n\n        return new JoinResult(responseFuture, coordinatorResult);\n    }\n\n    public JoinGroupResponseData joinClassicGroupAsDynamicMemberAndCompleteRebalance(\n        String groupId\n    ) throws Exception {\n        ClassicGroup group = createClassicGroup(groupId);\n\n        JoinGroupResponseData leaderJoinResponse =\n            joinClassicGroupAsDynamicMemberAndCompleteJoin(new JoinGroupRequestBuilder()\n                .withGroupId(groupId)\n                .withMemberId(UNKNOWN_MEMBER_ID)\n                .withDefaultProtocolTypeAndProtocols()\n                .withRebalanceTimeoutMs(10000)\n                .withSessionTimeoutMs(5000)\n                .build());\n\n        assertEquals(1, leaderJoinResponse.generationId());\n        assertTrue(group.isInState(COMPLETING_REBALANCE));\n\n        SyncResult syncResult = sendClassicGroupSync(new SyncGroupRequestBuilder()\n            .withGroupId(groupId)\n            .withMemberId(leaderJoinResponse.memberId())\n            .withGenerationId(leaderJoinResponse.generationId())\n            .build());\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newGroupMetadataRecord(group, group.groupAssignment(), MetadataVersion.latestTesting())),\n            syncResult.records\n        );\n        // Simulate a successful write to the log.\n        syncResult.appendFuture.complete(null);\n\n        assertTrue(syncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), syncResult.syncFuture.get().errorCode());\n        assertTrue(group.isInState(STABLE));\n\n        return leaderJoinResponse;\n    }\n\n    public JoinGroupResponseData joinClassicGroupAsDynamicMemberAndCompleteJoin(\n        JoinGroupRequestData request\n    ) throws ExecutionException, InterruptedException {\n        boolean requireKnownMemberId = true;\n        String newMemberId = request.memberId();\n\n        if (request.memberId().equals(UNKNOWN_MEMBER_ID)) {\n            // Since member id is required, we need another round to get the successful join group result.\n            JoinResult firstJoinResult = sendClassicGroupJoin(\n                request,\n                requireKnownMemberId\n            );\n            assertTrue(firstJoinResult.records.isEmpty());\n            assertTrue(firstJoinResult.joinFuture.isDone());\n            assertEquals(Errors.MEMBER_ID_REQUIRED.code(), firstJoinResult.joinFuture.get().errorCode());\n            newMemberId = firstJoinResult.joinFuture.get().memberId();\n        }\n\n        // Second round\n        JoinGroupRequestData secondRequest = new JoinGroupRequestData()\n            .setGroupId(request.groupId())\n            .setMemberId(newMemberId)\n            .setProtocolType(request.protocolType())\n            .setProtocols(request.protocols())\n            .setSessionTimeoutMs(request.sessionTimeoutMs())\n            .setRebalanceTimeoutMs(request.rebalanceTimeoutMs())\n            .setReason(request.reason());\n\n        JoinResult secondJoinResult = sendClassicGroupJoin(\n            secondRequest,\n            requireKnownMemberId\n        );\n\n        assertTrue(secondJoinResult.records.isEmpty());\n        List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = sleep(classicGroupInitialRebalanceDelayMs);\n        assertEquals(1, timeouts.size());\n        assertTrue(secondJoinResult.joinFuture.isDone());\n        assertEquals(Errors.NONE.code(), secondJoinResult.joinFuture.get().errorCode());\n\n        return secondJoinResult.joinFuture.get();\n    }\n\n    public JoinGroupResponseData joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    ) throws ExecutionException, InterruptedException {\n        return joinClassicGroupAndCompleteJoin(\n            request,\n            requireKnownMemberId,\n            supportSkippingAssignment,\n            classicGroupInitialRebalanceDelayMs\n        );\n    }\n\n    public JoinGroupResponseData joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment,\n        int advanceClockMs\n    ) throws ExecutionException, InterruptedException {\n        if (requireKnownMemberId && request.groupInstanceId().isEmpty()) {\n            return joinClassicGroupAsDynamicMemberAndCompleteJoin(request);\n        }\n\n        try {\n            JoinResult joinResult = sendClassicGroupJoin(\n                request,\n                requireKnownMemberId,\n                supportSkippingAssignment\n            );\n\n            sleep(advanceClockMs);\n            assertTrue(joinResult.joinFuture.isDone());\n            assertEquals(Errors.NONE.code(), joinResult.joinFuture.get().errorCode());\n            return joinResult.joinFuture.get();\n        } catch (Exception e) {\n            fail(\"Failed to due: \" + e.getMessage());\n        }\n        return null;\n    }\n\n    public SyncResult sendClassicGroupSync(SyncGroupRequestData request) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.SYNC_GROUP,\n                ApiKeys.SYNC_GROUP.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        CompletableFuture<SyncGroupResponseData> responseFuture = new CompletableFuture<>();\n\n        CoordinatorResult<Void, CoordinatorRecord> coordinatorResult = groupMetadataManager.classicGroupSync(\n            context,\n            request,\n            responseFuture\n        );\n\n        if (coordinatorResult.replayRecords()) {\n            coordinatorResult.records().forEach(this::replay);\n        }\n\n        return new SyncResult(responseFuture, coordinatorResult);\n    }\n\n    public RebalanceResult staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId\n    ) throws Exception {\n        return staticMembersJoinAndRebalance(\n            groupId,\n            leaderInstanceId,\n            followerInstanceId,\n            10000,\n            5000\n        );\n    }\n\n    public RebalanceResult staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId,\n        int rebalanceTimeoutMs,\n        int sessionTimeoutMs\n    ) throws Exception {\n        ClassicGroup group = createClassicGroup(groupId);\n\n        JoinGroupRequestData joinRequest = new JoinGroupRequestBuilder()\n            .withGroupId(groupId)\n            .withGroupInstanceId(leaderInstanceId)\n            .withMemberId(UNKNOWN_MEMBER_ID)\n            .withProtocolType(\"consumer\")\n            .withProtocolSuperset()\n            .withRebalanceTimeoutMs(rebalanceTimeoutMs)\n            .withSessionTimeoutMs(sessionTimeoutMs)\n            .build();\n\n        JoinResult leaderJoinResult = sendClassicGroupJoin(joinRequest);\n        JoinResult followerJoinResult = sendClassicGroupJoin(joinRequest.setGroupInstanceId(followerInstanceId));\n\n        assertTrue(leaderJoinResult.records.isEmpty());\n        assertTrue(followerJoinResult.records.isEmpty());\n        assertFalse(leaderJoinResult.joinFuture.isDone());\n        assertFalse(followerJoinResult.joinFuture.isDone());\n\n        // The goal for two timer advance is to let first group initial join complete and set newMemberAdded flag to false. Next advance is\n        // to trigger the rebalance as needed for follower delayed join. One large time advance won't help because we could only populate one\n        // delayed join from purgatory and the new delayed op is created at that time and never be triggered.\n        assertNoOrEmptyResult(sleep(classicGroupInitialRebalanceDelayMs));\n        assertNoOrEmptyResult(sleep(classicGroupInitialRebalanceDelayMs));\n\n        assertTrue(leaderJoinResult.joinFuture.isDone());\n        assertTrue(followerJoinResult.joinFuture.isDone());\n        assertEquals(Errors.NONE.code(), leaderJoinResult.joinFuture.get().errorCode());\n        assertEquals(Errors.NONE.code(), followerJoinResult.joinFuture.get().errorCode());\n        assertEquals(1, leaderJoinResult.joinFuture.get().generationId());\n        assertEquals(1, followerJoinResult.joinFuture.get().generationId());\n        assertEquals(2, group.numMembers());\n        assertEquals(1, group.generationId());\n        assertTrue(group.isInState(COMPLETING_REBALANCE));\n\n        String leaderId = leaderJoinResult.joinFuture.get().memberId();\n        String followerId = followerJoinResult.joinFuture.get().memberId();\n        List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment = new ArrayList<>();\n        assignment.add(new SyncGroupRequestData.SyncGroupRequestAssignment().setMemberId(leaderId)\n                                                                            .setAssignment(new byte[]{1}));\n        assignment.add(new SyncGroupRequestData.SyncGroupRequestAssignment().setMemberId(followerId)\n                                                                            .setAssignment(new byte[]{2}));\n\n        SyncGroupRequestData syncRequest = new SyncGroupRequestBuilder()\n            .withGroupId(groupId)\n            .withGroupInstanceId(leaderInstanceId)\n            .withMemberId(leaderId)\n            .withGenerationId(1)\n            .withAssignment(assignment)\n            .build();\n\n        SyncResult leaderSyncResult = sendClassicGroupSync(syncRequest);\n\n        // The generated record should contain the new assignment.\n        Map<String, byte[]> groupAssignment = assignment.stream().collect(Collectors.toMap(\n            SyncGroupRequestData.SyncGroupRequestAssignment::memberId, SyncGroupRequestData.SyncGroupRequestAssignment::assignment\n        ));\n        assertEquals(\n            Collections.singletonList(\n                CoordinatorRecordHelpers.newGroupMetadataRecord(group, groupAssignment, MetadataVersion.latestTesting())),\n            leaderSyncResult.records\n        );\n\n        // Simulate a successful write to the log.\n        leaderSyncResult.appendFuture.complete(null);\n\n        assertTrue(leaderSyncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), leaderSyncResult.syncFuture.get().errorCode());\n        assertTrue(group.isInState(STABLE));\n\n        SyncResult followerSyncResult = sendClassicGroupSync(\n            syncRequest.setGroupInstanceId(followerInstanceId)\n                       .setMemberId(followerId)\n                       .setAssignments(Collections.emptyList())\n        );\n\n        assertTrue(followerSyncResult.records.isEmpty());\n        assertTrue(followerSyncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), followerSyncResult.syncFuture.get().errorCode());\n        assertTrue(group.isInState(STABLE));\n\n        assertEquals(2, group.numMembers());\n        assertEquals(1, group.generationId());\n\n        return new RebalanceResult(\n            1,\n            leaderId,\n            leaderSyncResult.syncFuture.get().assignment(),\n            followerId,\n            followerSyncResult.syncFuture.get().assignment()\n        );\n    }\n\n    public PendingMemberGroupResult setupGroupWithPendingMember(ClassicGroup group) throws Exception {\n        // Add the first member\n        JoinGroupRequestData joinRequest = new JoinGroupRequestBuilder()\n            .withGroupId(group.groupId())\n            .withMemberId(UNKNOWN_MEMBER_ID)\n            .withDefaultProtocolTypeAndProtocols()\n            .withRebalanceTimeoutMs(10000)\n            .withSessionTimeoutMs(5000)\n            .build();\n\n        JoinGroupResponseData leaderJoinResponse =\n            joinClassicGroupAsDynamicMemberAndCompleteJoin(joinRequest);\n\n        List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment = new ArrayList<>();\n        assignment.add(new SyncGroupRequestData.SyncGroupRequestAssignment().setMemberId(leaderJoinResponse.memberId()));\n        SyncGroupRequestData syncRequest = new SyncGroupRequestBuilder()\n            .withGroupId(group.groupId())\n            .withMemberId(leaderJoinResponse.memberId())\n            .withGenerationId(leaderJoinResponse.generationId())\n            .withAssignment(assignment)\n            .build();\n\n        SyncResult syncResult = sendClassicGroupSync(syncRequest);\n\n        // Now the group is stable, with the one member that joined above\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newGroupMetadataRecord(group, group.groupAssignment(), MetadataVersion.latestTesting())),\n            syncResult.records\n        );\n        // Simulate a successful write to log.\n        syncResult.appendFuture.complete(null);\n\n        assertTrue(syncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), syncResult.syncFuture.get().errorCode());\n\n        // Start the join for the second member\n        JoinResult followerJoinResult = sendClassicGroupJoin(\n            joinRequest.setMemberId(UNKNOWN_MEMBER_ID)\n        );\n\n        assertTrue(followerJoinResult.records.isEmpty());\n        assertFalse(followerJoinResult.joinFuture.isDone());\n\n        JoinResult leaderJoinResult = sendClassicGroupJoin(\n            joinRequest.setMemberId(leaderJoinResponse.memberId())\n        );\n\n        assertTrue(leaderJoinResult.records.isEmpty());\n        assertTrue(group.isInState(COMPLETING_REBALANCE));\n        assertTrue(leaderJoinResult.joinFuture.isDone());\n        assertTrue(followerJoinResult.joinFuture.isDone());\n        assertEquals(Errors.NONE.code(), leaderJoinResult.joinFuture.get().errorCode());\n        assertEquals(Errors.NONE.code(), followerJoinResult.joinFuture.get().errorCode());\n        assertEquals(leaderJoinResult.joinFuture.get().generationId(), followerJoinResult.joinFuture.get().generationId());\n        assertEquals(leaderJoinResponse.memberId(), leaderJoinResult.joinFuture.get().leader());\n        assertEquals(leaderJoinResponse.memberId(), followerJoinResult.joinFuture.get().leader());\n\n        int nextGenerationId = leaderJoinResult.joinFuture.get().generationId();\n        String followerId = followerJoinResult.joinFuture.get().memberId();\n\n        // Stabilize the group\n        syncResult = sendClassicGroupSync(syncRequest.setGenerationId(nextGenerationId));\n\n        assertEquals(\n            Collections.singletonList(CoordinatorRecordHelpers.newGroupMetadataRecord(group, group.groupAssignment(), MetadataVersion.latestTesting())),\n            syncResult.records\n        );\n        // Simulate a successful write to log.\n        syncResult.appendFuture.complete(null);\n\n        assertTrue(syncResult.syncFuture.isDone());\n        assertEquals(Errors.NONE.code(), syncResult.syncFuture.get().errorCode());\n        assertTrue(group.isInState(STABLE));\n\n        // Re-join an existing member, to transition the group to PreparingRebalance state.\n        leaderJoinResult = sendClassicGroupJoin(\n            joinRequest.setMemberId(leaderJoinResponse.memberId()));\n\n        assertTrue(leaderJoinResult.records.isEmpty());\n        assertFalse(leaderJoinResult.joinFuture.isDone());\n        assertTrue(group.isInState(PREPARING_REBALANCE));\n\n        // Create a pending member in the group\n        JoinResult pendingMemberJoinResult = sendClassicGroupJoin(\n            joinRequest\n                .setMemberId(UNKNOWN_MEMBER_ID)\n                .setSessionTimeoutMs(2500),\n            true\n        );\n\n        assertTrue(pendingMemberJoinResult.records.isEmpty());\n        assertTrue(pendingMemberJoinResult.joinFuture.isDone());\n        assertEquals(Errors.MEMBER_ID_REQUIRED.code(), pendingMemberJoinResult.joinFuture.get().errorCode());\n        assertEquals(1, group.numPendingJoinMembers());\n\n        // Re-join the second existing member\n        followerJoinResult = sendClassicGroupJoin(\n            joinRequest.setMemberId(followerId).setSessionTimeoutMs(5000)\n        );\n\n        assertTrue(followerJoinResult.records.isEmpty());\n        assertFalse(followerJoinResult.joinFuture.isDone());\n        assertTrue(group.isInState(PREPARING_REBALANCE));\n        assertEquals(2, group.numMembers());\n        assertEquals(1, group.numPendingJoinMembers());\n\n        return new PendingMemberGroupResult(\n            leaderJoinResponse.memberId(),\n            followerId,\n            pendingMemberJoinResult.joinFuture.get()\n        );\n    }\n\n    public void verifySessionExpiration(ClassicGroup group, int timeoutMs) {\n        Set<String> expectedHeartbeatKeys = group.allMembers().stream()\n                                                 .map(member -> classicGroupHeartbeatKey(group.groupId(), member.memberId())).collect(Collectors.toSet());\n\n        // Member should be removed as session expires.\n        List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = sleep(timeoutMs);\n        List<CoordinatorRecord> expectedRecords = Collections.singletonList(newGroupMetadataRecord(\n            group.groupId(),\n            new GroupMetadataValue()\n                .setMembers(Collections.emptyList())\n                .setGeneration(group.generationId())\n                .setLeader(null)\n                .setProtocolType(\"consumer\")\n                .setProtocol(null)\n                .setCurrentStateTimestamp(time.milliseconds()),\n            MetadataVersion.latestTesting()\n        ));\n\n\n        Set<String> heartbeatKeys = timeouts.stream().map(timeout -> timeout.key).collect(Collectors.toSet());\n        assertEquals(expectedHeartbeatKeys, heartbeatKeys);\n\n        // Only the last member leaving the group should result in the empty group metadata record.\n        int timeoutsSize = timeouts.size();\n        assertEquals(expectedRecords, timeouts.get(timeoutsSize - 1).result.records());\n        assertNoOrEmptyResult(timeouts.subList(0, timeoutsSize - 1));\n        assertTrue(group.isInState(EMPTY));\n        assertEquals(0, group.numMembers());\n    }\n\n    public CoordinatorResult<HeartbeatResponseData, CoordinatorRecord> sendClassicGroupHeartbeat(\n        HeartbeatRequestData request\n    ) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.HEARTBEAT,\n                ApiKeys.HEARTBEAT.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        return groupMetadataManager.classicGroupHeartbeat(\n            context,\n            request\n        );\n    }\n\n    public List<ListGroupsResponseData.ListedGroup> sendListGroups(List<String> statesFilter, List<String> typesFilter) {\n        Set<String> statesFilterSet = new HashSet<>(statesFilter);\n        Set<String> typesFilterSet = new HashSet<>(typesFilter);\n        return groupMetadataManager.listGroups(statesFilterSet, typesFilterSet, lastCommittedOffset);\n    }\n\n    public List<ConsumerGroupDescribeResponseData.DescribedGroup> sendConsumerGroupDescribe(List<String> groupIds) {\n        return groupMetadataManager.consumerGroupDescribe(groupIds, lastCommittedOffset);\n    }\n\n    public List<DescribeGroupsResponseData.DescribedGroup> describeGroups(List<String> groupIds) {\n        return groupMetadataManager.describeGroups(groupIds, lastCommittedOffset);\n    }\n\n    public List<ShareGroupDescribeResponseData.DescribedGroup> sendShareGroupDescribe(List<String> groupIds) {\n        return groupMetadataManager.shareGroupDescribe(groupIds, lastCommittedOffset);\n    }\n\n    public void verifyHeartbeat(\n        String groupId,\n        JoinGroupResponseData joinResponse,\n        Errors expectedError\n    ) {\n        HeartbeatRequestData request = new HeartbeatRequestData()\n            .setGroupId(groupId)\n            .setMemberId(joinResponse.memberId())\n            .setGenerationId(joinResponse.generationId());\n\n        if (expectedError == Errors.UNKNOWN_MEMBER_ID) {\n            assertThrows(UnknownMemberIdException.class, () -> sendClassicGroupHeartbeat(request));\n        } else {\n            HeartbeatResponseData response = sendClassicGroupHeartbeat(request).response();\n            assertEquals(expectedError.code(), response.errorCode());\n        }\n    }\n\n    public List<JoinGroupResponseData> joinWithNMembers(\n        String groupId,\n        int numMembers,\n        int rebalanceTimeoutMs,\n        int sessionTimeoutMs\n    ) {\n        ClassicGroup group = createClassicGroup(groupId);\n        boolean requireKnownMemberId = true;\n\n        // First join requests\n        JoinGroupRequestData request = new JoinGroupRequestBuilder()\n            .withGroupId(groupId)\n            .withMemberId(UNKNOWN_MEMBER_ID)\n            .withDefaultProtocolTypeAndProtocols()\n            .withRebalanceTimeoutMs(rebalanceTimeoutMs)\n            .withSessionTimeoutMs(sessionTimeoutMs)\n            .build();\n\n        List<String> memberIds = IntStream.range(0, numMembers).mapToObj(i -> {\n            JoinResult joinResult = sendClassicGroupJoin(request, requireKnownMemberId);\n\n            assertTrue(joinResult.records.isEmpty());\n            assertTrue(joinResult.joinFuture.isDone());\n\n            try {\n                return joinResult.joinFuture.get().memberId();\n            } catch (Exception e) {\n                fail(\"Unexpected exception: \" + e.getMessage());\n            }\n            return null;\n        }).collect(Collectors.toList());\n\n        // Second join requests\n        List<CompletableFuture<JoinGroupResponseData>> secondJoinFutures = IntStream.range(0, numMembers).mapToObj(i -> {\n            JoinResult joinResult = sendClassicGroupJoin(request.setMemberId(memberIds.get(i)), requireKnownMemberId);\n\n            assertTrue(joinResult.records.isEmpty());\n            assertFalse(joinResult.joinFuture.isDone());\n\n            return joinResult.joinFuture;\n        }).collect(Collectors.toList());\n\n        // Advance clock by initial rebalance delay.\n        assertNoOrEmptyResult(sleep(classicGroupInitialRebalanceDelayMs));\n        secondJoinFutures.forEach(future -> assertFalse(future.isDone()));\n        // Advance clock by rebalance timeout to complete join phase.\n        assertNoOrEmptyResult(sleep(rebalanceTimeoutMs));\n\n        List<JoinGroupResponseData> joinResponses = secondJoinFutures.stream().map(future -> {\n            assertTrue(future.isDone());\n            try {\n                assertEquals(Errors.NONE.code(), future.get().errorCode());\n                return future.get();\n            } catch (Exception e) {\n                fail(\"Unexpected exception: \" + e.getMessage());\n            }\n            return null;\n        }).collect(Collectors.toList());\n\n        assertEquals(numMembers, group.numMembers());\n        assertTrue(group.isInState(COMPLETING_REBALANCE));\n\n        return joinResponses;\n    }\n\n    public CoordinatorResult<LeaveGroupResponseData, CoordinatorRecord> sendClassicGroupLeave(\n        LeaveGroupRequestData request\n    ) {\n        RequestContext context = new RequestContext(\n            new RequestHeader(\n                ApiKeys.LEAVE_GROUP,\n                ApiKeys.LEAVE_GROUP.latestVersion(),\n                \"client\",\n                0\n            ),\n            \"1\",\n            InetAddress.getLoopbackAddress(),\n            KafkaPrincipal.ANONYMOUS,\n            ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n            SecurityProtocol.PLAINTEXT,\n            ClientInformation.EMPTY,\n            false\n        );\n\n        return groupMetadataManager.classicGroupLeave(context, request);\n    }\n\n    public void verifyDescribeGroupsReturnsDeadGroup(String groupId) {\n        List<DescribeGroupsResponseData.DescribedGroup> describedGroups =\n            describeGroups(Collections.singletonList(groupId));\n\n        assertEquals(\n            Collections.singletonList(new DescribeGroupsResponseData.DescribedGroup()\n                .setGroupId(groupId)\n                .setGroupState(DEAD.toString())\n            ),\n            describedGroups\n        );\n    }\n\n    public void verifyClassicGroupSyncToConsumerGroup(\n        String groupId,\n        String memberId,\n        int generationId,\n        String protocolName,\n        String protocolType,\n        List<TopicPartition> topicPartitionList,\n        short version\n    ) throws Exception {\n        GroupMetadataManagerTestContext.SyncResult syncResult = sendClassicGroupSync(\n            new GroupMetadataManagerTestContext.SyncGroupRequestBuilder()\n                .withGroupId(groupId)\n                .withMemberId(memberId)\n                .withGenerationId(generationId)\n                .withProtocolName(protocolName)\n                .withProtocolType(protocolType)\n                .build()\n        );\n        assertEquals(Collections.emptyList(), syncResult.records);\n        assertFalse(syncResult.syncFuture.isDone());\n\n        // Simulate a successful write to log.\n        syncResult.appendFuture.complete(null);\n        assertSyncGroupResponseEquals(\n            new SyncGroupResponseData()\n                .setProtocolType(protocolType)\n                .setProtocolName(protocolName)\n                .setAssignment(ConsumerProtocol.serializeAssignment(\n                    new ConsumerPartitionAssignor.Assignment(topicPartitionList),\n                    version\n                ).array()),\n            syncResult.syncFuture.get()\n        );\n        assertSessionTimeout(groupId, memberId, 5000);\n        assertNoSyncTimeout(groupId, memberId);\n    }\n\n    public void verifyClassicGroupSyncToConsumerGroup(\n        String groupId,\n        String memberId,\n        int generationId,\n        String protocolName,\n        String protocolType,\n        List<TopicPartition> topicPartitionList\n    ) throws Exception {\n        verifyClassicGroupSyncToConsumerGroup(\n            groupId,\n            memberId,\n            generationId,\n            protocolName,\n            protocolType,\n            topicPartitionList,\n            ConsumerProtocolAssignment.HIGHEST_SUPPORTED_VERSION\n        );\n    }\n\n    private ApiMessage messageOrNull(ApiMessageAndVersion apiMessageAndVersion) {\n        if (apiMessageAndVersion == null) {\n            return null;\n        } else {\n            return apiMessageAndVersion.message();\n        }\n    }\n\n    public void replay(\n        CoordinatorRecord record\n    ) {\n        replay(record, null);\n    }\n\n    public void replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    ) {\n        ApiMessageAndVersion key = record.key();\n        ApiMessageAndVersion value = record.value();\n\n        if (key == null) {\n            throw new IllegalStateException(\"Received a null key in \" + record);\n        }\n\n        switch (key.version()) {\n            case GroupMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (GroupMetadataKey) key.message(),\n                    (GroupMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            case ConsumerGroupMemberMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupMemberMetadataKey) key.message(),\n                    (ConsumerGroupMemberMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            case ConsumerGroupMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupMetadataKey) key.message(),\n                    (ConsumerGroupMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            case ConsumerGroupPartitionMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupPartitionMetadataKey) key.message(),\n                    (ConsumerGroupPartitionMetadataValue) messageOrNull(value),\n                    groupType != null ? groupType : GroupType.CONSUMER\n                );\n                break;\n\n            case ConsumerGroupTargetAssignmentMemberKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupTargetAssignmentMemberKey) key.message(),\n                    (ConsumerGroupTargetAssignmentMemberValue) messageOrNull(value),\n                    groupType != null ? groupType : GroupType.CONSUMER\n                );\n                break;\n\n            case ConsumerGroupTargetAssignmentMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupTargetAssignmentMetadataKey) key.message(),\n                    (ConsumerGroupTargetAssignmentMetadataValue) messageOrNull(value),\n                    groupType != null ? groupType : GroupType.CONSUMER\n                );\n                break;\n\n            case ConsumerGroupCurrentMemberAssignmentKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ConsumerGroupCurrentMemberAssignmentKey) key.message(),\n                    (ConsumerGroupCurrentMemberAssignmentValue) messageOrNull(value),\n                    groupType != null ? groupType : GroupType.CONSUMER\n                );\n                break;\n\n            case ShareGroupMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ShareGroupMetadataKey) key.message(),\n                    (ShareGroupMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            case ShareGroupMemberMetadataKey.HIGHEST_SUPPORTED_VERSION:\n                groupMetadataManager.replay(\n                    (ShareGroupMemberMetadataKey) key.message(),\n                    (ShareGroupMemberMetadataValue) messageOrNull(value)\n                );\n                break;\n\n            default:\n                throw new IllegalStateException(\"Received an unknown record type \" + key.version()\n                    + \" in \" + record);\n        }\n\n        lastWrittenOffset++;\n        idempotentCreateSnapshot(lastWrittenOffset);\n    }\n\n    private void idempotentCreateSnapshot(long epoch) {\n        snapshotRegistry.getOrCreateSnapshot(epoch);\n    }\n\n    void onUnloaded() {\n        groupMetadataManager.onUnloaded();\n    }\n}",
                "methodCount": 87
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "assertNoOrEmptyResult",
                            "method_signature": "public static assertNoOrEmptyResult(List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toProtocols",
                            "method_signature": "public static toProtocols(String... protocolNames)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toConsumerProtocol",
                            "method_signature": "public static toConsumerProtocol(\n        List<String> topicNames,\n        List<TopicPartition> ownedPartitions\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "toConsumerProtocol",
                            "method_signature": "public static toConsumerProtocol(\n        List<String> topicNames,\n        List<TopicPartition> ownedPartitions,\n        short version\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "newGroupMetadataRecord",
                            "method_signature": "public static newGroupMetadataRecord(\n        String groupId,\n        GroupMetadataValue value,\n        MetadataVersion metadataVersion\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupId",
                            "method_signature": " withGroupId(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupInstanceId",
                            "method_signature": " withGroupInstanceId(String groupInstanceId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withMemberId",
                            "method_signature": " withMemberId(String memberId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withDefaultProtocolTypeAndProtocols",
                            "method_signature": " withDefaultProtocolTypeAndProtocols()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withProtocolSuperset",
                            "method_signature": " withProtocolSuperset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withProtocolType",
                            "method_signature": " withProtocolType(String protocolType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withProtocols",
                            "method_signature": " withProtocols(JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withRebalanceTimeoutMs",
                            "method_signature": " withRebalanceTimeoutMs(int rebalanceTimeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withSessionTimeoutMs",
                            "method_signature": " withSessionTimeoutMs(int sessionTimeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withReason",
                            "method_signature": " withReason(String reason)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupId",
                            "method_signature": " withGroupId(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupInstanceId",
                            "method_signature": " withGroupInstanceId(String groupInstanceId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withMemberId",
                            "method_signature": " withMemberId(String memberId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGenerationId",
                            "method_signature": " withGenerationId(int generationId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withProtocolType",
                            "method_signature": " withProtocolType(String protocolType)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withProtocolName",
                            "method_signature": " withProtocolName(String protocolName)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withAssignment",
                            "method_signature": " withAssignment(List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withMetadataImage",
                            "method_signature": "public withMetadataImage(MetadataImage metadataImage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConsumerGroupAssignors",
                            "method_signature": "public withConsumerGroupAssignors(List<ConsumerGroupPartitionAssignor> assignors)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConsumerGroup",
                            "method_signature": "public withConsumerGroup(ConsumerGroupBuilder builder)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConsumerGroupMaxSize",
                            "method_signature": "public withConsumerGroupMaxSize(int consumerGroupMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConsumerGroupMetadataRefreshIntervalMs",
                            "method_signature": "public withConsumerGroupMetadataRefreshIntervalMs(int consumerGroupMetadataRefreshIntervalMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withClassicGroupMaxSize",
                            "method_signature": "public withClassicGroupMaxSize(int classicGroupMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withClassicGroupInitialRebalanceDelayMs",
                            "method_signature": "public withClassicGroupInitialRebalanceDelayMs(int classicGroupInitialRebalanceDelayMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withClassicGroupMinSessionTimeoutMs",
                            "method_signature": "public withClassicGroupMinSessionTimeoutMs(int classicGroupMinSessionTimeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withClassicGroupMaxSessionTimeoutMs",
                            "method_signature": "public withClassicGroupMaxSessionTimeoutMs(int classicGroupMaxSessionTimeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConsumerGroupMigrationPolicy",
                            "method_signature": "public withConsumerGroupMigrationPolicy(ConsumerGroupMigrationPolicy consumerGroupMigrationPolicy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withShareGroupAssignor",
                            "method_signature": "public withShareGroupAssignor(ShareGroupPartitionAssignor shareGroupAssignor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withShareGroupMaxSize",
                            "method_signature": "public withShareGroupMaxSize(int shareGroupMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rollback",
                            "method_signature": "public rollback()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "consumerGroupState",
                            "method_signature": "public consumerGroupState(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shareGroupState",
                            "method_signature": "public shareGroupState(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "consumerGroupMemberState",
                            "method_signature": "public consumerGroupMemberState(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "consumerGroupHeartbeat",
                            "method_signature": "public consumerGroupHeartbeat(\n        ConsumerGroupHeartbeatRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "shareGroupHeartbeat",
                            "method_signature": "public shareGroupHeartbeat(\n        ShareGroupHeartbeatRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSessionTimeout",
                            "method_signature": "public assertSessionTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoSessionTimeout",
                            "method_signature": "public assertNoSessionTimeout(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertRebalanceTimeout",
                            "method_signature": "public assertRebalanceTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoRebalanceTimeout",
                            "method_signature": "public assertNoRebalanceTimeout(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertJoinTimeout",
                            "method_signature": "public assertJoinTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoJoinTimeout",
                            "method_signature": "public assertNoJoinTimeout(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertSyncTimeout",
                            "method_signature": "public assertSyncTimeout(\n        String groupId,\n        String memberId,\n        long delayMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertNoSyncTimeout",
                            "method_signature": "public assertNoSyncTimeout(\n        String groupId,\n        String memberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createClassicGroup",
                            "method_signature": " createClassicGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupJoin",
                            "method_signature": "public sendClassicGroupJoin(\n        JoinGroupRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupJoin",
                            "method_signature": "public sendClassicGroupJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupJoin",
                            "method_signature": "public sendClassicGroupJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinClassicGroupAsDynamicMemberAndCompleteRebalance",
                            "method_signature": "public joinClassicGroupAsDynamicMemberAndCompleteRebalance(\n        String groupId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinClassicGroupAsDynamicMemberAndCompleteJoin",
                            "method_signature": "public joinClassicGroupAsDynamicMemberAndCompleteJoin(\n        JoinGroupRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinClassicGroupAndCompleteJoin",
                            "method_signature": "public joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinClassicGroupAndCompleteJoin",
                            "method_signature": "public joinClassicGroupAndCompleteJoin(\n        JoinGroupRequestData request,\n        boolean requireKnownMemberId,\n        boolean supportSkippingAssignment,\n        int advanceClockMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupSync",
                            "method_signature": "public sendClassicGroupSync(SyncGroupRequestData request)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "staticMembersJoinAndRebalance",
                            "method_signature": "public staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "staticMembersJoinAndRebalance",
                            "method_signature": "public staticMembersJoinAndRebalance(\n        String groupId,\n        String leaderInstanceId,\n        String followerInstanceId,\n        int rebalanceTimeoutMs,\n        int sessionTimeoutMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifySessionExpiration",
                            "method_signature": "public verifySessionExpiration(ClassicGroup group, int timeoutMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupHeartbeat",
                            "method_signature": "public sendClassicGroupHeartbeat(\n        HeartbeatRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendListGroups",
                            "method_signature": "public sendListGroups(List<String> statesFilter, List<String> typesFilter)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendConsumerGroupDescribe",
                            "method_signature": "public sendConsumerGroupDescribe(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "describeGroups",
                            "method_signature": "public describeGroups(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendShareGroupDescribe",
                            "method_signature": "public sendShareGroupDescribe(List<String> groupIds)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyHeartbeat",
                            "method_signature": "public verifyHeartbeat(\n        String groupId,\n        JoinGroupResponseData joinResponse,\n        Errors expectedError\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "joinWithNMembers",
                            "method_signature": "public joinWithNMembers(\n        String groupId,\n        int numMembers,\n        int rebalanceTimeoutMs,\n        int sessionTimeoutMs\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sendClassicGroupLeave",
                            "method_signature": "public sendClassicGroupLeave(\n        LeaveGroupRequestData request\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyDescribeGroupsReturnsDeadGroup",
                            "method_signature": "public verifyDescribeGroupsReturnsDeadGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyClassicGroupSyncToConsumerGroup",
                            "method_signature": "public verifyClassicGroupSyncToConsumerGroup(\n        String groupId,\n        String memberId,\n        int generationId,\n        String protocolName,\n        String protocolType,\n        List<TopicPartition> topicPartitionList,\n        short version\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "verifyClassicGroupSyncToConsumerGroup",
                            "method_signature": "public verifyClassicGroupSyncToConsumerGroup(\n        String groupId,\n        String memberId,\n        int generationId,\n        String protocolName,\n        String protocolType,\n        List<TopicPartition> topicPartitionList\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(\n        CoordinatorRecord record\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(\n        CoordinatorRecord record,\n        GroupType groupType\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onUnloaded",
                            "method_signature": " onUnloaded()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "withMetadataImage",
                            "method_signature": "public withMetadataImage(MetadataImage metadataImage)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withReason",
                            "method_signature": " withReason(String reason)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withShareGroupMaxSize",
                            "method_signature": "public withShareGroupMaxSize(int shareGroupMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withAssignment",
                            "method_signature": " withAssignment(List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConsumerGroup",
                            "method_signature": "public withConsumerGroup(ConsumerGroupBuilder builder)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withClassicGroupMaxSize",
                            "method_signature": "public withClassicGroupMaxSize(int classicGroupMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConsumerGroupMaxSize",
                            "method_signature": "public withConsumerGroupMaxSize(int consumerGroupMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withConsumerGroupMigrationPolicy",
                            "method_signature": "public withConsumerGroupMigrationPolicy(ConsumerGroupMigrationPolicy consumerGroupMigrationPolicy)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGenerationId",
                            "method_signature": " withGenerationId(int generationId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupInstanceId",
                            "method_signature": " withGroupInstanceId(String groupInstanceId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withShareGroupAssignor",
                            "method_signature": "public withShareGroupAssignor(ShareGroupPartitionAssignor shareGroupAssignor)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rollback",
                            "method_signature": "public rollback()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupInstanceId",
                            "method_signature": " withGroupInstanceId(String groupInstanceId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withProtocols",
                            "method_signature": " withProtocols(JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "public withMetadataImage(MetadataImage metadataImage)": {
                    "first": {
                        "method_name": "withMetadataImage",
                        "method_signature": "public withMetadataImage(MetadataImage metadataImage)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.14880037161301174
                },
                "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)": {
                    "first": {
                        "method_name": "messageOrNull",
                        "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.18947883152560974
                },
                " withReason(String reason)": {
                    "first": {
                        "method_name": "withReason",
                        "method_signature": " withReason(String reason)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.24772717776573697
                },
                "public withShareGroupMaxSize(int shareGroupMaxSize)": {
                    "first": {
                        "method_name": "withShareGroupMaxSize",
                        "method_signature": "public withShareGroupMaxSize(int shareGroupMaxSize)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.24877951726163844
                },
                " withAssignment(List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment)": {
                    "first": {
                        "method_name": "withAssignment",
                        "method_signature": " withAssignment(List<SyncGroupRequestData.SyncGroupRequestAssignment> assignment)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2725113377822755
                },
                "public withConsumerGroup(ConsumerGroupBuilder builder)": {
                    "first": {
                        "method_name": "withConsumerGroup",
                        "method_signature": "public withConsumerGroup(ConsumerGroupBuilder builder)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.29209450901639006
                },
                "public withClassicGroupMaxSize(int classicGroupMaxSize)": {
                    "first": {
                        "method_name": "withClassicGroupMaxSize",
                        "method_signature": "public withClassicGroupMaxSize(int classicGroupMaxSize)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2931728900835028
                },
                "public withConsumerGroupMaxSize(int consumerGroupMaxSize)": {
                    "first": {
                        "method_name": "withConsumerGroupMaxSize",
                        "method_signature": "public withConsumerGroupMaxSize(int consumerGroupMaxSize)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2935368952025324
                },
                "public withConsumerGroupMigrationPolicy(ConsumerGroupMigrationPolicy consumerGroupMigrationPolicy)": {
                    "first": {
                        "method_name": "withConsumerGroupMigrationPolicy",
                        "method_signature": "public withConsumerGroupMigrationPolicy(ConsumerGroupMigrationPolicy consumerGroupMigrationPolicy)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.2943922188198769
                },
                " withGenerationId(int generationId)": {
                    "first": {
                        "method_name": "withGenerationId",
                        "method_signature": " withGenerationId(int generationId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.29794085028806583
                },
                " withGroupInstanceId(String groupInstanceId)": {
                    "first": {
                        "method_name": "withGroupInstanceId",
                        "method_signature": " withGroupInstanceId(String groupInstanceId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3154848524435535
                },
                "public withShareGroupAssignor(ShareGroupPartitionAssignor shareGroupAssignor)": {
                    "first": {
                        "method_name": "withShareGroupAssignor",
                        "method_signature": "public withShareGroupAssignor(ShareGroupPartitionAssignor shareGroupAssignor)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.30734230903634124
                },
                "public rollback()": {
                    "first": {
                        "method_name": "rollback",
                        "method_signature": "public rollback()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3080694831365844
                },
                " withProtocols(JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols)": {
                    "first": {
                        "method_name": "withProtocols",
                        "method_signature": " withProtocols(JoinGroupRequestData.JoinGroupRequestProtocolCollection protocols)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3245316828861138
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from private replay(producerId long, record CoordinatorRecord) : void in class org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 480,
                    "endLine": 509,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private replay(producerId long, record CoordinatorRecord) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 484,
                    "endLine": 484,
                    "startColumn": 13,
                    "endColumn": 69,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 480,
                    "endLine": 509,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private replay(producerId long, record CoordinatorRecord) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 484,
                    "endLine": 484,
                    "startColumn": 13,
                    "endColumn": 73,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastWrittenOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 607,
        "extraction_results": {
            "success": true,
            "newCommitHash": "e62eecd5e8915b6e6f83df0729e7ecd1b5103dce",
            "newBranchName": "extract-idempotentCreateSnapshot-replay-130af38"
        },
        "telemetry": {
            "id": "03753ff4-5342-44e1-855a-b020fadb30d7",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 483,
                "lineStart": 97,
                "lineEnd": 579,
                "bodyLineStart": 97,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                "sourceCode": "static class OffsetMetadataManagerTestContext {\n        public static class Builder {\n            private final MockTime time = new MockTime();\n            private final MockCoordinatorTimer<Void, CoordinatorRecord> timer = new MockCoordinatorTimer<>(time);\n            private final LogContext logContext = new LogContext();\n            private final SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n            private GroupMetadataManager groupMetadataManager = null;\n            private MetadataImage metadataImage = null;\n            private GroupCoordinatorConfig config = null;\n            private GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n\n            Builder withOffsetMetadataMaxSize(int offsetMetadataMaxSize) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(offsetMetadataMaxSize, 60000L, 24 * 60);\n                return this;\n            }\n\n            Builder withOffsetsRetentionMinutes(int offsetsRetentionMinutes) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, offsetsRetentionMinutes);\n                return this;\n            }\n\n            Builder withGroupMetadataManager(GroupMetadataManager groupMetadataManager) {\n                this.groupMetadataManager = groupMetadataManager;\n                return this;\n            }\n\n            OffsetMetadataManagerTestContext build() {\n                if (metadataImage == null) metadataImage = MetadataImage.EMPTY;\n                if (config == null) {\n                    config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, 24);\n                }\n\n                if (groupMetadataManager == null) {\n                    groupMetadataManager = new GroupMetadataManager.Builder()\n                        .withTime(time)\n                        .withTimer(timer)\n                        .withSnapshotRegistry(snapshotRegistry)\n                        .withLogContext(logContext)\n                        .withMetadataImage(metadataImage)\n                        .withConsumerGroupAssignors(Collections.singletonList(new RangeAssignor()))\n                        .withGroupCoordinatorMetricsShard(metrics)\n                        .build();\n                }\n\n                OffsetMetadataManager offsetMetadataManager = new OffsetMetadataManager.Builder()\n                    .withTime(time)\n                    .withLogContext(logContext)\n                    .withSnapshotRegistry(snapshotRegistry)\n                    .withMetadataImage(metadataImage)\n                    .withGroupMetadataManager(groupMetadataManager)\n                    .withGroupCoordinatorConfig(config)\n                    .withGroupCoordinatorMetricsShard(metrics)\n                    .build();\n\n                return new OffsetMetadataManagerTestContext(\n                    time,\n                    timer,\n                    snapshotRegistry,\n                    metrics,\n                    groupMetadataManager,\n                    offsetMetadataManager\n                );\n            }\n        }\n\n        final MockTime time;\n        final MockCoordinatorTimer<Void, CoordinatorRecord> timer;\n        final SnapshotRegistry snapshotRegistry;\n        final GroupCoordinatorMetricsShard metrics;\n        final GroupMetadataManager groupMetadataManager;\n        final OffsetMetadataManager offsetMetadataManager;\n\n        long lastCommittedOffset = 0L;\n        long lastWrittenOffset = 0L;\n\n        OffsetMetadataManagerTestContext(\n            MockTime time,\n            MockCoordinatorTimer<Void, CoordinatorRecord> timer,\n            SnapshotRegistry snapshotRegistry,\n            GroupCoordinatorMetricsShard metrics,\n            GroupMetadataManager groupMetadataManager,\n            OffsetMetadataManager offsetMetadataManager\n        ) {\n            this.time = time;\n            this.timer = timer;\n            this.snapshotRegistry = snapshotRegistry;\n            this.metrics = metrics;\n            this.groupMetadataManager = groupMetadataManager;\n            this.offsetMetadataManager = offsetMetadataManager;\n        }\n\n        public Group getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        ) {\n            switch (groupType) {\n                case CLASSIC:\n                    return groupMetadataManager.getOrMaybeCreateClassicGroup(\n                        groupId,\n                        true\n                    );\n                case CONSUMER:\n                    return groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n                        groupId,\n                        true\n                    );\n                default:\n                    throw new IllegalArgumentException(\"Invalid group type: \" + groupType);\n            }\n        }\n\n        public void commit() {\n            long lastCommittedOffset = this.lastCommittedOffset;\n            this.lastCommittedOffset = lastWrittenOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastCommittedOffset);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(\n            OffsetCommitRequestData request\n        ) {\n            return commitOffset(ApiKeys.OFFSET_COMMIT.latestVersion(), request);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.OFFSET_COMMIT,\n                    version,\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = offsetMetadataManager.commitOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.TXN_OFFSET_COMMIT,\n                    ApiKeys.TXN_OFFSET_COMMIT.latestVersion(),\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> result = offsetMetadataManager.commitTransactionalOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(record -> replay(\n                request.producerId(),\n                record\n            ));\n\n            return result;\n        }\n\n        public List<CoordinatorRecord> deletePartitions(\n            List<TopicPartition> topicPartitions\n        ) {\n            List<CoordinatorRecord> records = offsetMetadataManager.onPartitionsDeleted(topicPartitions);\n            records.forEach(this::replay);\n            return records;\n        }\n\n        public CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> deleteOffsets(\n            OffsetDeleteRequestData request\n        ) {\n            CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> result = offsetMetadataManager.deleteOffsets(request);\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public int deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        ) {\n            List<CoordinatorRecord> addedRecords = new ArrayList<>();\n            int numDeletedOffsets = offsetMetadataManager.deleteAllOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return numDeletedOffsets;\n        }\n\n        public boolean cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records) {\n            List<CoordinatorRecord> addedRecords = new ArrayList<>();\n            boolean isOffsetsEmptyForGroup = offsetMetadataManager.cleanupExpiredOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return isOffsetsEmptyForGroup;\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            return fetchOffsets(\n                groupId,\n                null,\n                -1,\n                topics,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch)\n                    .setTopics(topics),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        ) {\n            return fetchAllOffsets(\n                groupId,\n                null,\n                -1,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchAllOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> sleep(long ms) {\n            time.sleep(ms);\n            List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = timer.poll();\n            timeouts.forEach(timeout -> {\n                if (timeout.result.replayRecords()) {\n                    timeout.result.records().forEach(this::replay);\n                }\n            });\n            return timeouts;\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        ) {\n            commitOffset(\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                time.milliseconds()\n            );\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            commitOffset(\n                RecordBatch.NO_PRODUCER_ID,\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                commitTimestamp\n            );\n        }\n\n        public void commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            replay(producerId, CoordinatorRecordHelpers.newOffsetCommitRecord(\n                groupId,\n                topic,\n                partition,\n                new OffsetAndMetadata(\n                    offset,\n                    OptionalInt.of(leaderEpoch),\n                    \"metadata\",\n                    commitTimestamp,\n                    OptionalLong.empty()\n                ),\n                MetadataVersion.latestTesting()\n            ));\n        }\n\n        public void deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            replay(CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\n                groupId,\n                topic,\n                partition\n            ));\n        }\n\n        private ApiMessage messageOrNull(ApiMessageAndVersion apiMessageAndVersion) {\n            if (apiMessageAndVersion == null) {\n                return null;\n            } else {\n                return apiMessageAndVersion.message();\n            }\n        }\n\n        private void replay(\n            CoordinatorRecord record\n        ) {\n            replay(\n                RecordBatch.NO_PRODUCER_ID,\n                record\n            );\n        }\n\n        private void replay(\n            long producerId,\n            CoordinatorRecord record\n        ) {\n            idempotentCreateSnapshot();\n\n            ApiMessageAndVersion key = record.key();\n            ApiMessageAndVersion value = record.value();\n\n            if (key == null) {\n                throw new IllegalStateException(\"Received a null key in \" + record);\n            }\n\n            switch (key.version()) {\n                case OffsetCommitKey.HIGHEST_SUPPORTED_VERSION:\n                    offsetMetadataManager.replay(\n                        lastWrittenOffset,\n                        producerId,\n                        (OffsetCommitKey) key.message(),\n                        (OffsetCommitValue) messageOrNull(value)\n                    );\n                    break;\n\n                default:\n                    throw new IllegalStateException(\"Received an unknown record type \" + key.version()\n                        + \" in \" + record);\n            }\n\n            lastWrittenOffset++;\n        }\n\n        private void idempotentCreateSnapshot() {\n            snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n        }\n\n        private void replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        ) {\n            idempotentCreateSnapshot();\n            offsetMetadataManager.replayEndTransactionMarker(producerId, result);\n            lastWrittenOffset++;\n        }\n\n        public void testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        ) {\n            final OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection requestTopicCollection =\n                new OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection(Collections.singletonList(\n                    new OffsetDeleteRequestData.OffsetDeleteRequestTopic()\n                        .setName(topic)\n                        .setPartitions(Collections.singletonList(\n                            new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(partition)\n                        ))\n                ).iterator());\n\n            final OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection expectedResponsePartitionCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection();\n            expectedResponsePartitionCollection.add(\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartition()\n                    .setPartitionIndex(partition)\n                    .setErrorCode(expectedError.code())\n            );\n\n            final OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection expectedResponseTopicCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection(Collections.singletonList(\n                    new OffsetDeleteResponseData.OffsetDeleteResponseTopic()\n                        .setName(topic)\n                        .setPartitions(expectedResponsePartitionCollection)\n                ).iterator());\n\n            List<CoordinatorRecord> expectedRecords = Collections.emptyList();\n            if (hasOffset(groupId, topic, partition) && expectedError == Errors.NONE) {\n                expectedRecords = Collections.singletonList(\n                    CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(groupId, topic, partition)\n                );\n            }\n\n            final CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> coordinatorResult = deleteOffsets(\n                new OffsetDeleteRequestData()\n                    .setGroupId(groupId)\n                    .setTopics(requestTopicCollection)\n            );\n\n            assertEquals(new OffsetDeleteResponseData().setTopics(expectedResponseTopicCollection), coordinatorResult.response());\n            assertEquals(expectedRecords, coordinatorResult.records());\n        }\n\n        public boolean hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            return offsetMetadataManager.hasCommittedOffset(groupId, topic, partition) ||\n                offsetMetadataManager.hasPendingTransactionalOffsets(groupId, topic, partition);\n        }\n    }",
                "methodCount": 30
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetsRetentionMinutes",
                            "method_signature": " withOffsetsRetentionMinutes(int offsetsRetentionMinutes)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupMetadataManager",
                            "method_signature": " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deletePartitions",
                            "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testOffsetDeleteWith",
                            "method_signature": "public testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupMetadataManager",
                            "method_signature": " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deletePartitions",
                            "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)": {
                    "first": {
                        "method_name": "messageOrNull",
                        "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.1951732103114712
                },
                "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )": {
                    "first": {
                        "method_name": "getOrMaybeCreateGroup",
                        "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.33235663483995337
                },
                " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)": {
                    "first": {
                        "method_name": "withGroupMetadataManager",
                        "method_signature": " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3923341369615525
                },
                "private replay(\n            CoordinatorRecord record\n        )": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "private replay(\n            CoordinatorRecord record\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41822732736111484
                },
                "public sleep(long ms)": {
                    "first": {
                        "method_name": "sleep",
                        "method_signature": "public sleep(long ms)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.42748324689917644
                },
                "private idempotentCreateSnapshot()": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4658995962983482
                },
                "public commit()": {
                    "first": {
                        "method_name": "commit",
                        "method_signature": "public commit()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4769223867971554
                },
                "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )": {
                    "first": {
                        "method_name": "replayEndTransactionMarker",
                        "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5527603605163566
                },
                "public commitOffset(\n            OffsetCommitRequestData request\n        )": {
                    "first": {
                        "method_name": "commitOffset",
                        "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.554798794424395
                },
                "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )": {
                    "first": {
                        "method_name": "deletePartitions",
                        "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5703419768390383
                },
                "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )": {
                    "first": {
                        "method_name": "fetchAllOffsets",
                        "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5707181779656216
                },
                "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.582790582216309
                },
                "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )": {
                    "first": {
                        "method_name": "fetchOffsets",
                        "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5843501213452627
                },
                "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)": {
                    "first": {
                        "method_name": "cleanupExpiredOffsets",
                        "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5846060841768884
                },
                "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                    "first": {
                        "method_name": "hasOffset",
                        "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5978863420667276
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from private replayEndTransactionMarker(producerId long, result TransactionResult) : void in class org.apache.kafka.coordinator.group.OffsetMetadataManagerTest.OffsetMetadataManagerTestContext & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 511,
                    "endLine": 518,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private replayEndTransactionMarker(producerId long, result TransactionResult) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 515,
                    "endLine": 515,
                    "startColumn": 13,
                    "endColumn": 69,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 511,
                    "endLine": 518,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private replayEndTransactionMarker(producerId long, result TransactionResult) : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                    "startLine": 515,
                    "endLine": 515,
                    "startColumn": 13,
                    "endColumn": 73,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastWrittenOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 608,
        "extraction_results": {
            "success": true,
            "newCommitHash": "2d5adc292dd053243e3ad49fb9e5ee75e20fd598",
            "newBranchName": "extract-idempotentCreateSnapshot-replayEndTransactionMarker-130af38"
        },
        "telemetry": {
            "id": "0d60a97a-9cd8-4b20-939a-b7777e9403ad",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 483,
                "lineStart": 97,
                "lineEnd": 579,
                "bodyLineStart": 97,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/OffsetMetadataManagerTest.java",
                "sourceCode": "static class OffsetMetadataManagerTestContext {\n        public static class Builder {\n            private final MockTime time = new MockTime();\n            private final MockCoordinatorTimer<Void, CoordinatorRecord> timer = new MockCoordinatorTimer<>(time);\n            private final LogContext logContext = new LogContext();\n            private final SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n            private GroupMetadataManager groupMetadataManager = null;\n            private MetadataImage metadataImage = null;\n            private GroupCoordinatorConfig config = null;\n            private GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n\n            Builder withOffsetMetadataMaxSize(int offsetMetadataMaxSize) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(offsetMetadataMaxSize, 60000L, 24 * 60);\n                return this;\n            }\n\n            Builder withOffsetsRetentionMinutes(int offsetsRetentionMinutes) {\n                config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, offsetsRetentionMinutes);\n                return this;\n            }\n\n            Builder withGroupMetadataManager(GroupMetadataManager groupMetadataManager) {\n                this.groupMetadataManager = groupMetadataManager;\n                return this;\n            }\n\n            OffsetMetadataManagerTestContext build() {\n                if (metadataImage == null) metadataImage = MetadataImage.EMPTY;\n                if (config == null) {\n                    config = GroupCoordinatorConfigTest.createGroupCoordinatorConfig(4096, 60000L, 24);\n                }\n\n                if (groupMetadataManager == null) {\n                    groupMetadataManager = new GroupMetadataManager.Builder()\n                        .withTime(time)\n                        .withTimer(timer)\n                        .withSnapshotRegistry(snapshotRegistry)\n                        .withLogContext(logContext)\n                        .withMetadataImage(metadataImage)\n                        .withConsumerGroupAssignors(Collections.singletonList(new RangeAssignor()))\n                        .withGroupCoordinatorMetricsShard(metrics)\n                        .build();\n                }\n\n                OffsetMetadataManager offsetMetadataManager = new OffsetMetadataManager.Builder()\n                    .withTime(time)\n                    .withLogContext(logContext)\n                    .withSnapshotRegistry(snapshotRegistry)\n                    .withMetadataImage(metadataImage)\n                    .withGroupMetadataManager(groupMetadataManager)\n                    .withGroupCoordinatorConfig(config)\n                    .withGroupCoordinatorMetricsShard(metrics)\n                    .build();\n\n                return new OffsetMetadataManagerTestContext(\n                    time,\n                    timer,\n                    snapshotRegistry,\n                    metrics,\n                    groupMetadataManager,\n                    offsetMetadataManager\n                );\n            }\n        }\n\n        final MockTime time;\n        final MockCoordinatorTimer<Void, CoordinatorRecord> timer;\n        final SnapshotRegistry snapshotRegistry;\n        final GroupCoordinatorMetricsShard metrics;\n        final GroupMetadataManager groupMetadataManager;\n        final OffsetMetadataManager offsetMetadataManager;\n\n        long lastCommittedOffset = 0L;\n        long lastWrittenOffset = 0L;\n\n        OffsetMetadataManagerTestContext(\n            MockTime time,\n            MockCoordinatorTimer<Void, CoordinatorRecord> timer,\n            SnapshotRegistry snapshotRegistry,\n            GroupCoordinatorMetricsShard metrics,\n            GroupMetadataManager groupMetadataManager,\n            OffsetMetadataManager offsetMetadataManager\n        ) {\n            this.time = time;\n            this.timer = timer;\n            this.snapshotRegistry = snapshotRegistry;\n            this.metrics = metrics;\n            this.groupMetadataManager = groupMetadataManager;\n            this.offsetMetadataManager = offsetMetadataManager;\n        }\n\n        public Group getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        ) {\n            switch (groupType) {\n                case CLASSIC:\n                    return groupMetadataManager.getOrMaybeCreateClassicGroup(\n                        groupId,\n                        true\n                    );\n                case CONSUMER:\n                    return groupMetadataManager.getOrMaybeCreatePersistedConsumerGroup(\n                        groupId,\n                        true\n                    );\n                default:\n                    throw new IllegalArgumentException(\"Invalid group type: \" + groupType);\n            }\n        }\n\n        public void commit() {\n            long lastCommittedOffset = this.lastCommittedOffset;\n            this.lastCommittedOffset = lastWrittenOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastCommittedOffset);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(\n            OffsetCommitRequestData request\n        ) {\n            return commitOffset(ApiKeys.OFFSET_COMMIT.latestVersion(), request);\n        }\n\n        public CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.OFFSET_COMMIT,\n                    version,\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<OffsetCommitResponseData, CoordinatorRecord> result = offsetMetadataManager.commitOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        ) {\n            RequestContext context = new RequestContext(\n                new RequestHeader(\n                    ApiKeys.TXN_OFFSET_COMMIT,\n                    ApiKeys.TXN_OFFSET_COMMIT.latestVersion(),\n                    \"client\",\n                    0\n                ),\n                \"1\",\n                InetAddress.getLoopbackAddress(),\n                KafkaPrincipal.ANONYMOUS,\n                ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n                SecurityProtocol.PLAINTEXT,\n                ClientInformation.EMPTY,\n                false\n            );\n\n            CoordinatorResult<TxnOffsetCommitResponseData, CoordinatorRecord> result = offsetMetadataManager.commitTransactionalOffset(\n                context,\n                request\n            );\n\n            result.records().forEach(record -> replay(\n                request.producerId(),\n                record\n            ));\n\n            return result;\n        }\n\n        public List<CoordinatorRecord> deletePartitions(\n            List<TopicPartition> topicPartitions\n        ) {\n            List<CoordinatorRecord> records = offsetMetadataManager.onPartitionsDeleted(topicPartitions);\n            records.forEach(this::replay);\n            return records;\n        }\n\n        public CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> deleteOffsets(\n            OffsetDeleteRequestData request\n        ) {\n            CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> result = offsetMetadataManager.deleteOffsets(request);\n            result.records().forEach(this::replay);\n            return result;\n        }\n\n        public int deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        ) {\n            List<CoordinatorRecord> addedRecords = new ArrayList<>();\n            int numDeletedOffsets = offsetMetadataManager.deleteAllOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return numDeletedOffsets;\n        }\n\n        public boolean cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records) {\n            List<CoordinatorRecord> addedRecords = new ArrayList<>();\n            boolean isOffsetsEmptyForGroup = offsetMetadataManager.cleanupExpiredOffsets(groupId, addedRecords);\n            addedRecords.forEach(this::replay);\n\n            records.addAll(addedRecords);\n            return isOffsetsEmptyForGroup;\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            return fetchOffsets(\n                groupId,\n                null,\n                -1,\n                topics,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch)\n                    .setTopics(topics),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        ) {\n            return fetchAllOffsets(\n                groupId,\n                null,\n                -1,\n                committedOffset\n            );\n        }\n\n        public List<OffsetFetchResponseData.OffsetFetchResponseTopics> fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        ) {\n            OffsetFetchResponseData.OffsetFetchResponseGroup response = offsetMetadataManager.fetchAllOffsets(\n                new OffsetFetchRequestData.OffsetFetchRequestGroup()\n                    .setGroupId(groupId)\n                    .setMemberId(memberId)\n                    .setMemberEpoch(memberEpoch),\n                committedOffset\n            );\n            assertEquals(groupId, response.groupId());\n            return response.topics();\n        }\n\n        public List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> sleep(long ms) {\n            time.sleep(ms);\n            List<MockCoordinatorTimer.ExpiredTimeout<Void, CoordinatorRecord>> timeouts = timer.poll();\n            timeouts.forEach(timeout -> {\n                if (timeout.result.replayRecords()) {\n                    timeout.result.records().forEach(this::replay);\n                }\n            });\n            return timeouts;\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        ) {\n            commitOffset(\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                time.milliseconds()\n            );\n        }\n\n        public void commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            commitOffset(\n                RecordBatch.NO_PRODUCER_ID,\n                groupId,\n                topic,\n                partition,\n                offset,\n                leaderEpoch,\n                commitTimestamp\n            );\n        }\n\n        public void commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        ) {\n            replay(producerId, CoordinatorRecordHelpers.newOffsetCommitRecord(\n                groupId,\n                topic,\n                partition,\n                new OffsetAndMetadata(\n                    offset,\n                    OptionalInt.of(leaderEpoch),\n                    \"metadata\",\n                    commitTimestamp,\n                    OptionalLong.empty()\n                ),\n                MetadataVersion.latestTesting()\n            ));\n        }\n\n        public void deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            replay(CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(\n                groupId,\n                topic,\n                partition\n            ));\n        }\n\n        private ApiMessage messageOrNull(ApiMessageAndVersion apiMessageAndVersion) {\n            if (apiMessageAndVersion == null) {\n                return null;\n            } else {\n                return apiMessageAndVersion.message();\n            }\n        }\n\n        private void replay(\n            CoordinatorRecord record\n        ) {\n            replay(\n                RecordBatch.NO_PRODUCER_ID,\n                record\n            );\n        }\n\n        private void replay(\n            long producerId,\n            CoordinatorRecord record\n        ) {\n            idempotentCreateSnapshot();\n\n            ApiMessageAndVersion key = record.key();\n            ApiMessageAndVersion value = record.value();\n\n            if (key == null) {\n                throw new IllegalStateException(\"Received a null key in \" + record);\n            }\n\n            switch (key.version()) {\n                case OffsetCommitKey.HIGHEST_SUPPORTED_VERSION:\n                    offsetMetadataManager.replay(\n                        lastWrittenOffset,\n                        producerId,\n                        (OffsetCommitKey) key.message(),\n                        (OffsetCommitValue) messageOrNull(value)\n                    );\n                    break;\n\n                default:\n                    throw new IllegalStateException(\"Received an unknown record type \" + key.version()\n                        + \" in \" + record);\n            }\n\n            lastWrittenOffset++;\n        }\n\n        private void replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        ) {\n            idempotentCreateSnapshot();\n            offsetMetadataManager.replayEndTransactionMarker(producerId, result);\n            lastWrittenOffset++;\n        }\n\n        private void idempotentCreateSnapshot() {\n            snapshotRegistry.getOrCreateSnapshot(lastWrittenOffset);\n        }\n\n        public void testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        ) {\n            final OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection requestTopicCollection =\n                new OffsetDeleteRequestData.OffsetDeleteRequestTopicCollection(Collections.singletonList(\n                    new OffsetDeleteRequestData.OffsetDeleteRequestTopic()\n                        .setName(topic)\n                        .setPartitions(Collections.singletonList(\n                            new OffsetDeleteRequestData.OffsetDeleteRequestPartition().setPartitionIndex(partition)\n                        ))\n                ).iterator());\n\n            final OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection expectedResponsePartitionCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartitionCollection();\n            expectedResponsePartitionCollection.add(\n                new OffsetDeleteResponseData.OffsetDeleteResponsePartition()\n                    .setPartitionIndex(partition)\n                    .setErrorCode(expectedError.code())\n            );\n\n            final OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection expectedResponseTopicCollection =\n                new OffsetDeleteResponseData.OffsetDeleteResponseTopicCollection(Collections.singletonList(\n                    new OffsetDeleteResponseData.OffsetDeleteResponseTopic()\n                        .setName(topic)\n                        .setPartitions(expectedResponsePartitionCollection)\n                ).iterator());\n\n            List<CoordinatorRecord> expectedRecords = Collections.emptyList();\n            if (hasOffset(groupId, topic, partition) && expectedError == Errors.NONE) {\n                expectedRecords = Collections.singletonList(\n                    CoordinatorRecordHelpers.newOffsetCommitTombstoneRecord(groupId, topic, partition)\n                );\n            }\n\n            final CoordinatorResult<OffsetDeleteResponseData, CoordinatorRecord> coordinatorResult = deleteOffsets(\n                new OffsetDeleteRequestData()\n                    .setGroupId(groupId)\n                    .setTopics(requestTopicCollection)\n            );\n\n            assertEquals(new OffsetDeleteResponseData().setTopics(expectedResponseTopicCollection), coordinatorResult.response());\n            assertEquals(expectedRecords, coordinatorResult.records());\n        }\n\n        public boolean hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        ) {\n            return offsetMetadataManager.hasCommittedOffset(groupId, topic, partition) ||\n                offsetMetadataManager.hasPendingTransactionalOffsets(groupId, topic, partition);\n        }\n    }",
                "methodCount": 30
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "withOffsetMetadataMaxSize",
                            "method_signature": " withOffsetMetadataMaxSize(int offsetMetadataMaxSize)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withOffsetsRetentionMinutes",
                            "method_signature": " withOffsetsRetentionMinutes(int offsetsRetentionMinutes)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupMetadataManager",
                            "method_signature": " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": " build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            short version,\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitTransactionalOffset",
                            "method_signature": "public commitTransactionalOffset(\n            TxnOffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deletePartitions",
                            "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffsets",
                            "method_signature": "public deleteOffsets(\n            OffsetDeleteRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteAllOffsets",
                            "method_signature": "public deleteAllOffsets(\n            String groupId,\n            List<CoordinatorRecord> records\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            String memberId,\n            int memberEpoch,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            long producerId,\n            String groupId,\n            String topic,\n            int partition,\n            long offset,\n            int leaderEpoch,\n            long commitTimestamp\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deleteOffset",
                            "method_signature": "public deleteOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testOffsetDeleteWith",
                            "method_signature": "public testOffsetDeleteWith(\n            String groupId,\n            String topic,\n            int partition,\n            Errors expectedError\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "messageOrNull",
                            "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getOrMaybeCreateGroup",
                            "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "withGroupMetadataManager",
                            "method_signature": " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sleep",
                            "method_signature": "public sleep(long ms)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commit",
                            "method_signature": "public commit()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replayEndTransactionMarker",
                            "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitOffset",
                            "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deletePartitions",
                            "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchAllOffsets",
                            "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "fetchOffsets",
                            "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "cleanupExpiredOffsets",
                            "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasOffset",
                            "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)": {
                    "first": {
                        "method_name": "messageOrNull",
                        "method_signature": "private messageOrNull(ApiMessageAndVersion apiMessageAndVersion)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.1939756852192476
                },
                "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )": {
                    "first": {
                        "method_name": "getOrMaybeCreateGroup",
                        "method_signature": "public getOrMaybeCreateGroup(\n            Group.GroupType groupType,\n            String groupId\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3315382852788501
                },
                " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)": {
                    "first": {
                        "method_name": "withGroupMetadataManager",
                        "method_signature": " withGroupMetadataManager(GroupMetadataManager groupMetadataManager)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.390730785310973
                },
                "private replay(\n            CoordinatorRecord record\n        )": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "private replay(\n            CoordinatorRecord record\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41990546192452416
                },
                "public sleep(long ms)": {
                    "first": {
                        "method_name": "sleep",
                        "method_signature": "public sleep(long ms)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.428656888855491
                },
                "private idempotentCreateSnapshot()": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.46542097786619385
                },
                "public commit()": {
                    "first": {
                        "method_name": "commit",
                        "method_signature": "public commit()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4790163720631434
                },
                "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )": {
                    "first": {
                        "method_name": "replayEndTransactionMarker",
                        "method_signature": "private replayEndTransactionMarker(\n            long producerId,\n            TransactionResult result\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5531553437228104
                },
                "public commitOffset(\n            OffsetCommitRequestData request\n        )": {
                    "first": {
                        "method_name": "commitOffset",
                        "method_signature": "public commitOffset(\n            OffsetCommitRequestData request\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5562247929886831
                },
                "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )": {
                    "first": {
                        "method_name": "deletePartitions",
                        "method_signature": "public deletePartitions(\n            List<TopicPartition> topicPartitions\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5690387826010678
                },
                "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )": {
                    "first": {
                        "method_name": "fetchAllOffsets",
                        "method_signature": "public fetchAllOffsets(\n            String groupId,\n            long committedOffset\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5715592509044237
                },
                "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "private replay(\n            long producerId,\n            CoordinatorRecord record\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5842707068386135
                },
                "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )": {
                    "first": {
                        "method_name": "fetchOffsets",
                        "method_signature": "public fetchOffsets(\n            String groupId,\n            List<OffsetFetchRequestData.OffsetFetchRequestTopics> topics,\n            long committedOffset\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5853794858750969
                },
                "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)": {
                    "first": {
                        "method_name": "cleanupExpiredOffsets",
                        "method_signature": "public cleanupExpiredOffsets(String groupId, List<CoordinatorRecord> records)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5861988362937343
                },
                "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )": {
                    "first": {
                        "method_name": "hasOffset",
                        "method_signature": "public hasOffset(\n            String groupId,\n            String topic,\n            int partition\n        )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5982094281219734
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testTimelineGaugeCounters() : void in class org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsShardTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 48,
                    "endLine": 99,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testTimelineGaugeCounters() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 64,
                    "endLine": 64,
                    "startColumn": 9,
                    "endColumn": 52,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 48,
                    "endLine": 99,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testTimelineGaugeCounters() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 64,
                    "endLine": 64,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 90,
                    "endLine": 90,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(2000)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 609,
        "extraction_results": {
            "success": true,
            "newCommitHash": "a90c2d6088f885dc16d184ed13771ee9623b805d",
            "newBranchName": "extract-idempotentCreateSnapshot-testTimelineGaugeCounters-130af38"
        },
        "telemetry": {
            "id": "a47acbd8-dee2-4d19-ae47-9d0151d157e8",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 201,
                "lineStart": 46,
                "lineEnd": 246,
                "bodyLineStart": 46,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                "sourceCode": "public class GroupCoordinatorMetricsShardTest {\n\n    @Test\n    public void testTimelineGaugeCounters() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(snapshotRegistry, tp);\n\n        shard.incrementNumOffsets();\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD);\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        // The value should not be updated until the offset has been committed.\n        assertEquals(0, shard.numOffsets());\n        assertEquals(0, shard.numConsumerGroups());\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n\n        shard.commitUpTo(1000);\n        assertEquals(1, shard.numOffsets());\n        assertEquals(5, shard.numConsumerGroups());\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n\n        shard.decrementNumOffsets();\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD);\n\n        snapshotRegistry.getOrCreateSnapshot(2000);\n        shard.commitUpTo(2000);\n        assertEquals(0, shard.numOffsets());\n        assertEquals(0, shard.numConsumerGroups());\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(1000);\n    }\n\n    @Test\n    public void testGenericGroupStateTransitionMetrics() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(new SnapshotRegistry(new LogContext()), tp);\n        coordinatorMetrics.activateMetricsShard(shard);\n\n        LogContext logContext = new LogContext();\n        ClassicGroup group0 = new ClassicGroup(logContext, \"groupId0\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group1 = new ClassicGroup(logContext, \"groupId1\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group2 = new ClassicGroup(logContext, \"groupId2\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group3 = new ClassicGroup(logContext, \"groupId3\", EMPTY, Time.SYSTEM, shard);\n\n        IntStream.range(0, 4).forEach(__ -> shard.incrementNumClassicGroups(EMPTY));\n\n        assertEquals(4, shard.numClassicGroups());\n\n        group0.transitionTo(PREPARING_REBALANCE);\n        group0.transitionTo(COMPLETING_REBALANCE);\n        group1.transitionTo(PREPARING_REBALANCE);\n        group2.transitionTo(DEAD);\n\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.EMPTY));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.DEAD));\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.STABLE));\n\n        group0.transitionTo(STABLE);\n        group1.transitionTo(COMPLETING_REBALANCE);\n        group3.transitionTo(DEAD);\n\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.EMPTY));\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        assertEquals(2, shard.numClassicGroups(ClassicGroupState.DEAD));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.STABLE));\n\n        assertGaugeValue(\n            metrics,\n            metrics.metricName(\"group-count\", \"group-coordinator-metrics\", Collections.singletonMap(\"protocol\", \"classic\")),\n            4\n        );\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroups\"), 4);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsEmpty\"), 0);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsPreparingRebalance\"), 0);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsCompletingRebalance\"), 1);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsDead\"), 2);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsStable\"), 1);\n    }\n\n    @Test\n    public void testConsumerGroupStateTransitionMetrics() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(snapshotRegistry, tp);\n        coordinatorMetrics.activateMetricsShard(shard);\n\n        ConsumerGroup group0 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-0\",\n            shard\n        );\n        ConsumerGroup group1 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-1\",\n            shard\n        );\n        ConsumerGroup group2 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-2\",\n            shard\n        );\n        ConsumerGroup group3 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-3\",\n            shard\n        );\n\n        IntStream.range(0, 4).forEach(__ -> shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        shard.commitUpTo(1000);\n        assertEquals(4, shard.numConsumerGroups());\n        assertEquals(4, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n\n        ConsumerGroupMember member0 = group0.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member1 = group1.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member2 = group2.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member3 = group3.getOrMaybeCreateMember(\"member-id\", true);\n        group0.updateMember(member0);\n        group1.updateMember(member1);\n        group2.updateMember(member2);\n        group3.updateMember(member3);\n\n        snapshotRegistry.getOrCreateSnapshot(2000);\n        shard.commitUpTo(2000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(4, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        group2.setGroupEpoch(1);\n        group3.setGroupEpoch(1);\n\n        snapshotRegistry.getOrCreateSnapshot(3000);\n        shard.commitUpTo(3000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        group2.setTargetAssignmentEpoch(1);\n\n        // Set member2 to ASSIGNING state.\n        new ConsumerGroupMember.Builder(member2)\n            .setPartitionsPendingRevocation(Collections.singletonMap(Uuid.ZERO_UUID, Collections.singleton(0)))\n            .build();\n\n        snapshotRegistry.getOrCreateSnapshot(4000);\n        shard.commitUpTo(4000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        assertGaugeValue(metrics, metrics.metricName(\"group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"protocol\", \"consumer\")), 4);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.EMPTY.toString())), 0);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.ASSIGNING.toString())), 1);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.RECONCILING.toString())), 1);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.STABLE.toString())), 2);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.DEAD.toString())), 0);\n    }\n}",
                "methodCount": 4
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.44523321255057197
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testConsumerGroupStateTransitionMetrics() : void in class org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsShardTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 154,
                    "endLine": 241,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testConsumerGroupStateTransitionMetrics() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 187,
                    "endLine": 187,
                    "startColumn": 9,
                    "endColumn": 52,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 154,
                    "endLine": 241,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testConsumerGroupStateTransitionMetrics() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 187,
                    "endLine": 187,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 201,
                    "endLine": 201,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(2000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 209,
                    "endLine": 209,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(3000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                    "startLine": 222,
                    "endLine": 222,
                    "startColumn": 9,
                    "endColumn": 56,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(4000)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 610,
        "extraction_results": {
            "success": true,
            "newCommitHash": "e118294a728c0c6eae034ce5820762d2b6315112",
            "newBranchName": "extract-idempotentCreateSnapshot-testConsumerGroupStateTransitionMetrics-130af38"
        },
        "telemetry": {
            "id": "ce5cddce-6714-4407-8826-041af71ad58d",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 201,
                "lineStart": 46,
                "lineEnd": 246,
                "bodyLineStart": 46,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsShardTest.java",
                "sourceCode": "public class GroupCoordinatorMetricsShardTest {\n\n    @Test\n    public void testTimelineGaugeCounters() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(snapshotRegistry, tp);\n\n        shard.incrementNumOffsets();\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE);\n        shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD);\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        // The value should not be updated until the offset has been committed.\n        assertEquals(0, shard.numOffsets());\n        assertEquals(0, shard.numConsumerGroups());\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n\n        shard.commitUpTo(1000);\n        assertEquals(1, shard.numOffsets());\n        assertEquals(5, shard.numConsumerGroups());\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n\n        shard.decrementNumOffsets();\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE);\n        shard.decrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD);\n\n        snapshotRegistry.getOrCreateSnapshot(2000);\n        shard.commitUpTo(2000);\n        assertEquals(0, shard.numOffsets());\n        assertEquals(0, shard.numConsumerGroups());\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.DEAD));\n    }\n\n    @Test\n    public void testGenericGroupStateTransitionMetrics() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(new SnapshotRegistry(new LogContext()), tp);\n        coordinatorMetrics.activateMetricsShard(shard);\n\n        LogContext logContext = new LogContext();\n        ClassicGroup group0 = new ClassicGroup(logContext, \"groupId0\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group1 = new ClassicGroup(logContext, \"groupId1\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group2 = new ClassicGroup(logContext, \"groupId2\", EMPTY, Time.SYSTEM, shard);\n        ClassicGroup group3 = new ClassicGroup(logContext, \"groupId3\", EMPTY, Time.SYSTEM, shard);\n\n        IntStream.range(0, 4).forEach(__ -> shard.incrementNumClassicGroups(EMPTY));\n\n        assertEquals(4, shard.numClassicGroups());\n\n        group0.transitionTo(PREPARING_REBALANCE);\n        group0.transitionTo(COMPLETING_REBALANCE);\n        group1.transitionTo(PREPARING_REBALANCE);\n        group2.transitionTo(DEAD);\n\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.EMPTY));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.DEAD));\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.STABLE));\n\n        group0.transitionTo(STABLE);\n        group1.transitionTo(COMPLETING_REBALANCE);\n        group3.transitionTo(DEAD);\n\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.EMPTY));\n        assertEquals(0, shard.numClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        assertEquals(2, shard.numClassicGroups(ClassicGroupState.DEAD));\n        assertEquals(1, shard.numClassicGroups(ClassicGroupState.STABLE));\n\n        assertGaugeValue(\n            metrics,\n            metrics.metricName(\"group-count\", \"group-coordinator-metrics\", Collections.singletonMap(\"protocol\", \"classic\")),\n            4\n        );\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroups\"), 4);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsEmpty\"), 0);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsPreparingRebalance\"), 0);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsCompletingRebalance\"), 1);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsDead\"), 2);\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroupsStable\"), 1);\n    }\n\n    @Test\n    public void testConsumerGroupStateTransitionMetrics() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TopicPartition tp = new TopicPartition(\"__consumer_offsets\", 0);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(snapshotRegistry, tp);\n        coordinatorMetrics.activateMetricsShard(shard);\n\n        ConsumerGroup group0 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-0\",\n            shard\n        );\n        ConsumerGroup group1 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-1\",\n            shard\n        );\n        ConsumerGroup group2 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-2\",\n            shard\n        );\n        ConsumerGroup group3 = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-3\",\n            shard\n        );\n\n        IntStream.range(0, 4).forEach(__ -> shard.incrementNumConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        shard.commitUpTo(1000);\n        assertEquals(4, shard.numConsumerGroups());\n        assertEquals(4, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n\n        ConsumerGroupMember member0 = group0.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member1 = group1.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member2 = group2.getOrMaybeCreateMember(\"member-id\", true);\n        ConsumerGroupMember member3 = group3.getOrMaybeCreateMember(\"member-id\", true);\n        group0.updateMember(member0);\n        group1.updateMember(member1);\n        group2.updateMember(member2);\n        group3.updateMember(member3);\n\n        snapshotRegistry.getOrCreateSnapshot(2000);\n        shard.commitUpTo(2000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(4, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        group2.setGroupEpoch(1);\n        group3.setGroupEpoch(1);\n\n        snapshotRegistry.getOrCreateSnapshot(3000);\n        shard.commitUpTo(3000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        group2.setTargetAssignmentEpoch(1);\n\n        // Set member2 to ASSIGNING state.\n        new ConsumerGroupMember.Builder(member2)\n            .setPartitionsPendingRevocation(Collections.singletonMap(Uuid.ZERO_UUID, Collections.singleton(0)))\n            .build();\n\n        snapshotRegistry.getOrCreateSnapshot(4000);\n        shard.commitUpTo(4000);\n        assertEquals(0, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.EMPTY));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.ASSIGNING));\n        assertEquals(1, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.RECONCILING));\n        assertEquals(2, shard.numConsumerGroups(ConsumerGroup.ConsumerGroupState.STABLE));\n\n        assertGaugeValue(metrics, metrics.metricName(\"group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"protocol\", \"consumer\")), 4);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.EMPTY.toString())), 0);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.ASSIGNING.toString())), 1);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.RECONCILING.toString())), 1);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.STABLE.toString())), 2);\n        assertGaugeValue(metrics, metrics.metricName(\"consumer-group-count\", \"group-coordinator-metrics\",\n            Collections.singletonMap(\"state\", ConsumerGroup.ConsumerGroupState.DEAD.toString())), 0);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(1000);\n    }\n}",
                "methodCount": 4
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.44302328300117577
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public aggregateShards() : void in class org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 120,
                    "endLine": 172,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public aggregateShards() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 157,
                    "endLine": 157,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 120,
                    "endLine": 172,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public aggregateShards() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 157,
                    "endLine": 157,
                    "startColumn": 9,
                    "endColumn": 57,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry0.idempotentCreateSnapshot(1000)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                    "startLine": 158,
                    "endLine": 158,
                    "startColumn": 9,
                    "endColumn": 57,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry1.idempotentCreateSnapshot(1500)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 611,
        "extraction_results": {
            "success": true,
            "newCommitHash": "f76c771f07e85852f1fca4bd5e7c2e881fe59b87",
            "newBranchName": "extract-idempotentCreateSnapshot-aggregateShards-130af38"
        },
        "telemetry": {
            "id": "83142edc-4010-492a-a2eb-d064f1f19406",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 159,
                "lineStart": 50,
                "lineEnd": 208,
                "bodyLineStart": 50,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/metrics/GroupCoordinatorMetricsTest.java",
                "sourceCode": "public class GroupCoordinatorMetricsTest {\n\n    @Test\n    public void testMetricNames() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n\n        HashSet<org.apache.kafka.common.MetricName> expectedMetrics = new HashSet<>(Arrays.asList(\n            metrics.metricName(\"offset-commit-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-commit-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-expiration-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-expiration-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-deletion-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"offset-deletion-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"group-completed-rebalance-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"group-completed-rebalance-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"consumer-group-rebalance-rate\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\"consumer-group-rebalance-count\", GroupCoordinatorMetrics.METRICS_GROUP),\n            metrics.metricName(\n                \"group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"protocol\", \"classic\")),\n            metrics.metricName(\n                \"group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"protocol\", \"consumer\")),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.EMPTY.toString())),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.ASSIGNING.toString())),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.RECONCILING.toString())),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.STABLE.toString())),\n            metrics.metricName(\n                \"consumer-group-count\",\n                GroupCoordinatorMetrics.METRICS_GROUP,\n                Collections.singletonMap(\"state\", ConsumerGroupState.DEAD.toString()))\n        ));\n\n        try {\n            try (GroupCoordinatorMetrics ignored = new GroupCoordinatorMetrics(registry, metrics)) {\n                HashSet<String> expectedRegistry = new HashSet<>(Arrays.asList(\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumOffsets\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroups\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsPreparingRebalance\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsCompletingRebalance\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsStable\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsDead\",\n                    \"kafka.coordinator.group:type=GroupMetadataManager,name=NumGroupsEmpty\"\n                ));\n\n                assertMetricsForTypeEqual(registry, \"kafka.coordinator.group\", expectedRegistry);\n                expectedMetrics.forEach(metricName -> assertTrue(metrics.metrics().containsKey(metricName)));\n            }\n            assertMetricsForTypeEqual(registry, \"kafka.coordinator.group\", Collections.emptySet());\n            expectedMetrics.forEach(metricName -> assertFalse(metrics.metrics().containsKey(metricName)));\n        } finally {\n            registry.shutdown();\n        }\n    }\n\n    @Test\n    public void aggregateShards() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Metrics metrics = new Metrics();\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        SnapshotRegistry snapshotRegistry0 = new SnapshotRegistry(new LogContext());\n        SnapshotRegistry snapshotRegistry1 = new SnapshotRegistry(new LogContext());\n        TopicPartition tp0 = new TopicPartition(\"__consumer_offsets\", 0);\n        TopicPartition tp1 = new TopicPartition(\"__consumer_offsets\", 1);\n        GroupCoordinatorMetricsShard shard0 = coordinatorMetrics.newMetricsShard(snapshotRegistry0, tp0);\n        GroupCoordinatorMetricsShard shard1 = coordinatorMetrics.newMetricsShard(snapshotRegistry1, tp1);\n        coordinatorMetrics.activateMetricsShard(shard0);\n        coordinatorMetrics.activateMetricsShard(shard1);\n\n        IntStream.range(0, 5).forEach(__ -> shard0.incrementNumClassicGroups(ClassicGroupState.PREPARING_REBALANCE));\n        IntStream.range(0, 1).forEach(__ -> shard0.decrementNumClassicGroups(ClassicGroupState.COMPLETING_REBALANCE));\n        IntStream.range(0, 5).forEach(__ -> shard1.incrementNumClassicGroups(ClassicGroupState.STABLE));\n        IntStream.range(0, 4).forEach(__ -> shard1.incrementNumClassicGroups(ClassicGroupState.DEAD));\n        IntStream.range(0, 4).forEach(__ -> shard1.decrementNumClassicGroups(ClassicGroupState.EMPTY));\n\n        IntStream.range(0, 5).forEach(__ -> shard0.incrementNumConsumerGroups(ConsumerGroupState.ASSIGNING));\n        IntStream.range(0, 5).forEach(__ -> shard1.incrementNumConsumerGroups(ConsumerGroupState.RECONCILING));\n        IntStream.range(0, 3).forEach(__ -> shard1.decrementNumConsumerGroups(ConsumerGroupState.DEAD));\n\n        IntStream.range(0, 6).forEach(__ -> shard0.incrementNumOffsets());\n        IntStream.range(0, 2).forEach(__ -> shard1.incrementNumOffsets());\n        IntStream.range(0, 1).forEach(__ -> shard1.decrementNumOffsets());\n\n        assertEquals(4, shard0.numClassicGroups());\n        assertEquals(5, shard1.numClassicGroups());\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumGroups\"), 9);\n        assertGaugeValue(\n            metrics,\n            metrics.metricName(\"group-count\", METRICS_GROUP, Collections.singletonMap(\"protocol\", \"classic\")),\n            9\n        );\n\n        idempotentCreateSnapshot(snapshotRegistry0);\n        snapshotRegistry1.getOrCreateSnapshot(1500);\n        shard0.commitUpTo(1000);\n        shard1.commitUpTo(1500);\n\n        assertEquals(5, shard0.numConsumerGroups());\n        assertEquals(2, shard1.numConsumerGroups());\n        assertEquals(6, shard0.numOffsets());\n        assertEquals(1, shard1.numOffsets());\n        assertGaugeValue(\n            metrics,\n            metrics.metricName(\"group-count\", METRICS_GROUP, Collections.singletonMap(\"protocol\", \"consumer\")),\n            7\n        );\n        assertGaugeValue(registry, metricName(\"GroupMetadataManager\", \"NumOffsets\"), 7);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0) {\n        snapshotRegistry0.getOrCreateSnapshot(1000);\n    }\n\n    @Test\n    public void testGlobalSensors() {\n        MetricsRegistry registry = new MetricsRegistry();\n        Time time = new MockTime();\n        Metrics metrics = new Metrics(time);\n        GroupCoordinatorMetrics coordinatorMetrics = new GroupCoordinatorMetrics(registry, metrics);\n        GroupCoordinatorMetricsShard shard = coordinatorMetrics.newMetricsShard(\n            new SnapshotRegistry(new LogContext()), new TopicPartition(\"__consumer_offsets\", 0)\n        );\n\n        shard.record(CLASSIC_GROUP_COMPLETED_REBALANCES_SENSOR_NAME, 10);\n        assertMetricValue(metrics, metrics.metricName(\"group-completed-rebalance-rate\", GroupCoordinatorMetrics.METRICS_GROUP), 1.0 / 3.0);\n        assertMetricValue(metrics, metrics.metricName(\"group-completed-rebalance-count\", GroupCoordinatorMetrics.METRICS_GROUP), 10);\n\n        shard.record(OFFSET_COMMITS_SENSOR_NAME, 20);\n        assertMetricValue(metrics, metrics.metricName(\"offset-commit-rate\", GroupCoordinatorMetrics.METRICS_GROUP), 2.0 / 3.0);\n        assertMetricValue(metrics, metrics.metricName(\"offset-commit-count\", GroupCoordinatorMetrics.METRICS_GROUP), 20);\n\n        shard.record(OFFSET_EXPIRED_SENSOR_NAME, 30);\n        assertMetricValue(metrics, metrics.metricName(\"offset-expiration-rate\", GroupCoordinatorMetrics.METRICS_GROUP), 1.0);\n        assertMetricValue(metrics, metrics.metricName(\"offset-expiration-count\", GroupCoordinatorMetrics.METRICS_GROUP), 30);\n\n        shard.record(CONSUMER_GROUP_REBALANCES_SENSOR_NAME, 50);\n        assertMetricValue(metrics, metrics.metricName(\"consumer-group-rebalance-rate\", GroupCoordinatorMetrics.METRICS_GROUP), 5.0 / 3.0);\n        assertMetricValue(metrics, metrics.metricName(\"consumer-group-rebalance-count\", GroupCoordinatorMetrics.METRICS_GROUP), 50);\n    }\n\n    private void assertMetricValue(Metrics metrics, MetricName metricName, double val) {\n        assertEquals(val, metrics.metric(metricName).metricValue());\n    }\n}",
                "methodCount": 5
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertMetricValue",
                            "method_signature": "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "assertMetricValue",
                            "method_signature": "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry0)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4108297646013861
                },
                "private assertMetricValue(Metrics metrics, MetricName metricName, double val)": {
                    "first": {
                        "method_name": "assertMetricValue",
                        "method_signature": "private assertMetricValue(Metrics metrics, MetricName metricName, double val)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5190482215389689
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAsListedGroup() : void in class org.apache.kafka.coordinator.group.modern.consumer.ConsumerGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1104,
                    "endLine": 1121,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAsListedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1113,
                    "endLine": 1113,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1104,
                    "endLine": 1121,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAsListedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1113,
                    "endLine": 1113,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1118,
                    "endLine": 1118,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 612,
        "extraction_results": {
            "success": true,
            "newCommitHash": "8703298f378f5767ab7c5b8e0740c9b6b94fce07",
            "newBranchName": "extract-idempotentCreateSnapshot-testAsListedGroup-130af38"
        },
        "telemetry": {
            "id": "93c2fe39-f43b-4da9-a2d8-74a54b72b8d8",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1370,
                "lineStart": 74,
                "lineEnd": 1443,
                "bodyLineStart": 74,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                "sourceCode": "public class ConsumerGroupTest {\n\n    private ConsumerGroup createConsumerGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ConsumerGroup(\n            snapshotRegistry,\n            groupId,\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        // Create a member.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        consumerGroup.updateMember(member);\n\n        // Get that member back.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            consumerGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testNoStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Create a new member which is not static\n        consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        assertNull(consumerGroup.staticMember(\"instance-id\"));\n    }\n\n    @Test\n    public void testGetStaticMemberByInstanceId() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.staticMember(\"instance\"));\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n        assertEquals(member.memberId(), consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testRemoveStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n        assertNull(consumerGroup.staticMember(\"instance\"));\n        assertNull(consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsReassignedBeforeBeingRevoked() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(Collections.emptyMap())\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .setPartitionsPendingRevocation(Collections.emptyMap())\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsNotReleased() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        ConsumerGroupMember m2 = new ConsumerGroupMember.Builder(\"m2\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        // m2 should not be able to acquire foo-1 because the partition is\n        // still owned by another member.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.updateMember(m2));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        consumerGroup.removeMember(member.memberId());\n\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testWaitingOnUnreleasedPartition() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n        String memberId1 = Uuid.randomUuid().toString();\n        String memberId2 = Uuid.randomUuid().toString();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        consumerGroup.updateTargetAssignment(memberId1, new Assignment(mkAssignment(\n            mkTopicAssignment(fooTopicId, 1, 2, 3),\n            mkTopicAssignment(zarTopicId, 7, 8, 9)\n        )));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(memberId1)\n            .setMemberEpoch(10)\n            .setState(MemberState.UNRELEASED_PARTITIONS)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertFalse(consumerGroup.waitingOnUnreleasedPartition(member1));\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(memberId2)\n            .setMemberEpoch(10)\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 7)))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertTrue(consumerGroup.waitingOnUnreleasedPartition(member1));\n    }\n\n    @Test\n    public void testGroupState() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member2);\n        consumerGroup.setGroupEpoch(2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        consumerGroup.setTargetAssignmentEpoch(2);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        // Member 2 is not stable so the group stays in reconciling state.\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.UNREVOKED_PARTITIONS)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.UNREVOKED_PARTITIONS, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n\n        consumerGroup.removeMember(\"member1\");\n        consumerGroup.removeMember(\"member2\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n\n        assertEquals(Group.GroupType.parse(\"classic\"), Group.GroupType.CLASSIC);\n\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Consumer\"), Group.GroupType.CONSUMER);\n\n        // Test with invalid group type.\n        assertEquals(Group.GroupType.parse(\"Invalid\"), Group.GroupType.UNKNOWN);\n    }\n\n    @Test\n    public void testPreferredServerAssignor() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(\"uniform\")\n            .build();\n\n        // The group is empty so the preferred assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member1)\n        );\n\n        // Update the group with member 1.\n        consumerGroup.updateMember(member1);\n\n        // Member 1 is in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has been removed but this is not reflected in the group yet so\n        // we pass the removed member. The assignor should be range.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member1, null)\n        );\n\n        // Member 2 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member2)\n        );\n\n        // Update the group with member 2.\n        consumerGroup.updateMember(member2);\n\n        // Member 1 and 2 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Update the group with member 3.\n        consumerGroup.updateMember(member3);\n\n        // Member 1, 2 and 3 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Members without assignors\n        ConsumerGroupMember updatedMember1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(null)\n            .build();\n\n        // Member 1 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        Optional<String> assignor = consumerGroup.computePreferredServerAssignor(member1, updatedMember1);\n        assertTrue(assignor.equals(Optional.of(\"range\")) || assignor.equals(Optional.of(\"uniform\")));\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember1);\n\n        // Member 2 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.computePreferredServerAssignor(member2, updatedMember2)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember2);\n\n        // Only member 3 is left in the group so the assignor should be uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 3 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member3, updatedMember3)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember3);\n\n        // The group is empty so the assignor should be empty as well.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        consumerGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        consumerGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        consumerGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        ConsumerGroupMember member4 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        consumerGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(snapshotRegistry, \"test-group\", metricsShard);\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        consumerGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, group.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch() + 1);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch() + 1, group.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        group.setGroupEpoch(group.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        group.requestMetadataRefresh();\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateTransactionalOffsetCommit(short version) {\n        boolean isTransactional = true;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create a member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetCommit(\"member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        group.validateOffsetCommit(\"member-id\", \"\", 0, isTransactional, version);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        boolean isTransactional = false;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create members.\n        group.updateMember(\n            new ConsumerGroupMember\n                .Builder(\"new-protocol-member-id\").build()\n        );\n        group.updateMember(\n            new ConsumerGroupMember.Builder(\"old-protocol-member-id\")\n                .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n                .build()\n        );\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        if (version >= 9) {\n            assertThrows(StaleMemberEpochException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        }\n        assertThrows(IllegalGenerationException.class, () ->\n            group.validateOffsetCommit(\"old-protocol-member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        if (version >= 9) {\n            group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version);\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version));\n        }\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE.toString(), group.stateAsString(1));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-foo\",\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n\n        // Simulate a call from the admin client without member id and member epoch.\n        group.validateOffsetFetch(null, -1, Long.MAX_VALUE);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE));\n\n        // Create a member.\n        idempotentCreateSnapshot(snapshotRegistry);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // The member does not exist at last committed offset 0.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, 0));\n\n        // The member exists but the epoch is stale when the last committed offset is not considered.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 10, Long.MAX_VALUE));\n\n        // This should succeed.\n        group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        assertDoesNotThrow(consumerGroup::validateDeleteGroup);\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        long currentTimestamp = 30000L;\n        long commitTimestamp = 20000L;\n        long offsetsRetentionMs = 10000L;\n        OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(15000L, OptionalInt.empty(), \"\", commitTimestamp, OptionalLong.empty());\n        ConsumerGroup group = new ConsumerGroup(new SnapshotRegistry(new LogContext()), \"group-id\", mock(GroupCoordinatorMetricsShard.class));\n\n        Optional<OffsetExpirationCondition> offsetExpirationCondition = group.offsetExpirationCondition();\n        assertTrue(offsetExpirationCondition.isPresent());\n\n        OffsetExpirationConditionImpl condition = (OffsetExpirationConditionImpl) offsetExpirationCondition.get();\n        assertEquals(commitTimestamp, condition.baseTimestamp().apply(offsetAndMetadata));\n        assertTrue(condition.isOffsetExpired(offsetAndMetadata, currentTimestamp, offsetsRetentionMs));\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(consumerGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(consumerGroup.isSubscribedToTopic(\"bar\"));\n\n        consumerGroup.removeMember(\"member1\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"foo\"));\n\n        consumerGroup.removeMember(\"member2\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-id-1\", mock(GroupCoordinatorMetricsShard.class));\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .setServerAssignorName(\"assignorName\")\n                .build());\n        group.updateMember(new ConsumerGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ConsumerGroupDescribeResponseData.DescribedGroup expected = new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ConsumerGroup.ConsumerGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ConsumerGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                    .setSubscribedTopicRegex(\"\"),\n                new ConsumerGroupDescribeResponseData.Member().setMemberId(\"member2\")\n                    .setSubscribedTopicRegex(\"\")\n            ));\n        ConsumerGroupDescribeResponseData.DescribedGroup actual = group.asDescribedGroup(1, \"\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testStateTransitionMetrics() {\n        // Confirm metrics is not updated when a new ConsumerGroup is created but only when the group transitions\n        // its state.\n        GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(\n            new SnapshotRegistry(new LogContext()),\n            \"group-id\",\n            metrics\n        );\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(0)).onConsumerGroupStateTransition(null, ConsumerGroup.ConsumerGroupState.EMPTY);\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.EMPTY, ConsumerGroup.ConsumerGroupState.RECONCILING);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.RECONCILING, ConsumerGroup.ConsumerGroupState.ASSIGNING);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.ASSIGNING, ConsumerGroup.ConsumerGroupState.STABLE);\n\n        consumerGroup.removeMember(\"member\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.STABLE, ConsumerGroup.ConsumerGroupState.EMPTY);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(group.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(group.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(group.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    @Test\n    public void testClassicMembersSupportedProtocols() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> rangeProtocol = new ArrayList<>();\n        rangeProtocol.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> roundRobinAndRangeProtocols = new ArrayList<>();\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"roundrobin\")\n            .setMetadata(new byte[0]));\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(1, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"range\", \"sticky\"))));\n        assertFalse(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertFalse(consumerGroup.classicMembersSupportedProtocols().containsKey(\"roundrobin\"));\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n    }\n\n    @Test\n    public void testNumClassicProtocolMembers() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> protocols = new ArrayList<>();\n        protocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        // The group has member 1 (using the classic protocol).\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n\n        // The group has member 1 (using the classic protocol) and member 2 (using the consumer protocol).\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-1\"));\n        assertTrue(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the consumer protocol) and member 3 (using the consumer protocol).\n        consumerGroup.removeMember(member1.memberId());\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member-3\")\n            .build();\n        consumerGroup.updateMember(member3);\n        assertEquals(0, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the classic protocol).\n        consumerGroup.removeMember(member2.memberId());\n        member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n    }\n}",
                "methodCount": 34
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3480612588264244
                },
                "private createConsumerGroup(String groupId)": {
                    "first": {
                        "method_name": "createConsumerGroup",
                        "method_signature": "private createConsumerGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.589256282685003
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateTransactionalOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7076402683867435
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7250329443768034
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testValidateOffsetFetch() : void in class org.apache.kafka.coordinator.group.modern.consumer.ConsumerGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1123,
                    "endLine": 1153,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testValidateOffsetFetch() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1140,
                    "endLine": 1140,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1123,
                    "endLine": 1153,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testValidateOffsetFetch() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1140,
                    "endLine": 1140,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 613,
        "extraction_results": {
            "success": true,
            "newCommitHash": "34bebf3af2ff8ca4a4ac8a279765fc681c4b95ea",
            "newBranchName": "extract-idempotentCreateSnapshot-testValidateOffsetFetch-130af38"
        },
        "telemetry": {
            "id": "b08cbac1-c6c8-44f3-843d-0f6a3c0d4a7d",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1370,
                "lineStart": 74,
                "lineEnd": 1443,
                "bodyLineStart": 74,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                "sourceCode": "public class ConsumerGroupTest {\n\n    private ConsumerGroup createConsumerGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ConsumerGroup(\n            snapshotRegistry,\n            groupId,\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        // Create a member.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        consumerGroup.updateMember(member);\n\n        // Get that member back.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            consumerGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testNoStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Create a new member which is not static\n        consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        assertNull(consumerGroup.staticMember(\"instance-id\"));\n    }\n\n    @Test\n    public void testGetStaticMemberByInstanceId() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.staticMember(\"instance\"));\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n        assertEquals(member.memberId(), consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testRemoveStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n        assertNull(consumerGroup.staticMember(\"instance\"));\n        assertNull(consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsReassignedBeforeBeingRevoked() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(Collections.emptyMap())\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .setPartitionsPendingRevocation(Collections.emptyMap())\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsNotReleased() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        ConsumerGroupMember m2 = new ConsumerGroupMember.Builder(\"m2\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        // m2 should not be able to acquire foo-1 because the partition is\n        // still owned by another member.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.updateMember(m2));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        consumerGroup.removeMember(member.memberId());\n\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testWaitingOnUnreleasedPartition() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n        String memberId1 = Uuid.randomUuid().toString();\n        String memberId2 = Uuid.randomUuid().toString();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        consumerGroup.updateTargetAssignment(memberId1, new Assignment(mkAssignment(\n            mkTopicAssignment(fooTopicId, 1, 2, 3),\n            mkTopicAssignment(zarTopicId, 7, 8, 9)\n        )));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(memberId1)\n            .setMemberEpoch(10)\n            .setState(MemberState.UNRELEASED_PARTITIONS)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertFalse(consumerGroup.waitingOnUnreleasedPartition(member1));\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(memberId2)\n            .setMemberEpoch(10)\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 7)))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertTrue(consumerGroup.waitingOnUnreleasedPartition(member1));\n    }\n\n    @Test\n    public void testGroupState() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member2);\n        consumerGroup.setGroupEpoch(2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        consumerGroup.setTargetAssignmentEpoch(2);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        // Member 2 is not stable so the group stays in reconciling state.\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.UNREVOKED_PARTITIONS)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.UNREVOKED_PARTITIONS, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n\n        consumerGroup.removeMember(\"member1\");\n        consumerGroup.removeMember(\"member2\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n\n        assertEquals(Group.GroupType.parse(\"classic\"), Group.GroupType.CLASSIC);\n\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Consumer\"), Group.GroupType.CONSUMER);\n\n        // Test with invalid group type.\n        assertEquals(Group.GroupType.parse(\"Invalid\"), Group.GroupType.UNKNOWN);\n    }\n\n    @Test\n    public void testPreferredServerAssignor() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(\"uniform\")\n            .build();\n\n        // The group is empty so the preferred assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member1)\n        );\n\n        // Update the group with member 1.\n        consumerGroup.updateMember(member1);\n\n        // Member 1 is in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has been removed but this is not reflected in the group yet so\n        // we pass the removed member. The assignor should be range.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member1, null)\n        );\n\n        // Member 2 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member2)\n        );\n\n        // Update the group with member 2.\n        consumerGroup.updateMember(member2);\n\n        // Member 1 and 2 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Update the group with member 3.\n        consumerGroup.updateMember(member3);\n\n        // Member 1, 2 and 3 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Members without assignors\n        ConsumerGroupMember updatedMember1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(null)\n            .build();\n\n        // Member 1 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        Optional<String> assignor = consumerGroup.computePreferredServerAssignor(member1, updatedMember1);\n        assertTrue(assignor.equals(Optional.of(\"range\")) || assignor.equals(Optional.of(\"uniform\")));\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember1);\n\n        // Member 2 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.computePreferredServerAssignor(member2, updatedMember2)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember2);\n\n        // Only member 3 is left in the group so the assignor should be uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 3 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member3, updatedMember3)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember3);\n\n        // The group is empty so the assignor should be empty as well.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        consumerGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        consumerGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        consumerGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        ConsumerGroupMember member4 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        consumerGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(snapshotRegistry, \"test-group\", metricsShard);\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        consumerGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, group.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch() + 1);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch() + 1, group.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        group.setGroupEpoch(group.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        group.requestMetadataRefresh();\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateTransactionalOffsetCommit(short version) {\n        boolean isTransactional = true;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create a member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetCommit(\"member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        group.validateOffsetCommit(\"member-id\", \"\", 0, isTransactional, version);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        boolean isTransactional = false;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create members.\n        group.updateMember(\n            new ConsumerGroupMember\n                .Builder(\"new-protocol-member-id\").build()\n        );\n        group.updateMember(\n            new ConsumerGroupMember.Builder(\"old-protocol-member-id\")\n                .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n                .build()\n        );\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        if (version >= 9) {\n            assertThrows(StaleMemberEpochException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        }\n        assertThrows(IllegalGenerationException.class, () ->\n            group.validateOffsetCommit(\"old-protocol-member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        if (version >= 9) {\n            group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version);\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version));\n        }\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE.toString(), group.stateAsString(1));\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-foo\",\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n\n        // Simulate a call from the admin client without member id and member epoch.\n        group.validateOffsetFetch(null, -1, Long.MAX_VALUE);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE));\n\n        // Create a member.\n        idempotentCreateSnapshot(snapshotRegistry);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // The member does not exist at last committed offset 0.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, 0));\n\n        // The member exists but the epoch is stale when the last committed offset is not considered.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 10, Long.MAX_VALUE));\n\n        // This should succeed.\n        group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        assertDoesNotThrow(consumerGroup::validateDeleteGroup);\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        long currentTimestamp = 30000L;\n        long commitTimestamp = 20000L;\n        long offsetsRetentionMs = 10000L;\n        OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(15000L, OptionalInt.empty(), \"\", commitTimestamp, OptionalLong.empty());\n        ConsumerGroup group = new ConsumerGroup(new SnapshotRegistry(new LogContext()), \"group-id\", mock(GroupCoordinatorMetricsShard.class));\n\n        Optional<OffsetExpirationCondition> offsetExpirationCondition = group.offsetExpirationCondition();\n        assertTrue(offsetExpirationCondition.isPresent());\n\n        OffsetExpirationConditionImpl condition = (OffsetExpirationConditionImpl) offsetExpirationCondition.get();\n        assertEquals(commitTimestamp, condition.baseTimestamp().apply(offsetAndMetadata));\n        assertTrue(condition.isOffsetExpired(offsetAndMetadata, currentTimestamp, offsetsRetentionMs));\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(consumerGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(consumerGroup.isSubscribedToTopic(\"bar\"));\n\n        consumerGroup.removeMember(\"member1\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"foo\"));\n\n        consumerGroup.removeMember(\"member2\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-id-1\", mock(GroupCoordinatorMetricsShard.class));\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .setServerAssignorName(\"assignorName\")\n                .build());\n        group.updateMember(new ConsumerGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ConsumerGroupDescribeResponseData.DescribedGroup expected = new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ConsumerGroup.ConsumerGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ConsumerGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                    .setSubscribedTopicRegex(\"\"),\n                new ConsumerGroupDescribeResponseData.Member().setMemberId(\"member2\")\n                    .setSubscribedTopicRegex(\"\")\n            ));\n        ConsumerGroupDescribeResponseData.DescribedGroup actual = group.asDescribedGroup(1, \"\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testStateTransitionMetrics() {\n        // Confirm metrics is not updated when a new ConsumerGroup is created but only when the group transitions\n        // its state.\n        GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(\n            new SnapshotRegistry(new LogContext()),\n            \"group-id\",\n            metrics\n        );\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(0)).onConsumerGroupStateTransition(null, ConsumerGroup.ConsumerGroupState.EMPTY);\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.EMPTY, ConsumerGroup.ConsumerGroupState.RECONCILING);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.RECONCILING, ConsumerGroup.ConsumerGroupState.ASSIGNING);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.ASSIGNING, ConsumerGroup.ConsumerGroupState.STABLE);\n\n        consumerGroup.removeMember(\"member\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.STABLE, ConsumerGroup.ConsumerGroupState.EMPTY);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(group.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(group.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(group.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    @Test\n    public void testClassicMembersSupportedProtocols() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> rangeProtocol = new ArrayList<>();\n        rangeProtocol.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> roundRobinAndRangeProtocols = new ArrayList<>();\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"roundrobin\")\n            .setMetadata(new byte[0]));\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(1, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"range\", \"sticky\"))));\n        assertFalse(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertFalse(consumerGroup.classicMembersSupportedProtocols().containsKey(\"roundrobin\"));\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n    }\n\n    @Test\n    public void testNumClassicProtocolMembers() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> protocols = new ArrayList<>();\n        protocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        // The group has member 1 (using the classic protocol).\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n\n        // The group has member 1 (using the classic protocol) and member 2 (using the consumer protocol).\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-1\"));\n        assertTrue(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the consumer protocol) and member 3 (using the consumer protocol).\n        consumerGroup.removeMember(member1.memberId());\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member-3\")\n            .build();\n        consumerGroup.updateMember(member3);\n        assertEquals(0, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the classic protocol).\n        consumerGroup.removeMember(member2.memberId());\n        member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n    }\n}",
                "methodCount": 34
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.34690363445902683
                },
                "private createConsumerGroup(String groupId)": {
                    "first": {
                        "method_name": "createConsumerGroup",
                        "method_signature": "private createConsumerGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5865237238601102
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateTransactionalOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7078838593885659
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7263013023825688
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAsDescribedGroup() : void in class org.apache.kafka.coordinator.group.modern.consumer.ConsumerGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1243,
                    "endLine": 1276,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAsDescribedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1247,
                    "endLine": 1247,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1243,
                    "endLine": 1276,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAsDescribedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1247,
                    "endLine": 1247,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1256,
                    "endLine": 1256,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 614,
        "extraction_results": {
            "success": true,
            "newCommitHash": "7226b58c84508faac4518b8bf061266c17e4bdf7",
            "newBranchName": "extract-idempotentCreateSnapshot-testAsDescribedGroup-130af38"
        },
        "telemetry": {
            "id": "1701797e-e7fd-44d9-90f0-10be1959c76c",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1370,
                "lineStart": 74,
                "lineEnd": 1443,
                "bodyLineStart": 74,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                "sourceCode": "public class ConsumerGroupTest {\n\n    private ConsumerGroup createConsumerGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ConsumerGroup(\n            snapshotRegistry,\n            groupId,\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        // Create a member.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        consumerGroup.updateMember(member);\n\n        // Get that member back.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            consumerGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testNoStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Create a new member which is not static\n        consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        assertNull(consumerGroup.staticMember(\"instance-id\"));\n    }\n\n    @Test\n    public void testGetStaticMemberByInstanceId() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.staticMember(\"instance\"));\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n        assertEquals(member.memberId(), consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testRemoveStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n        assertNull(consumerGroup.staticMember(\"instance\"));\n        assertNull(consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsReassignedBeforeBeingRevoked() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(Collections.emptyMap())\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .setPartitionsPendingRevocation(Collections.emptyMap())\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsNotReleased() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        ConsumerGroupMember m2 = new ConsumerGroupMember.Builder(\"m2\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        // m2 should not be able to acquire foo-1 because the partition is\n        // still owned by another member.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.updateMember(m2));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        consumerGroup.removeMember(member.memberId());\n\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testWaitingOnUnreleasedPartition() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n        String memberId1 = Uuid.randomUuid().toString();\n        String memberId2 = Uuid.randomUuid().toString();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        consumerGroup.updateTargetAssignment(memberId1, new Assignment(mkAssignment(\n            mkTopicAssignment(fooTopicId, 1, 2, 3),\n            mkTopicAssignment(zarTopicId, 7, 8, 9)\n        )));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(memberId1)\n            .setMemberEpoch(10)\n            .setState(MemberState.UNRELEASED_PARTITIONS)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertFalse(consumerGroup.waitingOnUnreleasedPartition(member1));\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(memberId2)\n            .setMemberEpoch(10)\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 7)))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertTrue(consumerGroup.waitingOnUnreleasedPartition(member1));\n    }\n\n    @Test\n    public void testGroupState() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member2);\n        consumerGroup.setGroupEpoch(2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        consumerGroup.setTargetAssignmentEpoch(2);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        // Member 2 is not stable so the group stays in reconciling state.\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.UNREVOKED_PARTITIONS)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.UNREVOKED_PARTITIONS, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n\n        consumerGroup.removeMember(\"member1\");\n        consumerGroup.removeMember(\"member2\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n\n        assertEquals(Group.GroupType.parse(\"classic\"), Group.GroupType.CLASSIC);\n\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Consumer\"), Group.GroupType.CONSUMER);\n\n        // Test with invalid group type.\n        assertEquals(Group.GroupType.parse(\"Invalid\"), Group.GroupType.UNKNOWN);\n    }\n\n    @Test\n    public void testPreferredServerAssignor() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(\"uniform\")\n            .build();\n\n        // The group is empty so the preferred assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member1)\n        );\n\n        // Update the group with member 1.\n        consumerGroup.updateMember(member1);\n\n        // Member 1 is in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has been removed but this is not reflected in the group yet so\n        // we pass the removed member. The assignor should be range.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member1, null)\n        );\n\n        // Member 2 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member2)\n        );\n\n        // Update the group with member 2.\n        consumerGroup.updateMember(member2);\n\n        // Member 1 and 2 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Update the group with member 3.\n        consumerGroup.updateMember(member3);\n\n        // Member 1, 2 and 3 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Members without assignors\n        ConsumerGroupMember updatedMember1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(null)\n            .build();\n\n        // Member 1 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        Optional<String> assignor = consumerGroup.computePreferredServerAssignor(member1, updatedMember1);\n        assertTrue(assignor.equals(Optional.of(\"range\")) || assignor.equals(Optional.of(\"uniform\")));\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember1);\n\n        // Member 2 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.computePreferredServerAssignor(member2, updatedMember2)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember2);\n\n        // Only member 3 is left in the group so the assignor should be uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 3 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member3, updatedMember3)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember3);\n\n        // The group is empty so the assignor should be empty as well.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        consumerGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        consumerGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        consumerGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        ConsumerGroupMember member4 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        consumerGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(snapshotRegistry, \"test-group\", metricsShard);\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        consumerGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, group.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch() + 1);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch() + 1, group.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        group.setGroupEpoch(group.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        group.requestMetadataRefresh();\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateTransactionalOffsetCommit(short version) {\n        boolean isTransactional = true;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create a member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetCommit(\"member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        group.validateOffsetCommit(\"member-id\", \"\", 0, isTransactional, version);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        boolean isTransactional = false;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create members.\n        group.updateMember(\n            new ConsumerGroupMember\n                .Builder(\"new-protocol-member-id\").build()\n        );\n        group.updateMember(\n            new ConsumerGroupMember.Builder(\"old-protocol-member-id\")\n                .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n                .build()\n        );\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        if (version >= 9) {\n            assertThrows(StaleMemberEpochException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        }\n        assertThrows(IllegalGenerationException.class, () ->\n            group.validateOffsetCommit(\"old-protocol-member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        if (version >= 9) {\n            group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version);\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version));\n        }\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE.toString(), group.stateAsString(1));\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-foo\",\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n\n        // Simulate a call from the admin client without member id and member epoch.\n        group.validateOffsetFetch(null, -1, Long.MAX_VALUE);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE));\n\n        // Create a member.\n        idempotentCreateSnapshot(snapshotRegistry);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // The member does not exist at last committed offset 0.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, 0));\n\n        // The member exists but the epoch is stale when the last committed offset is not considered.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 10, Long.MAX_VALUE));\n\n        // This should succeed.\n        group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        assertDoesNotThrow(consumerGroup::validateDeleteGroup);\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        long currentTimestamp = 30000L;\n        long commitTimestamp = 20000L;\n        long offsetsRetentionMs = 10000L;\n        OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(15000L, OptionalInt.empty(), \"\", commitTimestamp, OptionalLong.empty());\n        ConsumerGroup group = new ConsumerGroup(new SnapshotRegistry(new LogContext()), \"group-id\", mock(GroupCoordinatorMetricsShard.class));\n\n        Optional<OffsetExpirationCondition> offsetExpirationCondition = group.offsetExpirationCondition();\n        assertTrue(offsetExpirationCondition.isPresent());\n\n        OffsetExpirationConditionImpl condition = (OffsetExpirationConditionImpl) offsetExpirationCondition.get();\n        assertEquals(commitTimestamp, condition.baseTimestamp().apply(offsetAndMetadata));\n        assertTrue(condition.isOffsetExpired(offsetAndMetadata, currentTimestamp, offsetsRetentionMs));\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(consumerGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(consumerGroup.isSubscribedToTopic(\"bar\"));\n\n        consumerGroup.removeMember(\"member1\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"foo\"));\n\n        consumerGroup.removeMember(\"member2\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-id-1\", mock(GroupCoordinatorMetricsShard.class));\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .setServerAssignorName(\"assignorName\")\n                .build());\n        group.updateMember(new ConsumerGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ConsumerGroupDescribeResponseData.DescribedGroup expected = new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ConsumerGroup.ConsumerGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ConsumerGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                    .setSubscribedTopicRegex(\"\"),\n                new ConsumerGroupDescribeResponseData.Member().setMemberId(\"member2\")\n                    .setSubscribedTopicRegex(\"\")\n            ));\n        ConsumerGroupDescribeResponseData.DescribedGroup actual = group.asDescribedGroup(1, \"\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testStateTransitionMetrics() {\n        // Confirm metrics is not updated when a new ConsumerGroup is created but only when the group transitions\n        // its state.\n        GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(\n            new SnapshotRegistry(new LogContext()),\n            \"group-id\",\n            metrics\n        );\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(0)).onConsumerGroupStateTransition(null, ConsumerGroup.ConsumerGroupState.EMPTY);\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.EMPTY, ConsumerGroup.ConsumerGroupState.RECONCILING);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.RECONCILING, ConsumerGroup.ConsumerGroupState.ASSIGNING);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.ASSIGNING, ConsumerGroup.ConsumerGroupState.STABLE);\n\n        consumerGroup.removeMember(\"member\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.STABLE, ConsumerGroup.ConsumerGroupState.EMPTY);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(group.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(group.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(group.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    @Test\n    public void testClassicMembersSupportedProtocols() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> rangeProtocol = new ArrayList<>();\n        rangeProtocol.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> roundRobinAndRangeProtocols = new ArrayList<>();\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"roundrobin\")\n            .setMetadata(new byte[0]));\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(1, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"range\", \"sticky\"))));\n        assertFalse(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertFalse(consumerGroup.classicMembersSupportedProtocols().containsKey(\"roundrobin\"));\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n    }\n\n    @Test\n    public void testNumClassicProtocolMembers() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> protocols = new ArrayList<>();\n        protocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        // The group has member 1 (using the classic protocol).\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n\n        // The group has member 1 (using the classic protocol) and member 2 (using the consumer protocol).\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-1\"));\n        assertTrue(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the consumer protocol) and member 3 (using the consumer protocol).\n        consumerGroup.removeMember(member1.memberId());\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member-3\")\n            .build();\n        consumerGroup.updateMember(member3);\n        assertEquals(0, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the classic protocol).\n        consumerGroup.removeMember(member2.memberId());\n        member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n    }\n}",
                "methodCount": 34
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3485749109925722
                },
                "private createConsumerGroup(String groupId)": {
                    "first": {
                        "method_name": "createConsumerGroup",
                        "method_signature": "private createConsumerGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5886221923120528
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateTransactionalOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7095486249473654
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7269159835694022
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testIsInStatesCaseInsensitive() : void in class org.apache.kafka.coordinator.group.modern.consumer.ConsumerGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1318,
                    "endLine": 1338,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testIsInStatesCaseInsensitive() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1327,
                    "endLine": 1327,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1318,
                    "endLine": 1338,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testIsInStatesCaseInsensitive() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1327,
                    "endLine": 1327,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                    "startLine": 1334,
                    "endLine": 1334,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 615,
        "extraction_results": {
            "success": true,
            "newCommitHash": "2afeb4dfa4330fb206a36754f2ee15817ba410df",
            "newBranchName": "extract-idempotentCreateSnapshot-testIsInStatesCaseInsensitive-130af38"
        },
        "telemetry": {
            "id": "8a4ff3d5-9a97-4396-8047-b021d29262be",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1370,
                "lineStart": 74,
                "lineEnd": 1443,
                "bodyLineStart": 74,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/consumer/ConsumerGroupTest.java",
                "sourceCode": "public class ConsumerGroupTest {\n\n    private ConsumerGroup createConsumerGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ConsumerGroup(\n            snapshotRegistry,\n            groupId,\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        // Create a member.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        consumerGroup.updateMember(member);\n\n        // Get that member back.\n        member = consumerGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            consumerGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testNoStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Create a new member which is not static\n        consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        assertNull(consumerGroup.staticMember(\"instance-id\"));\n    }\n\n    @Test\n    public void testGetStaticMemberByInstanceId() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(member, consumerGroup.staticMember(\"instance\"));\n        assertEquals(member, consumerGroup.getOrMaybeCreateMember(\"member\", false));\n        assertEquals(member.memberId(), consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = consumerGroup.getOrMaybeCreateMember(\"member\", true);\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testRemoveStaticMember() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .setInstanceId(\"instance\")\n            .build();\n\n        consumerGroup.updateMember(member);\n        assertTrue(consumerGroup.hasMember(\"member\"));\n\n        consumerGroup.removeMember(\"member\");\n        assertFalse(consumerGroup.hasMember(\"member\"));\n        assertNull(consumerGroup.staticMember(\"instance\"));\n        assertNull(consumerGroup.staticMemberId(\"instance\"));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(11, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsReassignedBeforeBeingRevoked() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(Collections.emptyMap())\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n\n        member = new ConsumerGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .setPartitionsPendingRevocation(Collections.emptyMap())\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(11, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpochWhenPartitionIsNotReleased() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        ConsumerGroupMember m2 = new ConsumerGroupMember.Builder(\"m2\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        // m2 should not be able to acquire foo-1 because the partition is\n        // still owned by another member.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.updateMember(m2));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ConsumerGroupMember m1 = new ConsumerGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        consumerGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> consumerGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        ConsumerGroupMember member;\n\n        member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(10, consumerGroup.currentPartitionEpoch(barTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 9));\n\n        consumerGroup.removeMember(member.memberId());\n\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 4));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 5));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(zarTopicId, 6));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 7));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 8));\n        assertEquals(-1, consumerGroup.currentPartitionEpoch(fooTopicId, 9));\n    }\n\n    @Test\n    public void testWaitingOnUnreleasedPartition() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n        String memberId1 = Uuid.randomUuid().toString();\n        String memberId2 = Uuid.randomUuid().toString();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        consumerGroup.updateTargetAssignment(memberId1, new Assignment(mkAssignment(\n            mkTopicAssignment(fooTopicId, 1, 2, 3),\n            mkTopicAssignment(zarTopicId, 7, 8, 9)\n        )));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(memberId1)\n            .setMemberEpoch(10)\n            .setState(MemberState.UNRELEASED_PARTITIONS)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(barTopicId, 4, 5, 6)))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertFalse(consumerGroup.waitingOnUnreleasedPartition(member1));\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(memberId2)\n            .setMemberEpoch(10)\n            .setPartitionsPendingRevocation(mkAssignment(\n                mkTopicAssignment(zarTopicId, 7)))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertTrue(consumerGroup.waitingOnUnreleasedPartition(member1));\n    }\n\n    @Test\n    public void testGroupState() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member2);\n        consumerGroup.setGroupEpoch(2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n\n        consumerGroup.setTargetAssignmentEpoch(2);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        // Member 2 is not stable so the group stays in reconciling state.\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.UNREVOKED_PARTITIONS)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.UNREVOKED_PARTITIONS, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(2)\n            .setPreviousMemberEpoch(1)\n            .build();\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(MemberState.STABLE, member2.state());\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n\n        consumerGroup.removeMember(\"member1\");\n        consumerGroup.removeMember(\"member2\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n\n        assertEquals(Group.GroupType.parse(\"classic\"), Group.GroupType.CLASSIC);\n\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Consumer\"), Group.GroupType.CONSUMER);\n\n        // Test with invalid group type.\n        assertEquals(Group.GroupType.parse(\"Invalid\"), Group.GroupType.UNKNOWN);\n    }\n\n    @Test\n    public void testPreferredServerAssignor() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(\"range\")\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(\"uniform\")\n            .build();\n\n        // The group is empty so the preferred assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member1)\n        );\n\n        // Update the group with member 1.\n        consumerGroup.updateMember(member1);\n\n        // Member 1 is in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 1 has been removed but this is not reflected in the group yet so\n        // we pass the removed member. The assignor should be range.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member1, null)\n        );\n\n        // Member 2 has got an updated assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.computePreferredServerAssignor(null, member2)\n        );\n\n        // Update the group with member 2.\n        consumerGroup.updateMember(member2);\n\n        // Member 1 and 2 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Update the group with member 3.\n        consumerGroup.updateMember(member3);\n\n        // Member 1, 2 and 3 are in the group so the assignor should be range.\n        assertEquals(\n            Optional.of(\"range\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Members without assignors\n        ConsumerGroupMember updatedMember1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setServerAssignorName(null)\n            .build();\n        ConsumerGroupMember updatedMember3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setServerAssignorName(null)\n            .build();\n\n        // Member 1 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        Optional<String> assignor = consumerGroup.computePreferredServerAssignor(member1, updatedMember1);\n        assertTrue(assignor.equals(Optional.of(\"range\")) || assignor.equals(Optional.of(\"uniform\")));\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember1);\n\n        // Member 2 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be range or uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.computePreferredServerAssignor(member2, updatedMember2)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember2);\n\n        // Only member 3 is left in the group so the assignor should be uniform.\n        assertEquals(\n            Optional.of(\"uniform\"),\n            consumerGroup.preferredServerAssignor()\n        );\n\n        // Member 3 has removed it assignor but this is not reflected in the group yet so\n        // we pass the updated member. The assignor should be empty.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.computePreferredServerAssignor(member3, updatedMember3)\n        );\n\n        // Update the group.\n        consumerGroup.updateMember(updatedMember3);\n\n        // The group is empty so the assignor should be empty as well.\n        assertEquals(\n            Optional.empty(),\n            consumerGroup.preferredServerAssignor()\n        );\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        consumerGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        consumerGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        consumerGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            consumerGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        consumerGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n\n        ConsumerGroupMember member4 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        consumerGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            consumerGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(snapshotRegistry, \"test-group\", metricsShard);\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        consumerGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        consumerGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        consumerGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            consumerGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, group.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch() + 1);\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch() + 1, group.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        group.setGroupEpoch(group.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        group.setMetadataRefreshDeadline(time.milliseconds() + 1000, group.groupEpoch());\n        assertFalse(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(group.groupEpoch(), group.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        group.requestMetadataRefresh();\n        assertTrue(group.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, group.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, group.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateTransactionalOffsetCommit(short version) {\n        boolean isTransactional = true;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create a member.\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetCommit(\"member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        group.validateOffsetCommit(\"member-id\", \"\", 0, isTransactional, version);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        boolean isTransactional = false;\n        ConsumerGroup group = createConsumerGroup(\"group-foo\");\n\n        // Simulate a call from the admin client without member id and member epoch.\n        // This should pass only if the group is empty.\n        group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"member-id\", null, 0, isTransactional, version));\n\n        // Create members.\n        group.updateMember(\n            new ConsumerGroupMember\n                .Builder(\"new-protocol-member-id\").build()\n        );\n        group.updateMember(\n            new ConsumerGroupMember.Builder(\"old-protocol-member-id\")\n                .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata())\n                .build()\n        );\n\n        // A call from the admin client should fail as the group is not empty.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetCommit(\"\", \"\", -1, isTransactional, version));\n\n        // The member epoch is stale.\n        if (version >= 9) {\n            assertThrows(StaleMemberEpochException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 10, isTransactional, version));\n        }\n        assertThrows(IllegalGenerationException.class, () ->\n            group.validateOffsetCommit(\"old-protocol-member-id\", \"\", 10, isTransactional, version));\n\n        // This should succeed.\n        if (version >= 9) {\n            group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version);\n        } else {\n            assertThrows(UnsupportedVersionException.class, () ->\n                group.validateOffsetCommit(\"new-protocol-member-id\", \"\", 0, isTransactional, version));\n        }\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE.toString(), group.stateAsString(1));\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(\n            snapshotRegistry,\n            \"group-foo\",\n            mock(GroupCoordinatorMetricsShard.class)\n        );\n\n        // Simulate a call from the admin client without member id and member epoch.\n        group.validateOffsetFetch(null, -1, Long.MAX_VALUE);\n\n        // The member does not exist.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE));\n\n        // Create a member.\n        idempotentCreateSnapshot(snapshotRegistry);\n        group.updateMember(new ConsumerGroupMember.Builder(\"member-id\").build());\n\n        // The member does not exist at last committed offset 0.\n        assertThrows(UnknownMemberIdException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 0, 0));\n\n        // The member exists but the epoch is stale when the last committed offset is not considered.\n        assertThrows(StaleMemberEpochException.class, () ->\n            group.validateOffsetFetch(\"member-id\", 10, Long.MAX_VALUE));\n\n        // This should succeed.\n        group.validateOffsetFetch(\"member-id\", 0, Long.MAX_VALUE);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        assertDoesNotThrow(consumerGroup::validateDeleteGroup);\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        consumerGroup.updateMember(member1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        assertThrows(GroupNotEmptyException.class, consumerGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        long currentTimestamp = 30000L;\n        long commitTimestamp = 20000L;\n        long offsetsRetentionMs = 10000L;\n        OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(15000L, OptionalInt.empty(), \"\", commitTimestamp, OptionalLong.empty());\n        ConsumerGroup group = new ConsumerGroup(new SnapshotRegistry(new LogContext()), \"group-id\", mock(GroupCoordinatorMetricsShard.class));\n\n        Optional<OffsetExpirationCondition> offsetExpirationCondition = group.offsetExpirationCondition();\n        assertTrue(offsetExpirationCondition.isPresent());\n\n        OffsetExpirationConditionImpl condition = (OffsetExpirationConditionImpl) offsetExpirationCondition.get();\n        assertEquals(commitTimestamp, condition.baseTimestamp().apply(offsetAndMetadata));\n        assertTrue(condition.isOffsetExpired(offsetAndMetadata, currentTimestamp, offsetsRetentionMs));\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ConsumerGroup consumerGroup = createConsumerGroup(\"group-foo\");\n\n        consumerGroup.updateMember(member1);\n        consumerGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            consumerGroup.computeSubscriptionMetadata(\n                consumerGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(consumerGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(consumerGroup.isSubscribedToTopic(\"bar\"));\n\n        consumerGroup.removeMember(\"member1\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"foo\"));\n\n        consumerGroup.removeMember(\"member2\");\n        assertFalse(consumerGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-id-1\", mock(GroupCoordinatorMetricsShard.class));\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY.toString(), group.stateAsString(0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .setServerAssignorName(\"assignorName\")\n                .build());\n        group.updateMember(new ConsumerGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ConsumerGroupDescribeResponseData.DescribedGroup expected = new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ConsumerGroup.ConsumerGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ConsumerGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                    .setSubscribedTopicRegex(\"\"),\n                new ConsumerGroupDescribeResponseData.Member().setMemberId(\"member2\")\n                    .setSubscribedTopicRegex(\"\")\n            ));\n        ConsumerGroupDescribeResponseData.DescribedGroup actual = group.asDescribedGroup(1, \"\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testStateTransitionMetrics() {\n        // Confirm metrics is not updated when a new ConsumerGroup is created but only when the group transitions\n        // its state.\n        GroupCoordinatorMetricsShard metrics = mock(GroupCoordinatorMetricsShard.class);\n        ConsumerGroup consumerGroup = new ConsumerGroup(\n            new SnapshotRegistry(new LogContext()),\n            \"group-id\",\n            metrics\n        );\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(0)).onConsumerGroupStateTransition(null, ConsumerGroup.ConsumerGroupState.EMPTY);\n\n        ConsumerGroupMember member = new ConsumerGroupMember.Builder(\"member\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        consumerGroup.updateMember(member);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.RECONCILING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.EMPTY, ConsumerGroup.ConsumerGroupState.RECONCILING);\n\n        consumerGroup.setGroupEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.ASSIGNING, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.RECONCILING, ConsumerGroup.ConsumerGroupState.ASSIGNING);\n\n        consumerGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.STABLE, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.ASSIGNING, ConsumerGroup.ConsumerGroupState.STABLE);\n\n        consumerGroup.removeMember(\"member\");\n\n        assertEquals(ConsumerGroup.ConsumerGroupState.EMPTY, consumerGroup.state());\n        verify(metrics, times(1)).onConsumerGroupStateTransition(ConsumerGroup.ConsumerGroupState.STABLE, ConsumerGroup.ConsumerGroupState.EMPTY);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        GroupCoordinatorMetricsShard metricsShard = new GroupCoordinatorMetricsShard(\n            snapshotRegistry,\n            Collections.emptyMap(),\n            new TopicPartition(\"__consumer_offsets\", 0)\n        );\n        ConsumerGroup group = new ConsumerGroup(snapshotRegistry, \"group-foo\", metricsShard);\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(group.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        group.updateMember(new ConsumerGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(group.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(group.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(group.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testClassicMembersSupportedProtocols() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> rangeProtocol = new ArrayList<>();\n        rangeProtocol.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> roundRobinAndRangeProtocols = new ArrayList<>();\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"roundrobin\")\n            .setMetadata(new byte[0]));\n        roundRobinAndRangeProtocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member1);\n\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(1, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"range\", \"sticky\"))));\n        assertFalse(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(rangeProtocol))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertFalse(consumerGroup.classicMembersSupportedProtocols().containsKey(\"roundrobin\"));\n\n        member1 = new ConsumerGroupMember.Builder(member1)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        member2 = new ConsumerGroupMember.Builder(member2)\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(roundRobinAndRangeProtocols))\n            .build();\n        consumerGroup.updateMember(member2);\n\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"range\"));\n        assertEquals(2, consumerGroup.classicMembersSupportedProtocols().get(\"roundrobin\"));\n        assertTrue(consumerGroup.supportsClassicProtocols(ConsumerProtocol.PROTOCOL_TYPE, new HashSet<>(Arrays.asList(\"sticky\", \"roundrobin\"))));\n    }\n\n    @Test\n    public void testNumClassicProtocolMembers() {\n        ConsumerGroup consumerGroup = createConsumerGroup(\"foo\");\n        List<ConsumerGroupMemberMetadataValue.ClassicProtocol> protocols = new ArrayList<>();\n        protocols.add(new ConsumerGroupMemberMetadataValue.ClassicProtocol()\n            .setName(\"range\")\n            .setMetadata(new byte[0]));\n\n        // The group has member 1 (using the classic protocol).\n        ConsumerGroupMember member1 = new ConsumerGroupMember.Builder(\"member-1\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member1);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n\n        // The group has member 1 (using the classic protocol) and member 2 (using the consumer protocol).\n        ConsumerGroupMember member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-1\"));\n        assertTrue(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the consumer protocol) and member 3 (using the consumer protocol).\n        consumerGroup.removeMember(member1.memberId());\n        ConsumerGroupMember member3 = new ConsumerGroupMember.Builder(\"member-3\")\n            .build();\n        consumerGroup.updateMember(member3);\n        assertEquals(0, consumerGroup.numClassicProtocolMembers());\n        assertFalse(consumerGroup.allMembersUseClassicProtocolExcept(\"member-2\"));\n\n        // The group has member 2 (using the classic protocol).\n        consumerGroup.removeMember(member2.memberId());\n        member2 = new ConsumerGroupMember.Builder(\"member-2\")\n            .setClassicMemberMetadata(new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                .setSupportedProtocols(protocols))\n            .build();\n        consumerGroup.updateMember(member2);\n        assertEquals(1, consumerGroup.numClassicProtocolMembers());\n    }\n}",
                "methodCount": 34
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createConsumerGroup",
                            "method_signature": "private createConsumerGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateTransactionalOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3465356000703552
                },
                "private createConsumerGroup(String groupId)": {
                    "first": {
                        "method_name": "createConsumerGroup",
                        "method_signature": "private createConsumerGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5874965379182631
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateTransactionalOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateTransactionalOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.708397850187187
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7260619945706963
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAsListedGroup() : void in class org.apache.kafka.coordinator.group.modern.share.ShareGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 677,
                    "endLine": 692,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAsListedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 681,
                    "endLine": 681,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 677,
                    "endLine": 692,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAsListedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 681,
                    "endLine": 681,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 687,
                    "endLine": 687,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 616,
        "extraction_results": {
            "success": true,
            "newCommitHash": "615b67dc17c192240f1a481c9167bb8eb032c9f8",
            "newBranchName": "extract-idempotentCreateSnapshot-testAsListedGroup-130af38"
        },
        "telemetry": {
            "id": "202cee4c-bbc9-4e62-980f-7ff87c7e9d2a",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 790,
                "lineStart": 56,
                "lineEnd": 845,
                "bodyLineStart": 56,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                "sourceCode": "public class ShareGroupTest {\n\n    @Test\n    public void testType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(Group.GroupType.SHARE, shareGroup.type());\n    }\n\n    @Test\n    public void testProtocolType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(\"share\", shareGroup.protocolType());\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        // Create a member.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        shareGroup.updateMember(member);\n\n        // Get that member back.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            shareGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ShareGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(member, shareGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        ShareGroupMember member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n        shareGroup.updateMember(member);\n        assertTrue(shareGroup.hasMember(\"member\"));\n\n        shareGroup.removeMember(\"member\");\n        assertFalse(shareGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 6));\n\n        member = new ShareGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ShareGroupMember m1 = new ShareGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        shareGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n\n        shareGroup.removeMember(member.memberId());\n\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testGroupState() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(ShareGroup.ShareGroupState.EMPTY, shareGroup.state());\n        assertEquals(\"Empty\", shareGroup.stateAsString());\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        shareGroup.updateMember(member1);\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertEquals(\"Stable\", shareGroup.stateAsString());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n        assertEquals(Group.GroupType.parse(\"share\"), Group.GroupType.SHARE);\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Share\"), Group.GroupType.SHARE);\n        assertEquals(Group.GroupType.parse(\"SHare\"), Group.GroupType.SHARE);\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        shareGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        shareGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        shareGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        ShareGroupMember member4 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        shareGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"test-group\");\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        shareGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, shareGroup.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch() + 1);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch() + 1, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        shareGroup.setGroupEpoch(shareGroup.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        shareGroup.requestMetadataRefresh();\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetCommit(null, null, -1, false, version));\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        assertEquals(ShareGroupState.STABLE, shareGroup.state(1));\n        assertEquals(\"Stable\", shareGroup.stateAsString(1));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::offsetExpirationCondition);\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetFetch(null, -1, -1));\n    }\n\n    @Test\n    public void testValidateOffsetDelete() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::validateOffsetDelete);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state());\n        assertDoesNotThrow(shareGroup::validateDeleteGroup);\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        shareGroup.updateMember(member1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        shareGroup.updateMember(member1);\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(shareGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(shareGroup.isSubscribedToTopic(\"bar\"));\n\n        shareGroup.removeMember(\"member1\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"foo\"));\n\n        shareGroup.removeMember(\"member2\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-id-1\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY.toString(), shareGroup.stateAsString(0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .build());\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ShareGroupDescribeResponseData.DescribedGroup expected = new ShareGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ShareGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ShareGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\")),\n                new ShareGroupDescribeResponseData.Member().setMemberId(\"member2\")\n            ));\n        ShareGroupDescribeResponseData.DescribedGroup actual = shareGroup.asDescribedGroup(1, \"assignorName\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    private ShareGroup createShareGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ShareGroup(\n            snapshotRegistry,\n            groupId\n        );\n    }\n}",
                "methodCount": 26
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.36472014915730905
                },
                "private createShareGroup(String groupId)": {
                    "first": {
                        "method_name": "createShareGroup",
                        "method_signature": "private createShareGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4998656321625178
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5829569333660806
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAsDescribedGroup() : void in class org.apache.kafka.coordinator.group.modern.share.ShareGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 785,
                    "endLine": 815,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAsDescribedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 789,
                    "endLine": 789,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 785,
                    "endLine": 815,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAsDescribedGroup() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 789,
                    "endLine": 789,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 797,
                    "endLine": 797,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 617,
        "extraction_results": {
            "success": true,
            "newCommitHash": "6763e67eb5b4151e73ba5a9ed1ee0dd90c71759f",
            "newBranchName": "extract-idempotentCreateSnapshot-testAsDescribedGroup-130af38"
        },
        "telemetry": {
            "id": "4790d528-e6b8-46d4-991d-63caf338500f",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 790,
                "lineStart": 56,
                "lineEnd": 845,
                "bodyLineStart": 56,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                "sourceCode": "public class ShareGroupTest {\n\n    @Test\n    public void testType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(Group.GroupType.SHARE, shareGroup.type());\n    }\n\n    @Test\n    public void testProtocolType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(\"share\", shareGroup.protocolType());\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        // Create a member.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        shareGroup.updateMember(member);\n\n        // Get that member back.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            shareGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ShareGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(member, shareGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        ShareGroupMember member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n        shareGroup.updateMember(member);\n        assertTrue(shareGroup.hasMember(\"member\"));\n\n        shareGroup.removeMember(\"member\");\n        assertFalse(shareGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 6));\n\n        member = new ShareGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ShareGroupMember m1 = new ShareGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        shareGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n\n        shareGroup.removeMember(member.memberId());\n\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testGroupState() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(ShareGroup.ShareGroupState.EMPTY, shareGroup.state());\n        assertEquals(\"Empty\", shareGroup.stateAsString());\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        shareGroup.updateMember(member1);\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertEquals(\"Stable\", shareGroup.stateAsString());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n        assertEquals(Group.GroupType.parse(\"share\"), Group.GroupType.SHARE);\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Share\"), Group.GroupType.SHARE);\n        assertEquals(Group.GroupType.parse(\"SHare\"), Group.GroupType.SHARE);\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        shareGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        shareGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        shareGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        ShareGroupMember member4 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        shareGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"test-group\");\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        shareGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, shareGroup.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch() + 1);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch() + 1, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        shareGroup.setGroupEpoch(shareGroup.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        shareGroup.requestMetadataRefresh();\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetCommit(null, null, -1, false, version));\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        assertEquals(ShareGroupState.STABLE, shareGroup.state(1));\n        assertEquals(\"Stable\", shareGroup.stateAsString(1));\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::offsetExpirationCondition);\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetFetch(null, -1, -1));\n    }\n\n    @Test\n    public void testValidateOffsetDelete() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::validateOffsetDelete);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state());\n        assertDoesNotThrow(shareGroup::validateDeleteGroup);\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        shareGroup.updateMember(member1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        shareGroup.updateMember(member1);\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(shareGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(shareGroup.isSubscribedToTopic(\"bar\"));\n\n        shareGroup.removeMember(\"member1\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"foo\"));\n\n        shareGroup.removeMember(\"member2\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-id-1\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY.toString(), shareGroup.stateAsString(0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .build());\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ShareGroupDescribeResponseData.DescribedGroup expected = new ShareGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ShareGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ShareGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\")),\n                new ShareGroupDescribeResponseData.Member().setMemberId(\"member2\")\n            ));\n        ShareGroupDescribeResponseData.DescribedGroup actual = shareGroup.asDescribedGroup(1, \"assignorName\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    private ShareGroup createShareGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ShareGroup(\n            snapshotRegistry,\n            groupId\n        );\n    }\n}",
                "methodCount": 26
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3640488639576087
                },
                "private createShareGroup(String groupId)": {
                    "first": {
                        "method_name": "createShareGroup",
                        "method_signature": "private createShareGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4999138067992805
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.583150826600613
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testIsInStatesCaseInsensitive() : void in class org.apache.kafka.coordinator.group.modern.share.ShareGroupTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 817,
                    "endLine": 832,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testIsInStatesCaseInsensitive() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 821,
                    "endLine": 821,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 817,
                    "endLine": 832,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testIsInStatesCaseInsensitive() : void"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 821,
                    "endLine": 821,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                },
                {
                    "filePath": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                    "startLine": 828,
                    "endLine": 828,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 618,
        "extraction_results": {
            "success": true,
            "newCommitHash": "7e477e8eadf30c6142f19bbb76cff3b1bf7d8587",
            "newBranchName": "extract-idempotentCreateSnapshot-testIsInStatesCaseInsensitive-130af38"
        },
        "telemetry": {
            "id": "7a78b923-d26f-4033-b0cd-013adf21f04f",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 790,
                "lineStart": 56,
                "lineEnd": 845,
                "bodyLineStart": 56,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/modern/share/ShareGroupTest.java",
                "sourceCode": "public class ShareGroupTest {\n\n    @Test\n    public void testType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(Group.GroupType.SHARE, shareGroup.type());\n    }\n\n    @Test\n    public void testProtocolType() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(\"share\", shareGroup.protocolType());\n    }\n\n    @Test\n    public void testGetOrCreateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        // Create a member.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", true);\n        assertEquals(\"member-id\", member.memberId());\n\n        // Add member to the group.\n        shareGroup.updateMember(member);\n\n        // Get that member back.\n        member = shareGroup.getOrMaybeCreateMember(\"member-id\", false);\n        assertEquals(\"member-id\", member.memberId());\n\n        assertThrows(UnknownMemberIdException.class, () ->\n            shareGroup.getOrMaybeCreateMember(\"does-not-exist\", false));\n    }\n\n    @Test\n    public void testUpdateMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n\n        member = new ShareGroupMember.Builder(member)\n            .setSubscribedTopicNames(Arrays.asList(\"foo\", \"bar\"))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(member, shareGroup.getOrMaybeCreateMember(\"member\", false));\n    }\n\n    @Test\n    public void testRemoveMember() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        ShareGroupMember member = shareGroup.getOrMaybeCreateMember(\"member\", true);\n        shareGroup.updateMember(member);\n        assertTrue(shareGroup.hasMember(\"member\"));\n\n        shareGroup.removeMember(\"member\");\n        assertFalse(shareGroup.hasMember(\"member\"));\n\n    }\n\n    @Test\n    public void testUpdatingMemberUpdatesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 4));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 5));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(barTopicId, 6));\n\n        member = new ShareGroupMember.Builder(member)\n            .setMemberEpoch(11)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(barTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 1));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 2));\n        assertEquals(11, shareGroup.currentPartitionEpoch(barTopicId, 3));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testRemovePartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        // Removing should fail because there is no epoch set.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        ));\n\n        ShareGroupMember m1 = new ShareGroupMember.Builder(\"m1\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)))\n            .build();\n\n        shareGroup.updateMember(m1);\n\n        // Removing should fail because the expected epoch is incorrect.\n        assertThrows(IllegalStateException.class, () -> shareGroup.removePartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testAddPartitionEpochs() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            10\n        );\n\n        // Changing the epoch should fail because the owner of the partition\n        // should remove it first.\n        assertThrows(IllegalStateException.class, () -> shareGroup.addPartitionEpochs(\n            mkAssignment(\n                mkTopicAssignment(fooTopicId, 1)\n            ),\n            11\n        ));\n    }\n\n    @Test\n    public void testDeletingMemberRemovesPartitionEpoch() {\n        Uuid fooTopicId = Uuid.randomUuid();\n\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        ShareGroupMember member;\n\n        member = new ShareGroupMember.Builder(\"member\")\n            .setMemberEpoch(10)\n            .setAssignedPartitions(mkAssignment(\n                mkTopicAssignment(fooTopicId, 1, 2, 3)))\n            .build();\n\n        shareGroup.updateMember(member);\n\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(10, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n\n        shareGroup.removeMember(member.memberId());\n\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 1));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 2));\n        assertEquals(-1, shareGroup.currentPartitionEpoch(fooTopicId, 3));\n    }\n\n    @Test\n    public void testGroupState() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n        assertEquals(ShareGroup.ShareGroupState.EMPTY, shareGroup.state());\n        assertEquals(\"Empty\", shareGroup.stateAsString());\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setState(MemberState.STABLE)\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n\n        shareGroup.updateMember(member1);\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(MemberState.STABLE, member1.state());\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertEquals(\"Stable\", shareGroup.stateAsString());\n    }\n\n    @Test\n    public void testGroupTypeFromString() {\n        assertEquals(Group.GroupType.parse(\"share\"), Group.GroupType.SHARE);\n        // Test case insensitivity.\n        assertEquals(Group.GroupType.parse(\"Share\"), Group.GroupType.SHARE);\n        assertEquals(Group.GroupType.parse(\"SHare\"), Group.GroupType.SHARE);\n    }\n\n    @Test\n    public void testUpdateSubscriptionMetadata() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n        Uuid zarTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addTopic(zarTopicId, \"zar\", 3)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Collections.singletonList(\"zar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member1),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member1.\n        shareGroup.updateMember(member1);\n\n        // It should return foo now.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member2),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating the group with member2.\n        shareGroup.updateMember(member2);\n\n        // It should return foo and bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member2, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Removing member1 results in returning bar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(member1, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, member3),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Updating group with member3.\n        shareGroup.updateMember(member3);\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1, member 2 and member 3\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member1, member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 2 and member 3.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(new HashSet<>(Arrays.asList(member2, member3))),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // Compute while taking into account removal of member 1.\n        assertEquals(\n            mkMap(\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.singleton(member1)),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        // It should return foo, bar and zar.\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2))),\n                mkEntry(\"zar\", new TopicMetadata(zarTopicId, \"zar\", 3, mkMapOfPartitionRacks(3)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(Collections.emptySet()),\n                image.topics(),\n                image.cluster()\n            )\n        );\n    }\n\n    @Test\n    public void testUpdateSubscribedTopicNamesAndSubscriptionType() {\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n        ShareGroupMember member3 = new ShareGroupMember.Builder(\"member3\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // It should be empty by default.\n        assertEquals(\n            Collections.emptyMap(),\n            shareGroup.subscribedTopicNames()\n        );\n\n        // It should be Homogeneous by default.\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member1);\n\n        // It should be Homogeneous since there is just 1 member\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.updateMember(member3);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        shareGroup.removeMember(member1.memberId());\n\n        assertEquals(\n            HOMOGENEOUS,\n            shareGroup.subscriptionType()\n        );\n\n        ShareGroupMember member4 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Arrays.asList(\"bar\", \"foo\", \"zar\"))\n            .build();\n\n        shareGroup.updateMember(member4);\n\n        assertEquals(\n            HETEROGENEOUS,\n            shareGroup.subscriptionType()\n        );\n    }\n\n    @Test\n    public void testUpdateInvertedAssignment() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"test-group\");\n        Uuid topicId = Uuid.randomUuid();\n        String memberId1 = \"member1\";\n        String memberId2 = \"member2\";\n\n        // Initial assignment for member1\n        Assignment initialAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, initialAssignment);\n\n        // Verify that partition 0 is assigned to member1.\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(0, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1\n        Assignment newAssignment = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId1)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member2 to add partition 1\n        Assignment newAssignment2 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(1))\n        ));\n        shareGroup.updateTargetAssignment(memberId2, newAssignment2);\n\n        // Verify that partition 1 is assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // New assignment for member1 to revoke partition 1 and assign partition 0\n        Assignment newAssignment1 = new Assignment(Collections.singletonMap(\n            topicId,\n            new HashSet<>(Collections.singletonList(0))\n        ));\n        shareGroup.updateTargetAssignment(memberId1, newAssignment1);\n\n        // Verify that partition 1 is still assigned to member2 and partition 0 is assigned to member1\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(\n                    mkEntry(0, memberId1),\n                    mkEntry(1, memberId2)\n                ))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n\n        // Test remove target assignment for member1\n        shareGroup.removeTargetAssignment(memberId1);\n\n        // Verify that partition 0 is no longer assigned and partition 1 is still assigned to member2\n        assertEquals(\n            mkMap(\n                mkEntry(topicId, mkMap(mkEntry(1, memberId2)))\n            ),\n            shareGroup.invertedTargetAssignment()\n        );\n    }\n\n    @Test\n    public void testMetadataRefreshDeadline() {\n        MockTime time = new MockTime();\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        // Group epoch starts at 0.\n        assertEquals(0, shareGroup.groupEpoch());\n\n        // The refresh time deadline should be empty when the group is created or loaded.\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance past the deadline. The metadata should have expired.\n        time.sleep(1001L);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n\n        // Set the refresh time deadline with a higher group epoch. The metadata is considered\n        // as expired because the group epoch attached to the deadline is higher than the\n        // current group epoch.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch() + 1);\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch() + 1, shareGroup.metadataRefreshDeadline().epoch);\n\n        // Advance the group epoch.\n        shareGroup.setGroupEpoch(shareGroup.groupEpoch() + 1);\n\n        // Set the refresh deadline. The metadata remains valid because the deadline\n        // has not past and the group epoch is correct.\n        shareGroup.setMetadataRefreshDeadline(time.milliseconds() + 1000, shareGroup.groupEpoch());\n        assertFalse(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(time.milliseconds() + 1000, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(shareGroup.groupEpoch(), shareGroup.metadataRefreshDeadline().epoch);\n\n        // Request metadata refresh. The metadata expires immediately.\n        shareGroup.requestMetadataRefresh();\n        assertTrue(shareGroup.hasMetadataExpired(time.milliseconds()));\n        assertEquals(0L, shareGroup.metadataRefreshDeadline().deadlineMs);\n        assertEquals(0, shareGroup.metadataRefreshDeadline().epoch);\n    }\n\n    @ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public void testValidateOffsetCommit(short version) {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetCommit(null, null, -1, false, version));\n    }\n\n    @Test\n    public void testAsListedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state(0));\n        assertEquals(\"Empty\", shareGroup.stateAsString(0));\n        assertEquals(ShareGroupState.STABLE, shareGroup.state(1));\n        assertEquals(\"Stable\", shareGroup.stateAsString(1));\n    }\n\n    @Test\n    public void testOffsetExpirationCondition() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::offsetExpirationCondition);\n    }\n\n    @Test\n    public void testValidateOffsetFetch() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, () ->\n            shareGroup.validateOffsetFetch(null, -1, -1));\n    }\n\n    @Test\n    public void testValidateOffsetDelete() {\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n        assertThrows(UnsupportedOperationException.class, shareGroup::validateOffsetDelete);\n    }\n\n    @Test\n    public void testValidateDeleteGroup() {\n        ShareGroup shareGroup = createShareGroup(\"foo\");\n\n        assertEquals(ShareGroupState.EMPTY, shareGroup.state());\n        assertDoesNotThrow(shareGroup::validateDeleteGroup);\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setMemberEpoch(1)\n            .setPreviousMemberEpoch(0)\n            .build();\n        shareGroup.updateMember(member1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setGroupEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n\n        shareGroup.setTargetAssignmentEpoch(1);\n\n        assertEquals(ShareGroupState.STABLE, shareGroup.state());\n        assertThrows(GroupNotEmptyException.class, shareGroup::validateDeleteGroup);\n    }\n\n    @Test\n    public void testIsSubscribedToTopic() {\n        Uuid fooTopicId = Uuid.randomUuid();\n        Uuid barTopicId = Uuid.randomUuid();\n\n        MetadataImage image = new MetadataImageBuilder()\n            .addTopic(fooTopicId, \"foo\", 1)\n            .addTopic(barTopicId, \"bar\", 2)\n            .addRacks()\n            .build();\n\n        ShareGroupMember member1 = new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build();\n        ShareGroupMember member2 = new ShareGroupMember.Builder(\"member2\")\n            .setSubscribedTopicNames(Collections.singletonList(\"bar\"))\n            .build();\n\n        ShareGroup shareGroup = createShareGroup(\"group-foo\");\n\n        shareGroup.updateMember(member1);\n        shareGroup.updateMember(member2);\n\n        assertEquals(\n            mkMap(\n                mkEntry(\"foo\", new TopicMetadata(fooTopicId, \"foo\", 1, mkMapOfPartitionRacks(1))),\n                mkEntry(\"bar\", new TopicMetadata(barTopicId, \"bar\", 2, mkMapOfPartitionRacks(2)))\n            ),\n            shareGroup.computeSubscriptionMetadata(\n                shareGroup.computeSubscribedTopicNames(null, null),\n                image.topics(),\n                image.cluster()\n            )\n        );\n\n        assertTrue(shareGroup.isSubscribedToTopic(\"foo\"));\n        assertTrue(shareGroup.isSubscribedToTopic(\"bar\"));\n\n        shareGroup.removeMember(\"member1\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"foo\"));\n\n        shareGroup.removeMember(\"member2\");\n        assertFalse(shareGroup.isSubscribedToTopic(\"bar\"));\n    }\n\n    @Test\n    public void testAsDescribedGroup() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-id-1\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(ShareGroupState.EMPTY.toString(), shareGroup.stateAsString(0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n                .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n                .build());\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member2\")\n                .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n\n        ShareGroupDescribeResponseData.DescribedGroup expected = new ShareGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(\"group-id-1\")\n            .setGroupState(ShareGroupState.STABLE.toString())\n            .setGroupEpoch(0)\n            .setAssignmentEpoch(0)\n            .setAssignorName(\"assignorName\")\n            .setMembers(Arrays.asList(\n                new ShareGroupDescribeResponseData.Member()\n                    .setMemberId(\"member1\")\n                    .setSubscribedTopicNames(Collections.singletonList(\"foo\")),\n                new ShareGroupDescribeResponseData.Member().setMemberId(\"member2\")\n            ));\n        ShareGroupDescribeResponseData.DescribedGroup actual = shareGroup.asDescribedGroup(1, \"assignorName\",\n            new MetadataImageBuilder().build().topics());\n\n        assertEquals(expected, actual);\n    }\n\n    @Test\n    public void testIsInStatesCaseInsensitive() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        ShareGroup shareGroup = new ShareGroup(snapshotRegistry, \"group-foo\");\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"Empty\"), 0));\n\n        shareGroup.updateMember(new ShareGroupMember.Builder(\"member1\")\n            .setSubscribedTopicNames(Collections.singletonList(\"foo\"))\n            .build());\n        snapshotRegistry.getOrCreateSnapshot(1);\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"empty\"), 0));\n        assertTrue(shareGroup.isInStates(Collections.singleton(\"stable\"), 1));\n        assertFalse(shareGroup.isInStates(Collections.singleton(\"empty\"), 1));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    private ShareGroup createShareGroup(String groupId) {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        return new ShareGroup(\n            snapshotRegistry,\n            groupId\n        );\n    }\n}",
                "methodCount": 26
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "createShareGroup",
                            "method_signature": "private createShareGroup(String groupId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testValidateOffsetCommit",
                            "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.36655271729213784
                },
                "private createShareGroup(String groupId)": {
                    "first": {
                        "method_name": "createShareGroup",
                        "method_signature": "private createShareGroup(String groupId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5012584262292796
                },
                "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)": {
                    "first": {
                        "method_name": "testValidateOffsetCommit",
                        "method_signature": "@ParameterizedTest\n    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)\n    public testValidateOffsetCommit(short version)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5828965041147405
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testAddEntriesWithSnapshots() : Map<Integer,String> in class org.apache.kafka.jmh.timeline.TimelineHashMapBenchmark & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                    "startLine": 70,
                    "endLine": 90,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testAddEntriesWithSnapshots() : Map<Integer,String>"
                },
                {
                    "filePath": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                    "startLine": 81,
                    "endLine": 81,
                    "startColumn": 17,
                    "endColumn": 61,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                    "startLine": 70,
                    "endLine": 90,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testAddEntriesWithSnapshots() : Map<Integer,String>"
                },
                {
                    "filePath": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                    "startLine": 81,
                    "endLine": 81,
                    "startColumn": 17,
                    "endColumn": 65,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(epoch)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 619,
        "extraction_results": {
            "success": true,
            "newCommitHash": "ebceb5d1219775a9e83c01195bca606578cff305",
            "newBranchName": "extract-idempotentCreateSnapshot-testAddEntriesWithSnapshots-130af38"
        },
        "telemetry": {
            "id": "4082ae18-ec70-4cad-96b9-245665b9271a",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 58,
                "lineStart": 38,
                "lineEnd": 95,
                "bodyLineStart": 38,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java",
                "sourceCode": "@State(Scope.Benchmark)\n@Fork(value = 1)\n@Warmup(iterations = 3)\n@Measurement(iterations = 10)\n@BenchmarkMode(Mode.AverageTime)\n@OutputTimeUnit(TimeUnit.MILLISECONDS)\n\npublic class TimelineHashMapBenchmark {\n    private static final int NUM_ENTRIES = 1_000_000;\n\n    @Benchmark\n    public Map<Integer, String> testAddEntriesInHashMap() {\n        HashMap<Integer, String> map = new HashMap<>(NUM_ENTRIES);\n        for (int i = 0; i < NUM_ENTRIES; i++) {\n            int key = (int) (0xffffffff & ((i * 2862933555777941757L) + 3037000493L));\n            map.put(key, String.valueOf(key));\n        }\n        return map;\n    }\n\n    @Benchmark\n    public Map<Integer, String> testAddEntriesInTimelineMap() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TimelineHashMap<Integer, String> map =\n            new TimelineHashMap<>(snapshotRegistry, NUM_ENTRIES);\n        for (int i = 0; i < NUM_ENTRIES; i++) {\n            int key = (int) (0xffffffff & ((i * 2862933555777941757L) + 3037000493L));\n            map.put(key, String.valueOf(key));\n        }\n        return map;\n    }\n\n    @Benchmark\n    public Map<Integer, String> testAddEntriesWithSnapshots() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        TimelineHashMap<Integer, String> map =\n            new TimelineHashMap<>(snapshotRegistry, NUM_ENTRIES);\n        long epoch = 0;\n        int j = 0;\n        for (int i = 0; i < NUM_ENTRIES; i++) {\n            int key = (int) (0xffffffff & ((i * 2862933555777941757L) + 3037000493L));\n            if (j > 10 && key % 3 == 0) {\n                snapshotRegistry.deleteSnapshotsUpTo(epoch - 1000);\n                idempotentCreateSnapshot(snapshotRegistry, epoch);\n                j = 0;\n            } else {\n                j++;\n            }\n            map.put(key, String.valueOf(key));\n            epoch++;\n        }\n        return map;\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch) {\n        snapshotRegistry.getOrCreateSnapshot(epoch);\n    }\n}",
                "methodCount": 4
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "testAddEntriesInHashMap",
                            "method_signature": "@Benchmark\n    public testAddEntriesInHashMap()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testAddEntriesInTimelineMap",
                            "method_signature": "@Benchmark\n    public testAddEntriesInTimelineMap()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testAddEntriesWithSnapshots",
                            "method_signature": "@Benchmark\n    public testAddEntriesWithSnapshots()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testAddEntriesInHashMap",
                            "method_signature": "@Benchmark\n    public testAddEntriesInHashMap()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testAddEntriesWithSnapshots",
                            "method_signature": "@Benchmark\n    public testAddEntriesWithSnapshots()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testAddEntriesInTimelineMap",
                            "method_signature": "@Benchmark\n    public testAddEntriesInTimelineMap()",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry, long epoch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.512615868483097
                },
                "@Benchmark\n    public testAddEntriesInHashMap()": {
                    "first": {
                        "method_name": "testAddEntriesInHashMap",
                        "method_signature": "@Benchmark\n    public testAddEntriesInHashMap()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7661834806339528
                },
                "@Benchmark\n    public testAddEntriesWithSnapshots()": {
                    "first": {
                        "method_name": "testAddEntriesWithSnapshots",
                        "method_signature": "@Benchmark\n    public testAddEntriesWithSnapshots()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.9248052654351167
                },
                "@Benchmark\n    public testAddEntriesInTimelineMap()": {
                    "first": {
                        "method_name": "testAddEntriesInTimelineMap",
                        "method_signature": "@Benchmark\n    public testAddEntriesInTimelineMap()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.9274206790892942
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from private OffsetControlManager(logContext LogContext, snapshotRegistry SnapshotRegistry, metrics QuorumControllerMetrics, time Time) in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 142,
                    "endLine": 164,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private OffsetControlManager(logContext LogContext, snapshotRegistry SnapshotRegistry, metrics QuorumControllerMetrics, time Time)"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 159,
                    "endLine": 159,
                    "startColumn": 9,
                    "endColumn": 51,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 142,
                    "endLine": 164,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private OffsetControlManager(logContext LogContext, snapshotRegistry SnapshotRegistry, metrics QuorumControllerMetrics, time Time)"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 159,
                    "endLine": 159,
                    "startColumn": 9,
                    "endColumn": 55,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(-1L)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 620,
        "extraction_results": {
            "success": false
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package activate(newNextWriteOffset long) : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 236,
                    "endLine": 255,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package activate(newNextWriteOffset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 252,
                    "endLine": 252,
                    "startColumn": 9,
                    "endColumn": 64,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 236,
                    "endLine": 255,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package activate(newNextWriteOffset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 252,
                    "endLine": 252,
                    "startColumn": 9,
                    "endColumn": 68,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastStableOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 621,
        "extraction_results": {
            "success": true,
            "newCommitHash": "9eea6895d62aec7465dfbc4811e8b0bc61445662",
            "newBranchName": "extract-idempotentCreateSnapshot-activate-130af38"
        },
        "telemetry": {
            "id": "90d52a46-db45-4794-853a-8aec61ac27da",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        idempotentCreateSnapshot();\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    private void idempotentCreateSnapshot() {\n        snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                idempotentCreateSnapshot();\n            }\n        }\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        snapshotRegistry.getOrCreateSnapshot(offset - 1);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                " active()": {
                    "first": {
                        "method_name": "active",
                        "method_signature": " active()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5125917034066154
                },
                " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                    "first": {
                        "method_name": "handleCommitBatch",
                        "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5143307686392088
                },
                "public replay(AbortTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5498298207858505
                },
                "private idempotentCreateSnapshot()": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5512868797525027
                },
                "public replay(EndTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(EndTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5610877351004279
                },
                "public replay(BeginTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5771845191316811
                },
                "public build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": "public build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5922036435962503
                },
                " maybeAdvanceLastStableOffset()": {
                    "first": {
                        "method_name": "maybeAdvanceLastStableOffset",
                        "method_signature": " maybeAdvanceLastStableOffset()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6081402838099608
                },
                " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                    "first": {
                        "method_name": "beginLoadSnapshot",
                        "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.642456211327002
                },
                " endLoadSnapshot(long timestamp)": {
                    "first": {
                        "method_name": "endLoadSnapshot",
                        "method_signature": " endLoadSnapshot(long timestamp)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6445369651428627
                },
                " handleScheduleAtomicAppend(long endOffset)": {
                    "first": {
                        "method_name": "handleScheduleAtomicAppend",
                        "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.65794584225965
                },
                " deactivate()": {
                    "first": {
                        "method_name": "deactivate",
                        "method_signature": " deactivate()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6726638949989693
                },
                " activate(long newNextWriteOffset)": {
                    "first": {
                        "method_name": "activate",
                        "method_signature": " activate(long newNextWriteOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7189283661732085
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package handleScheduleAtomicAppend(endOffset long) : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 292,
                    "endLine": 310,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package handleScheduleAtomicAppend(endOffset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 301,
                    "endLine": 301,
                    "startColumn": 9,
                    "endColumn": 57,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 292,
                    "endLine": 310,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package handleScheduleAtomicAppend(endOffset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 301,
                    "endLine": 301,
                    "startColumn": 9,
                    "endColumn": 61,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(endOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 622,
        "extraction_results": {
            "success": true,
            "newCommitHash": "2e8ab6ec5591a47e6a5217eac0b790aacf3695fb",
            "newBranchName": "extract-idempotentCreateSnapshot-handleScheduleAtomicAppend-130af38"
        },
        "telemetry": {
            "id": "707fe11c-921f-40c9-b381-1d004ded058f",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        idempotentCreateSnapshot(lastStableOffset);\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        idempotentCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    private void idempotentCreateSnapshot(long endOffset) {\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                idempotentCreateSnapshot(lastStableOffset);\n            }\n        }\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        idempotentCreateSnapshot(offset - 1);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                " active()": {
                    "first": {
                        "method_name": "active",
                        "method_signature": " active()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5112370144224989
                },
                " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                    "first": {
                        "method_name": "handleCommitBatch",
                        "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5126655198753425
                },
                "public replay(AbortTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5510365274530321
                },
                "private idempotentCreateSnapshot(long endOffset)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(long endOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5510502060696468
                },
                "public replay(EndTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(EndTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5598858034347816
                },
                "public replay(BeginTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5761798496264882
                },
                "public build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": "public build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5945218994885361
                },
                " maybeAdvanceLastStableOffset()": {
                    "first": {
                        "method_name": "maybeAdvanceLastStableOffset",
                        "method_signature": " maybeAdvanceLastStableOffset()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6088439669570727
                },
                " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                    "first": {
                        "method_name": "beginLoadSnapshot",
                        "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.639581494334759
                },
                " endLoadSnapshot(long timestamp)": {
                    "first": {
                        "method_name": "endLoadSnapshot",
                        "method_signature": " endLoadSnapshot(long timestamp)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6418961294892257
                },
                " handleScheduleAtomicAppend(long endOffset)": {
                    "first": {
                        "method_name": "handleScheduleAtomicAppend",
                        "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6485778588136819
                },
                " deactivate()": {
                    "first": {
                        "method_name": "deactivate",
                        "method_signature": " deactivate()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6697900339745598
                },
                " activate(long newNextWriteOffset)": {
                    "first": {
                        "method_name": "activate",
                        "method_signature": " activate(long newNextWriteOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7226699456326655
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package maybeAdvanceLastStableOffset() : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 312,
                    "endLine": 329,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package maybeAdvanceLastStableOffset() : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 326,
                    "endLine": 326,
                    "startColumn": 17,
                    "endColumn": 72,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 312,
                    "endLine": 329,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package maybeAdvanceLastStableOffset() : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 326,
                    "endLine": 326,
                    "startColumn": 17,
                    "endColumn": 76,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(lastStableOffset)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 623,
        "extraction_results": {
            "success": true,
            "newCommitHash": "f02c9361287ce0b3d5874fb1da2a212cf97ae04f",
            "newBranchName": "extract-idempotentCreateSnapshot-maybeAdvanceLastStableOffset-130af38"
        },
        "telemetry": {
            "id": "0b3199c4-5420-434c-8693-9eccbede098b",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        idempotentCreateSnapshot();\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                idempotentCreateSnapshot();\n            }\n        }\n    }\n\n    private void idempotentCreateSnapshot() {\n        snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        snapshotRegistry.getOrCreateSnapshot(offset - 1);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                    "first": {
                        "method_name": "handleCommitBatch",
                        "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5110187249003356
                },
                " active()": {
                    "first": {
                        "method_name": "active",
                        "method_signature": " active()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5128851571769361
                },
                "public replay(AbortTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5485222449608803
                },
                "private idempotentCreateSnapshot()": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5511860143244525
                },
                "public replay(EndTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(EndTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5553225022841389
                },
                "public replay(BeginTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5742409152695633
                },
                "public build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": "public build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5950104625232243
                },
                " maybeAdvanceLastStableOffset()": {
                    "first": {
                        "method_name": "maybeAdvanceLastStableOffset",
                        "method_signature": " maybeAdvanceLastStableOffset()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6100213828697582
                },
                " endLoadSnapshot(long timestamp)": {
                    "first": {
                        "method_name": "endLoadSnapshot",
                        "method_signature": " endLoadSnapshot(long timestamp)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6433963438025715
                },
                " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                    "first": {
                        "method_name": "beginLoadSnapshot",
                        "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6450579794514504
                },
                " handleScheduleAtomicAppend(long endOffset)": {
                    "first": {
                        "method_name": "handleScheduleAtomicAppend",
                        "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6570626689508942
                },
                " deactivate()": {
                    "first": {
                        "method_name": "deactivate",
                        "method_signature": " deactivate()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6692273909957315
                },
                " activate(long newNextWriteOffset)": {
                    "first": {
                        "method_name": "activate",
                        "method_signature": " activate(long newNextWriteOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7181196687939895
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from package endLoadSnapshot(timestamp long) : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 354,
                    "endLine": 376,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "package endLoadSnapshot(timestamp long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 365,
                    "endLine": 365,
                    "startColumn": 9,
                    "endColumn": 79,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 354,
                    "endLine": 376,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "package endLoadSnapshot(timestamp long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 365,
                    "endLine": 365,
                    "startColumn": 9,
                    "endColumn": 83,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "this.snapshotRegistry.idempotentCreateSnapshot(currentSnapshotId.offset())"
                }
            ],
            "isStatic": false
        },
        "ref_id": 624,
        "extraction_results": {
            "success": true,
            "newCommitHash": "e4bf466a438e0f393135b84ed4b5611c7608ac3e",
            "newBranchName": "extract-idempotentCreateSnapshot-endLoadSnapshot-130af38"
        },
        "telemetry": {
            "id": "d726b916-479d-4ffb-9e98-643fccb78350",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n            }\n        }\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        idempotentCreateSnapshot();\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    private void idempotentCreateSnapshot() {\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        snapshotRegistry.getOrCreateSnapshot(offset - 1);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                " active()": {
                    "first": {
                        "method_name": "active",
                        "method_signature": " active()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5131587640061114
                },
                " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                    "first": {
                        "method_name": "handleCommitBatch",
                        "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5141522299155448
                },
                "public replay(AbortTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5530488328218044
                },
                "public replay(EndTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(EndTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5592126823793617
                },
                "private idempotentCreateSnapshot()": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5631970598511161
                },
                "public replay(BeginTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5792405782022997
                },
                "public build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": "public build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5918403389971755
                },
                " maybeAdvanceLastStableOffset()": {
                    "first": {
                        "method_name": "maybeAdvanceLastStableOffset",
                        "method_signature": " maybeAdvanceLastStableOffset()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6103719568638888
                },
                " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                    "first": {
                        "method_name": "beginLoadSnapshot",
                        "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6397624047209258
                },
                " endLoadSnapshot(long timestamp)": {
                    "first": {
                        "method_name": "endLoadSnapshot",
                        "method_signature": " endLoadSnapshot(long timestamp)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.642863072780253
                },
                " handleScheduleAtomicAppend(long endOffset)": {
                    "first": {
                        "method_name": "handleScheduleAtomicAppend",
                        "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6592053510199872
                },
                " deactivate()": {
                    "first": {
                        "method_name": "deactivate",
                        "method_signature": " deactivate()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6712922957250084
                },
                " activate(long newNextWriteOffset)": {
                    "first": {
                        "method_name": "activate",
                        "method_signature": " activate(long newNextWriteOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.719501830770968
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public replay(message BeginTransactionRecord, offset long) : void in class org.apache.kafka.controller.OffsetControlManager & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 378,
                    "endLine": 389,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public replay(message BeginTransactionRecord, offset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 386,
                    "endLine": 386,
                    "startColumn": 9,
                    "endColumn": 58,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 378,
                    "endLine": 389,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public replay(message BeginTransactionRecord, offset long) : void"
                },
                {
                    "filePath": "metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                    "startLine": 386,
                    "endLine": 386,
                    "startColumn": 9,
                    "endColumn": 62,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(offset - 1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 625,
        "extraction_results": {
            "success": true,
            "newCommitHash": "d0d1ef2e01ed6532f34cdbb087e241a4f6880007",
            "newBranchName": "extract-idempotentCreateSnapshot-replay-130af38"
        },
        "telemetry": {
            "id": "ff8f56f3-94d6-4634-953f-a1c7c09df2ed",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 390,
                "lineStart": 37,
                "lineEnd": 426,
                "bodyLineStart": 37,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/main/java/org/apache/kafka/controller/OffsetControlManager.java",
                "sourceCode": "/**\n * Manages read and write offsets, and in-memory snapshots.\n *\n * Also manages the following metrics:\n *      kafka.controller:type=KafkaController,name=ActiveControllerCount\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordLagMs\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordOffset\n *      kafka.controller:type=KafkaController,name=LastAppliedRecordTimestamp\n *      kafka.controller:type=KafkaController,name=LastCommittedRecordOffset\n */\nclass OffsetControlManager {\n    public static class Builder {\n        private LogContext logContext = null;\n        private SnapshotRegistry snapshotRegistry = null;\n        private QuorumControllerMetrics metrics = null;\n        private Time time = Time.SYSTEM;\n\n        Builder setLogContext(LogContext logContext) {\n            this.logContext = logContext;\n            return this;\n        }\n\n        Builder setSnapshotRegistry(SnapshotRegistry snapshotRegistry) {\n            this.snapshotRegistry = snapshotRegistry;\n            return this;\n        }\n\n        Builder setMetrics(QuorumControllerMetrics metrics) {\n            this.metrics = metrics;\n            return this;\n        }\n\n        Builder setTime(Time time) {\n            this.time = time;\n            return this;\n        }\n\n        public OffsetControlManager build() {\n            if (logContext == null) logContext = new LogContext();\n            if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);\n            if (metrics == null) {\n                metrics = new QuorumControllerMetrics(Optional.empty(), time, false);\n            }\n            return new OffsetControlManager(logContext,\n                    snapshotRegistry,\n                    metrics,\n                    time);\n        }\n    }\n\n    /**\n     * The slf4j logger.\n     */\n    private final Logger log;\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The quorum controller metrics.\n     */\n    private final QuorumControllerMetrics metrics;\n\n    /**\n     * The clock.\n     */\n    private final Time time;\n\n    /**\n     * The ID of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private OffsetAndEpoch currentSnapshotId;\n\n    /**\n     * The name of the snapshot that we're currently replaying, or null if there is none.\n     */\n    private String currentSnapshotName;\n\n    /**\n     * The latest committed offset.\n     */\n    private long lastCommittedOffset;\n\n    /**\n     * The latest committed epoch.\n     */\n    private int lastCommittedEpoch;\n\n    /**\n     * The latest offset that it is safe to read from.\n     */\n    private long lastStableOffset;\n\n    /**\n     * The offset of the transaction we're in, or -1 if we are not in one.\n     */\n    private long transactionStartOffset;\n\n    /**\n     * The next offset we should write to, or -1 if the controller is not active. Exclusive offset.\n     */\n    private long nextWriteOffset;\n\n    private OffsetControlManager(\n        LogContext logContext,\n        SnapshotRegistry snapshotRegistry,\n        QuorumControllerMetrics metrics,\n        Time time\n    ) {\n        this.log = logContext.logger(OffsetControlManager.class);\n        this.snapshotRegistry = snapshotRegistry;\n        this.metrics = metrics;\n        this.time = time;\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        snapshotRegistry.getOrCreateSnapshot(-1L);\n        metrics.setActive(false);\n        metrics.setLastCommittedRecordOffset(-1L);\n        metrics.setLastAppliedRecordOffset(-1L);\n        metrics.setLastAppliedRecordTimestamp(-1L);\n    }\n\n    /**\n     *  @return The SnapshotRegistry used by this offset control manager.\n     */\n    SnapshotRegistry snapshotRegistry() {\n        return snapshotRegistry;\n    }\n\n    /**\n     * @return QuorumControllerMetrics managed by this offset control manager.\n     */\n    QuorumControllerMetrics metrics() {\n        return metrics;\n    }\n\n    /**\n     * @return the ID of the current snapshot.\n     */\n    OffsetAndEpoch currentSnapshotId() {\n        return currentSnapshotId;\n    }\n\n    /**\n     * @return the name of the current snapshot.\n     */\n    String currentSnapshotName() {\n        return currentSnapshotName;\n    }\n\n    /**\n     * @return the last committed offset.\n     */\n    long lastCommittedOffset() {\n        return lastCommittedOffset;\n    }\n\n    /**\n     * @return the last committed epoch.\n     */\n    int lastCommittedEpoch() {\n        return lastCommittedEpoch;\n    }\n\n    /**\n     * @return the latest offset that it is safe to read from.\n     */\n    long lastStableOffset() {\n        return lastStableOffset;\n    }\n\n    /**\n     * @return the transaction start offset, or -1 if there is no transaction.\n     */\n    long transactionStartOffset() {\n        return transactionStartOffset;\n    }\n\n    /**\n     * @return the next offset that the active controller should write to.\n     */\n    long nextWriteOffset() {\n        return nextWriteOffset;\n    }\n\n    /**\n     * @return true only if the manager is active.\n     */\n    boolean active() {\n        return nextWriteOffset != -1L;\n    }\n\n    /**\n     * Called when the QuorumController becomes active.\n     *\n     * @param newNextWriteOffset The new next write offset to use. Must be non-negative.\n     */\n    void activate(long newNextWriteOffset) {\n        if (active()) {\n            throw new RuntimeException(\"Can't activate already active OffsetControlManager.\");\n        }\n        if (newNextWriteOffset < 0) {\n            throw new RuntimeException(\"Invalid negative newNextWriteOffset \" +\n                    newNextWriteOffset + \".\");\n        }\n        // Before switching to active, create an in-memory snapshot at the last committed\n        // offset. This is required because the active controller assumes that there is always\n        // an in-memory snapshot at the last committed offset.\n        snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n        this.nextWriteOffset = newNextWriteOffset;\n        metrics.setActive(true);\n    }\n\n    /**\n     * Called when the QuorumController becomes inactive.\n     */\n    void deactivate() {\n        if (!active()) {\n            throw new RuntimeException(\"Can't deactivate inactive OffsetControlManager.\");\n        }\n        metrics.setActive(false);\n        metrics.setLastAppliedRecordOffset(lastStableOffset);\n        this.nextWriteOffset = -1L;\n        if (!snapshotRegistry.hasSnapshot(lastStableOffset)) {\n            throw new RuntimeException(\"Unable to reset to last stable offset \" + lastStableOffset +\n                    \". No in-memory snapshot found for this offset.\");\n        }\n        snapshotRegistry.revertToSnapshot(lastStableOffset);\n    }\n\n    /**\n     * Handle the callback from the Raft layer indicating that a batch was committed.\n     *\n     * @param batch The batch that has been committed.\n     */\n    void handleCommitBatch(Batch<ApiMessageAndVersion> batch) {\n        this.lastCommittedOffset = batch.lastOffset();\n        this.lastCommittedEpoch = batch.epoch();\n        maybeAdvanceLastStableOffset();\n        metrics.setLastCommittedRecordOffset(batch.lastOffset());\n        if (!active()) {\n            // On standby controllers, the last applied record offset is equals to the last\n            // committed offset.\n            metrics.setLastAppliedRecordOffset(batch.lastOffset());\n            metrics.setLastAppliedRecordTimestamp(batch.appendTimestamp());\n        }\n    }\n\n    /**\n     * Called by the active controller after it has invoked scheduleAtomicAppend to schedule some\n     * records to be written.\n     *\n     * @param endOffset The offset of the last record that was written.\n     */\n    void handleScheduleAtomicAppend(long endOffset) {\n        this.nextWriteOffset = endOffset + 1;\n\n        snapshotRegistry.getOrCreateSnapshot(endOffset);\n\n        metrics.setLastAppliedRecordOffset(endOffset);\n\n        // This is not truly the append timestamp. The KRaft client doesn't expose the append\n        // time when scheduling a write. This is good enough because this is called right after\n        // the records were given to the KRAft client for appending and the default append linger\n        // for KRaft is 25ms.\n        metrics.setLastAppliedRecordTimestamp(time.milliseconds());\n    }\n\n    /**\n     * Advance the last stable offset if needed.\n     */\n    void maybeAdvanceLastStableOffset() {\n        long newLastStableOffset;\n        if (transactionStartOffset == -1L) {\n            newLastStableOffset = lastCommittedOffset;\n        } else {\n            newLastStableOffset = Math.min(transactionStartOffset - 1, lastCommittedOffset);\n        }\n        if (lastStableOffset < newLastStableOffset) {\n            lastStableOffset = newLastStableOffset;\n            snapshotRegistry.deleteSnapshotsUpTo(lastStableOffset);\n            if (!active()) {\n                snapshotRegistry.getOrCreateSnapshot(lastStableOffset);\n            }\n        }\n    }\n\n    /**\n     * Called before we load a Raft snapshot.\n     *\n     * @param snapshotId The Raft snapshot offset and epoch.\n     */\n    void beginLoadSnapshot(OffsetAndEpoch snapshotId) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"Can't begin reading snapshot for \" + snapshotId +\n                    \", because we are already reading \" + currentSnapshotId);\n        }\n        this.currentSnapshotId = snapshotId;\n        this.currentSnapshotName = Snapshots.filenameFromSnapshotId(snapshotId);\n        log.info(\"Starting to load snapshot {}. Previous lastCommittedOffset was {}. Previous \" +\n                \"transactionStartOffset was {}.\", currentSnapshotName, lastCommittedOffset,\n                transactionStartOffset);\n        this.snapshotRegistry.reset();\n        this.lastCommittedOffset = -1L;\n        this.lastCommittedEpoch = -1;\n        this.lastStableOffset = -1L;\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n    }\n\n    /**\n     * Called after we have finished loading a Raft snapshot.\n     *\n     * @param timestamp The timestamp of the snapshot.\n     */\n    void endLoadSnapshot(long timestamp) {\n        if (currentSnapshotId == null) {\n            throw new RuntimeException(\"Can't end loading snapshot, because there is no \" +\n                    \"current snapshot.\");\n        }\n        log.info(\"Successfully loaded snapshot {}.\", currentSnapshotName);\n        this.snapshotRegistry.getOrCreateSnapshot(currentSnapshotId.offset());\n        this.lastCommittedOffset = currentSnapshotId.offset();\n        this.lastCommittedEpoch = currentSnapshotId.epoch();\n        this.lastStableOffset = currentSnapshotId.offset();\n        this.transactionStartOffset = -1L;\n        this.nextWriteOffset = -1L;\n        metrics.setLastCommittedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordOffset(currentSnapshotId.offset());\n        metrics.setLastAppliedRecordTimestamp(timestamp);\n        this.currentSnapshotId = null;\n        this.currentSnapshotName = null;\n    }\n\n    public void replay(BeginTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"BeginTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset != -1L) {\n            throw new RuntimeException(\"Can't replay a BeginTransactionRecord at \" + offset +\n                \" because the transaction at \" + transactionStartOffset + \" was never closed.\");\n        }\n        idempotentCreateSnapshot(offset);\n        transactionStartOffset = offset;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    private void idempotentCreateSnapshot(long offset) {\n        snapshotRegistry.getOrCreateSnapshot(offset - 1);\n    }\n\n    public void replay(EndTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"EndTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an EndTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}.\", message, offset);\n    }\n\n    public void replay(AbortTransactionRecord message, long offset) {\n        if (currentSnapshotId != null) {\n            throw new RuntimeException(\"AbortTransactionRecord cannot appear within a snapshot.\");\n        }\n        if (transactionStartOffset == -1L) {\n            throw new RuntimeException(\"Can't replay an AbortTransactionRecord at \" + offset +\n                    \" because there is no open transaction.\");\n        }\n        long preTransactionOffset = transactionStartOffset - 1;\n        snapshotRegistry.revertToSnapshot(preTransactionOffset);\n        transactionStartOffset = -1L;\n        log.info(\"Replayed {} at offset {}. Reverted to offset {}.\",\n                message, offset, preTransactionOffset);\n    }\n\n    // VisibleForTesting\n    void setNextWriteOffset(long newNextWriteOffset) {\n        this.nextWriteOffset = newNextWriteOffset;\n    }\n}",
                "methodCount": 28
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "active",
                            "method_signature": " active()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleCommitBatch",
                            "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(EndTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "replay",
                            "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "build",
                            "method_signature": "public build()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeAdvanceLastStableOffset",
                            "method_signature": " maybeAdvanceLastStableOffset()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "beginLoadSnapshot",
                            "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "endLoadSnapshot",
                            "method_signature": " endLoadSnapshot(long timestamp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleScheduleAtomicAppend",
                            "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "deactivate",
                            "method_signature": " deactivate()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "activate",
                            "method_signature": " activate(long newNextWriteOffset)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                " active()": {
                    "first": {
                        "method_name": "active",
                        "method_signature": " active()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5115448572053831
                },
                " handleCommitBatch(Batch<ApiMessageAndVersion> batch)": {
                    "first": {
                        "method_name": "handleCommitBatch",
                        "method_signature": " handleCommitBatch(Batch<ApiMessageAndVersion> batch)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5116387671050442
                },
                "private idempotentCreateSnapshot(long offset)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5422637127365022
                },
                "public replay(AbortTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(AbortTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5523984330264016
                },
                "public replay(EndTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(EndTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5637615135045143
                },
                "public replay(BeginTransactionRecord message, long offset)": {
                    "first": {
                        "method_name": "replay",
                        "method_signature": "public replay(BeginTransactionRecord message, long offset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5690658981992537
                },
                "public build()": {
                    "first": {
                        "method_name": "build",
                        "method_signature": "public build()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.5930944273235845
                },
                " maybeAdvanceLastStableOffset()": {
                    "first": {
                        "method_name": "maybeAdvanceLastStableOffset",
                        "method_signature": " maybeAdvanceLastStableOffset()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6132881796658732
                },
                " beginLoadSnapshot(OffsetAndEpoch snapshotId)": {
                    "first": {
                        "method_name": "beginLoadSnapshot",
                        "method_signature": " beginLoadSnapshot(OffsetAndEpoch snapshotId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6390118378197097
                },
                " endLoadSnapshot(long timestamp)": {
                    "first": {
                        "method_name": "endLoadSnapshot",
                        "method_signature": " endLoadSnapshot(long timestamp)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6456418867502444
                },
                " handleScheduleAtomicAppend(long endOffset)": {
                    "first": {
                        "method_name": "handleScheduleAtomicAppend",
                        "method_signature": " handleScheduleAtomicAppend(long endOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6595121955404046
                },
                " deactivate()": {
                    "first": {
                        "method_name": "deactivate",
                        "method_signature": " deactivate()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.6694947113408708
                },
                " activate(long newNextWriteOffset)": {
                    "first": {
                        "method_name": "activate",
                        "method_signature": " activate(long newNextWriteOffset)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7221218062025735
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testLoadSnapshot() : void in class org.apache.kafka.controller.AclControlManagerTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                    "startLine": 203,
                    "endLine": 238,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testLoadSnapshot() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                    "startLine": 206,
                    "endLine": 206,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                    "startLine": 203,
                    "endLine": 238,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testLoadSnapshot() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                    "startLine": 206,
                    "endLine": 206,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(0)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 626,
        "extraction_results": {
            "success": true,
            "newCommitHash": "41521201e2c0ea4f6a97d434a4a1b58e6b71b232",
            "newBranchName": "extract-idempotentCreateSnapshot-testLoadSnapshot-130af38"
        },
        "telemetry": {
            "id": "1d31a406-7236-4d50-88f0-06d5e0e79b7a",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 265,
                "lineStart": 81,
                "lineEnd": 345,
                "bodyLineStart": 81,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
                "sourceCode": "@Timeout(value = 40)\npublic class AclControlManagerTest {\n    /**\n     * Verify that validateNewAcl catches invalid ACLs.\n     */\n    @Test\n    public void testValidateNewAcl() {\n        AclControlManager.validateNewAcl(new AclBinding(\n            new ResourcePattern(TOPIC, \"*\", LITERAL),\n            new AccessControlEntry(\"User:*\", \"*\", ALTER, ALLOW)));\n        assertEquals(\"Invalid patternType UNKNOWN\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateNewAcl(new AclBinding(\n                    new ResourcePattern(TOPIC, \"*\", PatternType.UNKNOWN),\n                    new AccessControlEntry(\"User:*\", \"*\", ALTER, ALLOW)))).\n                getMessage());\n        assertEquals(\"Invalid resourceType UNKNOWN\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateNewAcl(new AclBinding(\n                    new ResourcePattern(ResourceType.UNKNOWN, \"*\", LITERAL),\n                    new AccessControlEntry(\"User:*\", \"*\", ALTER, ALLOW)))).\n                getMessage());\n        assertEquals(\"Invalid operation UNKNOWN\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateNewAcl(new AclBinding(\n                    new ResourcePattern(TOPIC, \"*\", LITERAL),\n                    new AccessControlEntry(\"User:*\", \"*\", AclOperation.UNKNOWN, ALLOW)))).\n                getMessage());\n        assertEquals(\"Invalid permissionType UNKNOWN\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateNewAcl(new AclBinding(\n                    new ResourcePattern(TOPIC, \"*\", LITERAL),\n                    new AccessControlEntry(\"User:*\", \"*\", ALTER, AclPermissionType.UNKNOWN)))).\n                getMessage());\n    }\n\n    /**\n     * Verify that validateFilter catches invalid filters.\n     */\n    @Test\n    public void testValidateFilter() {\n        AclControlManager.validateFilter(new AclBindingFilter(\n            new ResourcePatternFilter(ResourceType.ANY, \"*\", LITERAL),\n            new AccessControlEntryFilter(\"User:*\", \"*\", AclOperation.ANY, AclPermissionType.ANY)));\n        assertEquals(\"Unknown patternFilter.\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateFilter(new AclBindingFilter(\n                    new ResourcePatternFilter(ResourceType.ANY, \"*\", PatternType.UNKNOWN),\n                    new AccessControlEntryFilter(\"User:*\", \"*\", AclOperation.ANY, AclPermissionType.ANY)))).\n                getMessage());\n        assertEquals(\"Unknown entryFilter.\",\n            assertThrows(InvalidRequestException.class, () ->\n                AclControlManager.validateFilter(new AclBindingFilter(\n                    new ResourcePatternFilter(ResourceType.ANY, \"*\", MATCH),\n                    new AccessControlEntryFilter(\"User:*\", \"*\", AclOperation.ANY, AclPermissionType.UNKNOWN)))).\n                getMessage());\n    }\n\n    static class MockClusterMetadataAuthorizer implements ClusterMetadataAuthorizer {\n        Map<Uuid, StandardAcl> acls = Collections.emptyMap();\n\n        @Override\n        public void setAclMutator(AclMutator aclMutator) {\n            // do nothing\n        }\n\n        @Override\n        public AclMutator aclMutatorOrException() {\n            throw new NotControllerException(\"The current node is not the active controller.\");\n        }\n\n        @Override\n        public void completeInitialLoad() {\n            // do nothing\n        }\n\n        @Override\n        public void completeInitialLoad(Exception e) {\n            // do nothing\n        }\n\n        @Override\n        public void loadSnapshot(Map<Uuid, StandardAcl> acls) {\n            this.acls = new HashMap<>(acls);\n        }\n\n        @Override\n        public void addAcl(Uuid id, StandardAcl acl) {\n            // do nothing\n        }\n\n        @Override\n        public void removeAcl(Uuid id) {\n            // do nothing\n        }\n\n        @Override\n        public Map<Endpoint, ? extends CompletionStage<Void>> start(AuthorizerServerInfo serverInfo) {\n            return null; // do nothing\n        }\n\n        @Override\n        public List<AuthorizationResult> authorize(AuthorizableRequestContext requestContext, List<Action> actions) {\n            return null; // do nothing\n        }\n\n        @Override\n        public Iterable<AclBinding> acls(AclBindingFilter filter) {\n            return null; // do nothing\n        }\n\n        @Override\n        public void close() throws IOException {\n            // do nothing\n        }\n\n        @Override\n        public void configure(Map<String, ?> configs) {\n            // do nothing\n        }\n    }\n\n    @Test\n    public void testLoadSnapshot() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        idempotentCreateSnapshot(snapshotRegistry);\n        AclControlManager manager = new AclControlManager.Builder().\n            setSnapshotRegistry(snapshotRegistry).\n            build();\n\n        // Load TEST_ACLS into the AclControlManager.\n        Set<ApiMessageAndVersion> loadedAcls = new HashSet<>();\n        for (StandardAclWithId acl : TEST_ACLS) {\n            AccessControlEntryRecord record = acl.toRecord();\n            assertTrue(loadedAcls.add(new ApiMessageAndVersion(record, (short) 0)));\n            manager.replay(acl.toRecord());\n        }\n\n        // Verify that the ACLs stored in the AclControlManager match the ones we expect.\n        Set<ApiMessageAndVersion> foundAcls = new HashSet<>();\n        for (Map.Entry<Uuid, StandardAcl> entry : manager.idToAcl().entrySet()) {\n            foundAcls.add(new ApiMessageAndVersion(\n                    new StandardAclWithId(entry.getKey(), entry.getValue()).toRecord(), (short) 0));\n        }\n        assertEquals(loadedAcls, foundAcls);\n\n        // Once we complete the snapshot load, the ACLs should be reflected in the authorizer.\n        MockClusterMetadataAuthorizer authorizer = new MockClusterMetadataAuthorizer();\n        authorizer.loadSnapshot(manager.idToAcl());\n        assertEquals(new HashSet<>(StandardAclTest.TEST_ACLS), new HashSet<>(authorizer.acls.values()));\n\n        // Test reverting to an empty state and then completing the snapshot load without\n        // setting an authorizer. This simulates the case where the user didn't configure\n        // a cluster metadata authorizer.\n        snapshotRegistry.revertToSnapshot(0);\n        authorizer.loadSnapshot(manager.idToAcl());\n        assertTrue(manager.idToAcl().isEmpty());\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(0);\n    }\n\n    @Test\n    public void testAddAndDelete() {\n        AclControlManager manager = new AclControlManager.Builder().build();\n        MockClusterMetadataAuthorizer authorizer = new MockClusterMetadataAuthorizer();\n        authorizer.loadSnapshot(manager.idToAcl());\n        manager.replay(StandardAclWithIdTest.TEST_ACLS.get(0).toRecord());\n        manager.replay(new RemoveAccessControlEntryRecord().\n            setId(TEST_ACLS.get(0).id()));\n        assertTrue(manager.idToAcl().isEmpty());\n    }\n\n    @Test\n    public void testCreateAclDeleteAcl() {\n        AclControlManager manager = new AclControlManager.Builder().build();\n        MockClusterMetadataAuthorizer authorizer = new MockClusterMetadataAuthorizer();\n        authorizer.loadSnapshot(manager.idToAcl());\n\n        List<AclBinding> toCreate = new ArrayList<>();\n        for (int i = 0; i < 3; i++) {\n            toCreate.add(TEST_ACLS.get(i).toBinding());\n        }\n        toCreate.add(new AclBinding(\n            new ResourcePattern(TOPIC, \"*\", PatternType.UNKNOWN),\n            new AccessControlEntry(\"User:*\", \"*\", ALTER, ALLOW)));\n\n        ControllerResult<List<AclCreateResult>> createResult = manager.createAcls(toCreate);\n\n        List<AclCreateResult> expectedResults = new ArrayList<>();\n        for (int i = 0; i < 3; i++) {\n            expectedResults.add(AclCreateResult.SUCCESS);\n        }\n        expectedResults.add(new AclCreateResult(\n            new InvalidRequestException(\"Invalid patternType UNKNOWN\")));\n\n        for (int i = 0; i < expectedResults.size(); i++) {\n            AclCreateResult expectedResult = expectedResults.get(i);\n            if (expectedResult.exception().isPresent()) {\n                assertEquals(expectedResult.exception().get().getMessage(),\n                    createResult.response().get(i).exception().get().getMessage());\n            } else {\n                assertFalse(createResult.response().get(i).exception().isPresent());\n            }\n        }\n        RecordTestUtils.replayAll(manager, createResult.records());\n        assertFalse(manager.idToAcl().isEmpty());\n\n        ControllerResult<List<AclDeleteResult>> deleteResult =\n            manager.deleteAcls(Arrays.asList(\n                new AclBindingFilter(\n                    new ResourcePatternFilter(ResourceType.ANY, null, LITERAL),\n                        AccessControlEntryFilter.ANY),\n                new AclBindingFilter(\n                    new ResourcePatternFilter(ResourceType.UNKNOWN, null, LITERAL),\n                        AccessControlEntryFilter.ANY)));\n        assertEquals(2, deleteResult.response().size());\n        Set<AclBinding> deleted = new HashSet<>();\n        for (AclDeleteResult.AclBindingDeleteResult result :\n                deleteResult.response().get(0).aclBindingDeleteResults()) {\n            assertEquals(Optional.empty(), result.exception());\n            deleted.add(result.aclBinding());\n        }\n        assertEquals(new HashSet<>(Arrays.asList(\n            TEST_ACLS.get(0).toBinding(),\n                TEST_ACLS.get(2).toBinding())), deleted);\n        assertEquals(InvalidRequestException.class,\n            deleteResult.response().get(1).exception().get().getClass());\n        RecordTestUtils.replayAll(manager, deleteResult.records());\n\n        Iterator<Map.Entry<Uuid, StandardAcl>> iterator = manager.idToAcl().entrySet().iterator();\n        assertEquals(TEST_ACLS.get(1).acl(), iterator.next().getValue());\n        assertFalse(iterator.hasNext());\n    }\n\n    @Test\n    public void testDeleteDedupe() {\n        AclControlManager manager = new AclControlManager.Builder().build();\n        MockClusterMetadataAuthorizer authorizer = new MockClusterMetadataAuthorizer();\n        authorizer.loadSnapshot(manager.idToAcl());\n\n        AclBinding aclBinding = new AclBinding(new ResourcePattern(TOPIC, \"topic-1\", LITERAL),\n                new AccessControlEntry(\"User:user\", \"10.0.0.1\", AclOperation.ALL, ALLOW));\n\n        ControllerResult<List<AclCreateResult>> createResult = manager.createAcls(Collections.singletonList(aclBinding));\n        Uuid id = ((AccessControlEntryRecord) createResult.records().get(0).message()).id();\n        assertEquals(1, createResult.records().size());\n\n        ControllerResult<List<AclDeleteResult>> deleteAclResultsAnyFilter = manager.deleteAcls(Collections.singletonList(AclBindingFilter.ANY));\n        assertEquals(1, deleteAclResultsAnyFilter.records().size());\n        assertEquals(id, ((RemoveAccessControlEntryRecord) deleteAclResultsAnyFilter.records().get(0).message()).id());\n        assertEquals(1, deleteAclResultsAnyFilter.response().size());\n\n        ControllerResult<List<AclDeleteResult>> deleteAclResultsSpecificFilter = manager.deleteAcls(Collections.singletonList(aclBinding.toFilter()));\n        assertEquals(1, deleteAclResultsSpecificFilter.records().size());\n        assertEquals(id, ((RemoveAccessControlEntryRecord) deleteAclResultsSpecificFilter.records().get(0).message()).id());\n        assertEquals(1, deleteAclResultsSpecificFilter.response().size());\n\n        ControllerResult<List<AclDeleteResult>> deleteAclResultsBothFilters = manager.deleteAcls(Arrays.asList(AclBindingFilter.ANY, aclBinding.toFilter()));\n        assertEquals(1, deleteAclResultsBothFilters.records().size());\n        assertEquals(id, ((RemoveAccessControlEntryRecord) deleteAclResultsBothFilters.records().get(0).message()).id());\n        assertEquals(2, deleteAclResultsBothFilters.response().size());\n    }\n}",
                "methodCount": 19
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3954550031395101
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testUpdateFeatures() : void in class org.apache.kafka.controller.FeatureControlManagerTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 95,
                    "endLine": 125,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testUpdateFeatures() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 103,
                    "endLine": 103,
                    "startColumn": 9,
                    "endColumn": 50,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 94,
                    "endLine": 124,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testUpdateFeatures() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 102,
                    "endLine": 102,
                    "startColumn": 9,
                    "endColumn": 54,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(-1)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 627,
        "extraction_results": {
            "success": true,
            "newCommitHash": "37798d9b1c134ab3b36c3c31004ba933de6687bd",
            "newBranchName": "extract-idempotentCreateSnapshot-testUpdateFeatures-130af38"
        },
        "telemetry": {
            "id": "6e6cc5f3-2001-483e-91eb-7749a37b011b",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 405,
                "lineStart": 54,
                "lineEnd": 458,
                "bodyLineStart": 54,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                "sourceCode": "@Timeout(value = 40)\npublic class FeatureControlManagerTest {\n\n    @SuppressWarnings(\"unchecked\")\n    private static Map<String, VersionRange> rangeMap(Object... args) {\n        Map<String, VersionRange> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 3) {\n            String feature = (String) args[i];\n            Number low = (Number) args[i + 1];\n            Number high = (Number) args[i + 2];\n            result.put(feature, VersionRange.of(low.shortValue(), high.shortValue()));\n        }\n        return result;\n    }\n\n    private static Map<String, Short> versionMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    public static QuorumFeatures features(Object... args) {\n        Map<String, VersionRange> features = QuorumFeatures.defaultFeatureMap(true);\n        features.putAll(rangeMap(args));\n        return new QuorumFeatures(0, features, emptyList());\n    }\n\n    private static Map<String, Short> updateMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    @Test\n    public void testUpdateFeatures() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(\"foo\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(new FinalizedControllerFeatures(Collections.singletonMap(\"metadata.version\", (short) 4), -1),\n            manager.finalizedFeatures(-1));\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 3 for feature foo. Local controller 0 only supports versions 1-2\"))),\n            manager.updateFeatures(updateMap(\"foo\", 3),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false));\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n                updateMap(\"foo\", 2, \"bar\", 1), Collections.emptyMap(),\n                false);\n        Map<String, ApiError> expectedMap = new HashMap<>();\n        expectedMap.put(\"foo\", ApiError.NONE);\n        expectedMap.put(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 1 for feature bar. Local controller 0 does not support this feature.\"));\n        assertEquals(expectedMap, result.response());\n        List<ApiMessageAndVersion> expectedMessages = new ArrayList<>();\n        expectedMessages.add(new ApiMessageAndVersion(new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2),\n            (short) 0));\n        assertEquals(expectedMessages, result.records());\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(-1);\n    }\n\n    @Test\n    public void testReplay() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureLevelRecord record = new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2);\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setLogContext(logContext).\n                setQuorumFeatures(features(\"foo\", 1, 2)).\n                setSnapshotRegistry(snapshotRegistry).\n                setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n                build();\n        manager.replay(record);\n        snapshotRegistry.getOrCreateSnapshot(123);\n        assertEquals(new FinalizedControllerFeatures(versionMap(\"metadata.version\", 4, \"foo\", 2), 123),\n            manager.finalizedFeatures(123));\n    }\n\n    static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(\n        List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges,\n        List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges\n    ) {\n        return new ClusterFeatureSupportDescriber() {\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> brokerSupported() {\n                return brokerRanges.iterator();\n            }\n\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> controllerSupported() {\n                return controllerRanges.iterator();\n            }\n        };\n    }\n\n    @Test\n    public void testUpdateFeaturesErrorCases() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 0, 3)).\n            setSnapshotRegistry(snapshotRegistry).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(5, singletonMap(\"bar\", VersionRange.of(0, 3)))),\n                emptyList())).\n            build();\n\n        assertEquals(ControllerResult.atomicOf(emptyList(),\n            Collections.singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature foo. Broker 5 does not support this feature.\"))),\n                    manager.updateFeatures(updateMap(\"foo\", 3),\n                        Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        false));\n\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            updateMap(\"bar\", 3), Collections.emptyMap(), false);\n        assertEquals(Collections.singletonMap(\"bar\", ApiError.NONE), result.response());\n        manager.replay((FeatureLevelRecord) result.records().get(0).message());\n        snapshotRegistry.getOrCreateSnapshot(3);\n\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 2 for feature bar. Can't downgrade the version of this feature \" +\n                    \"without setting the upgrade type to either safe or unsafe downgrade.\"))),\n            manager.updateFeatures(updateMap(\"bar\", 2), Collections.emptyMap(), false));\n\n        assertEquals(\n            ControllerResult.atomicOf(\n                Collections.singletonList(\n                    new ApiMessageAndVersion(\n                        new FeatureLevelRecord()\n                            .setName(\"bar\")\n                            .setFeatureLevel((short) 2),\n                        (short) 0\n                    )\n                ),\n                Collections.singletonMap(\"bar\", ApiError.NONE)\n            ),\n            manager.updateFeatures(\n                updateMap(\"bar\", 2),\n                Collections.singletonMap(\"bar\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false)\n        );\n    }\n\n    @Test\n    public void testReplayRecords() throws Exception {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        ControllerResult<Map<String, ApiError>> result = manager.\n            updateFeatures(updateMap(\"foo\", 5, \"bar\", 1), Collections.emptyMap(), false);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_3_IV0, manager.metadataVersion());\n        assertEquals(Optional.of((short) 5), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"bar\"));\n        assertEquals(new HashSet<>(Arrays.asList(\n            MetadataVersion.FEATURE_NAME, \"foo\", \"bar\")),\n                manager.finalizedFeatures(Long.MAX_VALUE).featureNames());\n    }\n\n    private static final FeatureControlManager.Builder TEST_MANAGER_BUILDER1 =\n        new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_3_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV2);\n\n    @Test\n    public void testApplyMetadataVersionChangeRecord() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        manager.replay(new FeatureLevelRecord().\n            setName(MetadataVersion.FEATURE_NAME).\n            setFeatureLevel(MetadataVersion.IBP_3_3_IV3.featureLevel()));\n        assertEquals(MetadataVersion.IBP_3_3_IV3, manager.metadataVersion());\n    }\n\n    @Test\n    public void testCannotDowngradeToVersionBeforeMinimumSupportedKraftVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature metadata.version. Local controller 0 only \" +\n                \"supports versions 4-7\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUnsafeDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUpgradeToLowerVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 4 for feature metadata.version. Can't downgrade the \" +\n                \"version of this feature without setting the upgrade type to either safe or \" +\n                \"unsafe downgrade.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCanUpgradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUseSafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Refusing to perform the requested downgrade because \" +\n                \"it might delete metadata information.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testUnsafeDowngradeIsTemporarilyDisabled() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                                \"Invalid metadata.version 4. Unsafe metadata downgrade is not supported in this version.\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Disabled\n    @Test\n    public void testCanUseUnsafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCanUseSafeDowngradeIfMetadataDidNotChange() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                        MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV1.featureLevel())).\n                setMetadataVersion(MetadataVersion.IBP_3_1_IV0).\n                setMinimumBootstrapVersion(MetadataVersion.IBP_3_0_IV0).\n                build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_0_IV1.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCannotDowngradeBefore3_3_IV0() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                        \"Invalid metadata.version 3. Unable to set a metadata.version less than 3.3-IV0\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCreateFeatureLevelRecords() {\n        Map<String, VersionRange> localSupportedFeatures = new HashMap<>();\n        localSupportedFeatures.put(MetadataVersion.FEATURE_NAME, VersionRange.of(\n            MetadataVersion.IBP_3_0_IV1.featureLevel(), MetadataVersion.latestTesting().featureLevel()));\n        localSupportedFeatures.put(\"foo\", VersionRange.of(0, 2));\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(new QuorumFeatures(0, localSupportedFeatures, emptyList())).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(1, singletonMap(\"foo\", VersionRange.of(0, 3)))),\n                emptyList())).\n                build();\n        ControllerResult<Map<String, ApiError>> result  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 1),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UPGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 1), (short) 0)),\n                        Collections.singletonMap(\"foo\", ApiError.NONE)), result);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n\n        ControllerResult<Map<String, ApiError>> result2  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 0),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                        new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 0), (short) 0)),\n                Collections.singletonMap(\"foo\", ApiError.NONE)), result2);\n        RecordTestUtils.replayAll(manager, result2.records());\n        assertEquals(Optional.empty(), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n    }\n\n    @Test\n    public void testNoMetadataVersionChangeDuringMigration() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                    MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_5_IV1.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_4_IV0).\n            build();\n        BootstrapMetadata bootstrapMetadata = BootstrapMetadata.fromVersion(MetadataVersion.IBP_3_4_IV0, \"FeatureControlManagerTest\");\n        RecordTestUtils.replayAll(manager, bootstrapMetadata.records());\n        RecordTestUtils.replayOne(manager, ZkMigrationState.PRE_MIGRATION.toRecord());\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 10. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n\n        // Complete the migration\n        RecordTestUtils.replayOne(manager, ZkMigrationState.POST_MIGRATION.toRecord());\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n            singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n            false);\n        assertEquals(Errors.NONE, result.response().get(MetadataVersion.FEATURE_NAME).error());\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_5_IV1, manager.metadataVersion());\n    }\n}",
                "methodCount": 25
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3709228592777801
                },
                "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)": {
                    "first": {
                        "method_name": "rangeMap",
                        "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3786155032226575
                },
                "private static versionMap(Object... args)": {
                    "first": {
                        "method_name": "versionMap",
                        "method_signature": "private static versionMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3841412431138385
                },
                "private static updateMap(Object... args)": {
                    "first": {
                        "method_name": "updateMap",
                        "method_signature": "private static updateMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4234007526040948
                },
                "public static features(Object... args)": {
                    "first": {
                        "method_name": "features",
                        "method_signature": "public static features(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.43765337557552575
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testReplay() : void in class org.apache.kafka.controller.FeatureControlManagerTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 127,
                    "endLine": 145,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testReplay() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 134,
                    "endLine": 134,
                    "startColumn": 9,
                    "endColumn": 50,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 126,
                    "endLine": 144,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testReplay() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 133,
                    "endLine": 133,
                    "startColumn": 9,
                    "endColumn": 54,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(-1)"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 141,
                    "endLine": 141,
                    "startColumn": 9,
                    "endColumn": 55,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(123)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 628,
        "extraction_results": {
            "success": true,
            "newCommitHash": "8f45b80fb602932dd6473cbc2a55946fd956e4b5",
            "newBranchName": "extract-idempotentCreateSnapshot-testReplay-130af38"
        },
        "telemetry": {
            "id": "ca7845a6-1750-49c0-a12c-80727b8c7d8c",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 405,
                "lineStart": 54,
                "lineEnd": 458,
                "bodyLineStart": 54,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                "sourceCode": "@Timeout(value = 40)\npublic class FeatureControlManagerTest {\n\n    @SuppressWarnings(\"unchecked\")\n    private static Map<String, VersionRange> rangeMap(Object... args) {\n        Map<String, VersionRange> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 3) {\n            String feature = (String) args[i];\n            Number low = (Number) args[i + 1];\n            Number high = (Number) args[i + 2];\n            result.put(feature, VersionRange.of(low.shortValue(), high.shortValue()));\n        }\n        return result;\n    }\n\n    private static Map<String, Short> versionMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    public static QuorumFeatures features(Object... args) {\n        Map<String, VersionRange> features = QuorumFeatures.defaultFeatureMap(true);\n        features.putAll(rangeMap(args));\n        return new QuorumFeatures(0, features, emptyList());\n    }\n\n    private static Map<String, Short> updateMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    @Test\n    public void testUpdateFeatures() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(\"foo\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        idempotentCreateSnapshot(snapshotRegistry);\n        assertEquals(new FinalizedControllerFeatures(Collections.singletonMap(\"metadata.version\", (short) 4), -1),\n            manager.finalizedFeatures(-1));\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 3 for feature foo. Local controller 0 only supports versions 1-2\"))),\n            manager.updateFeatures(updateMap(\"foo\", 3),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false));\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n                updateMap(\"foo\", 2, \"bar\", 1), Collections.emptyMap(),\n                false);\n        Map<String, ApiError> expectedMap = new HashMap<>();\n        expectedMap.put(\"foo\", ApiError.NONE);\n        expectedMap.put(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 1 for feature bar. Local controller 0 does not support this feature.\"));\n        assertEquals(expectedMap, result.response());\n        List<ApiMessageAndVersion> expectedMessages = new ArrayList<>();\n        expectedMessages.add(new ApiMessageAndVersion(new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2),\n            (short) 0));\n        assertEquals(expectedMessages, result.records());\n    }\n\n    @Test\n    public void testReplay() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureLevelRecord record = new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2);\n\n        idempotentCreateSnapshot(snapshotRegistry);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setLogContext(logContext).\n                setQuorumFeatures(features(\"foo\", 1, 2)).\n                setSnapshotRegistry(snapshotRegistry).\n                setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n                build();\n        manager.replay(record);\n        snapshotRegistry.getOrCreateSnapshot(123);\n        assertEquals(new FinalizedControllerFeatures(versionMap(\"metadata.version\", 4, \"foo\", 2), 123),\n            manager.finalizedFeatures(123));\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(-1);\n    }\n\n    static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(\n        List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges,\n        List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges\n    ) {\n        return new ClusterFeatureSupportDescriber() {\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> brokerSupported() {\n                return brokerRanges.iterator();\n            }\n\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> controllerSupported() {\n                return controllerRanges.iterator();\n            }\n        };\n    }\n\n    @Test\n    public void testUpdateFeaturesErrorCases() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 0, 3)).\n            setSnapshotRegistry(snapshotRegistry).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(5, singletonMap(\"bar\", VersionRange.of(0, 3)))),\n                emptyList())).\n            build();\n\n        assertEquals(ControllerResult.atomicOf(emptyList(),\n            Collections.singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature foo. Broker 5 does not support this feature.\"))),\n                    manager.updateFeatures(updateMap(\"foo\", 3),\n                        Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        false));\n\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            updateMap(\"bar\", 3), Collections.emptyMap(), false);\n        assertEquals(Collections.singletonMap(\"bar\", ApiError.NONE), result.response());\n        manager.replay((FeatureLevelRecord) result.records().get(0).message());\n        snapshotRegistry.getOrCreateSnapshot(3);\n\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 2 for feature bar. Can't downgrade the version of this feature \" +\n                    \"without setting the upgrade type to either safe or unsafe downgrade.\"))),\n            manager.updateFeatures(updateMap(\"bar\", 2), Collections.emptyMap(), false));\n\n        assertEquals(\n            ControllerResult.atomicOf(\n                Collections.singletonList(\n                    new ApiMessageAndVersion(\n                        new FeatureLevelRecord()\n                            .setName(\"bar\")\n                            .setFeatureLevel((short) 2),\n                        (short) 0\n                    )\n                ),\n                Collections.singletonMap(\"bar\", ApiError.NONE)\n            ),\n            manager.updateFeatures(\n                updateMap(\"bar\", 2),\n                Collections.singletonMap(\"bar\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false)\n        );\n    }\n\n    @Test\n    public void testReplayRecords() throws Exception {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        ControllerResult<Map<String, ApiError>> result = manager.\n            updateFeatures(updateMap(\"foo\", 5, \"bar\", 1), Collections.emptyMap(), false);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_3_IV0, manager.metadataVersion());\n        assertEquals(Optional.of((short) 5), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"bar\"));\n        assertEquals(new HashSet<>(Arrays.asList(\n            MetadataVersion.FEATURE_NAME, \"foo\", \"bar\")),\n                manager.finalizedFeatures(Long.MAX_VALUE).featureNames());\n    }\n\n    private static final FeatureControlManager.Builder TEST_MANAGER_BUILDER1 =\n        new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_3_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV2);\n\n    @Test\n    public void testApplyMetadataVersionChangeRecord() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        manager.replay(new FeatureLevelRecord().\n            setName(MetadataVersion.FEATURE_NAME).\n            setFeatureLevel(MetadataVersion.IBP_3_3_IV3.featureLevel()));\n        assertEquals(MetadataVersion.IBP_3_3_IV3, manager.metadataVersion());\n    }\n\n    @Test\n    public void testCannotDowngradeToVersionBeforeMinimumSupportedKraftVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature metadata.version. Local controller 0 only \" +\n                \"supports versions 4-7\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUnsafeDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUpgradeToLowerVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 4 for feature metadata.version. Can't downgrade the \" +\n                \"version of this feature without setting the upgrade type to either safe or \" +\n                \"unsafe downgrade.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCanUpgradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUseSafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Refusing to perform the requested downgrade because \" +\n                \"it might delete metadata information.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testUnsafeDowngradeIsTemporarilyDisabled() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                                \"Invalid metadata.version 4. Unsafe metadata downgrade is not supported in this version.\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Disabled\n    @Test\n    public void testCanUseUnsafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCanUseSafeDowngradeIfMetadataDidNotChange() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                        MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV1.featureLevel())).\n                setMetadataVersion(MetadataVersion.IBP_3_1_IV0).\n                setMinimumBootstrapVersion(MetadataVersion.IBP_3_0_IV0).\n                build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_0_IV1.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCannotDowngradeBefore3_3_IV0() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                        \"Invalid metadata.version 3. Unable to set a metadata.version less than 3.3-IV0\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCreateFeatureLevelRecords() {\n        Map<String, VersionRange> localSupportedFeatures = new HashMap<>();\n        localSupportedFeatures.put(MetadataVersion.FEATURE_NAME, VersionRange.of(\n            MetadataVersion.IBP_3_0_IV1.featureLevel(), MetadataVersion.latestTesting().featureLevel()));\n        localSupportedFeatures.put(\"foo\", VersionRange.of(0, 2));\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(new QuorumFeatures(0, localSupportedFeatures, emptyList())).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(1, singletonMap(\"foo\", VersionRange.of(0, 3)))),\n                emptyList())).\n                build();\n        ControllerResult<Map<String, ApiError>> result  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 1),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UPGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 1), (short) 0)),\n                        Collections.singletonMap(\"foo\", ApiError.NONE)), result);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n\n        ControllerResult<Map<String, ApiError>> result2  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 0),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                        new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 0), (short) 0)),\n                Collections.singletonMap(\"foo\", ApiError.NONE)), result2);\n        RecordTestUtils.replayAll(manager, result2.records());\n        assertEquals(Optional.empty(), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n    }\n\n    @Test\n    public void testNoMetadataVersionChangeDuringMigration() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                    MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_5_IV1.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_4_IV0).\n            build();\n        BootstrapMetadata bootstrapMetadata = BootstrapMetadata.fromVersion(MetadataVersion.IBP_3_4_IV0, \"FeatureControlManagerTest\");\n        RecordTestUtils.replayAll(manager, bootstrapMetadata.records());\n        RecordTestUtils.replayOne(manager, ZkMigrationState.PRE_MIGRATION.toRecord());\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 10. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n\n        // Complete the migration\n        RecordTestUtils.replayOne(manager, ZkMigrationState.POST_MIGRATION.toRecord());\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n            singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n            false);\n        assertEquals(Errors.NONE, result.response().get(MetadataVersion.FEATURE_NAME).error());\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_5_IV1, manager.metadataVersion());\n    }\n}",
                "methodCount": 25
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.37054625644332484
                },
                "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)": {
                    "first": {
                        "method_name": "rangeMap",
                        "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3758463718712335
                },
                "private static versionMap(Object... args)": {
                    "first": {
                        "method_name": "versionMap",
                        "method_signature": "private static versionMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.38324297957991427
                },
                "private static updateMap(Object... args)": {
                    "first": {
                        "method_name": "updateMap",
                        "method_signature": "private static updateMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.42233578702176866
                },
                "public static features(Object... args)": {
                    "first": {
                        "method_name": "features",
                        "method_signature": "public static features(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.43624428675365556
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "4e862c0903195eb11a715271c29304d28036273d",
        "url": "https://github.com/apache/kafka/commit/4e862c0903195eb11a715271c29304d28036273d",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public idempotentCreateSnapshot(epoch long) : void extracted from public testUpdateFeaturesErrorCases() : void in class org.apache.kafka.controller.FeatureControlManagerTest & moved to class org.apache.kafka.timeline.SnapshotRegistry",
            "leftSideLocations": [
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 164,
                    "endLine": 213,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testUpdateFeaturesErrorCases() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 188,
                    "endLine": 188,
                    "startColumn": 9,
                    "endColumn": 49,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 208,
                    "endLine": 218,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public idempotentCreateSnapshot(epoch long) : void"
                },
                {
                    "filePath": "server-common/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java",
                    "startLine": 217,
                    "endLine": 217,
                    "startColumn": 9,
                    "endColumn": 36,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 163,
                    "endLine": 212,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testUpdateFeaturesErrorCases() : void"
                },
                {
                    "filePath": "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                    "startLine": 187,
                    "endLine": 187,
                    "startColumn": 9,
                    "endColumn": 53,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "snapshotRegistry.idempotentCreateSnapshot(3)"
                }
            ],
            "isStatic": false
        },
        "ref_id": 629,
        "extraction_results": {
            "success": true,
            "newCommitHash": "bed236d73c831ae8aa19c7bdaed32680482e4f73",
            "newBranchName": "extract-idempotentCreateSnapshot-testUpdateFeaturesErrorCases-130af38"
        },
        "telemetry": {
            "id": "647fb86f-761e-49d3-9f52-f54bc61f36da",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 405,
                "lineStart": 54,
                "lineEnd": 458,
                "bodyLineStart": 54,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
                "sourceCode": "@Timeout(value = 40)\npublic class FeatureControlManagerTest {\n\n    @SuppressWarnings(\"unchecked\")\n    private static Map<String, VersionRange> rangeMap(Object... args) {\n        Map<String, VersionRange> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 3) {\n            String feature = (String) args[i];\n            Number low = (Number) args[i + 1];\n            Number high = (Number) args[i + 2];\n            result.put(feature, VersionRange.of(low.shortValue(), high.shortValue()));\n        }\n        return result;\n    }\n\n    private static Map<String, Short> versionMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    public static QuorumFeatures features(Object... args) {\n        Map<String, VersionRange> features = QuorumFeatures.defaultFeatureMap(true);\n        features.putAll(rangeMap(args));\n        return new QuorumFeatures(0, features, emptyList());\n    }\n\n    private static Map<String, Short> updateMap(Object... args) {\n        Map<String, Short> result = new HashMap<>();\n        for (int i = 0; i < args.length; i += 2) {\n            String feature = (String) args[i];\n            Number ver = (Number) args[i + 1];\n            result.put(feature, ver.shortValue());\n        }\n        return result;\n    }\n\n    @Test\n    public void testUpdateFeatures() {\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(\"foo\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        snapshotRegistry.getOrCreateSnapshot(-1);\n        assertEquals(new FinalizedControllerFeatures(Collections.singletonMap(\"metadata.version\", (short) 4), -1),\n            manager.finalizedFeatures(-1));\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 3 for feature foo. Local controller 0 only supports versions 1-2\"))),\n            manager.updateFeatures(updateMap(\"foo\", 3),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false));\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n                updateMap(\"foo\", 2, \"bar\", 1), Collections.emptyMap(),\n                false);\n        Map<String, ApiError> expectedMap = new HashMap<>();\n        expectedMap.put(\"foo\", ApiError.NONE);\n        expectedMap.put(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 1 for feature bar. Local controller 0 does not support this feature.\"));\n        assertEquals(expectedMap, result.response());\n        List<ApiMessageAndVersion> expectedMessages = new ArrayList<>();\n        expectedMessages.add(new ApiMessageAndVersion(new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2),\n            (short) 0));\n        assertEquals(expectedMessages, result.records());\n    }\n\n    @Test\n    public void testReplay() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureLevelRecord record = new FeatureLevelRecord().\n            setName(\"foo\").setFeatureLevel((short) 2);\n\n        snapshotRegistry.getOrCreateSnapshot(-1);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setLogContext(logContext).\n                setQuorumFeatures(features(\"foo\", 1, 2)).\n                setSnapshotRegistry(snapshotRegistry).\n                setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n                build();\n        manager.replay(record);\n        snapshotRegistry.getOrCreateSnapshot(123);\n        assertEquals(new FinalizedControllerFeatures(versionMap(\"metadata.version\", 4, \"foo\", 2), 123),\n            manager.finalizedFeatures(123));\n    }\n\n    static ClusterFeatureSupportDescriber createFakeClusterFeatureSupportDescriber(\n        List<Map.Entry<Integer, Map<String, VersionRange>>> brokerRanges,\n        List<Map.Entry<Integer, Map<String, VersionRange>>> controllerRanges\n    ) {\n        return new ClusterFeatureSupportDescriber() {\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> brokerSupported() {\n                return brokerRanges.iterator();\n            }\n\n            @Override\n            public Iterator<Map.Entry<Integer, Map<String, VersionRange>>> controllerSupported() {\n                return controllerRanges.iterator();\n            }\n        };\n    }\n\n    @Test\n    public void testUpdateFeaturesErrorCases() {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 0, 3)).\n            setSnapshotRegistry(snapshotRegistry).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(5, singletonMap(\"bar\", VersionRange.of(0, 3)))),\n                emptyList())).\n            build();\n\n        assertEquals(ControllerResult.atomicOf(emptyList(),\n            Collections.singletonMap(\"foo\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature foo. Broker 5 does not support this feature.\"))),\n                    manager.updateFeatures(updateMap(\"foo\", 3),\n                        Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        false));\n\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            updateMap(\"bar\", 3), Collections.emptyMap(), false);\n        assertEquals(Collections.singletonMap(\"bar\", ApiError.NONE), result.response());\n        manager.replay((FeatureLevelRecord) result.records().get(0).message());\n        idempotentCreateSnapshot(snapshotRegistry);\n\n        assertEquals(ControllerResult.atomicOf(emptyList(), Collections.\n                singletonMap(\"bar\", new ApiError(Errors.INVALID_UPDATE_VERSION,\n                    \"Invalid update version 2 for feature bar. Can't downgrade the version of this feature \" +\n                    \"without setting the upgrade type to either safe or unsafe downgrade.\"))),\n            manager.updateFeatures(updateMap(\"bar\", 2), Collections.emptyMap(), false));\n\n        assertEquals(\n            ControllerResult.atomicOf(\n                Collections.singletonList(\n                    new ApiMessageAndVersion(\n                        new FeatureLevelRecord()\n                            .setName(\"bar\")\n                            .setFeatureLevel((short) 2),\n                        (short) 0\n                    )\n                ),\n                Collections.singletonMap(\"bar\", ApiError.NONE)\n            ),\n            manager.updateFeatures(\n                updateMap(\"bar\", 2),\n                Collections.singletonMap(\"bar\", FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                false)\n        );\n    }\n\n    private void idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry) {\n        snapshotRegistry.getOrCreateSnapshot(3);\n    }\n\n    @Test\n    public void testReplayRecords() throws Exception {\n        LogContext logContext = new LogContext();\n        SnapshotRegistry snapshotRegistry = new SnapshotRegistry(logContext);\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setLogContext(logContext).\n            setQuorumFeatures(features(\"foo\", 1, 5, \"bar\", 1, 2)).\n            setSnapshotRegistry(snapshotRegistry).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        ControllerResult<Map<String, ApiError>> result = manager.\n            updateFeatures(updateMap(\"foo\", 5, \"bar\", 1), Collections.emptyMap(), false);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_3_IV0, manager.metadataVersion());\n        assertEquals(Optional.of((short) 5), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"bar\"));\n        assertEquals(new HashSet<>(Arrays.asList(\n            MetadataVersion.FEATURE_NAME, \"foo\", \"bar\")),\n                manager.finalizedFeatures(Long.MAX_VALUE).featureNames());\n    }\n\n    private static final FeatureControlManager.Builder TEST_MANAGER_BUILDER1 =\n        new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_3_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV2);\n\n    @Test\n    public void testApplyMetadataVersionChangeRecord() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        manager.replay(new FeatureLevelRecord().\n            setName(MetadataVersion.FEATURE_NAME).\n            setFeatureLevel(MetadataVersion.IBP_3_3_IV3.featureLevel()));\n        assertEquals(MetadataVersion.IBP_3_3_IV3, manager.metadataVersion());\n    }\n\n    @Test\n    public void testCannotDowngradeToVersionBeforeMinimumSupportedKraftVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 3 for feature metadata.version. Local controller 0 only \" +\n                \"supports versions 4-7\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUnsafeDowngradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 7 for feature metadata.version. Can't downgrade to a \" +\n                \"newer version.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUpgradeToLowerVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid update version 4 for feature metadata.version. Can't downgrade the \" +\n                \"version of this feature without setting the upgrade type to either safe or \" +\n                \"unsafe downgrade.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCanUpgradeToHigherVersion() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n    }\n\n    @Test\n    public void testCannotUseSafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Refusing to perform the requested downgrade because \" +\n                \"it might delete metadata information.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n    }\n\n    @Test\n    public void testUnsafeDowngradeIsTemporarilyDisabled() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                                \"Invalid metadata.version 4. Unsafe metadata downgrade is not supported in this version.\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Disabled\n    @Test\n    public void testCanUseUnsafeDowngradeIfMetadataChanged() {\n        FeatureControlManager manager = TEST_MANAGER_BUILDER1.build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCanUseSafeDowngradeIfMetadataDidNotChange() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n                setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                        MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV1.featureLevel())).\n                setMetadataVersion(MetadataVersion.IBP_3_1_IV0).\n                setMinimumBootstrapVersion(MetadataVersion.IBP_3_0_IV0).\n                build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_0_IV1.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCannotDowngradeBefore3_3_IV0() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_3_IV0).\n            build();\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                        singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                        \"Invalid metadata.version 3. Unable to set a metadata.version less than 3.3-IV0\"))),\n                manager.updateFeatures(\n                        singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_2_IV0.featureLevel()),\n                        singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                        true));\n    }\n\n    @Test\n    public void testCreateFeatureLevelRecords() {\n        Map<String, VersionRange> localSupportedFeatures = new HashMap<>();\n        localSupportedFeatures.put(MetadataVersion.FEATURE_NAME, VersionRange.of(\n            MetadataVersion.IBP_3_0_IV1.featureLevel(), MetadataVersion.latestTesting().featureLevel()));\n        localSupportedFeatures.put(\"foo\", VersionRange.of(0, 2));\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(new QuorumFeatures(0, localSupportedFeatures, emptyList())).\n            setClusterFeatureSupportDescriber(createFakeClusterFeatureSupportDescriber(\n                Collections.singletonList(new SimpleImmutableEntry<>(1, singletonMap(\"foo\", VersionRange.of(0, 3)))),\n                emptyList())).\n                build();\n        ControllerResult<Map<String, ApiError>> result  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 1),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UPGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 1), (short) 0)),\n                        Collections.singletonMap(\"foo\", ApiError.NONE)), result);\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(Optional.of((short) 1), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n\n        ControllerResult<Map<String, ApiError>> result2  = manager.updateFeatures(\n                Collections.singletonMap(\"foo\", (short) 0),\n                Collections.singletonMap(\"foo\", FeatureUpdate.UpgradeType.UNSAFE_DOWNGRADE),\n                false);\n        assertEquals(ControllerResult.atomicOf(Collections.singletonList(new ApiMessageAndVersion(\n                        new FeatureLevelRecord().setName(\"foo\").setFeatureLevel((short) 0), (short) 0)),\n                Collections.singletonMap(\"foo\", ApiError.NONE)), result2);\n        RecordTestUtils.replayAll(manager, result2.records());\n        assertEquals(Optional.empty(), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));\n    }\n\n    @Test\n    public void testNoMetadataVersionChangeDuringMigration() {\n        FeatureControlManager manager = new FeatureControlManager.Builder().\n            setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,\n                    MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_5_IV1.featureLevel())).\n            setMetadataVersion(MetadataVersion.IBP_3_4_IV0).\n            build();\n        BootstrapMetadata bootstrapMetadata = BootstrapMetadata.fromVersion(MetadataVersion.IBP_3_4_IV0, \"FeatureControlManagerTest\");\n        RecordTestUtils.replayAll(manager, bootstrapMetadata.records());\n        RecordTestUtils.replayOne(manager, ZkMigrationState.PRE_MIGRATION.toRecord());\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n            singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 10. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n                true));\n\n        assertEquals(ControllerResult.of(Collections.emptyList(),\n                singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,\n                \"Invalid metadata.version 4. Unable to modify metadata.version while a ZK migration is in progress.\"))),\n            manager.updateFeatures(\n                singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),\n                singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),\n                true));\n\n        // Complete the migration\n        RecordTestUtils.replayOne(manager, ZkMigrationState.POST_MIGRATION.toRecord());\n        ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(\n            singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),\n            singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),\n            false);\n        assertEquals(Errors.NONE, result.response().get(MetadataVersion.FEATURE_NAME).error());\n        RecordTestUtils.replayAll(manager, result.records());\n        assertEquals(MetadataVersion.IBP_3_5_IV1, manager.metadataVersion());\n    }\n}",
                "methodCount": 25
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "rangeMap",
                            "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "versionMap",
                            "method_signature": "private static versionMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "idempotentCreateSnapshot",
                            "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "updateMap",
                            "method_signature": "private static updateMap(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "features",
                            "method_signature": "public static features(Object... args)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)": {
                    "first": {
                        "method_name": "rangeMap",
                        "method_signature": "@SuppressWarnings(\"unchecked\")\n    private static rangeMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3787569842864361
                },
                "private static versionMap(Object... args)": {
                    "first": {
                        "method_name": "versionMap",
                        "method_signature": "private static versionMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.38476394576743
                },
                "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)": {
                    "first": {
                        "method_name": "idempotentCreateSnapshot",
                        "method_signature": "private idempotentCreateSnapshot(SnapshotRegistry snapshotRegistry)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.38740872259346915
                },
                "private static updateMap(Object... args)": {
                    "first": {
                        "method_name": "updateMap",
                        "method_signature": "private static updateMap(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4226024914510052
                },
                "public static features(Object... args)": {
                    "first": {
                        "method_name": "features",
                        "method_signature": "public static features(Object... args)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4397284130415836
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "b4d5f163a9c5a480efe0a1de3bebf12e35be2a7f",
        "url": "https://github.com/apache/kafka/commit/b4d5f163a9c5a480efe0a1de3bebf12e35be2a7f",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public votedKey() : Optional<ReplicaKey> extracted from public testElectionTimeout() : void in class org.apache.kafka.raft.VotedStateTest & moved to class org.apache.kafka.raft.UnattachedState",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 57,
                    "endLine": 78,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public testElectionTimeout() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                    "startLine": 63,
                    "endLine": 63,
                    "startColumn": 9,
                    "endColumn": 50,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/UnattachedState.java",
                    "startLine": 99,
                    "endLine": 101,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public votedKey() : Optional<ReplicaKey>"
                },
                {
                    "filePath": "raft/src/main/java/org/apache/kafka/raft/UnattachedState.java",
                    "startLine": 100,
                    "endLine": 100,
                    "startColumn": 9,
                    "endColumn": 25,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/UnattachedStateWithVoteTest.java",
                    "startLine": 59,
                    "endLine": 80,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public testElectionTimeout() : void"
                },
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/UnattachedStateWithVoteTest.java",
                    "startLine": 65,
                    "endLine": 65,
                    "startColumn": 32,
                    "endColumn": 48,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "state.votedKey()"
                }
            ],
            "isStatic": false
        },
        "ref_id": 630,
        "extraction_results": {
            "success": true,
            "newCommitHash": "0545041c57c66aa4d42a43367f40b0139da63df4",
            "newBranchName": "extract-votedKey-testElectionTimeout-e1b2ade"
        },
        "telemetry": {
            "id": "e7f3d2d3-4881-486e-bf2f-2cedfda6dca3",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 93,
                "lineStart": 35,
                "lineEnd": 127,
                "bodyLineStart": 35,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java",
                "sourceCode": "class VotedStateTest {\n\n    private final MockTime time = new MockTime();\n    private final LogContext logContext = new LogContext();\n    private final int epoch = 5;\n    private final int votedId = 1;\n    private final int electionTimeoutMs = 10000;\n\n    private VotedState newVotedState(\n        Uuid votedDirectoryId\n    ) {\n        return new VotedState(\n            time,\n            epoch,\n            ReplicaKey.of(votedId, votedDirectoryId),\n            Collections.emptySet(),\n            Optional.empty(),\n            electionTimeoutMs,\n            logContext\n        );\n    }\n\n    @Test\n    public void testElectionTimeout() {\n        VotedState state = newVotedState(ReplicaKey.NO_DIRECTORY_ID);\n        ReplicaKey votedKey  = ReplicaKey.of(votedId, ReplicaKey.NO_DIRECTORY_ID);\n\n        assertEquals(epoch, state.epoch());\n        votedKey(state, votedKey);\n        assertEquals(\n            ElectionState.withVotedCandidate(epoch, votedKey, Collections.emptySet()),\n            state.election()\n        );\n        assertEquals(electionTimeoutMs, state.remainingElectionTimeMs(time.milliseconds()));\n        assertFalse(state.hasElectionTimeoutExpired(time.milliseconds()));\n\n        time.sleep(5000);\n        assertEquals(electionTimeoutMs - 5000, state.remainingElectionTimeMs(time.milliseconds()));\n        assertFalse(state.hasElectionTimeoutExpired(time.milliseconds()));\n\n        time.sleep(5000);\n        assertEquals(0, state.remainingElectionTimeMs(time.milliseconds()));\n        assertTrue(state.hasElectionTimeoutExpired(time.milliseconds()));\n    }\n\n    private void votedKey(VotedState state, ReplicaKey votedKey) {\n        assertEquals(votedKey, state.votedKey());\n    }\n\n    @ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public void testCanGrantVoteWithoutDirectoryId(boolean isLogUpToDate) {\n        VotedState state = newVotedState(ReplicaKey.NO_DIRECTORY_ID);\n\n        assertTrue(\n            state.canGrantVote(ReplicaKey.of(votedId, ReplicaKey.NO_DIRECTORY_ID), isLogUpToDate)\n        );\n        assertTrue(\n            state.canGrantVote(\n                ReplicaKey.of(votedId, Uuid.randomUuid()),\n                isLogUpToDate\n            )\n        );\n\n        assertFalse(\n            state.canGrantVote(ReplicaKey.of(votedId + 1, ReplicaKey.NO_DIRECTORY_ID), isLogUpToDate)\n        );\n    }\n\n    @Test\n    void testCanGrantVoteWithDirectoryId() {\n        Uuid votedDirectoryId = Uuid.randomUuid();\n        VotedState state = newVotedState(votedDirectoryId);\n\n        assertTrue(state.canGrantVote(ReplicaKey.of(votedId, votedDirectoryId), false));\n\n        assertFalse(\n            state.canGrantVote(ReplicaKey.of(votedId, Uuid.randomUuid()), false)\n        );\n        assertFalse(state.canGrantVote(ReplicaKey.of(votedId, ReplicaKey.NO_DIRECTORY_ID), false));\n\n        assertFalse(state.canGrantVote(ReplicaKey.of(votedId + 1, votedDirectoryId), false));\n        assertFalse(state.canGrantVote(ReplicaKey.of(votedId + 1, ReplicaKey.NO_DIRECTORY_ID), false));\n    }\n\n    @Test\n    void testLeaderEndpoints() {\n        Uuid votedDirectoryId = Uuid.randomUuid();\n        VotedState state = newVotedState(votedDirectoryId);\n\n        assertEquals(Endpoints.empty(), state.leaderEndpoints());\n    }\n}",
                "methodCount": 6
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "newVotedState",
                            "method_signature": "private newVotedState(\n        Uuid votedDirectoryId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "votedKey",
                            "method_signature": "private votedKey(VotedState state, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testCanGrantVoteWithoutDirectoryId",
                            "method_signature": "@ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public testCanGrantVoteWithoutDirectoryId(boolean isLogUpToDate)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "votedKey",
                            "method_signature": "private votedKey(VotedState state, ReplicaKey votedKey)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "newVotedState",
                            "method_signature": "private newVotedState(\n        Uuid votedDirectoryId\n    )",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "testCanGrantVoteWithoutDirectoryId",
                            "method_signature": "@ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public testCanGrantVoteWithoutDirectoryId(boolean isLogUpToDate)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                "private votedKey(VotedState state, ReplicaKey votedKey)": {
                    "first": {
                        "method_name": "votedKey",
                        "method_signature": "private votedKey(VotedState state, ReplicaKey votedKey)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7425211199944738
                },
                "private newVotedState(\n        Uuid votedDirectoryId\n    )": {
                    "first": {
                        "method_name": "newVotedState",
                        "method_signature": "private newVotedState(\n        Uuid votedDirectoryId\n    )",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7751253568223777
                },
                "@ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public testCanGrantVoteWithoutDirectoryId(boolean isLogUpToDate)": {
                    "first": {
                        "method_name": "testCanGrantVoteWithoutDirectoryId",
                        "method_signature": "@ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public testCanGrantVoteWithoutDirectoryId(boolean isLogUpToDate)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.7784537771232207
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "8d29bc1fa8ce75b6807b957ea35ad39e8dd9e04a",
        "url": "https://github.com/apache/kafka/commit/8d29bc1fa8ce75b6807b957ea35ad39e8dd9e04a",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public newShareGroupSubscriptionMetadataRecord(groupId String, newSubscriptionMetadata Map<String,TopicMetadata>) : CoordinatorRecord extracted from private shareGroupHeartbeat(groupId String, memberId String, memberEpoch int, rackId String, clientId String, clientHost String, subscribedTopicNames List<String>) : CoordinatorResult<ShareGroupHeartbeatResponseData,CoordinatorRecord> in class org.apache.kafka.coordinator.group.GroupMetadataManager & moved to class org.apache.kafka.coordinator.group.CoordinatorRecordHelpers",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2062,
                    "endLine": 2213,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private shareGroupHeartbeat(groupId String, memberId String, memberEpoch int, rackId String, clientId String, clientHost String, subscribedTopicNames List<String>) : CoordinatorResult<ShareGroupHeartbeatResponseData,CoordinatorRecord>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2089,
                    "endLine": 2089,
                    "startColumn": 9,
                    "endColumn": 73,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2211,
                    "endLine": 2211,
                    "startColumn": 9,
                    "endColumn": 45,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 646,
                    "endLine": 688,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public newShareGroupSubscriptionMetadataRecord(groupId String, newSubscriptionMetadata Map<String,TopicMetadata>) : CoordinatorRecord"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 659,
                    "endLine": 659,
                    "startColumn": 13,
                    "endColumn": 108,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 662,
                    "endLine": 667,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2055,
                    "endLine": 2201,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private shareGroupHeartbeat(groupId String, memberId String, memberEpoch int, rackId String, clientId String, clientHost String, subscribedTopicNames List<String>) : CoordinatorResult<ShareGroupHeartbeatResponseData,CoordinatorRecord>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2143,
                    "endLine": 2143,
                    "startColumn": 29,
                    "endColumn": 99,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "newShareGroupSubscriptionMetadataRecord(groupId,subscriptionMetadata)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 657,
                    "endLine": 657,
                    "startColumn": 9,
                    "endColumn": 89,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 658,
                    "endLine": 675,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 677,
                    "endLine": 687,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 631,
        "extraction_results": {
            "success": true,
            "newCommitHash": "66511a520a3e8ea05d6531fa7fe6989099db9164",
            "newBranchName": "extract-newShareGroupSubscriptionMetadataRecord-shareGroupHeartbeat-ad08ec6"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "8d29bc1fa8ce75b6807b957ea35ad39e8dd9e04a",
        "url": "https://github.com/apache/kafka/commit/8d29bc1fa8ce75b6807b957ea35ad39e8dd9e04a",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public newShareGroupSubscriptionMetadataRecord(groupId String, newSubscriptionMetadata Map<String,TopicMetadata>) : CoordinatorRecord extracted from private shareGroupFenceMember(group ShareGroup, member ShareGroupMember, response T) : CoordinatorResult<T,CoordinatorRecord> in class org.apache.kafka.coordinator.group.GroupMetadataManager & moved to class org.apache.kafka.coordinator.group.CoordinatorRecordHelpers",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2766,
                    "endLine": 2813,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private shareGroupFenceMember(group ShareGroup, member ShareGroupMember, response T) : CoordinatorResult<T,CoordinatorRecord>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2780,
                    "endLine": 2780,
                    "startColumn": 9,
                    "endColumn": 67,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2810,
                    "endLine": 2810,
                    "startColumn": 9,
                    "endColumn": 45,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 646,
                    "endLine": 688,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public newShareGroupSubscriptionMetadataRecord(groupId String, newSubscriptionMetadata Map<String,TopicMetadata>) : CoordinatorRecord"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 659,
                    "endLine": 659,
                    "startColumn": 13,
                    "endColumn": 108,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 662,
                    "endLine": 667,
                    "startColumn": 17,
                    "endColumn": 19,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2756,
                    "endLine": 2794,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private shareGroupFenceMember(group ShareGroup, member ShareGroupMember, response T) : CoordinatorResult<T,CoordinatorRecord>"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
                    "startLine": 2784,
                    "endLine": 2784,
                    "startColumn": 25,
                    "endColumn": 103,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "newShareGroupSubscriptionMetadataRecord(group.groupId(),subscriptionMetadata)"
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 657,
                    "endLine": 657,
                    "startColumn": 9,
                    "endColumn": 89,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 658,
                    "endLine": 675,
                    "startColumn": 9,
                    "endColumn": 12,
                    "codeElementType": "EXPRESSION_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/CoordinatorRecordHelpers.java",
                    "startLine": 677,
                    "endLine": 687,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "added statement in extracted method declaration",
                    "codeElement": null
                }
            ],
            "isStatic": true
        },
        "ref_id": 632,
        "extraction_results": {
            "success": true,
            "newCommitHash": "24a13d857757026051ebda38f04cfbee3cf54b2e",
            "newBranchName": "extract-newShareGroupSubscriptionMetadataRecord-shareGroupFenceMember-ad08ec6"
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "932e84096a96e199c0772c81bddf3ea3789377ba",
        "url": "https://github.com/apache/kafka/commit/932e84096a96e199c0772c81bddf3ea3789377ba",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public onClose() : boolean extracted from public acknowledgeOnClose(acknowledgementsMap Map<TopicIdPartition,Acknowledgements>, deadlineMs long) : CompletableFuture<Void> in class org.apache.kafka.clients.consumer.internals.ShareConsumeRequestManager & moved to class org.apache.kafka.clients.consumer.internals.ShareConsumeRequestManager.AcknowledgeRequestState",
            "leftSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                    "startLine": 433,
                    "endLine": 496,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "public acknowledgeOnClose(acknowledgementsMap Map<TopicIdPartition,Acknowledgements>, deadlineMs long) : CompletableFuture<Void>"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                    "startLine": 457,
                    "endLine": 457,
                    "startColumn": 25,
                    "endColumn": 49,
                    "codeElementType": "IF_STATEMENT_CONDITION",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                    "startLine": 990,
                    "endLine": 992,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public onClose() : boolean"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                    "startLine": 991,
                    "endLine": 991,
                    "startColumn": 13,
                    "endColumn": 64,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                    "startLine": 434,
                    "endLine": 502,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "public acknowledgeOnClose(acknowledgementsMap Map<TopicIdPartition,Acknowledgements>, deadlineMs long) : CompletableFuture<Void>"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                    "startLine": 452,
                    "endLine": 498,
                    "startColumn": 9,
                    "endColumn": 11,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "sessionHandlers.forEach((nodeId,sessionHandler) -> {\n  Node node=cluster.nodeById(nodeId);\n  if (node != null) {\n    Map<TopicIdPartition,Acknowledgements> acknowledgementsMapForNode=new HashMap<>();\n    for (    TopicIdPartition tip : sessionHandler.sessionPartitions()) {\n      Acknowledgements acknowledgements=acknowledgementsMap.getOrDefault(tip,Acknowledgements.empty());\n      if (fetchAcknowledgementsMap.get(tip) != null) {\n        acknowledgements.merge(fetchAcknowledgementsMap.remove(tip));\n      }\n      if (acknowledgements != null && !acknowledgements.isEmpty()) {\n        acknowledgementsMapForNode.put(tip,acknowledgements);\n        metricsManager.recordAcknowledgementSent(acknowledgements.size());\n        log.debug(\"Added closing acknowledge request for partition {} to node {}\",tip.topicPartition(),node.id());\n        resultCount.incrementAndGet();\n      }\n    }\n    acknowledgeRequestStates.putIfAbsent(nodeId,new Pair<>(null,null));\n    if (acknowledgeRequestStates.get(nodeId).getSyncRequest() != null && !acknowledgeRequestStates.get(nodeId).getSyncRequest().isEmpty()) {\n      log.error(\"Attempt to call close() when there is an existing sync request for node {}-{}\",node.id(),acknowledgeRequestStates.get(nodeId).getSyncRequest());\n      closeFuture.completeExceptionally(new IllegalStateException(\"Attempt to call close() when there is an existing sync request for node : \" + node.id()));\n    }\n else {\n      acknowledgeRequestStates.get(nodeId).setSyncRequest(new AcknowledgeRequestState(logContext,ShareConsumeRequestManager.class.getSimpleName() + \":3\",deadlineMs,retryBackoffMs,retryBackoffMaxMs,sessionHandler,nodeId,acknowledgementsMapForNode,this::handleShareAcknowledgeCloseSuccess,this::handleShareAcknowledgeCloseFailure,resultHandler,AcknowledgeRequestType.CLOSE));\n    }\n  }\n}\n)"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                    "startLine": 482,
                    "endLine": 494,
                    "startColumn": 21,
                    "endColumn": 23,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "acknowledgeRequestStates.get(nodeId).setSyncRequest(new AcknowledgeRequestState(logContext,ShareConsumeRequestManager.class.getSimpleName() + \":3\",deadlineMs,retryBackoffMs,retryBackoffMaxMs,sessionHandler,nodeId,acknowledgementsMapForNode,this::handleShareAcknowledgeCloseSuccess,this::handleShareAcknowledgeCloseFailure,resultHandler,AcknowledgeRequestType.CLOSE))"
                }
            ],
            "isStatic": false
        },
        "ref_id": 633,
        "extraction_results": {
            "success": true,
            "newCommitHash": "9194b45995ef8c7fbbc4b94df1922f14be6036c8",
            "newBranchName": "extract-onClose-acknowledgeOnClose-f6bfa94"
        },
        "telemetry": {
            "id": "08faf94a-7324-402c-8971-0db040d373d7",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1025,
                "lineStart": 63,
                "lineEnd": 1087,
                "bodyLineStart": 63,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ShareConsumeRequestManager.java",
                "sourceCode": "/**\n * {@code ShareConsumeRequestManager} is responsible for generating {@link ShareFetchRequest} and\n * {@link ShareAcknowledgeRequest} to fetch and acknowledge records being delivered for a consumer\n * in a share group.\n */\n@SuppressWarnings(\"NPathComplexity\")\npublic class ShareConsumeRequestManager implements RequestManager, MemberStateListener, Closeable {\n    private final Time time;\n    private final Logger log;\n    private final LogContext logContext;\n    private final String groupId;\n    private final ConsumerMetadata metadata;\n    private final SubscriptionState subscriptions;\n    private final FetchConfig fetchConfig;\n    protected final ShareFetchBuffer shareFetchBuffer;\n    private final BackgroundEventHandler backgroundEventHandler;\n    private final Map<Integer, ShareSessionHandler> sessionHandlers;\n    private final Set<Integer> nodesWithPendingRequests;\n    private final ShareFetchMetricsManager metricsManager;\n    private final IdempotentCloser idempotentCloser = new IdempotentCloser();\n    private Uuid memberId;\n    private boolean fetchMoreRecords = false;\n    private final Map<TopicIdPartition, Acknowledgements> fetchAcknowledgementsMap;\n    private final Map<Integer, Pair<AcknowledgeRequestState>> acknowledgeRequestStates;\n    private final long retryBackoffMs;\n    private final long retryBackoffMaxMs;\n    private boolean closing = false;\n    private final CompletableFuture<Void> closeFuture;\n\n    ShareConsumeRequestManager(final Time time,\n                               final LogContext logContext,\n                               final String groupId,\n                               final ConsumerMetadata metadata,\n                               final SubscriptionState subscriptions,\n                               final FetchConfig fetchConfig,\n                               final ShareFetchBuffer shareFetchBuffer,\n                               final BackgroundEventHandler backgroundEventHandler,\n                               final ShareFetchMetricsManager metricsManager,\n                               final long retryBackoffMs,\n                               final long retryBackoffMaxMs) {\n        this.time = time;\n        this.log = logContext.logger(ShareConsumeRequestManager.class);\n        this.logContext = logContext;\n        this.groupId = groupId;\n        this.metadata = metadata;\n        this.subscriptions = subscriptions;\n        this.fetchConfig = fetchConfig;\n        this.shareFetchBuffer = shareFetchBuffer;\n        this.backgroundEventHandler = backgroundEventHandler;\n        this.metricsManager = metricsManager;\n        this.retryBackoffMs = retryBackoffMs;\n        this.retryBackoffMaxMs = retryBackoffMaxMs;\n        this.sessionHandlers = new HashMap<>();\n        this.nodesWithPendingRequests = new HashSet<>();\n        this.acknowledgeRequestStates = new HashMap<>();\n        this.fetchAcknowledgementsMap = new HashMap<>();\n        this.closeFuture = new CompletableFuture<>();\n    }\n\n    @Override\n    public PollResult poll(long currentTimeMs) {\n        if (memberId == null) {\n            return PollResult.EMPTY;\n        }\n\n        // Send any pending acknowledgements before fetching more records.\n        PollResult pollResult = processAcknowledgements(currentTimeMs);\n        if (pollResult != null) {\n            return pollResult;\n        }\n\n        if (!fetchMoreRecords || closing) {\n            return PollResult.EMPTY;\n        }\n\n        Map<Node, ShareSessionHandler> handlerMap = new HashMap<>();\n        Map<String, Uuid> topicIds = metadata.topicIds();\n        for (TopicPartition partition : partitionsToFetch()) {\n            Optional<Node> leaderOpt = metadata.currentLeader(partition).leader;\n\n            if (!leaderOpt.isPresent()) {\n                log.debug(\"Requesting metadata update for partition {} since current leader node is missing\", partition);\n                metadata.requestUpdate(false);\n                continue;\n            }\n\n            Uuid topicId = topicIds.get(partition.topic());\n            if (topicId == null) {\n                log.debug(\"Requesting metadata update for partition {} since topic ID is missing\", partition);\n                metadata.requestUpdate(false);\n                continue;\n            }\n\n            Node node = leaderOpt.get();\n            if (nodesWithPendingRequests.contains(node.id())) {\n                log.trace(\"Skipping fetch for partition {} because previous fetch request to {} has not been processed\", partition, node.id());\n            } else {\n                // if there is a leader and no in-flight requests, issue a new fetch\n                ShareSessionHandler handler = handlerMap.computeIfAbsent(node,\n                        k -> sessionHandlers.computeIfAbsent(node.id(), n -> new ShareSessionHandler(logContext, n, memberId)));\n\n                TopicIdPartition tip = new TopicIdPartition(topicId, partition);\n                Acknowledgements acknowledgementsToSend = fetchAcknowledgementsMap.get(tip);\n                if (onClose(acknowledgementsToSend)) {\n                    metricsManager.recordAcknowledgementSent(acknowledgementsToSend.size());\n                }\n                handler.addPartitionToFetch(tip, acknowledgementsToSend);\n\n                log.debug(\"Added fetch request for partition {} to node {}\", partition, node.id());\n            }\n        }\n\n        Map<Node, ShareFetchRequest.Builder> builderMap = new LinkedHashMap<>();\n        for (Map.Entry<Node, ShareSessionHandler> entry : handlerMap.entrySet()) {\n            builderMap.put(entry.getKey(), entry.getValue().newShareFetchBuilder(groupId, fetchConfig));\n        }\n\n        List<UnsentRequest> requests = builderMap.entrySet().stream().map(entry -> {\n            Node target = entry.getKey();\n            log.trace(\"Building ShareFetch request to send to node {}\", target.id());\n            ShareFetchRequest.Builder requestBuilder = entry.getValue();\n\n            nodesWithPendingRequests.add(target.id());\n\n            BiConsumer<ClientResponse, Throwable> responseHandler = (clientResponse, error) -> {\n                if (error != null) {\n                    handleShareFetchFailure(target, requestBuilder.data(), error);\n                } else {\n                    handleShareFetchSuccess(target, requestBuilder.data(), clientResponse);\n                }\n            };\n            return new UnsentRequest(requestBuilder, Optional.of(target)).whenComplete(responseHandler);\n        }).collect(Collectors.toList());\n\n        return new PollResult(requests);\n    }\n\n    public void fetch(Map<TopicIdPartition, Acknowledgements> acknowledgementsMap) {\n        if (!fetchMoreRecords) {\n            log.debug(\"Fetch more data\");\n            fetchMoreRecords = true;\n        }\n        acknowledgementsMap.forEach((tip, acks) -> fetchAcknowledgementsMap.merge(tip, acks, Acknowledgements::merge));\n    }\n\n    /**\n     * Process acknowledgeRequestStates and prepares a list of acknowledgements to be sent in the poll().\n     *\n     * @param currentTimeMs the current time in ms.\n     *\n     * @return the PollResult containing zero or more acknowledgements.\n     */\n    private PollResult processAcknowledgements(long currentTimeMs) {\n        List<UnsentRequest> unsentRequests = new ArrayList<>();\n        AtomicBoolean isAsyncDone = new AtomicBoolean();\n        for (Map.Entry<Integer, Pair<AcknowledgeRequestState>> requestStates : acknowledgeRequestStates.entrySet()) {\n            int nodeId = requestStates.getKey();\n\n            if (!isNodeFree(nodeId)) {\n                log.trace(\"Skipping acknowledge request because previous request to {} has not been processed, so acks are not sent\", nodeId);\n            } else {\n                isAsyncDone.set(false);\n                // For commitAsync\n                maybeBuildRequest(requestStates.getValue().getAsyncRequest(), currentTimeMs, true, isAsyncDone).ifPresent(unsentRequests::add);\n                // Check to ensure we start processing commitSync/close only if there are no commitAsync requests left to process.\n                if (!isNodeFree(nodeId)) {\n                    log.trace(\"Skipping acknowledge request because previous request to {} has not been processed, so acks are not sent\", nodeId);\n                } else if (isAsyncDone.get()) {\n                    maybeBuildRequest(requestStates.getValue().getSyncRequest(), currentTimeMs, false, isAsyncDone).ifPresent(unsentRequests::add);\n                }\n            }\n        }\n\n        PollResult pollResult = null;\n        if (!unsentRequests.isEmpty()) {\n            pollResult = new PollResult(unsentRequests);\n        } else if (checkAndRemoveCompletedAcknowledgements()) {\n            // Return empty result until all the acknowledgement request states are processed\n            pollResult = PollResult.EMPTY;\n        } else if (closing) {\n            if (!closeFuture.isDone()) {\n                log.trace(\"Completing acknowledgement on close\");\n                closeFuture.complete(null);\n            }\n            pollResult = PollResult.EMPTY;\n        }\n\n        return pollResult;\n    }\n\n    private boolean isNodeFree(int nodeId) {\n        return !nodesWithPendingRequests.contains(nodeId);\n    }\n\n    private Optional<UnsentRequest> maybeBuildRequest(AcknowledgeRequestState acknowledgeRequestState,\n                                                      long currentTimeMs,\n                                                      boolean onCommitAsync,\n                                                      AtomicBoolean isAsyncDone) {\n        if (acknowledgeRequestState == null || (!acknowledgeRequestState.onClose && acknowledgeRequestState.isEmpty())) {\n            if (onCommitAsync) {\n                isAsyncDone.set(true);\n            }\n            return Optional.empty();\n        } else if (!acknowledgeRequestState.maybeExpire()) {\n            if (acknowledgeRequestState.canSendRequest(currentTimeMs)) {\n                acknowledgeRequestState.onSendAttempt(currentTimeMs);\n                if (onCommitAsync) {\n                    isAsyncDone.set(true);\n                }\n                return Optional.of(acknowledgeRequestState.buildRequest(currentTimeMs));\n            } else {\n                // We wait for the backoff before we can send this request.\n                if (onCommitAsync) {\n                    isAsyncDone.set(false);\n                }\n            }\n        } else {\n            // Fill in TimeoutException\n            for (TopicIdPartition tip : acknowledgeRequestState.incompleteAcknowledgements.keySet()) {\n                metricsManager.recordFailedAcknowledgements(acknowledgeRequestState.getIncompleteAcknowledgementsCount(tip));\n                acknowledgeRequestState.handleAcknowledgeTimedOut(tip);\n            }\n            acknowledgeRequestState.incompleteAcknowledgements.clear();\n            if (onCommitAsync) {\n                isAsyncDone.set(true);\n            }\n        }\n        return Optional.empty();\n    }\n\n    /**\n     * Prunes the empty acknowledgementRequestStates.\n     * Returns true if there are still some acknowledgements left to be processed.\n     */\n    private boolean checkAndRemoveCompletedAcknowledgements() {\n        boolean areAnyAcksLeft = false;\n        Iterator<Map.Entry<Integer, Pair<AcknowledgeRequestState>>> iterator = acknowledgeRequestStates.entrySet().iterator();\n        while (iterator.hasNext()) {\n            Map.Entry<Integer, Pair<AcknowledgeRequestState>> acknowledgeRequestStatePair = iterator.next();\n            if (isRequestStateInProgress(acknowledgeRequestStatePair.getValue().getAsyncRequest()) || isRequestStateInProgress(acknowledgeRequestStatePair.getValue().getSyncRequest())) {\n                areAnyAcksLeft = true;\n            } else if (!closing) {\n                iterator.remove();\n            }\n        }\n        if (!acknowledgeRequestStates.isEmpty()) areAnyAcksLeft = true;\n        return areAnyAcksLeft;\n    }\n\n    private boolean isRequestStateInProgress(AcknowledgeRequestState acknowledgeRequestState) {\n        return acknowledgeRequestState != null && !(acknowledgeRequestState.isEmpty());\n    }\n\n    /**\n     * Enqueue an AcknowledgeRequestState to be picked up on the next poll\n     *\n     * @param acknowledgementsMap The acknowledgements to commit\n     * @param deadlineMs          Time until which the request will be retried if it fails with\n     *                            an expected retriable error.\n     *\n     * @return The future which completes when the acknowledgements finished\n     */\n    public CompletableFuture<Map<TopicIdPartition, Acknowledgements>> commitSync(\n            final Map<TopicIdPartition, Acknowledgements> acknowledgementsMap,\n            final long deadlineMs) {\n        final AtomicInteger resultCount = new AtomicInteger();\n        final CompletableFuture<Map<TopicIdPartition, Acknowledgements>> future = new CompletableFuture<>();\n        final ResultHandler resultHandler = new ResultHandler(resultCount, Optional.of(future));\n\n        final Cluster cluster = metadata.fetch();\n\n        sessionHandlers.forEach((nodeId, sessionHandler) -> {\n            Node node = cluster.nodeById(nodeId);\n            if (node != null) {\n                acknowledgeRequestStates.putIfAbsent(nodeId, new Pair<>(null, null));\n\n                // Ensure there is no commitSync()/close() request already present as they are blocking calls\n                // and only one request can be active at a time.\n                if (acknowledgeRequestStates.get(nodeId).getSyncRequest() != null && !acknowledgeRequestStates.get(nodeId).getSyncRequest().isEmpty()) {\n                    log.error(\"Attempt to call commitSync() when there is an existing sync request for node {}\", node.id());\n                    future.completeExceptionally(\n                            new IllegalStateException(\"Attempt to call commitSync() when there is an existing sync request for node : \" + node.id()));\n                } else {\n                    Map<TopicIdPartition, Acknowledgements> acknowledgementsMapForNode = new HashMap<>();\n                    for (TopicIdPartition tip : sessionHandler.sessionPartitions()) {\n                        Acknowledgements acknowledgements = acknowledgementsMap.get(tip);\n                        if (onClose(acknowledgements)) {\n                            acknowledgementsMapForNode.put(tip, acknowledgements);\n\n                            metricsManager.recordAcknowledgementSent(acknowledgements.size());\n                            log.debug(\"Added sync acknowledge request for partition {} to node {}\", tip.topicPartition(), node.id());\n                            resultCount.incrementAndGet();\n                        }\n                    }\n\n\n                    // There can only be one commitSync()/close() happening at a time. So per node, there will be one acknowledge request state representing commitSync() and close().\n                    acknowledgeRequestStates.get(nodeId).setSyncRequest(new AcknowledgeRequestState(logContext,\n                            ShareConsumeRequestManager.class.getSimpleName() + \":1\",\n                            deadlineMs,\n                            retryBackoffMs,\n                            retryBackoffMaxMs,\n                            sessionHandler,\n                            nodeId,\n                            acknowledgementsMapForNode,\n                            this::handleShareAcknowledgeSuccess,\n                            this::handleShareAcknowledgeFailure,\n                            resultHandler\n                    ));\n                }\n            }\n        });\n\n        resultHandler.completeIfEmpty();\n        return future;\n    }\n\n    /**\n     * Enqueue an AcknowledgeRequestState to be picked up on the next poll.\n     *\n     * @param acknowledgementsMap The acknowledgements to commit\n     */\n    public void commitAsync(final Map<TopicIdPartition, Acknowledgements> acknowledgementsMap) {\n        final Cluster cluster = metadata.fetch();\n        final AtomicInteger resultCount = new AtomicInteger();\n        final ResultHandler resultHandler = new ResultHandler(resultCount, Optional.empty());\n\n        sessionHandlers.forEach((nodeId, sessionHandler) -> {\n            Node node = cluster.nodeById(nodeId);\n            if (node != null) {\n                Map<TopicIdPartition, Acknowledgements> acknowledgementsMapForNode = new HashMap<>();\n\n                acknowledgeRequestStates.putIfAbsent(nodeId, new Pair<>(null, null));\n\n                for (TopicIdPartition tip : sessionHandler.sessionPartitions()) {\n                    Acknowledgements acknowledgements = acknowledgementsMap.get(tip);\n                    if (onClose(acknowledgements)) {\n                        acknowledgementsMapForNode.put(tip, acknowledgements);\n\n                        metricsManager.recordAcknowledgementSent(acknowledgements.size());\n                        log.debug(\"Added async acknowledge request for partition {} to node {}\", tip.topicPartition(), node.id());\n                        resultCount.incrementAndGet();\n                        AcknowledgeRequestState asyncRequestState = acknowledgeRequestStates.get(nodeId).getAsyncRequest();\n                        if (asyncRequestState == null) {\n                            acknowledgeRequestStates.get(nodeId).setAsyncRequest(new AcknowledgeRequestState(logContext,\n                                    ShareConsumeRequestManager.class.getSimpleName() + \":2\",\n                                    Long.MAX_VALUE,\n                                    retryBackoffMs,\n                                    retryBackoffMaxMs,\n                                    sessionHandler,\n                                    nodeId,\n                                    acknowledgementsMapForNode,\n                                    this::handleShareAcknowledgeSuccess,\n                                    this::handleShareAcknowledgeFailure,\n                                    resultHandler\n                            ));\n                        } else {\n                            Acknowledgements prevAcks = asyncRequestState.acknowledgementsToSend.putIfAbsent(tip, acknowledgements);\n                            if (onClose(prevAcks)) {\n                                asyncRequestState.acknowledgementsToSend.get(tip).merge(acknowledgements);\n                            }\n                        }\n                    }\n                }\n            }\n        });\n\n        resultHandler.completeIfEmpty();\n    }\n\n    /**\n     * Enqueue the final AcknowledgeRequestState used to commit the final acknowledgements and\n     * close the share sessions.\n     *\n     * @param acknowledgementsMap The acknowledgements to commit\n     * @param deadlineMs          Time until which the request will be retried if it fails with\n     *                            an expected retriable error.\n     *\n     * @return The future which completes when the acknowledgements finished\n     */\n    public CompletableFuture<Void> acknowledgeOnClose(final Map<TopicIdPartition, Acknowledgements> acknowledgementsMap,\n                                                      final long deadlineMs) {\n        final Cluster cluster = metadata.fetch();\n        final AtomicInteger resultCount = new AtomicInteger();\n        final ResultHandler resultHandler = new ResultHandler(resultCount, Optional.empty());\n\n        closing = true;\n\n        sessionHandlers.forEach((nodeId, sessionHandler) -> {\n            Node node = cluster.nodeById(nodeId);\n            if (node != null) {\n                Map<TopicIdPartition, Acknowledgements> acknowledgementsMapForNode = new HashMap<>();\n                for (TopicIdPartition tip : sessionHandler.sessionPartitions()) {\n                    Acknowledgements acknowledgements = acknowledgementsMap.get(tip);\n                    if (onClose(acknowledgements)) {\n                        acknowledgementsMapForNode.put(tip, acknowledgements);\n\n                        metricsManager.recordAcknowledgementSent(acknowledgements.size());\n                        log.debug(\"Added closing acknowledge request for partition {} to node {}\", tip.topicPartition(), node.id());\n                        resultCount.incrementAndGet();\n                    }\n                }\n\n                acknowledgeRequestStates.putIfAbsent(nodeId, new Pair<>(null, null));\n\n                // Ensure there is no commitSync()/close() request already present as they are blocking calls\n                // and only one request can be active at a time.\n                if (acknowledgeRequestStates.get(nodeId).getSyncRequest() != null && !acknowledgeRequestStates.get(nodeId).getSyncRequest().isEmpty()) {\n                    log.error(\"Attempt to call close() when there is an existing sync request for node {}-{}\", node.id(), acknowledgeRequestStates.get(nodeId).getSyncRequest());\n                    closeFuture.completeExceptionally(\n                            new IllegalStateException(\"Attempt to call close() when there is an existing sync request for node : \" + node.id()));\n                } else {\n                    // There can only be one commitSync()/close() happening at a time. So per node, there will be one acknowledge request state.\n                    acknowledgeRequestStates.get(nodeId).setSyncRequest(new AcknowledgeRequestState(logContext,\n                            ShareConsumeRequestManager.class.getSimpleName() + \":3\",\n                            deadlineMs,\n                            retryBackoffMs,\n                            retryBackoffMaxMs,\n                            sessionHandler,\n                            nodeId,\n                            acknowledgementsMapForNode,\n                            this::handleShareAcknowledgeCloseSuccess,\n                            this::handleShareAcknowledgeCloseFailure,\n                            resultHandler,\n                            true\n                    ));\n\n                }\n            }\n        });\n\n        resultHandler.completeIfEmpty();\n        return closeFuture;\n    }\n\n    private boolean onClose(Acknowledgements acknowledgements) {\n        return acknowledgements != null;\n    }\n\n    private void handleShareFetchSuccess(Node fetchTarget,\n                                         @SuppressWarnings(\"unused\") ShareFetchRequestData requestData,\n                                         ClientResponse resp) {\n        try {\n            log.debug(\"Completed ShareFetch request from node {} successfully\", fetchTarget.id());\n            final ShareFetchResponse response = (ShareFetchResponse) resp.responseBody();\n            final ShareSessionHandler handler = sessionHandler(fetchTarget.id());\n\n            if (handler == null) {\n                log.error(\"Unable to find ShareSessionHandler for node {}. Ignoring ShareFetch response.\",\n                        fetchTarget.id());\n                return;\n            }\n\n            final short requestVersion = resp.requestHeader().apiVersion();\n\n            if (!handler.handleResponse(response, requestVersion)) {\n                if (response.error() == Errors.UNKNOWN_TOPIC_ID) {\n                    metadata.requestUpdate(false);\n                }\n                return;\n            }\n\n            final Map<TopicIdPartition, ShareFetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n\n            response.data().responses().forEach(topicResponse ->\n                    topicResponse.partitions().forEach(partition ->\n                            responseData.put(new TopicIdPartition(topicResponse.topicId(),\n                                    partition.partitionIndex(),\n                                    metadata.topicNames().get(topicResponse.topicId())), partition)));\n\n            final Set<TopicPartition> partitions = responseData.keySet().stream().map(TopicIdPartition::topicPartition).collect(Collectors.toSet());\n            final ShareFetchMetricsAggregator shareFetchMetricsAggregator = new ShareFetchMetricsAggregator(metricsManager, partitions);\n\n            for (Map.Entry<TopicIdPartition, ShareFetchResponseData.PartitionData> entry : responseData.entrySet()) {\n                TopicIdPartition tip = entry.getKey();\n\n                ShareFetchResponseData.PartitionData partitionData = entry.getValue();\n\n                log.debug(\"ShareFetch for partition {} returned fetch data {}\", tip, partitionData);\n\n                Acknowledgements acks = fetchAcknowledgementsMap.remove(tip);\n                if (onClose(acks)) {\n                    if (partitionData.acknowledgeErrorCode() != Errors.NONE.code()) {\n                        metricsManager.recordFailedAcknowledgements(acks.size());\n                    }\n                    acks.setAcknowledgeErrorCode(Errors.forCode(partitionData.acknowledgeErrorCode()));\n                    Map<TopicIdPartition, Acknowledgements> acksMap = Collections.singletonMap(tip, acks);\n                    ShareAcknowledgementCommitCallbackEvent event = new ShareAcknowledgementCommitCallbackEvent(acksMap);\n                    backgroundEventHandler.add(event);\n                }\n\n                ShareCompletedFetch completedFetch = new ShareCompletedFetch(\n                        logContext,\n                        BufferSupplier.create(),\n                        tip,\n                        partitionData,\n                        shareFetchMetricsAggregator,\n                        requestVersion);\n                shareFetchBuffer.add(completedFetch);\n\n                if (!partitionData.acquiredRecords().isEmpty()) {\n                    fetchMoreRecords = false;\n                }\n            }\n\n            metricsManager.recordLatency(resp.requestLatencyMs());\n        } finally {\n            log.debug(\"Removing pending request for node {} - success\", fetchTarget.id());\n            nodesWithPendingRequests.remove(fetchTarget.id());\n        }\n    }\n\n    private void handleShareFetchFailure(Node fetchTarget,\n                                         ShareFetchRequestData requestData,\n                                         Throwable error) {\n        try {\n            log.debug(\"Completed ShareFetch request from node {} unsuccessfully {}\", fetchTarget.id(), Errors.forException(error));\n            final ShareSessionHandler handler = sessionHandler(fetchTarget.id());\n            if (handler != null) {\n                handler.handleError(error);\n            }\n\n            requestData.topics().forEach(topic -> topic.partitions().forEach(partition -> {\n                TopicIdPartition tip = new TopicIdPartition(topic.topicId(),\n                        partition.partitionIndex(),\n                        metadata.topicNames().get(topic.topicId()));\n\n                Acknowledgements acks = fetchAcknowledgementsMap.remove(tip);\n                if (onClose(acks)) {\n                    metricsManager.recordFailedAcknowledgements(acks.size());\n                    acks.setAcknowledgeErrorCode(Errors.forException(error));\n                    Map<TopicIdPartition, Acknowledgements> acksMap = Collections.singletonMap(tip, acks);\n                    ShareAcknowledgementCommitCallbackEvent event = new ShareAcknowledgementCommitCallbackEvent(acksMap);\n                    backgroundEventHandler.add(event);\n                }\n            }));\n        } finally {\n            log.debug(\"Removing pending request for node {} - failed\", fetchTarget.id());\n            nodesWithPendingRequests.remove(fetchTarget.id());\n        }\n    }\n\n    private void handleShareAcknowledgeSuccess(Node fetchTarget,\n                                               ShareAcknowledgeRequestData requestData,\n                                               AcknowledgeRequestState acknowledgeRequestState,\n                                               ClientResponse resp,\n                                               long currentTimeMs) {\n        try {\n            log.debug(\"Completed ShareAcknowledge request from node {} successfully\", fetchTarget.id());\n            final ShareAcknowledgeResponse response = (ShareAcknowledgeResponse) resp.responseBody();\n            final ShareSessionHandler handler = acknowledgeRequestState.sessionHandler();\n\n            final short requestVersion = resp.requestHeader().apiVersion();\n\n            if (!handler.handleResponse(response, requestVersion)) {\n                acknowledgeRequestState.onFailedAttempt(currentTimeMs);\n                if (response.error().exception() instanceof RetriableException && !acknowledgeRequestState.onClose) {\n                    // We retry the request until the timer expires, unless we are closing.\n                    acknowledgeRequestState.retryRequest();\n                } else {\n                    response.data().responses().forEach(shareAcknowledgeTopicResponse -> shareAcknowledgeTopicResponse.partitions().forEach(partitionData -> {\n                        TopicIdPartition tip = new TopicIdPartition(shareAcknowledgeTopicResponse.topicId(),\n                                partitionData.partitionIndex(),\n                                metadata.topicNames().get(shareAcknowledgeTopicResponse.topicId()));\n\n                        acknowledgeRequestState.handleAcknowledgeErrorCode(tip, response.error());\n                        metricsManager.recordLatency(resp.requestLatencyMs());\n                    }));\n                }\n            } else {\n                AtomicBoolean shouldRetry = new AtomicBoolean(false);\n                // Check all partition level error codes\n                response.data().responses().forEach(shareAcknowledgeTopicResponse -> shareAcknowledgeTopicResponse.partitions().forEach(partitionData -> {\n                    Errors partitionError = Errors.forCode(partitionData.errorCode());\n                    TopicIdPartition tip = new TopicIdPartition(shareAcknowledgeTopicResponse.topicId(),\n                            partitionData.partitionIndex(),\n                            metadata.topicNames().get(shareAcknowledgeTopicResponse.topicId()));\n                    if (partitionError.exception() != null) {\n                        if (partitionError.exception() instanceof RetriableException && !acknowledgeRequestState.onClose) {\n                            // Move to incomplete acknowledgements to retry\n                            acknowledgeRequestState.moveToIncompleteAcks(tip);\n                            shouldRetry.set(true);\n                        } else {\n                            metricsManager.recordFailedAcknowledgements(acknowledgeRequestState.getInFlightAcknowledgementsCount(tip));\n                            acknowledgeRequestState.handleAcknowledgeErrorCode(tip, partitionError);\n                        }\n                    } else {\n                        acknowledgeRequestState.handleAcknowledgeErrorCode(tip, partitionError);\n                    }\n                }));\n\n                if (shouldRetry.get()) {\n                    acknowledgeRequestState.onFailedAttempt(currentTimeMs);\n                } else {\n                    acknowledgeRequestState.onSuccessfulAttempt(currentTimeMs);\n                }\n                acknowledgeRequestState.processingComplete();\n            }\n            metricsManager.recordLatency(resp.requestLatencyMs());\n        } finally {\n            log.debug(\"Removing pending request for node {} - success\", fetchTarget.id());\n            nodesWithPendingRequests.remove(fetchTarget.id());\n        }\n    }\n\n    private void handleShareAcknowledgeFailure(Node fetchTarget,\n                                               ShareAcknowledgeRequestData requestData,\n                                               AcknowledgeRequestState acknowledgeRequestState,\n                                               Throwable error,\n                                               long currentTimeMs) {\n        try {\n            log.debug(\"Completed ShareAcknowledge request from node {} unsuccessfully {}\", fetchTarget.id(), Errors.forException(error));\n            acknowledgeRequestState.sessionHandler().handleError(error);\n            acknowledgeRequestState.onFailedAttempt(currentTimeMs);\n\n            requestData.topics().forEach(topic -> topic.partitions().forEach(partition -> {\n                TopicIdPartition tip = new TopicIdPartition(topic.topicId(),\n                        partition.partitionIndex(),\n                        metadata.topicNames().get(topic.topicId()));\n                metricsManager.recordFailedAcknowledgements(acknowledgeRequestState.getInFlightAcknowledgementsCount(tip));\n                acknowledgeRequestState.handleAcknowledgeErrorCode(tip, Errors.forException(error));\n            }));\n        } finally {\n            log.debug(\"Removing pending request for node {} - failed\", fetchTarget.id());\n            nodesWithPendingRequests.remove(fetchTarget.id());\n        }\n    }\n\n    private void handleShareAcknowledgeCloseSuccess(Node fetchTarget,\n                                                    ShareAcknowledgeRequestData requestData,\n                                                    AcknowledgeRequestState acknowledgeRequestState,\n                                                    ClientResponse resp,\n                                                    long currentTimeMs) {\n        try {\n            log.debug(\"Completed ShareAcknowledge on close request from node {} successfully\", fetchTarget.id());\n            final ShareAcknowledgeResponse response = (ShareAcknowledgeResponse) resp.responseBody();\n\n            response.data().responses().forEach(topic -> topic.partitions().forEach(partition -> {\n                TopicIdPartition tip = new TopicIdPartition(topic.topicId(),\n                        partition.partitionIndex(),\n                        metadata.topicNames().get(topic.topicId()));\n                if (partition.errorCode() != Errors.NONE.code()) {\n                    metricsManager.recordFailedAcknowledgements(acknowledgeRequestState.getInFlightAcknowledgementsCount(tip));\n                }\n                acknowledgeRequestState.handleAcknowledgeErrorCode(tip, Errors.forCode(partition.errorCode()));\n            }));\n\n            acknowledgeRequestState.onSuccessfulAttempt(currentTimeMs);\n            metricsManager.recordLatency(resp.requestLatencyMs());\n            acknowledgeRequestState.processingComplete();\n        } finally {\n            log.debug(\"Removing pending request for node {} - success\", fetchTarget.id());\n            nodesWithPendingRequests.remove(fetchTarget.id());\n            sessionHandlers.remove(fetchTarget.id());\n        }\n    }\n\n    private void handleShareAcknowledgeCloseFailure(Node fetchTarget,\n                                                    ShareAcknowledgeRequestData requestData,\n                                                    AcknowledgeRequestState acknowledgeRequestState,\n                                                    Throwable error,\n                                                    long currentTimeMs) {\n        try {\n            log.debug(\"Completed ShareAcknowledge on close request from node {} unsuccessfully {}\", fetchTarget.id(), Errors.forException(error));\n            acknowledgeRequestState.sessionHandler().handleError(error);\n            acknowledgeRequestState.onFailedAttempt(currentTimeMs);\n\n            requestData.topics().forEach(topic -> topic.partitions().forEach(partition -> {\n                TopicIdPartition tip = new TopicIdPartition(topic.topicId(),\n                        partition.partitionIndex(),\n                        metadata.topicNames().get(topic.topicId()));\n                metricsManager.recordFailedAcknowledgements(acknowledgeRequestState.getInFlightAcknowledgementsCount(tip));\n                acknowledgeRequestState.handleAcknowledgeErrorCode(tip, Errors.forException(error));\n            }));\n        } finally {\n            log.debug(\"Removing pending request for node {} - failed\", fetchTarget.id());\n            nodesWithPendingRequests.remove(fetchTarget.id());\n            sessionHandlers.remove(fetchTarget.id());\n        }\n    }\n\n    private List<TopicPartition> partitionsToFetch() {\n        return subscriptions.fetchablePartitions(tp -> true);\n    }\n\n    public ShareSessionHandler sessionHandler(int node) {\n        return sessionHandlers.get(node);\n    }\n\n    boolean hasCompletedFetches() {\n        return !shareFetchBuffer.isEmpty();\n    }\n\n    protected void closeInternal() {\n        Utils.closeQuietly(shareFetchBuffer, \"shareFetchBuffer\");\n    }\n\n    public void close() {\n        idempotentCloser.close(this::closeInternal);\n    }\n\n    @Override\n    public void onMemberEpochUpdated(Optional<Integer> memberEpochOpt, Optional<String> memberIdOpt) {\n        memberIdOpt.ifPresent(s -> memberId = Uuid.fromString(s));\n    }\n\n    /**\n     * Represents a request to acknowledge delivery that can be retried or aborted.\n     */\n    public class AcknowledgeRequestState extends TimedRequestState {\n\n        /**\n         * The share session handler.\n         */\n        private final ShareSessionHandler sessionHandler;\n\n        /**\n         * The node to send the request to.\n         */\n        private final int nodeId;\n\n        /**\n         * The map of acknowledgements to send\n         */\n        private final Map<TopicIdPartition, Acknowledgements> acknowledgementsToSend;\n\n        /**\n         * The map of acknowledgements to be retried in the next attempt.\n         */\n        private final Map<TopicIdPartition, Acknowledgements> incompleteAcknowledgements;\n\n        /**\n         * The in-flight acknowledgements\n         */\n        private final Map<TopicIdPartition, Acknowledgements> inFlightAcknowledgements;\n\n        /**\n         * The handler to call on a successful response from ShareAcknowledge.\n         */\n        private final ResponseHandler<ClientResponse> successHandler;\n\n        /**\n         * The handler to call on a failed response from ShareAcknowledge.\n         */\n        private final ResponseHandler<Throwable> errorHandler;\n\n        /**\n         * This handles completing a future when all results are known.\n         */\n        private final ResultHandler resultHandler;\n\n        /**\n         * Whether this is the final acknowledge request state before the consumer closes.\n         */\n        private final boolean onClose;\n\n        AcknowledgeRequestState(LogContext logContext,\n                                String owner,\n                                long deadlineMs,\n                                long retryBackoffMs,\n                                long retryBackoffMaxMs,\n                                ShareSessionHandler sessionHandler,\n                                int nodeId,\n                                Map<TopicIdPartition, Acknowledgements> acknowledgementsMap,\n                                ResponseHandler<ClientResponse> successHandler,\n                                ResponseHandler<Throwable> errorHandler,\n                                ResultHandler resultHandler) {\n            this(logContext, owner, deadlineMs, retryBackoffMs, retryBackoffMaxMs, sessionHandler, nodeId,\n                    acknowledgementsMap, successHandler, errorHandler, resultHandler, false);\n        }\n\n        AcknowledgeRequestState(LogContext logContext,\n                                String owner,\n                                long deadlineMs,\n                                long retryBackoffMs,\n                                long retryBackoffMaxMs,\n                                ShareSessionHandler sessionHandler,\n                                int nodeId,\n                                Map<TopicIdPartition, Acknowledgements> acknowledgementsMap,\n                                ResponseHandler<ClientResponse> successHandler,\n                                ResponseHandler<Throwable> errorHandler,\n                                ResultHandler resultHandler,\n                                boolean onClose) {\n            super(logContext, owner, retryBackoffMs, retryBackoffMaxMs, deadlineTimer(time, deadlineMs));\n            this.sessionHandler = sessionHandler;\n            this.nodeId = nodeId;\n            this.successHandler = successHandler;\n            this.errorHandler = errorHandler;\n            this.acknowledgementsToSend = acknowledgementsMap;\n            this.resultHandler = resultHandler;\n            this.onClose = onClose;\n            this.inFlightAcknowledgements = new HashMap<>();\n            this.incompleteAcknowledgements = new HashMap<>();\n        }\n\n        UnsentRequest buildRequest(long currentTimeMs) {\n            // If this is the closing request, close the share session by setting the final epoch\n            if (onClose) {\n                sessionHandler.notifyClose();\n            }\n\n            Map<TopicIdPartition, Acknowledgements> finalAcknowledgementsToSend = new HashMap<>(\n                    incompleteAcknowledgements.isEmpty() ? acknowledgementsToSend : incompleteAcknowledgements);\n\n            for (Map.Entry<TopicIdPartition, Acknowledgements> entry : finalAcknowledgementsToSend.entrySet()) {\n                sessionHandler.addPartitionToFetch(entry.getKey(), entry.getValue());\n            }\n\n            ShareAcknowledgeRequest.Builder requestBuilder = sessionHandler.newShareAcknowledgeBuilder(groupId, fetchConfig);\n            Node nodeToSend = metadata.fetch().nodeById(nodeId);\n\n            nodesWithPendingRequests.add(nodeId);\n\n            BiConsumer<ClientResponse, Throwable> responseHandler = (clientResponse, error) -> {\n                if (error != null) {\n                    errorHandler.handle(nodeToSend, requestBuilder.data(), this, error, currentTimeMs);\n                    processingComplete();\n                } else {\n                    successHandler.handle(nodeToSend, requestBuilder.data(), this, clientResponse, currentTimeMs);\n                    if (onClose && !closeFuture.isDone()) {\n                        closeFuture.complete(null);\n                    }\n                }\n            };\n\n            if (requestBuilder == null) {\n                handleSessionErrorCode(Errors.SHARE_SESSION_NOT_FOUND);\n                return null;\n            } else {\n                inFlightAcknowledgements.putAll(finalAcknowledgementsToSend);\n                if (incompleteAcknowledgements.isEmpty()) {\n                    acknowledgementsToSend.clear();\n                } else {\n                    incompleteAcknowledgements.clear();\n                }\n                return new UnsentRequest(requestBuilder, Optional.of(nodeToSend)).whenComplete(responseHandler);\n            }\n        }\n\n        int getInFlightAcknowledgementsCount(TopicIdPartition tip) {\n            Acknowledgements acks = inFlightAcknowledgements.get(tip);\n            if (acks == null) {\n                return 0;\n            } else {\n                return acks.size();\n            }\n        }\n\n        int getIncompleteAcknowledgementsCount(TopicIdPartition tip) {\n            Acknowledgements acks = incompleteAcknowledgements.get(tip);\n            if (acks == null) {\n                return 0;\n            } else {\n                return acks.size();\n            }\n        }\n\n        int getAcknowledgementsToSendCount(TopicIdPartition tip) {\n            Acknowledgements acks = acknowledgementsToSend.get(tip);\n            if (acks == null) {\n                return 0;\n            } else {\n                return acks.size();\n            }\n        }\n\n        boolean isEmpty() {\n            return acknowledgementsToSend.isEmpty() &&\n                    incompleteAcknowledgements.isEmpty() &&\n                    inFlightAcknowledgements.isEmpty();\n        }\n\n        /**\n         * Sets the error code in the acknowledgements and sends the response\n         * through a background event.\n         */\n        void handleAcknowledgeErrorCode(TopicIdPartition tip, Errors acknowledgeErrorCode) {\n            Acknowledgements acks = inFlightAcknowledgements.get(tip);\n            if (onClose(acks)) {\n                acks.setAcknowledgeErrorCode(acknowledgeErrorCode);\n            }\n            resultHandler.complete(tip, acks);\n        }\n\n        /**\n         * Sets the error code for the acknowledgements which were timed out\n         * after some retries.\n         */\n        void handleAcknowledgeTimedOut(TopicIdPartition tip) {\n            Acknowledgements acks = incompleteAcknowledgements.get(tip);\n            if (onClose(acks)) {\n                acks.setAcknowledgeErrorCode(Errors.REQUEST_TIMED_OUT);\n            }\n            resultHandler.complete(tip, acks);\n        }\n\n        /**\n         * Set the error code for all remaining acknowledgements in the event\n         * of a session error which prevents the remains acknowledgements from\n         * being sent.\n         */\n        void handleSessionErrorCode(Errors errorCode) {\n            inFlightAcknowledgements.forEach((tip, acks) -> {\n                if (onClose(acks)) {\n                    acks.setAcknowledgeErrorCode(errorCode);\n                }\n                resultHandler.complete(tip, acks);\n            });\n            processingComplete();\n        }\n\n        ShareSessionHandler sessionHandler() {\n            return sessionHandler;\n        }\n\n        void processingComplete() {\n            inFlightAcknowledgements.clear();\n            resultHandler.completeIfEmpty();\n        }\n\n        void retryRequest() {\n            incompleteAcknowledgements.putAll(inFlightAcknowledgements);\n            inFlightAcknowledgements.clear();\n        }\n\n        boolean maybeExpire() {\n            return numAttempts > 0 && isExpired();\n        }\n\n        public void moveToIncompleteAcks(TopicIdPartition tip) {\n            Acknowledgements acks = inFlightAcknowledgements.remove(tip);\n            if (onClose(acks)) {\n                Acknowledgements existingAcks = incompleteAcknowledgements.putIfAbsent(tip, acks);\n                if (onClose(existingAcks)) {\n                    incompleteAcknowledgements.get(tip).merge(acks);\n                }\n            }\n        }\n    }\n\n    /**\n     * Defines the contract for handling responses from brokers.\n     * @param <T> Type of response, usually either {@link ClientResponse} or {@link Throwable}\n     */\n    @FunctionalInterface\n    private interface ResponseHandler<T> {\n        /**\n         * Handle the response from the given {@link Node target}\n         */\n        void handle(Node target, ShareAcknowledgeRequestData request, AcknowledgeRequestState requestState, T response, long currentTimeMs);\n    }\n\n    /**\n     * Sends a ShareAcknowledgeCommitCallback event to the application when it is done\n     * processing all the remaining acknowledgement request states.\n     * Also manages completing the future for synchronous acknowledgement commit by counting\n     * down the results as they are known and completing the future at the end.\n     */\n    class ResultHandler {\n        private final Map<TopicIdPartition, Acknowledgements> result;\n        private final AtomicInteger remainingResults;\n        private final Optional<CompletableFuture<Map<TopicIdPartition, Acknowledgements>>> future;\n\n        ResultHandler(final AtomicInteger remainingResults,\n                      final Optional<CompletableFuture<Map<TopicIdPartition, Acknowledgements>>> future) {\n            result = new HashMap<>();\n            this.remainingResults = remainingResults;\n            this.future = future;\n        }\n\n        /**\n         * Handle the result of a ShareAcknowledge request sent to one or more nodes and\n         * signal the completion when all results are known.\n         */\n        public void complete(TopicIdPartition partition, Acknowledgements acknowledgements) {\n            if (onClose(acknowledgements)) {\n                result.put(partition, acknowledgements);\n            }\n            if (remainingResults.decrementAndGet() == 0) {\n                ShareAcknowledgementCommitCallbackEvent event = new ShareAcknowledgementCommitCallbackEvent(result);\n                backgroundEventHandler.add(event);\n                future.ifPresent(future -> future.complete(result));\n            }\n        }\n\n        /**\n         * Handles the case where there are no results pending after initialization.\n         */\n        public void completeIfEmpty() {\n            if (remainingResults.get() == 0) {\n                future.ifPresent(future -> future.complete(result));\n            }\n        }\n    }\n\n    static class Pair<V> {\n        private V asyncRequest;\n        private V syncRequest;\n\n        public Pair(V asyncRequest, V syncRequest) {\n            this.asyncRequest = asyncRequest;\n            this.syncRequest = syncRequest;\n        }\n\n        public void setAsyncRequest(V asyncRequest) {\n            this.asyncRequest = asyncRequest;\n        }\n\n        public void setSyncRequest(V second) {\n            this.syncRequest = second;\n        }\n\n        public V getAsyncRequest() {\n            return asyncRequest;\n        }\n\n        public V getSyncRequest() {\n            return syncRequest;\n        }\n    }\n\n    Pair<AcknowledgeRequestState> requestStates(int nodeId) {\n        return acknowledgeRequestStates.get(nodeId);\n    }\n}",
                "methodCount": 49
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "fetch",
                            "method_signature": "public fetch(Map<TopicIdPartition, Acknowledgements> acknowledgementsMap)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processAcknowledgements",
                            "method_signature": "private processAcknowledgements(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isNodeFree",
                            "method_signature": "private isNodeFree(int nodeId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeBuildRequest",
                            "method_signature": "private maybeBuildRequest(AcknowledgeRequestState acknowledgeRequestState,\n                                                      long currentTimeMs,\n                                                      boolean onCommitAsync,\n                                                      AtomicBoolean isAsyncDone)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "checkAndRemoveCompletedAcknowledgements",
                            "method_signature": "private checkAndRemoveCompletedAcknowledgements()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRequestStateInProgress",
                            "method_signature": "private isRequestStateInProgress(AcknowledgeRequestState acknowledgeRequestState)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitSync",
                            "method_signature": "public commitSync(\n            final Map<TopicIdPartition, Acknowledgements> acknowledgementsMap,\n            final long deadlineMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "commitAsync",
                            "method_signature": "public commitAsync(final Map<TopicIdPartition, Acknowledgements> acknowledgementsMap)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "acknowledgeOnClose",
                            "method_signature": "public acknowledgeOnClose(final Map<TopicIdPartition, Acknowledgements> acknowledgementsMap,\n                                                      final long deadlineMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onClose",
                            "method_signature": "private onClose(Acknowledgements acknowledgements)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleShareFetchSuccess",
                            "method_signature": "private handleShareFetchSuccess(Node fetchTarget,\n                                         @SuppressWarnings(\"unused\") ShareFetchRequestData requestData,\n                                         ClientResponse resp)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleShareFetchFailure",
                            "method_signature": "private handleShareFetchFailure(Node fetchTarget,\n                                         ShareFetchRequestData requestData,\n                                         Throwable error)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleShareAcknowledgeSuccess",
                            "method_signature": "private handleShareAcknowledgeSuccess(Node fetchTarget,\n                                               ShareAcknowledgeRequestData requestData,\n                                               AcknowledgeRequestState acknowledgeRequestState,\n                                               ClientResponse resp,\n                                               long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleShareAcknowledgeFailure",
                            "method_signature": "private handleShareAcknowledgeFailure(Node fetchTarget,\n                                               ShareAcknowledgeRequestData requestData,\n                                               AcknowledgeRequestState acknowledgeRequestState,\n                                               Throwable error,\n                                               long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleShareAcknowledgeCloseSuccess",
                            "method_signature": "private handleShareAcknowledgeCloseSuccess(Node fetchTarget,\n                                                    ShareAcknowledgeRequestData requestData,\n                                                    AcknowledgeRequestState acknowledgeRequestState,\n                                                    ClientResponse resp,\n                                                    long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleShareAcknowledgeCloseFailure",
                            "method_signature": "private handleShareAcknowledgeCloseFailure(Node fetchTarget,\n                                                    ShareAcknowledgeRequestData requestData,\n                                                    AcknowledgeRequestState acknowledgeRequestState,\n                                                    Throwable error,\n                                                    long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionsToFetch",
                            "method_signature": "private partitionsToFetch()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sessionHandler",
                            "method_signature": "public sessionHandler(int node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "hasCompletedFetches",
                            "method_signature": " hasCompletedFetches()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeInternal",
                            "method_signature": "protected closeInternal()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "buildRequest",
                            "method_signature": " buildRequest(long currentTimeMs)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getInFlightAcknowledgementsCount",
                            "method_signature": " getInFlightAcknowledgementsCount(TopicIdPartition tip)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getIncompleteAcknowledgementsCount",
                            "method_signature": " getIncompleteAcknowledgementsCount(TopicIdPartition tip)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getAcknowledgementsToSendCount",
                            "method_signature": " getAcknowledgementsToSendCount(TopicIdPartition tip)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isEmpty",
                            "method_signature": " isEmpty()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleAcknowledgeErrorCode",
                            "method_signature": " handleAcknowledgeErrorCode(TopicIdPartition tip, Errors acknowledgeErrorCode)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleAcknowledgeTimedOut",
                            "method_signature": " handleAcknowledgeTimedOut(TopicIdPartition tip)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleSessionErrorCode",
                            "method_signature": " handleSessionErrorCode(Errors errorCode)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processingComplete",
                            "method_signature": " processingComplete()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "retryRequest",
                            "method_signature": " retryRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "maybeExpire",
                            "method_signature": " maybeExpire()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "moveToIncompleteAcks",
                            "method_signature": "public moveToIncompleteAcks(TopicIdPartition tip)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "complete",
                            "method_signature": "public complete(TopicIdPartition partition, Acknowledgements acknowledgements)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeIfEmpty",
                            "method_signature": "public completeIfEmpty()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "requestStates",
                            "method_signature": " requestStates(int nodeId)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                },
                {
                    "iteration_num": -2,
                    "suggested_move_methods": [
                        {
                            "method_name": "maybeExpire",
                            "method_signature": " maybeExpire()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "close",
                            "method_signature": "public close()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "sessionHandler",
                            "method_signature": "public sessionHandler(int node)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isNodeFree",
                            "method_signature": "private isNodeFree(int nodeId)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isEmpty",
                            "method_signature": " isEmpty()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "onClose",
                            "method_signature": "private onClose(Acknowledgements acknowledgements)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "completeIfEmpty",
                            "method_signature": "public completeIfEmpty()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "processingComplete",
                            "method_signature": " processingComplete()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "handleSessionErrorCode",
                            "method_signature": " handleSessionErrorCode(Errors errorCode)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "partitionsToFetch",
                            "method_signature": "private partitionsToFetch()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getAcknowledgementsToSendCount",
                            "method_signature": " getAcknowledgementsToSendCount(TopicIdPartition tip)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "getInFlightAcknowledgementsCount",
                            "method_signature": " getInFlightAcknowledgementsCount(TopicIdPartition tip)",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "closeInternal",
                            "method_signature": "protected closeInternal()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "retryRequest",
                            "method_signature": " retryRequest()",
                            "target_class": "",
                            "rationale": ""
                        },
                        {
                            "method_name": "isRequestStateInProgress",
                            "method_signature": "private isRequestStateInProgress(AcknowledgeRequestState acknowledgeRequestState)",
                            "target_class": "",
                            "rationale": ""
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "methodCompatibilityScores": {
                " maybeExpire()": {
                    "first": {
                        "method_name": "maybeExpire",
                        "method_signature": " maybeExpire()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.30878010049210836
                },
                "public close()": {
                    "first": {
                        "method_name": "close",
                        "method_signature": "public close()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3158673034790973
                },
                "public sessionHandler(int node)": {
                    "first": {
                        "method_name": "sessionHandler",
                        "method_signature": "public sessionHandler(int node)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3315160601770215
                },
                "private isNodeFree(int nodeId)": {
                    "first": {
                        "method_name": "isNodeFree",
                        "method_signature": "private isNodeFree(int nodeId)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3378563778342318
                },
                " isEmpty()": {
                    "first": {
                        "method_name": "isEmpty",
                        "method_signature": " isEmpty()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.3588404848295017
                },
                "private onClose(Acknowledgements acknowledgements)": {
                    "first": {
                        "method_name": "onClose",
                        "method_signature": "private onClose(Acknowledgements acknowledgements)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.40951997392485046
                },
                "public completeIfEmpty()": {
                    "first": {
                        "method_name": "completeIfEmpty",
                        "method_signature": "public completeIfEmpty()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.41551618738590024
                },
                " processingComplete()": {
                    "first": {
                        "method_name": "processingComplete",
                        "method_signature": " processingComplete()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.42077612998881897
                },
                " handleSessionErrorCode(Errors errorCode)": {
                    "first": {
                        "method_name": "handleSessionErrorCode",
                        "method_signature": " handleSessionErrorCode(Errors errorCode)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4354464800180104
                },
                "private partitionsToFetch()": {
                    "first": {
                        "method_name": "partitionsToFetch",
                        "method_signature": "private partitionsToFetch()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4362348691598568
                },
                " getAcknowledgementsToSendCount(TopicIdPartition tip)": {
                    "first": {
                        "method_name": "getAcknowledgementsToSendCount",
                        "method_signature": " getAcknowledgementsToSendCount(TopicIdPartition tip)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4436631619261442
                },
                " getInFlightAcknowledgementsCount(TopicIdPartition tip)": {
                    "first": {
                        "method_name": "getInFlightAcknowledgementsCount",
                        "method_signature": " getInFlightAcknowledgementsCount(TopicIdPartition tip)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.44852187506375363
                },
                "protected closeInternal()": {
                    "first": {
                        "method_name": "closeInternal",
                        "method_signature": "protected closeInternal()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4687326520727215
                },
                " retryRequest()": {
                    "first": {
                        "method_name": "retryRequest",
                        "method_signature": " retryRequest()",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.4744256129511621
                },
                "private isRequestStateInProgress(AcknowledgeRequestState acknowledgeRequestState)": {
                    "first": {
                        "method_name": "isRequestStateInProgress",
                        "method_signature": "private isRequestStateInProgress(AcknowledgeRequestState acknowledgeRequestState)",
                        "target_class": "",
                        "rationale": ""
                    },
                    "second": 0.47478991656494657
                }
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "d67c18b4aec66fec5d145c9ee737c7a469b8635c",
        "url": "https://github.com/apache/kafka/commit/d67c18b4aec66fec5d145c9ee737c7a469b8635c",
        "move_method_refactoring": {
            "type": "Extract And Move Method",
            "description": "Extract And Move Method public forConsumer(requireTimestamp boolean, isolationLevel IsolationLevel) : Builder extracted from private sendListOffsetRequest(node Node, timestampsToSearch Map<TopicPartition,ListOffsetsPartition>, requireTimestamp boolean) : RequestFuture<ListOffsetResult> in class org.apache.kafka.clients.consumer.internals.OffsetFetcher & moved to class org.apache.kafka.common.requests.ListOffsetsRequest.Builder",
            "leftSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetFetcher.java",
                    "startLine": 382,
                    "endLine": 407,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration before extraction",
                    "codeElement": "private sendListOffsetRequest(node Node, timestampsToSearch Map<TopicPartition,ListOffsetsPartition>, requireTimestamp boolean) : RequestFuture<ListOffsetResult>"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetFetcher.java",
                    "startLine": 393,
                    "endLine": 395,
                    "startColumn": 9,
                    "endColumn": 93,
                    "codeElementType": "VARIABLE_DECLARATION_STATEMENT",
                    "description": "extracted code from source method declaration",
                    "codeElement": null
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/requests/ListOffsetsRequest.java",
                    "startLine": 61,
                    "endLine": 64,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "extracted method declaration",
                    "codeElement": "public forConsumer(requireTimestamp boolean, isolationLevel IsolationLevel) : Builder"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/common/requests/ListOffsetsRequest.java",
                    "startLine": 63,
                    "endLine": 63,
                    "startColumn": 13,
                    "endColumn": 87,
                    "codeElementType": "RETURN_STATEMENT",
                    "description": "extracted code to extracted method declaration",
                    "codeElement": null
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetFetcher.java",
                    "startLine": 382,
                    "endLine": 407,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "source method declaration after extraction",
                    "codeElement": "private sendListOffsetRequest(node Node, timestampsToSearch Map<TopicPartition,ListOffsetsPartition>, requireTimestamp boolean) : RequestFuture<ListOffsetResult>"
                },
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetFetcher.java",
                    "startLine": 393,
                    "endLine": 394,
                    "startColumn": 46,
                    "endColumn": 63,
                    "codeElementType": "METHOD_INVOCATION",
                    "description": "extracted method invocation",
                    "codeElement": "ListOffsetsRequest.Builder.forConsumer(requireTimestamp,isolationLevel)"
                }
            ],
            "isStatic": true
        },
        "ref_id": 634,
        "extraction_results": {
            "success": true,
            "newCommitHash": "f843a212e5cfdf88c76494ff4f3dae817d2545f0",
            "newBranchName": "extract-forConsumer-sendListOffsetRequest-3d436f5"
        }
    }
]