{"id":"44ee65c5-572b-4ac6-bcc1-9f5435ad19e9","methodCount":12,"hostFunctionTelemetryData":{"hostFunctionSize":149,"lineStart":165,"lineEnd":313,"bodyLineStart":165,"language":"java","filePath":"/Users/fraol/Documents/MM-assist-dataset/kafka/tools/src/test/java/org/apache/kafka/tools/FeatureCommandTest.java","sourceCode":"class FeatureCommandUnitTest {\n    @Test\n    public void testLevelToString() {\n        assertEquals(\"5\", FeatureCommand.levelToString(\"foo.bar\", (short) 5));\n        assertEquals(\"3.3-IV0\",\n                FeatureCommand.levelToString(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()));\n    }\n\n    @Test\n    public void testMetadataVersionsToString() {\n        assertEquals(\"3.3-IV0, 3.3-IV1, 3.3-IV2, 3.3-IV3\",\n                FeatureCommand.metadataVersionsToString(MetadataVersion.IBP_3_3_IV0, MetadataVersion.IBP_3_3_IV3));\n    }\n\n    @Test\n    public void testdowngradeType() {\n        assertEquals(SAFE_DOWNGRADE, FeatureCommand.downgradeType(\n                new Namespace(singletonMap(\"unsafe\", Boolean.FALSE))));\n        assertEquals(UNSAFE_DOWNGRADE, FeatureCommand.downgradeType(\n                new Namespace(singletonMap(\"unsafe\", Boolean.TRUE))));\n        assertEquals(SAFE_DOWNGRADE, FeatureCommand.downgradeType(new Namespace(emptyMap())));\n    }\n\n    @Test\n    public void testParseNameAndLevel() {\n        assertArrayEquals(new String[]{\"foo.bar\", \"5\"}, FeatureCommand.parseNameAndLevel(\"foo.bar\u003d5\"));\n        assertArrayEquals(new String[]{\"quux\", \"0\"}, FeatureCommand.parseNameAndLevel(\"quux\u003d0\"));\n        assertTrue(assertThrows(RuntimeException.class, () -\u003e FeatureCommand.parseNameAndLevel(\"baaz\"))\n                .getMessage().contains(\"Can\u0027t parse feature\u003dlevel string baaz: equals sign not found.\"));\n        assertTrue(assertThrows(RuntimeException.class, () -\u003e FeatureCommand.parseNameAndLevel(\"w\u003dtf\"))\n                .getMessage().contains(\"Can\u0027t parse feature\u003dlevel string w\u003dtf: unable to parse tf as a short.\"));\n    }\n\n    private static MockAdminClient buildAdminClient() {\n        Map\u003cString, Short\u003e minSupportedFeatureLevels \u003d new HashMap\u003c\u003e();\n        minSupportedFeatureLevels.put(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel());\n        minSupportedFeatureLevels.put(\"foo.bar\", (short) 0);\n\n        Map\u003cString, Short\u003e featureLevels \u003d new HashMap\u003c\u003e();\n        featureLevels.put(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV2.featureLevel());\n        featureLevels.put(\"foo.bar\", (short) 5);\n\n        Map\u003cString, Short\u003e maxSupportedFeatureLevels \u003d new HashMap\u003c\u003e();\n        maxSupportedFeatureLevels.put(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV3.featureLevel());\n        maxSupportedFeatureLevels.put(\"foo.bar\", (short) 10);\n\n        return new MockAdminClient.Builder().\n                minSupportedFeatureLevels(minSupportedFeatureLevels).\n                featureLevels(featureLevels).\n                maxSupportedFeatureLevels(maxSupportedFeatureLevels).build();\n    }\n\n    @Test\n    public void testHandleDescribe() {\n        String describeResult \u003d ToolsTestUtils.captureStandardOut(() -\u003e {\n            try {\n                FeatureCommand.handleDescribe(buildAdminClient());\n            } catch (Exception e) {\n                throw new RuntimeException(e);\n            }\n        });\n        assertEquals(format(\"Feature: foo.bar\\tSupportedMinVersion: 0\\tSupportedMaxVersion: 10\\tFinalizedVersionLevel: 5\\tEpoch: 123%n\" +\n                \"Feature: metadata.version\\tSupportedMinVersion: 3.3-IV0\\tSupportedMaxVersion: 3.3-IV3\\tFinalizedVersionLevel: 3.3-IV2\\tEpoch: 123\"), describeResult);\n    }\n\n    @Test\n    public void testHandleUpgrade() {\n        Map\u003cString, Object\u003e namespace \u003d new HashMap\u003c\u003e();\n        namespace.put(\"metadata\", \"3.3-IV1\");\n        namespace.put(\"feature\", Collections.singletonList(\"foo.bar\u003d6\"));\n        namespace.put(\"dry_run\", false);\n        String upgradeOutput \u003d ToolsTestUtils.captureStandardOut(() -\u003e {\n            Throwable t \u003d assertThrows(TerseException.class, () -\u003e FeatureCommand.handleUpgrade(new Namespace(namespace), buildAdminClient()));\n            assertTrue(t.getMessage().contains(\"1 out of 2 operation(s) failed.\"));\n        });\n        assertEquals(format(\"foo.bar was upgraded to 6.%n\" +\n                \"Could not upgrade metadata.version to 5. Can\u0027t upgrade to lower version.\"), upgradeOutput);\n    }\n\n    @Test\n    public void testHandleUpgradeDryRun() {\n        Map\u003cString, Object\u003e namespace \u003d new HashMap\u003c\u003e();\n        namespace.put(\"metadata\", \"3.3-IV1\");\n        namespace.put(\"feature\", Collections.singletonList(\"foo.bar\u003d6\"));\n        namespace.put(\"dry_run\", true);\n        String upgradeOutput \u003d ToolsTestUtils.captureStandardOut(() -\u003e {\n            Throwable t \u003d assertThrows(TerseException.class, () -\u003e FeatureCommand.handleUpgrade(new Namespace(namespace), buildAdminClient()));\n            assertTrue(t.getMessage().contains(\"1 out of 2 operation(s) failed.\"));\n        });\n        assertEquals(format(\"foo.bar can be upgraded to 6.%n\" +\n                \"Can not upgrade metadata.version to 5. Can\u0027t upgrade to lower version.\"), upgradeOutput);\n    }\n\n    @Test\n    public void testHandleDowngrade() {\n        Map\u003cString, Object\u003e namespace \u003d new HashMap\u003c\u003e();\n        namespace.put(\"metadata\", \"3.3-IV3\");\n        namespace.put(\"feature\", Collections.singletonList(\"foo.bar\u003d1\"));\n        namespace.put(\"dry_run\", false);\n        String downgradeOutput \u003d ToolsTestUtils.captureStandardOut(() -\u003e {\n            Throwable t \u003d assertThrows(TerseException.class, () -\u003e FeatureCommand.handleDowngrade(new Namespace(namespace), buildAdminClient()));\n            assertTrue(t.getMessage().contains(\"1 out of 2 operation(s) failed.\"));\n        });\n        assertEquals(format(\"foo.bar was downgraded to 1.%n\" +\n                \"Could not downgrade metadata.version to 7. Can\u0027t downgrade to newer version.\"), downgradeOutput);\n    }\n\n    @Test\n    public void testHandleDowngradeDryRun() {\n        Map\u003cString, Object\u003e namespace \u003d new HashMap\u003c\u003e();\n        namespace.put(\"metadata\", \"3.3-IV3\");\n        namespace.put(\"feature\", Collections.singletonList(\"foo.bar\u003d1\"));\n        namespace.put(\"dry_run\", true);\n        String downgradeOutput \u003d ToolsTestUtils.captureStandardOut(() -\u003e {\n            Throwable t \u003d assertThrows(TerseException.class, () -\u003e FeatureCommand.handleDowngrade(new Namespace(namespace), buildAdminClient()));\n            assertTrue(t.getMessage().contains(\"1 out of 2 operation(s) failed.\"));\n        });\n        assertEquals(format(\"foo.bar can be downgraded to 1.%n\" +\n                \"Can not downgrade metadata.version to 7. Can\u0027t downgrade to newer version.\"), downgradeOutput);\n    }\n\n    @Test\n    public void testHandleDisable() {\n        Map\u003cString, Object\u003e namespace \u003d new HashMap\u003c\u003e();\n        namespace.put(\"feature\", Arrays.asList(\"foo.bar\", \"metadata.version\", \"quux\"));\n        namespace.put(\"dry_run\", false);\n        String disableOutput \u003d ToolsTestUtils.captureStandardOut(() -\u003e {\n            Throwable t \u003d assertThrows(TerseException.class, () -\u003e FeatureCommand.handleDisable(new Namespace(namespace), buildAdminClient()));\n            assertTrue(t.getMessage().contains(\"1 out of 3 operation(s) failed.\"));\n        });\n        assertEquals(format(\"foo.bar was disabled.%n\" +\n                \"Could not disable metadata.version. Can\u0027t downgrade below 4%n\" +\n                \"quux was disabled.\"), disableOutput);\n    }\n\n    @Test\n    public void testHandleDisableDryRun() {\n        Map\u003cString, Object\u003e namespace \u003d new HashMap\u003c\u003e();\n        namespace.put(\"feature\", Arrays.asList(\"foo.bar\", \"metadata.version\", \"quux\"));\n        namespace.put(\"dry_run\", true);\n        String disableOutput \u003d ToolsTestUtils.captureStandardOut(() -\u003e {\n            Throwable t \u003d assertThrows(TerseException.class, () -\u003e FeatureCommand.handleDisable(new Namespace(namespace), buildAdminClient()));\n            assertTrue(t.getMessage().contains(\"1 out of 3 operation(s) failed.\"));\n        });\n        assertEquals(format(\"foo.bar can be disabled.%n\" +\n                \"Can not disable metadata.version. Can\u0027t downgrade below 4%n\" +\n                \"quux can be disabled.\"), disableOutput);\n    }\n}","methodCount":12},"candidatesTelemetryData":{"numberOfSuggestions":3,"candidates":[{"lineStart":197,"lineEnd":214,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method buildAdminClient to class AssignmentTestUtils","description":"move method buildAdminClient to PsiClass:AssignmentTestUtils\nRationale: The buildAdminClient() method is responsible for creating a MockAdminClient, which is closely related to the assignment logic in Kafka. Moving it to AssignmentTestUtils aligns with the Single Responsibility Principle, as it centralizes mock client creation for testing assignments. This enhances cohesion and makes the method more reusable in assignment-related tests. However, care must be taken to ensure that the method does not introduce unnecessary dependencies on the assignment logic.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":197,"lineEnd":214,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method buildAdminClient to class ClientUtils","description":"move method buildAdminClient to PsiClass:ClientUtils\nRationale: ClientUtils is a suitable target for buildAdminClient() as it relates to client creation and configuration. This move would enhance the utility of ClientUtils for various client-related operations, adhering to the Open/Closed Principle. However, it may lead to a tightly coupled design if not managed properly.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":197,"lineEnd":214,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method buildAdminClient to class ConsumerGroupCommandTestUtils","description":"move method buildAdminClient to PsiClass:ConsumerGroupCommandTestUtils\nRationale: The method is relevant for creating mock clients that can be used in consumer group command tests. Relocating it here would improve the cohesion of test utilities related to consumer groups, adhering to the Single Responsibility Principle. This move would also facilitate easier testing of consumer group functionalities. However, it may limit the method\u0027s usability in other contexts outside consumer group commands.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false}]},"iterationData":[{"iteration_num":1,"suggested_move_methods":[{"method_name":"buildAdminClient","method_signature":"private static MockAdminClient buildAdminClient()","target_class":"MockAdminClientTestUtils","rationale":"The buildAdminClient method is responsible for creating and configuring a MockAdminClient instance, which is a utility function that does not belong in the test class. Moving it to a dedicated utility class will improve code organization and reusability."}],"llm_response_time":1456},{"iteration_num":2,"suggested_move_methods":[{"method_name":"buildAdminClient","method_signature":"private static MockAdminClient buildAdminClient()","target_class":"MockAdminClientTestUtils","rationale":"The method buildAdminClient() is responsible for creating and configuring a MockAdminClient instance, which is a utility function that could be better organized in a dedicated utility class for test setup."}],"llm_response_time":1356},{"iteration_num":3,"suggested_move_methods":[{"method_name":"buildAdminClient","method_signature":"private static MockAdminClient buildAdminClient()","target_class":"MockAdminClientFactory","rationale":"The method buildAdminClient() is responsible for creating and configuring a MockAdminClient instance. Moving it to a dedicated factory class (MockAdminClientFactory) would improve separation of concerns and enhance test readability."}],"llm_response_time":1724},{"iteration_num":-1,"suggested_move_methods":[{"method_name":"buildAdminClient","method_signature":"private static buildAdminClient()","target_class":"","rationale":""}],"llm_response_time":0},{"iteration_num":-2,"suggested_move_methods":[{"method_name":"buildAdminClient","method_signature":"private static buildAdminClient()","target_class":"","rationale":""}],"llm_response_time":0}],"methodCompatibilityScores":{"private static buildAdminClient()":{"first":{"method_name":"buildAdminClient","method_signature":"private static buildAdminClient()","target_class":"","rationale":""},"second":0.5982165960014815}},"llmMethodPriority":{"priority_method_names":["buildAdminClient"],"llm_response_time":631},"targetClassMap":{"buildAdminClient":{"target_classes":[{"class_name":"CsvUtils","similarity_score":0.2896953082939204},{"class_name":"ConsumerGroupCommandTestUtils","similarity_score":0.34925651638496286},{"class_name":"ToolsTestUtils","similarity_score":0.3749597139270878},{"class_name":"ToolsUtils","similarity_score":0.3250481933958116},{"class_name":"CommandLineUtils","similarity_score":0.1876423936463348},{"class_name":"CommandLineUtilsTest","similarity_score":0.20988433757790592},{"class_name":"AuthorizerUtils","similarity_score":0.17301336577538542},{"class_name":"CommandUtils","similarity_score":0.27446990439187824},{"class_name":"ControllerMetricsTestUtils","similarity_score":0.359351092065849},{"class_name":"ControllerRequestContextUtil","similarity_score":0.28242192204282246},{"class_name":"ControlRecordUtils","similarity_score":0.30167547477580436},{"class_name":"SystemTestUtil","similarity_score":0.25262669283851935},{"class_name":"FutureUtils","similarity_score":0.14729750787289733},{"class_name":"RequestTestUtils","similarity_score":0.2811316875405122},{"class_name":"RequestUtils","similarity_score":0.2620133544003864},{"class_name":"FetchUtils","similarity_score":0.01931812693721734},{"class_name":"TaskAssignmentUtils","similarity_score":0.0878630540423186},{"class_name":"TaskBuilder","similarity_score":0.2553088298005587},{"class_name":"SinkUtils","similarity_score":0.2823546315027159},{"class_name":"ClaimValidationUtils","similarity_score":0.07669015580713771},{"class_name":"AdminClientTestUtils","similarity_score":0.2944208835381293},{"class_name":"AdminUtils","similarity_score":0.21111745318195643},{"class_name":"ApiUtils","similarity_score":0.23900244784011815},{"class_name":"SmokeTestUtil","similarity_score":0.2633895763420765},{"class_name":"TestCloseable","similarity_score":0.3292905005426437},{"class_name":"ConcurrencyUtils","similarity_score":0.07772809805068724},{"class_name":"RetryUtil","similarity_score":0.1413518753084809},{"class_name":"TestSslUtils","similarity_score":0.43388304144357065},{"class_name":"TestUtil","similarity_score":0.23160277792655037},{"class_name":"TestUtils","similarity_score":0.2588874783695628},{"class_name":"GraphGraceSearchUtil","similarity_score":0.2607274230136563},{"class_name":"ConfigUtils","similarity_score":0.1761510876572276},{"class_name":"ExceptionUtils","similarity_score":0.2493916090006894},{"class_name":"ClientMetricsTestUtils","similarity_score":0.3578810105803186},{"class_name":"ConnectIntegrationTestUtils","similarity_score":0.16001877663808625},{"class_name":"ThreadUtils","similarity_score":0.22104487652606572},{"class_name":"ClientTelemetryUtils","similarity_score":0.30313945127737846},{"class_name":"ClientUtils","similarity_score":0.33211654433381127},{"class_name":"ConnectorUtils","similarity_score":0.17842328297159243},{"class_name":"CollectionUtils","similarity_score":0.28708787159108196},{"class_name":"ConnectUtils","similarity_score":0.12856845491366292},{"class_name":"TieredStorageTestUtils","similarity_score":0.10526379574861731},{"class_name":"AssignmentTestUtil","similarity_score":0.29554004461167865},{"class_name":"AssignmentTestUtils","similarity_score":0.40664095878142864},{"class_name":"StateManagerUtil","similarity_score":0.17156481467096385},{"class_name":"SSLUtils","similarity_score":0.2733774824522567},{"class_name":"Util","similarity_score":0.2133181777013039},{"class_name":"Utils","similarity_score":0.1861451990046811},{"class_name":"UtilsTest","similarity_score":0.3489976557810871},{"class_name":"ConsumerProtocolUtils","similarity_score":0.2642686460020288}],"target_classes_sorted_by_llm":["AssignmentTestUtils","ClientUtils","ConsumerGroupCommandTestUtils","ToolsTestUtils","ClientMetricsTestUtils","TestSslUtils","UtilsTest","ToolsUtils","TestCloseable","ControllerMetricsTestUtils"],"llm_response_time":13511,"similarity_computation_time":145,"similarity_metric":"tfidf"}}}
{"id":"bbee0edd-2bd9-4948-9659-eb4ef59034b0","methodCount":48,"hostFunctionTelemetryData":{"hostFunctionSize":1230,"lineStart":47,"lineEnd":1276,"bodyLineStart":47,"language":"java","filePath":"/Users/fraol/Documents/MM-assist-dataset/kafka/connect/api/src/main/java/org/apache/kafka/connect/data/Values.java","sourceCode":"/**\n * Utility for converting from one Connect value to a different form. This is useful when the caller expects a value of a particular type\n * but is uncertain whether the actual value is one that isn\u0027t directly that type but can be converted into that type.\n *\n * \u003cp\u003eFor example, a caller might expect a particular {@link org.apache.kafka.connect.header.Header} to contain a {@link Type#INT64}\n * value, when in fact that header contains a string representation of a 32-bit integer. Here, the caller can use the methods in this\n * class to convert the value to the desired type:\n * \u003cpre\u003e\n *     Header header \u003d ...\n *     long value \u003d Values.convertToLong(header.schema(), header.value());\n * \u003c/pre\u003e\n *\n * \u003cp\u003eThis class is able to convert any value to a string representation as well as parse those string representations back into most of\n * the types. The only exception is {@link Struct} values that require a schema and thus cannot be parsed from a simple string.\n */\npublic class Values {\n\n    private static final Logger LOG \u003d LoggerFactory.getLogger(Values.class);\n\n    private static final TimeZone UTC \u003d TimeZone.getTimeZone(\"UTC\");\n    private static final SchemaAndValue NULL_SCHEMA_AND_VALUE \u003d new SchemaAndValue(null, null);\n    private static final SchemaAndValue TRUE_SCHEMA_AND_VALUE \u003d new SchemaAndValue(Schema.BOOLEAN_SCHEMA, Boolean.TRUE);\n    private static final SchemaAndValue FALSE_SCHEMA_AND_VALUE \u003d new SchemaAndValue(Schema.BOOLEAN_SCHEMA, Boolean.FALSE);\n    private static final Schema ARRAY_SELECTOR_SCHEMA \u003d SchemaBuilder.array(Schema.STRING_SCHEMA).build();\n    private static final Schema MAP_SELECTOR_SCHEMA \u003d SchemaBuilder.map(Schema.STRING_SCHEMA, Schema.STRING_SCHEMA).build();\n    private static final Schema STRUCT_SELECTOR_SCHEMA \u003d SchemaBuilder.struct().build();\n    private static final String TRUE_LITERAL \u003d Boolean.TRUE.toString();\n    private static final String FALSE_LITERAL \u003d Boolean.FALSE.toString();\n    private static final long MILLIS_PER_DAY \u003d 24 * 60 * 60 * 1000;\n    private static final String NULL_VALUE \u003d \"null\";\n    static final String ISO_8601_DATE_FORMAT_PATTERN \u003d \"yyyy-MM-dd\";\n    static final String ISO_8601_TIME_FORMAT_PATTERN \u003d \"HH:mm:ss.SSS\u0027Z\u0027\";\n    static final String ISO_8601_TIMESTAMP_FORMAT_PATTERN \u003d ISO_8601_DATE_FORMAT_PATTERN + \"\u0027T\u0027\" + ISO_8601_TIME_FORMAT_PATTERN;\n    private static final Set\u003cString\u003e TEMPORAL_LOGICAL_TYPE_NAMES \u003d\n            Collections.unmodifiableSet(\n                    new HashSet\u003c\u003e(\n                            Arrays.asList(Time.LOGICAL_NAME,\n                            Timestamp.LOGICAL_NAME,\n                            Date.LOGICAL_NAME\n                            )\n                    )\n            );\n\n    private static final String QUOTE_DELIMITER \u003d \"\\\"\";\n    private static final String COMMA_DELIMITER \u003d \",\";\n    private static final String ENTRY_DELIMITER \u003d \":\";\n    private static final String ARRAY_BEGIN_DELIMITER \u003d \"[\";\n    private static final String ARRAY_END_DELIMITER \u003d \"]\";\n    private static final String MAP_BEGIN_DELIMITER \u003d \"{\";\n    private static final String MAP_END_DELIMITER \u003d \"}\";\n    private static final int ISO_8601_DATE_LENGTH \u003d ISO_8601_DATE_FORMAT_PATTERN.length();\n    private static final int ISO_8601_TIME_LENGTH \u003d ISO_8601_TIME_FORMAT_PATTERN.length() - 2; // subtract single quotes\n    private static final int ISO_8601_TIMESTAMP_LENGTH \u003d ISO_8601_TIMESTAMP_FORMAT_PATTERN.length() - 4; // subtract single quotes\n\n    private static final Pattern TWO_BACKSLASHES \u003d Pattern.compile(\"\\\\\\\\\");\n\n    private static final Pattern DOUBLE_QUOTE \u003d Pattern.compile(\"\\\"\");\n\n    /**\n     * Convert the specified value to a {@link Type#BOOLEAN} value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a boolean.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a boolean, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a boolean\n     */\n    public static Boolean convertToBoolean(Schema schema, Object value) throws DataException {\n        return (Boolean) convertTo(Schema.OPTIONAL_BOOLEAN_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT8} byte value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a byte.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a byte, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a byte\n     */\n    public static Byte convertToByte(Schema schema, Object value) throws DataException {\n        return (Byte) convertTo(Schema.OPTIONAL_INT8_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT16} short value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a short.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a short, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a short\n     */\n    public static Short convertToShort(Schema schema, Object value) throws DataException {\n        return (Short) convertTo(Schema.OPTIONAL_INT16_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT32} int value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to an integer.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as an integer, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to an integer\n     */\n    public static Integer convertToInteger(Schema schema, Object value) throws DataException {\n        return (Integer) convertTo(Schema.OPTIONAL_INT32_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT64} long value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a long.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a long, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a long\n     */\n    public static Long convertToLong(Schema schema, Object value) throws DataException {\n        return (Long) convertTo(Schema.OPTIONAL_INT64_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#FLOAT32} float value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a floating point number.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a float, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a float\n     */\n    public static Float convertToFloat(Schema schema, Object value) throws DataException {\n        return (Float) convertTo(Schema.OPTIONAL_FLOAT32_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#FLOAT64} double value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a floating point number.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a double, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a double\n     */\n    public static Double convertToDouble(Schema schema, Object value) throws DataException {\n        return (Double) convertTo(Schema.OPTIONAL_FLOAT64_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#STRING} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a string, or null if the supplied value was null\n     */\n    public static String convertToString(Schema schema, Object value) {\n        return (String) convertTo(Schema.OPTIONAL_STRING_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#ARRAY} value. If the value is a string representation of an array, this method\n     * will parse the string and its elements to infer the schemas for those elements. Thus, this method supports\n     * arrays of other primitives and structured types. If the value is already an array (or list), this method simply casts and\n     * returns it.\n     *\n     * \u003cp\u003eThis method currently does not use the schema, though it may be used in the future.\u003c/p\u003e\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a list, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a list value\n     */\n    public static List\u003c?\u003e convertToList(Schema schema, Object value) {\n        return (List\u003c?\u003e) convertTo(ARRAY_SELECTOR_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#MAP} value. If the value is a string representation of a map, this method\n     * will parse the string and its entries to infer the schemas for those entries. Thus, this method supports\n     * maps with primitives and structured keys and values. If the value is already a map, this method simply casts and returns it.\n     *\n     * \u003cp\u003eThis method currently does not use the schema, though it may be used in the future.\u003c/p\u003e\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a map, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a map value\n     */\n    public static Map\u003c?, ?\u003e convertToMap(Schema schema, Object value) {\n        return (Map\u003c?, ?\u003e) convertTo(MAP_SELECTOR_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#STRUCT} value. Structs cannot be converted from other types, so this method returns\n     * a struct only if the supplied value is a struct. If not a struct, this method throws an exception.\n     *\n     * \u003cp\u003eThis method currently does not use the schema, though it may be used in the future.\u003c/p\u003e\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a struct, or null if the supplied value was null\n     * @throws DataException if the value is not a struct\n     */\n    public static Struct convertToStruct(Schema schema, Object value) {\n        return (Struct) convertTo(STRUCT_SELECTOR_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Time#SCHEMA time} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a time, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a time value\n     */\n    public static java.util.Date convertToTime(Schema schema, Object value) {\n        return (java.util.Date) convertTo(Time.SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Date#SCHEMA date} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a date, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a date value\n     */\n    public static java.util.Date convertToDate(Schema schema, Object value) {\n        return (java.util.Date) convertTo(Date.SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Timestamp#SCHEMA timestamp} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a timestamp, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a timestamp value\n     */\n    public static java.util.Date convertToTimestamp(Schema schema, Object value) {\n        return (java.util.Date) convertTo(Timestamp.SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Decimal decimal} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a decimal, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a decimal value\n     */\n    public static BigDecimal convertToDecimal(Schema schema, Object value, int scale) {\n        return (BigDecimal) convertTo(Decimal.schema(scale), schema, value);\n    }\n\n    /**\n     * If possible infer a schema for the given value.\n     *\n     * @param value the value whose schema is to be inferred; may be null\n     * @return the inferred schema, or null if the value is null or no schema could be inferred\n     */\n    public static Schema inferSchema(Object value) {\n        if (value instanceof String) {\n            return Schema.STRING_SCHEMA;\n        }\n        if (value instanceof Boolean) {\n            return Schema.BOOLEAN_SCHEMA;\n        }\n        if (value instanceof Byte) {\n            return Schema.INT8_SCHEMA;\n        }\n        if (value instanceof Short) {\n            return Schema.INT16_SCHEMA;\n        }\n        if (value instanceof Integer) {\n            return Schema.INT32_SCHEMA;\n        }\n        if (value instanceof Long) {\n            return Schema.INT64_SCHEMA;\n        }\n        if (value instanceof Float) {\n            return Schema.FLOAT32_SCHEMA;\n        }\n        if (value instanceof Double) {\n            return Schema.FLOAT64_SCHEMA;\n        }\n        if (value instanceof byte[] || value instanceof ByteBuffer) {\n            return Schema.BYTES_SCHEMA;\n        }\n        if (value instanceof List) {\n            List\u003c?\u003e list \u003d (List\u003c?\u003e) value;\n            if (list.isEmpty()) {\n                return null;\n            }\n            SchemaDetector detector \u003d new SchemaDetector();\n            for (Object element : list) {\n                if (!detector.canDetect(element)) {\n                    return null;\n                }\n            }\n            return SchemaBuilder.array(detector.schema()).build();\n        }\n        if (value instanceof Map) {\n            Map\u003c?, ?\u003e map \u003d (Map\u003c?, ?\u003e) value;\n            if (map.isEmpty()) {\n                return null;\n            }\n            SchemaDetector keyDetector \u003d new SchemaDetector();\n            SchemaDetector valueDetector \u003d new SchemaDetector();\n            for (Map.Entry\u003c?, ?\u003e entry : map.entrySet()) {\n                if (!keyDetector.canDetect(entry.getKey()) || !valueDetector.canDetect(entry.getValue())) {\n                    return null;\n                }\n            }\n            return SchemaBuilder.map(keyDetector.schema(), valueDetector.schema()).build();\n        }\n        if (value instanceof Struct) {\n            return ((Struct) value).schema();\n        }\n        return null;\n    }\n\n\n    /**\n     * Parse the specified string representation of a value into its schema and value.\n     *\n     * @param value the string form of the value\n     * @return the schema and value; never null, but whose schema and value may be null\n     * @see #convertToString\n     */\n    public static SchemaAndValue parseString(String value) {\n        if (value \u003d\u003d null) {\n            return NULL_SCHEMA_AND_VALUE;\n        }\n        if (value.isEmpty()) {\n            return new SchemaAndValue(Schema.STRING_SCHEMA, value);\n        }\n        Parser parser \u003d new Parser(value);\n        return parse(parser, false);\n    }\n\n    /**\n     * Convert the value to the desired type.\n     *\n     * @param toSchema   the schema for the desired type; may not be null\n     * @param fromSchema the schema for the supplied value; may be null if not known\n     * @return the converted value; never null\n     * @throws DataException if the value could not be converted to the desired type\n     */\n    protected static Object convertTo(Schema toSchema, Schema fromSchema, Object value) throws DataException {\n        if (value \u003d\u003d null) {\n            if (toSchema.isOptional()) {\n                return null;\n            }\n            throw new DataException(\"Unable to convert a null value to a schema that requires a value\");\n        }\n        switch (toSchema.type()) {\n            case BYTES:\n                if (Decimal.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof ByteBuffer) {\n                        value \u003d Utils.toArray((ByteBuffer) value);\n                    }\n                    if (value instanceof byte[]) {\n                        return Decimal.toLogical(toSchema, (byte[]) value);\n                    }\n                    if (value instanceof BigDecimal) {\n                        return value;\n                    }\n                    if (value instanceof Number) {\n                        // Not already a decimal, so treat it as a double ...\n                        double converted \u003d ((Number) value).doubleValue();\n                        return BigDecimal.valueOf(converted);\n                    }\n                    if (value instanceof String) {\n                        return new BigDecimal(value.toString());\n                    }\n                }\n                if (value instanceof ByteBuffer) {\n                    return Utils.toArray((ByteBuffer) value);\n                }\n                if (value instanceof byte[]) {\n                    return value;\n                }\n                if (value instanceof BigDecimal) {\n                    return Decimal.fromLogical(toSchema, (BigDecimal) value);\n                }\n                break;\n            case STRING:\n                StringBuilder sb \u003d new StringBuilder();\n                append(sb, value, false);\n                return sb.toString();\n            case BOOLEAN:\n                if (value instanceof Boolean) {\n                    return value;\n                }\n                if (value instanceof String) {\n                    SchemaAndValue parsed \u003d parseString(value.toString());\n                    if (parsed.value() instanceof Boolean) {\n                        return parsed.value();\n                    }\n                }\n                return asLong(value, fromSchema, null) \u003d\u003d 0L ? Boolean.FALSE : Boolean.TRUE;\n            case INT8:\n                if (value instanceof Byte) {\n                    return value;\n                }\n                return (byte) asLong(value, fromSchema, null);\n            case INT16:\n                if (value instanceof Short) {\n                    return value;\n                }\n                return (short) asLong(value, fromSchema, null);\n            case INT32:\n                if (Date.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof String) {\n                        SchemaAndValue parsed \u003d parseString(value.toString());\n                        value \u003d parsed.value();\n                    }\n                    if (value instanceof java.util.Date) {\n                        if (fromSchema !\u003d null) {\n                            String fromSchemaName \u003d fromSchema.name();\n                            if (Date.LOGICAL_NAME.equals(fromSchemaName)) {\n                                return value;\n                            }\n                            if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {\n                                // Just get the number of days from this timestamp\n                                long millis \u003d ((java.util.Date) value).getTime();\n                                int days \u003d (int) (millis / MILLIS_PER_DAY); // truncates\n                                return Date.toLogical(toSchema, days);\n                            }\n                        } else {\n                            // There is no fromSchema, so no conversion is needed\n                            return value;\n                        }\n                    }\n                    long numeric \u003d asLong(value, fromSchema, null);\n                    return Date.toLogical(toSchema, (int) numeric);\n                }\n                if (Time.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof String) {\n                        SchemaAndValue parsed \u003d parseString(value.toString());\n                        value \u003d parsed.value();\n                    }\n                    if (value instanceof java.util.Date) {\n                        if (fromSchema !\u003d null) {\n                            String fromSchemaName \u003d fromSchema.name();\n                            if (Time.LOGICAL_NAME.equals(fromSchemaName)) {\n                                return value;\n                            }\n                            if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {\n                                // Just get the time portion of this timestamp\n                                Calendar calendar \u003d Calendar.getInstance(UTC);\n                                calendar.setTime((java.util.Date) value);\n                                calendar.set(Calendar.YEAR, 1970);\n                                calendar.set(Calendar.MONTH, 0); // Months are zero-based\n                                calendar.set(Calendar.DAY_OF_MONTH, 1);\n                                return Time.toLogical(toSchema, (int) calendar.getTimeInMillis());\n                            }\n                        } else {\n                            // There is no fromSchema, so no conversion is needed\n                            return value;\n                        }\n                    }\n                    long numeric \u003d asLong(value, fromSchema, null);\n                    return Time.toLogical(toSchema, (int) numeric);\n                }\n                if (value instanceof Integer) {\n                    return value;\n                }\n                return (int) asLong(value, fromSchema, null);\n            case INT64:\n                if (Timestamp.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof String) {\n                        SchemaAndValue parsed \u003d parseString(value.toString());\n                        value \u003d parsed.value();\n                    }\n                    if (value instanceof java.util.Date) {\n                        java.util.Date date \u003d (java.util.Date) value;\n                        if (fromSchema !\u003d null) {\n                            String fromSchemaName \u003d fromSchema.name();\n                            if (Date.LOGICAL_NAME.equals(fromSchemaName)) {\n                                int days \u003d Date.fromLogical(fromSchema, date);\n                                long millis \u003d days * MILLIS_PER_DAY;\n                                return Timestamp.toLogical(toSchema, millis);\n                            }\n                            if (Time.LOGICAL_NAME.equals(fromSchemaName)) {\n                                long millis \u003d Time.fromLogical(fromSchema, date);\n                                return Timestamp.toLogical(toSchema, millis);\n                            }\n                            if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {\n                                return value;\n                            }\n                        } else {\n                            // There is no fromSchema, so no conversion is needed\n                            return value;\n                        }\n                    }\n                    long numeric \u003d asLong(value, fromSchema, null);\n                    return Timestamp.toLogical(toSchema, numeric);\n                }\n                if (value instanceof Long) {\n                    return value;\n                }\n                return asLong(value, fromSchema, null);\n            case FLOAT32:\n                if (value instanceof Float) {\n                    return value;\n                }\n                return (float) asDouble(value, fromSchema, null);\n            case FLOAT64:\n                if (value instanceof Double) {\n                    return value;\n                }\n                return asDouble(value, fromSchema, null);\n            case ARRAY:\n                if (value instanceof String) {\n                    SchemaAndValue schemaAndValue \u003d parseString(value.toString());\n                    value \u003d schemaAndValue.value();\n                }\n                if (value instanceof List) {\n                    return value;\n                }\n                break;\n            case MAP:\n                if (value instanceof String) {\n                    SchemaAndValue schemaAndValue \u003d parseString(value.toString());\n                    value \u003d schemaAndValue.value();\n                }\n                if (value instanceof Map) {\n                    return value;\n                }\n                break;\n            case STRUCT:\n                if (value instanceof Struct) {\n                    return value;\n                }\n        }\n        throw new DataException(\"Unable to convert \" + value + \" (\" + value.getClass() + \") to \" + toSchema);\n    }\n\n    /**\n     * Convert the specified value to the desired scalar value type.\n     *\n     * @param value      the value to be converted; may not be null\n     * @param fromSchema the schema for the current value type; may not be null\n     * @param error      any previous error that should be included in an exception message; may be null\n     * @return the long value after conversion; never null\n     * @throws DataException if the value could not be converted to a long\n     */\n    protected static long asLong(Object value, Schema fromSchema, Throwable error) {\n        try {\n            if (value instanceof Number) {\n                Number number \u003d (Number) value;\n                return number.longValue();\n            }\n            if (value instanceof String) {\n                return new BigDecimal(value.toString()).longValue();\n            }\n        } catch (NumberFormatException e) {\n            error \u003d e;\n            // fall through\n        }\n        if (fromSchema !\u003d null) {\n            String schemaName \u003d fromSchema.name();\n            if (value instanceof java.util.Date) {\n                if (Date.LOGICAL_NAME.equals(schemaName)) {\n                    return Date.fromLogical(fromSchema, (java.util.Date) value);\n                }\n                if (Time.LOGICAL_NAME.equals(schemaName)) {\n                    return Time.fromLogical(fromSchema, (java.util.Date) value);\n                }\n                if (Timestamp.LOGICAL_NAME.equals(schemaName)) {\n                    return Timestamp.fromLogical(fromSchema, (java.util.Date) value);\n                }\n            }\n            throw new DataException(\"Unable to convert \" + value + \" (\" + value.getClass() + \") to \" + fromSchema, error);\n        }\n        throw new DataException(\"Unable to convert \" + value + \" (\" + value.getClass() + \") to a number\", error);\n    }\n\n    /**\n     * Convert the specified value with the desired floating point type.\n     *\n     * @param value  the value to be converted; may not be null\n     * @param schema the schema for the current value type; may not be null\n     * @param error  any previous error that should be included in an exception message; may be null\n     * @return the double value after conversion; never null\n     * @throws DataException if the value could not be converted to a double\n     */\n    protected static double asDouble(Object value, Schema schema, Throwable error) {\n        try {\n            if (value instanceof Number) {\n                Number number \u003d (Number) value;\n                return number.doubleValue();\n            }\n            if (value instanceof String) {\n                return new BigDecimal(value.toString()).doubleValue();\n            }\n        } catch (NumberFormatException e) {\n            error \u003d e;\n            // fall through\n        }\n        return asLong(value, schema, error);\n    }\n\n    protected static void append(StringBuilder sb, Object value, boolean embedded) {\n        if (value \u003d\u003d null) {\n            sb.append(NULL_VALUE);\n        } else if (value instanceof Number) {\n            sb.append(value);\n        } else if (value instanceof Boolean) {\n            sb.append(value);\n        } else if (value instanceof String) {\n            if (embedded) {\n                String escaped \u003d escape((String) value);\n                sb.append(\u0027\"\u0027).append(escaped).append(\u0027\"\u0027);\n            } else {\n                sb.append(value);\n            }\n        } else if (value instanceof byte[]) {\n            value \u003d Base64.getEncoder().encodeToString((byte[]) value);\n            if (embedded) {\n                sb.append(\u0027\"\u0027).append(value).append(\u0027\"\u0027);\n            } else {\n                sb.append(value);\n            }\n        } else if (value instanceof ByteBuffer) {\n            byte[] bytes \u003d Utils.readBytes((ByteBuffer) value);\n            append(sb, bytes, embedded);\n        } else if (value instanceof List) {\n            List\u003c?\u003e list \u003d (List\u003c?\u003e) value;\n            sb.append(\u0027[\u0027);\n            appendIterable(sb, list.iterator());\n            sb.append(\u0027]\u0027);\n        } else if (value instanceof Map) {\n            Map\u003c?, ?\u003e map \u003d (Map\u003c?, ?\u003e) value;\n            sb.append(\u0027{\u0027);\n            appendIterable(sb, map.entrySet().iterator());\n            sb.append(\u0027}\u0027);\n        } else if (value instanceof Struct) {\n            Struct struct \u003d (Struct) value;\n            Schema schema \u003d struct.schema();\n            boolean first \u003d true;\n            sb.append(\u0027{\u0027);\n            for (Field field : schema.fields()) {\n                if (first) {\n                    first \u003d false;\n                } else {\n                    sb.append(\u0027,\u0027);\n                }\n                append(sb, field.name(), true);\n                sb.append(\u0027:\u0027);\n                append(sb, struct.get(field), true);\n            }\n            sb.append(\u0027}\u0027);\n        } else if (value instanceof Map.Entry) {\n            Map.Entry\u003c?, ?\u003e entry \u003d (Map.Entry\u003c?, ?\u003e) value;\n            append(sb, entry.getKey(), true);\n            sb.append(\u0027:\u0027);\n            append(sb, entry.getValue(), true);\n        } else if (value instanceof java.util.Date) {\n            java.util.Date dateValue \u003d (java.util.Date) value;\n            String formatted \u003d dateFormatFor(dateValue).format(dateValue);\n            sb.append(formatted);\n        } else {\n            throw new DataException(\"Failed to serialize unexpected value type \" + value.getClass().getName() + \": \" + value);\n        }\n    }\n\n    protected static void appendIterable(StringBuilder sb, Iterator\u003c?\u003e iter) {\n        if (iter.hasNext()) {\n            append(sb, iter.next(), true);\n            while (iter.hasNext()) {\n                sb.append(\u0027,\u0027);\n                append(sb, iter.next(), true);\n            }\n        }\n    }\n\n    protected static String escape(String value) {\n        String replace1 \u003d TWO_BACKSLASHES.matcher(value).replaceAll(\"\\\\\\\\\\\\\\\\\");\n        return DOUBLE_QUOTE.matcher(replace1).replaceAll(\"\\\\\\\\\\\"\");\n    }\n\n    public static DateFormat dateFormatFor(java.util.Date value) {\n        if (value.getTime() \u003c MILLIS_PER_DAY) {\n            return new SimpleDateFormat(ISO_8601_TIME_FORMAT_PATTERN);\n        }\n        if (value.getTime() % MILLIS_PER_DAY \u003d\u003d 0) {\n            return new SimpleDateFormat(ISO_8601_DATE_FORMAT_PATTERN);\n        }\n        return new SimpleDateFormat(ISO_8601_TIMESTAMP_FORMAT_PATTERN);\n    }\n\n    protected static boolean canParseSingleTokenLiteral(Parser parser, boolean embedded, String tokenLiteral) {\n        int startPosition \u003d parser.mark();\n        // If the next token is what we expect, then either...\n        if (parser.canConsume(tokenLiteral)) {\n            //   ...we\u0027re reading an embedded value, in which case the next token will be handled appropriately\n            //      by the caller if it\u0027s something like an end delimiter for a map or array, or a comma to\n            //      separate multiple embedded values...\n            //   ...or it\u0027s being parsed as part of a top-level string, in which case, any other tokens should\n            //      cause use to stop parsing this single-token literal as such and instead just treat it like\n            //      a string. For example, the top-level string \"true}\" will be tokenized as the tokens \"true\" and\n            //      \"}\", but should ultimately be parsed as just the string \"true}\" instead of the boolean true.\n            if (embedded || !parser.hasNext()) {\n                return true;\n            }\n        }\n        parser.rewindTo(startPosition);\n        return false;\n    }\n\n    protected static SchemaAndValue parse(Parser parser, boolean embedded) throws NoSuchElementException {\n        if (!parser.hasNext()) {\n            return null;\n        }\n        if (embedded) {\n            if (parser.canConsume(QUOTE_DELIMITER)) {\n                StringBuilder sb \u003d new StringBuilder();\n                while (parser.hasNext()) {\n                    if (parser.canConsume(QUOTE_DELIMITER)) {\n                        break;\n                    }\n                    sb.append(parser.next());\n                }\n                String content \u003d sb.toString();\n                // We can parse string literals as temporal logical types, but all others\n                // are treated as strings\n                SchemaAndValue parsed \u003d parseString(content);\n                if (parsed !\u003d null \u0026\u0026 TEMPORAL_LOGICAL_TYPE_NAMES.contains(parsed.schema().name())) {\n                    return parsed;\n                }\n                return new SchemaAndValue(Schema.STRING_SCHEMA, content);\n            }\n        }\n\n        if (canParseSingleTokenLiteral(parser, embedded, NULL_VALUE)) {\n            return null;\n        }\n        if (canParseSingleTokenLiteral(parser, embedded, TRUE_LITERAL)) {\n            return TRUE_SCHEMA_AND_VALUE;\n        }\n        if (canParseSingleTokenLiteral(parser, embedded, FALSE_LITERAL)) {\n            return FALSE_SCHEMA_AND_VALUE;\n        }\n\n        int startPosition \u003d parser.mark();\n\n        try {\n            if (parser.canConsume(ARRAY_BEGIN_DELIMITER)) {\n                List\u003cObject\u003e result \u003d new ArrayList\u003c\u003e();\n                boolean compatible \u003d true;\n                Schema elementSchema \u003d null;\n                while (parser.hasNext()) {\n                    if (parser.canConsume(ARRAY_END_DELIMITER)) {\n                        Schema listSchema;\n                        if (elementSchema !\u003d null \u0026\u0026 compatible) {\n                            listSchema \u003d SchemaBuilder.array(elementSchema).schema();\n                            result \u003d alignListEntriesWithSchema(listSchema, result);\n                        } else {\n                            // Every value is null\n                            listSchema \u003d SchemaBuilder.arrayOfNull().build();\n                        }\n                        return new SchemaAndValue(listSchema, result);\n                    }\n\n                    if (parser.canConsume(COMMA_DELIMITER)) {\n                        throw new DataException(\"Unable to parse an empty array element: \" + parser.original());\n                    }\n                    SchemaAndValue element \u003d parse(parser, true);\n                    elementSchema \u003d commonSchemaFor(elementSchema, element);\n                    if (elementSchema \u003d\u003d null \u0026\u0026 element !\u003d null \u0026\u0026 element.schema() !\u003d null) {\n                        compatible \u003d false;\n                    }\n                    result.add(element !\u003d null ? element.value() : null);\n\n                    int currentPosition \u003d parser.mark();\n                    if (parser.canConsume(ARRAY_END_DELIMITER)) {\n                        parser.rewindTo(currentPosition);\n                    } else if (!parser.canConsume(COMMA_DELIMITER)) {\n                        throw new DataException(\"Array elements missing \u0027\" + COMMA_DELIMITER + \"\u0027 delimiter\");\n                    }\n                }\n\n                // Missing either a comma or an end delimiter\n                if (COMMA_DELIMITER.equals(parser.previous())) {\n                    throw new DataException(\"Array is missing element after \u0027,\u0027: \" + parser.original());\n                }\n                throw new DataException(\"Array is missing terminating \u0027]\u0027: \" + parser.original());\n            }\n\n            if (parser.canConsume(MAP_BEGIN_DELIMITER)) {\n                Map\u003cObject, Object\u003e result \u003d new LinkedHashMap\u003c\u003e();\n                boolean keyCompatible \u003d true;\n                Schema keySchema \u003d null;\n                boolean valueCompatible \u003d true;\n                Schema valueSchema \u003d null;\n                while (parser.hasNext()) {\n                    if (parser.canConsume(MAP_END_DELIMITER)) {\n                        Schema mapSchema;\n                        if (keySchema !\u003d null \u0026\u0026 valueSchema !\u003d null \u0026\u0026 keyCompatible \u0026\u0026 valueCompatible) {\n                            mapSchema \u003d SchemaBuilder.map(keySchema, valueSchema).build();\n                            result \u003d alignMapKeysAndValuesWithSchema(mapSchema, result);\n                        } else if (keySchema !\u003d null \u0026\u0026 keyCompatible) {\n                            mapSchema \u003d SchemaBuilder.mapWithNullValues(keySchema);\n                            result \u003d alignMapKeysWithSchema(mapSchema, result);\n                        } else {\n                            mapSchema \u003d SchemaBuilder.mapOfNull().build();\n                        }\n                        return new SchemaAndValue(mapSchema, result);\n                    }\n\n                    if (parser.canConsume(COMMA_DELIMITER)) {\n                        throw new DataException(\"Unable to parse a map entry with no key or value: \" + parser.original());\n                    }\n                    SchemaAndValue key \u003d parse(parser, true);\n                    if (key \u003d\u003d null || key.value() \u003d\u003d null) {\n                        throw new DataException(\"Map entry may not have a null key: \" + parser.original());\n                    }\n\n                    if (!parser.canConsume(ENTRY_DELIMITER)) {\n                        throw new DataException(\"Map entry is missing \u0027\" + ENTRY_DELIMITER\n                                                + \"\u0027 at \" + parser.position()\n                                                + \" in \" + parser.original());\n                    }\n                    SchemaAndValue value \u003d parse(parser, true);\n                    Object entryValue \u003d value !\u003d null ? value.value() : null;\n                    result.put(key.value(), entryValue);\n\n                    parser.canConsume(COMMA_DELIMITER);\n                    keySchema \u003d commonSchemaFor(keySchema, key);\n                    if (keySchema \u003d\u003d null \u0026\u0026 key.schema() !\u003d null) {\n                        keyCompatible \u003d false;\n                    }\n                    valueSchema \u003d commonSchemaFor(valueSchema, value);\n                    if (valueSchema \u003d\u003d null \u0026\u0026 value !\u003d null \u0026\u0026 value.schema() !\u003d null) {\n                        valueCompatible \u003d false;\n                    }\n                }\n                // Missing either a comma or an end delimiter\n                if (COMMA_DELIMITER.equals(parser.previous())) {\n                    throw new DataException(\"Map is missing element after \u0027,\u0027: \" + parser.original());\n                }\n                throw new DataException(\"Map is missing terminating \u0027}\u0027: \" + parser.original());\n            }\n        } catch (DataException e) {\n            LOG.trace(\"Unable to parse the value as a map or an array; reverting to string\", e);\n            parser.rewindTo(startPosition);\n        }\n\n        String token \u003d parser.next();\n        if (Utils.isBlank(token)) {\n            return new SchemaAndValue(Schema.STRING_SCHEMA, token);\n        }\n        token \u003d token.trim();\n\n        char firstChar \u003d token.charAt(0);\n        boolean firstCharIsDigit \u003d Character.isDigit(firstChar);\n\n        // Temporal types are more restrictive, so try them first\n        if (firstCharIsDigit) {\n            // The time and timestamp literals may be split into 5 tokens since an unescaped colon\n            // is a delimiter. Check these first since the first of these tokens is a simple numeric\n            int position \u003d parser.mark();\n            String remainder \u003d parser.next(4);\n            if (remainder !\u003d null) {\n                String timeOrTimestampStr \u003d token + remainder;\n                SchemaAndValue temporal \u003d parseAsTemporal(timeOrTimestampStr);\n                if (temporal !\u003d null) {\n                    return temporal;\n                }\n            }\n            // No match was found using the 5 tokens, so rewind and see if the current token has a date, time, or timestamp\n            parser.rewindTo(position);\n            SchemaAndValue temporal \u003d parseAsTemporal(token);\n            if (temporal !\u003d null) {\n                return temporal;\n            }\n        }\n        if (firstCharIsDigit || firstChar \u003d\u003d \u0027+\u0027 || firstChar \u003d\u003d \u0027-\u0027) {\n            try {\n                // Try to parse as a number ...\n                BigDecimal decimal \u003d new BigDecimal(token);\n                try {\n                    return new SchemaAndValue(Schema.INT8_SCHEMA, decimal.byteValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                try {\n                    return new SchemaAndValue(Schema.INT16_SCHEMA, decimal.shortValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                try {\n                    return new SchemaAndValue(Schema.INT32_SCHEMA, decimal.intValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                try {\n                    return new SchemaAndValue(Schema.INT64_SCHEMA, decimal.longValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                float fValue \u003d decimal.floatValue();\n                if (fValue !\u003d Float.NEGATIVE_INFINITY \u0026\u0026 fValue !\u003d Float.POSITIVE_INFINITY\n                    \u0026\u0026 decimal.scale() !\u003d 0) {\n                    return new SchemaAndValue(Schema.FLOAT32_SCHEMA, fValue);\n                }\n                double dValue \u003d decimal.doubleValue();\n                if (dValue !\u003d Double.NEGATIVE_INFINITY \u0026\u0026 dValue !\u003d Double.POSITIVE_INFINITY\n                    \u0026\u0026 decimal.scale() !\u003d 0) {\n                    return new SchemaAndValue(Schema.FLOAT64_SCHEMA, dValue);\n                }\n                Schema schema \u003d Decimal.schema(decimal.scale());\n                return new SchemaAndValue(schema, decimal);\n            } catch (NumberFormatException e) {\n                // can\u0027t parse as a number\n            }\n        }\n        if (embedded) {\n            throw new DataException(\"Failed to parse embedded value\");\n        }\n        // At this point, the only thing this non-embedded value can be is a string.\n        return new SchemaAndValue(Schema.STRING_SCHEMA, parser.original());\n    }\n\n    private static SchemaAndValue parseAsTemporal(String token) {\n        if (token \u003d\u003d null) {\n            return null;\n        }\n        // If the colons were escaped, we\u0027ll see the escape chars and need to remove them\n        token \u003d token.replace(\"\\\\:\", \":\");\n        int tokenLength \u003d token.length();\n        if (tokenLength \u003d\u003d ISO_8601_TIME_LENGTH) {\n            try {\n                return new SchemaAndValue(Time.SCHEMA, new SimpleDateFormat(ISO_8601_TIME_FORMAT_PATTERN).parse(token));\n            } catch (ParseException e) {\n              // not a valid date\n            }\n        } else if (tokenLength \u003d\u003d ISO_8601_TIMESTAMP_LENGTH) {\n            try {\n                return new SchemaAndValue(Timestamp.SCHEMA, new SimpleDateFormat(ISO_8601_TIMESTAMP_FORMAT_PATTERN).parse(token));\n            } catch (ParseException e) {\n              // not a valid date\n            }\n        } else if (tokenLength \u003d\u003d ISO_8601_DATE_LENGTH) {\n            try {\n                return new SchemaAndValue(Date.SCHEMA, new SimpleDateFormat(ISO_8601_DATE_FORMAT_PATTERN).parse(token));\n            } catch (ParseException e) {\n                // not a valid date\n            }\n        }\n        return null;\n    }\n\n    protected static Schema commonSchemaFor(Schema previous, SchemaAndValue latest) {\n        if (latest \u003d\u003d null) {\n            return previous;\n        }\n        if (previous \u003d\u003d null) {\n            return latest.schema();\n        }\n        Schema newSchema \u003d latest.schema();\n        Type previousType \u003d previous.type();\n        Type newType \u003d newSchema.type();\n        if (previousType !\u003d newType) {\n            switch (previous.type()) {\n                case INT8:\n                    if (newType \u003d\u003d Type.INT16 || newType \u003d\u003d Type.INT32 || newType \u003d\u003d Type.INT64 || newType \u003d\u003d Type.FLOAT32 || newType \u003d\u003d\n                                                                                                                              Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case INT16:\n                    if (newType \u003d\u003d Type.INT8) {\n                        return previous;\n                    }\n                    if (newType \u003d\u003d Type.INT32 || newType \u003d\u003d Type.INT64 || newType \u003d\u003d Type.FLOAT32 || newType \u003d\u003d Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case INT32:\n                    if (newType \u003d\u003d Type.INT8 || newType \u003d\u003d Type.INT16) {\n                        return previous;\n                    }\n                    if (newType \u003d\u003d Type.INT64 || newType \u003d\u003d Type.FLOAT32 || newType \u003d\u003d Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case INT64:\n                    if (newType \u003d\u003d Type.INT8 || newType \u003d\u003d Type.INT16 || newType \u003d\u003d Type.INT32) {\n                        return previous;\n                    }\n                    if (newType \u003d\u003d Type.FLOAT32 || newType \u003d\u003d Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case FLOAT32:\n                    if (newType \u003d\u003d Type.INT8 || newType \u003d\u003d Type.INT16 || newType \u003d\u003d Type.INT32 || newType \u003d\u003d Type.INT64) {\n                        return previous;\n                    }\n                    if (newType \u003d\u003d Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case FLOAT64:\n                    if (newType \u003d\u003d Type.INT8 || newType \u003d\u003d Type.INT16 || newType \u003d\u003d Type.INT32 || newType \u003d\u003d Type.INT64 || newType \u003d\u003d\n                                                                                                                           Type.FLOAT32) {\n                        return previous;\n                    }\n                    break;\n            }\n            return null;\n        }\n        if (previous.isOptional() \u003d\u003d newSchema.isOptional()) {\n            // Use the optional one\n            return previous.isOptional() ? previous : newSchema;\n        }\n        if (!previous.equals(newSchema)) {\n            return null;\n        }\n        return previous;\n    }\n\n    protected static List\u003cObject\u003e alignListEntriesWithSchema(Schema schema, List\u003cObject\u003e input) {\n        Schema valueSchema \u003d schema.valueSchema();\n        List\u003cObject\u003e result \u003d new ArrayList\u003c\u003e();\n        for (Object value : input) {\n            Object newValue \u003d convertTo(valueSchema, null, value);\n            result.add(newValue);\n        }\n        return result;\n    }\n\n    protected static Map\u003cObject, Object\u003e alignMapKeysAndValuesWithSchema(Schema mapSchema, Map\u003cObject, Object\u003e input) {\n        Schema keySchema \u003d mapSchema.keySchema();\n        Schema valueSchema \u003d mapSchema.valueSchema();\n        Map\u003cObject, Object\u003e result \u003d new LinkedHashMap\u003c\u003e();\n        for (Map.Entry\u003c?, ?\u003e entry : input.entrySet()) {\n            Object newKey \u003d convertTo(keySchema, null, entry.getKey());\n            Object newValue \u003d convertTo(valueSchema, null, entry.getValue());\n            result.put(newKey, newValue);\n        }\n        return result;\n    }\n\n    protected static Map\u003cObject, Object\u003e alignMapKeysWithSchema(Schema mapSchema, Map\u003cObject, Object\u003e input) {\n        Schema keySchema \u003d mapSchema.keySchema();\n        Map\u003cObject, Object\u003e result \u003d new LinkedHashMap\u003c\u003e();\n        for (Map.Entry\u003c?, ?\u003e entry : input.entrySet()) {\n            Object newKey \u003d convertTo(keySchema, null, entry.getKey());\n            result.put(newKey, entry.getValue());\n        }\n        return result;\n    }\n\n    protected static class SchemaDetector {\n        private Type knownType \u003d null;\n        private boolean optional \u003d false;\n\n        public SchemaDetector() {\n        }\n\n        public boolean canDetect(Object value) {\n            if (value \u003d\u003d null) {\n                optional \u003d true;\n                return true;\n            }\n            Schema schema \u003d inferSchema(value);\n            if (schema \u003d\u003d null) {\n                return false;\n            }\n            if (knownType \u003d\u003d null) {\n                knownType \u003d schema.type();\n            } else if (knownType !\u003d schema.type()) {\n                return false;\n            }\n            return true;\n        }\n\n        public Schema schema() {\n            SchemaBuilder builder \u003d SchemaBuilder.type(knownType);\n            if (optional) {\n                builder.optional();\n            }\n            return builder.schema();\n        }\n    }\n\n    protected static class Parser {\n        private final String original;\n        private final CharacterIterator iter;\n        private String nextToken \u003d null;\n        private String previousToken \u003d null;\n\n        public Parser(String original) {\n            this.original \u003d original;\n            this.iter \u003d new StringCharacterIterator(this.original);\n        }\n\n        public int position() {\n            return iter.getIndex();\n        }\n\n        public int mark() {\n            return iter.getIndex() - (nextToken !\u003d null ? nextToken.length() : 0);\n        }\n\n        public void rewindTo(int position) {\n            iter.setIndex(position);\n            nextToken \u003d null;\n            previousToken \u003d null;\n        }\n\n        public String original() {\n            return original;\n        }\n\n        public boolean hasNext() {\n            return nextToken !\u003d null || canConsumeNextToken();\n        }\n\n        protected boolean canConsumeNextToken() {\n            return iter.getEndIndex() \u003e iter.getIndex();\n        }\n\n        public String next() {\n            if (nextToken !\u003d null) {\n                previousToken \u003d nextToken;\n                nextToken \u003d null;\n            } else {\n                previousToken \u003d consumeNextToken();\n            }\n            return previousToken;\n        }\n\n        public String next(int n) {\n            int current \u003d mark();\n            int start \u003d mark();\n            for (int i \u003d 0; i !\u003d n; ++i) {\n                if (!hasNext()) {\n                    rewindTo(start);\n                    return null;\n                }\n                next();\n            }\n            return original.substring(current, position());\n        }\n\n        private String consumeNextToken() throws NoSuchElementException {\n            boolean escaped \u003d false;\n            int start \u003d iter.getIndex();\n            char c \u003d iter.current();\n            while (canConsumeNextToken()) {\n                switch (c) {\n                    case \u0027\\\\\u0027:\n                        escaped \u003d !escaped;\n                        break;\n                    case \u0027:\u0027:\n                    case \u0027,\u0027:\n                    case \u0027{\u0027:\n                    case \u0027}\u0027:\n                    case \u0027[\u0027:\n                    case \u0027]\u0027:\n                    case \u0027\\\"\u0027:\n                        if (!escaped) {\n                            if (start \u003c iter.getIndex()) {\n                                // Return the previous token\n                                return original.substring(start, iter.getIndex());\n                            }\n                            // Consume and return this delimiter as a token\n                            iter.next();\n                            return original.substring(start, start + 1);\n                        }\n                        // escaped, so continue\n                        escaped \u003d false;\n                        break;\n                    default:\n                        // If escaped, then we don\u0027t care what was escaped\n                        escaped \u003d false;\n                        break;\n                }\n                c \u003d iter.next();\n            }\n            return original.substring(start, iter.getIndex());\n        }\n\n        public String previous() {\n            return previousToken;\n        }\n\n        public boolean canConsume(String expected) {\n            return canConsume(expected, true);\n        }\n\n        public boolean canConsume(String expected, boolean ignoreLeadingAndTrailingWhitespace) {\n            if (isNext(expected, ignoreLeadingAndTrailingWhitespace)) {\n                // consume this token ...\n                nextToken \u003d null;\n                return true;\n            }\n            return false;\n        }\n\n        protected boolean isNext(String expected, boolean ignoreLeadingAndTrailingWhitespace) {\n            if (nextToken \u003d\u003d null) {\n                if (!hasNext()) {\n                    return false;\n                }\n                // There\u0027s another token, so consume it\n                nextToken \u003d consumeNextToken();\n            }\n            if (ignoreLeadingAndTrailingWhitespace) {\n                while (Utils.isBlank(nextToken) \u0026\u0026 canConsumeNextToken()) {\n                    nextToken \u003d consumeNextToken();\n                }\n            }\n            return ignoreLeadingAndTrailingWhitespace\n                ? nextToken.trim().equals(expected)\n                : nextToken.equals(expected);\n        }\n    }\n}","methodCount":48},"candidatesTelemetryData":{"numberOfSuggestions":0,"candidates":[]},"processingTime":{"llmResponseTime":-1,"pluginProcessingTime":-1,"totalTime":61705},"iterationData":[{"iteration_num":1,"suggested_move_methods":[{"method_name":"parseString","method_signature":"public static SchemaAndValue parseString(String value)","target_class":"Parser","rationale":"The parseString method is responsible for parsing string representations of values, which aligns closely with the functionality of the Parser class. Moving it there would enhance cohesion."},{"method_name":"inferSchema","method_signature":"public static Schema inferSchema(Object value)","target_class":"SchemaDetector","rationale":"The inferSchema method is used to determine the schema of a given value, which is a core responsibility of the SchemaDetector class. Moving it there would improve the organization of schema-related logic."},{"method_name":"append","method_signature":"protected static void append(StringBuilder sb, Object value, boolean embedded)","target_class":"Utils","rationale":"The append method is a utility function for converting various types to a string representation. It would be more appropriate in a utility class like Utils, which can handle general-purpose methods."},{"method_name":"dateFormatFor","method_signature":"public static DateFormat dateFormatFor(java.util.Date value)","target_class":"DateUtils","rationale":"The dateFormatFor method is focused on formatting dates, which is a utility function that would fit well in a dedicated DateUtils class, separating date handling from value conversion."},{"method_name":"alignListEntriesWithSchema","method_signature":"protected static List\u003cObject\u003e alignListEntriesWithSchema(Schema schema, List\u003cObject\u003e input)","target_class":"SchemaUtils","rationale":"The alignListEntriesWithSchema method is concerned with aligning list entries to a schema, which is a specific utility function that would be better placed in a SchemaUtils class dedicated to schema-related operations."},{"method_name":"alignMapKeysAndValuesWithSchema","method_signature":"protected static Map\u003cObject, Object\u003e alignMapKeysAndValuesWithSchema(Schema mapSchema, Map\u003cObject, Object\u003e input)","target_class":"SchemaUtils","rationale":"Similar to alignListEntriesWithSchema, this method deals with aligning map entries to a schema, making it suitable for a SchemaUtils class."},{"method_name":"alignMapKeysWithSchema","method_signature":"protected static Map\u003cObject, Object\u003e alignMapKeysWithSchema(Schema mapSchema, Map\u003cObject, Object\u003e input)","target_class":"SchemaUtils","rationale":"This method also deals with schema alignment for maps and should be grouped with other schema utility functions in a SchemaUtils class."}],"llm_response_time":8751},{"iteration_num":2,"suggested_move_methods":[{"method_name":"parseString","method_signature":"public static SchemaAndValue parseString(String value)","target_class":"Parser","rationale":"The parseString method is responsible for parsing string representations of values, which aligns with the functionality of the Parser class. Moving it there would enhance cohesion."},{"method_name":"inferSchema","method_signature":"public static Schema inferSchema(Object value)","target_class":"SchemaDetector","rationale":"The inferSchema method is used to determine the schema of a given value, which is closely related to the functionality of the SchemaDetector class. Moving it there would improve the organization of schema-related logic."},{"method_name":"dateFormatFor","method_signature":"public static DateFormat dateFormatFor(java.util.Date value)","target_class":"DateUtils","rationale":"The dateFormatFor method is focused on formatting dates, which is a utility function that would be more appropriately placed in a dedicated DateUtils class."},{"method_name":"escape","method_signature":"protected static String escape(String value)","target_class":"StringUtils","rationale":"The escape method is a utility function for escaping strings, which would fit well in a StringUtils class, enhancing the separation of concerns."},{"method_name":"append","method_signature":"protected static void append(StringBuilder sb, Object value, boolean embedded)","target_class":"StringUtils","rationale":"The append method is responsible for appending various types of values to a StringBuilder, which is a string manipulation utility and should be part of a StringUtils class."},{"method_name":"appendIterable","method_signature":"protected static void appendIterable(StringBuilder sb, Iterator\u003c?\u003e iter)","target_class":"StringUtils","rationale":"The appendIterable method is a utility for appending iterables to a StringBuilder, which aligns with string manipulation tasks and should be moved to a StringUtils class."}],"llm_response_time":6923},{"iteration_num":3,"suggested_move_methods":[{"method_name":"parseString","method_signature":"public static SchemaAndValue parseString(String value)","target_class":"Parser","rationale":"The parseString method is responsible for parsing string representations into schemas and values, which aligns with the functionality of the Parser class."},{"method_name":"inferSchema","method_signature":"public static Schema inferSchema(Object value)","target_class":"SchemaDetector","rationale":"The inferSchema method determines the schema based on the value type, which is a core responsibility of the SchemaDetector class."},{"method_name":"append","method_signature":"protected static void append(StringBuilder sb, Object value, boolean embedded)","target_class":"Utils","rationale":"The append method is a utility function for appending various types of values to a StringBuilder, which fits better in a utility class like Utils."},{"method_name":"dateFormatFor","method_signature":"public static DateFormat dateFormatFor(java.util.Date value)","target_class":"DateUtils","rationale":"The dateFormatFor method is focused on formatting dates, which is a utility function that would be more appropriate in a dedicated DateUtils class."}],"llm_response_time":6725},{"iteration_num":-1,"suggested_move_methods":[{"method_name":"convertToBoolean","method_signature":"public static convertToBoolean(Schema schema, Object value)","target_class":"","rationale":""},{"method_name":"convertToByte","method_signature":"public static convertToByte(Schema schema, Object value)","target_class":"","rationale":""},{"method_name":"convertToShort","method_signature":"public static convertToShort(Schema schema, Object value)","target_class":"","rationale":""},{"method_name":"convertToInteger","method_signature":"public static convertToInteger(Schema schema, Object value)","target_class":"","rationale":""},{"method_name":"convertToLong","method_signature":"public static convertToLong(Schema schema, Object value)","target_class":"","rationale":""},{"method_name":"convertToFloat","method_signature":"public static convertToFloat(Schema schema, Object value)","target_class":"","rationale":""},{"method_name":"convertToDouble","method_signature":"public static convertToDouble(Schema schema, Object value)","target_class":"","rationale":""},{"method_name":"convertToString","method_signature":"public static convertToString(Schema schema, Object value)","target_class":"","rationale":""},{"method_name":"convertToList","method_signature":"public static convertToList(Schema schema, Object value)","target_class":"","rationale":""},{"method_name":"convertToMap","method_signature":"public static convertToMap(Schema schema, Object value)","target_class":"","rationale":""},{"method_name":"convertToStruct","method_signature":"public static convertToStruct(Schema schema, Object value)","target_class":"","rationale":""},{"method_name":"convertToTime","method_signature":"public static convertToTime(Schema schema, Object value)","target_class":"","rationale":""},{"method_name":"convertToDate","method_signature":"public static convertToDate(Schema schema, Object value)","target_class":"","rationale":""},{"method_name":"convertToTimestamp","method_signature":"public static convertToTimestamp(Schema schema, Object value)","target_class":"","rationale":""},{"method_name":"convertToDecimal","method_signature":"public static convertToDecimal(Schema schema, Object value, int scale)","target_class":"","rationale":""},{"method_name":"inferSchema","method_signature":"public static inferSchema(Object value)","target_class":"","rationale":""},{"method_name":"parseString","method_signature":"public static parseString(String value)","target_class":"","rationale":""},{"method_name":"convertTo","method_signature":"protected static convertTo(Schema toSchema, Schema fromSchema, Object value)","target_class":"","rationale":""},{"method_name":"asLong","method_signature":"protected static asLong(Object value, Schema fromSchema, Throwable error)","target_class":"","rationale":""},{"method_name":"asDouble","method_signature":"protected static asDouble(Object value, Schema schema, Throwable error)","target_class":"","rationale":""},{"method_name":"append","method_signature":"protected static append(StringBuilder sb, Object value, boolean embedded)","target_class":"","rationale":""},{"method_name":"appendIterable","method_signature":"protected static appendIterable(StringBuilder sb, Iterator\u003c?\u003e iter)","target_class":"","rationale":""},{"method_name":"escape","method_signature":"protected static escape(String value)","target_class":"","rationale":""},{"method_name":"dateFormatFor","method_signature":"public static dateFormatFor(java.util.Date value)","target_class":"","rationale":""},{"method_name":"canParseSingleTokenLiteral","method_signature":"protected static canParseSingleTokenLiteral(Parser parser, boolean embedded, String tokenLiteral)","target_class":"","rationale":""},{"method_name":"parse","method_signature":"protected static parse(Parser parser, boolean embedded)","target_class":"","rationale":""},{"method_name":"parseAsTemporal","method_signature":"private static parseAsTemporal(String token)","target_class":"","rationale":""},{"method_name":"commonSchemaFor","method_signature":"protected static commonSchemaFor(Schema previous, SchemaAndValue latest)","target_class":"","rationale":""},{"method_name":"alignListEntriesWithSchema","method_signature":"protected static alignListEntriesWithSchema(Schema schema, List\u003cObject\u003e input)","target_class":"","rationale":""},{"method_name":"alignMapKeysAndValuesWithSchema","method_signature":"protected static alignMapKeysAndValuesWithSchema(Schema mapSchema, Map\u003cObject, Object\u003e input)","target_class":"","rationale":""},{"method_name":"alignMapKeysWithSchema","method_signature":"protected static alignMapKeysWithSchema(Schema mapSchema, Map\u003cObject, Object\u003e input)","target_class":"","rationale":""},{"method_name":"canDetect","method_signature":"public canDetect(Object value)","target_class":"","rationale":""},{"method_name":"schema","method_signature":"public schema()","target_class":"","rationale":""},{"method_name":"position","method_signature":"public position()","target_class":"","rationale":""},{"method_name":"mark","method_signature":"public mark()","target_class":"","rationale":""},{"method_name":"rewindTo","method_signature":"public rewindTo(int position)","target_class":"","rationale":""},{"method_name":"hasNext","method_signature":"public hasNext()","target_class":"","rationale":""},{"method_name":"canConsumeNextToken","method_signature":"protected canConsumeNextToken()","target_class":"","rationale":""},{"method_name":"next","method_signature":"public next()","target_class":"","rationale":""},{"method_name":"next","method_signature":"public next(int n)","target_class":"","rationale":""},{"method_name":"consumeNextToken","method_signature":"private consumeNextToken()","target_class":"","rationale":""},{"method_name":"canConsume","method_signature":"public canConsume(String expected)","target_class":"","rationale":""},{"method_name":"canConsume","method_signature":"public canConsume(String expected, boolean ignoreLeadingAndTrailingWhitespace)","target_class":"","rationale":""},{"method_name":"isNext","method_signature":"protected isNext(String expected, boolean ignoreLeadingAndTrailingWhitespace)","target_class":"","rationale":""}],"llm_response_time":0},{"iteration_num":-2,"suggested_move_methods":[{"method_name":"position","method_signature":"public position()","target_class":"","rationale":""},{"method_name":"canConsumeNextToken","method_signature":"protected canConsumeNextToken()","target_class":"","rationale":""},{"method_name":"rewindTo","method_signature":"public rewindTo(int position)","target_class":"","rationale":""},{"method_name":"mark","method_signature":"public mark()","target_class":"","rationale":""},{"method_name":"next","method_signature":"public next(int n)","target_class":"","rationale":""},{"method_name":"canConsume","method_signature":"public canConsume(String expected)","target_class":"","rationale":""},{"method_name":"canConsume","method_signature":"public canConsume(String expected, boolean ignoreLeadingAndTrailingWhitespace)","target_class":"","rationale":""},{"method_name":"appendIterable","method_signature":"protected static appendIterable(StringBuilder sb, Iterator\u003c?\u003e iter)","target_class":"","rationale":""},{"method_name":"hasNext","method_signature":"public hasNext()","target_class":"","rationale":""},{"method_name":"next","method_signature":"public next()","target_class":"","rationale":""},{"method_name":"isNext","method_signature":"protected isNext(String expected, boolean ignoreLeadingAndTrailingWhitespace)","target_class":"","rationale":""},{"method_name":"escape","method_signature":"protected static escape(String value)","target_class":"","rationale":""},{"method_name":"consumeNextToken","method_signature":"private consumeNextToken()","target_class":"","rationale":""},{"method_name":"alignMapKeysWithSchema","method_signature":"protected static alignMapKeysWithSchema(Schema mapSchema, Map\u003cObject, Object\u003e input)","target_class":"","rationale":""},{"method_name":"canParseSingleTokenLiteral","method_signature":"protected static canParseSingleTokenLiteral(Parser parser, boolean embedded, String tokenLiteral)","target_class":"","rationale":""}],"llm_response_time":0}],"methodCompatibilityScores":{"public position()":{"first":{"method_name":"position","method_signature":"public position()","target_class":"","rationale":""},"second":0.06928953738726655},"protected canConsumeNextToken()":{"first":{"method_name":"canConsumeNextToken","method_signature":"protected canConsumeNextToken()","target_class":"","rationale":""},"second":0.08649203878161088},"public rewindTo(int position)":{"first":{"method_name":"rewindTo","method_signature":"public rewindTo(int position)","target_class":"","rationale":""},"second":0.14617584821797563},"public mark()":{"first":{"method_name":"mark","method_signature":"public mark()","target_class":"","rationale":""},"second":0.14791187634595818},"public next(int n)":{"first":{"method_name":"next","method_signature":"public next(int n)","target_class":"","rationale":""},"second":0.17369007885866095},"public canConsume(String expected)":{"first":{"method_name":"canConsume","method_signature":"public canConsume(String expected)","target_class":"","rationale":""},"second":0.18634904930883728},"public canConsume(String expected, boolean ignoreLeadingAndTrailingWhitespace)":{"first":{"method_name":"canConsume","method_signature":"public canConsume(String expected, boolean ignoreLeadingAndTrailingWhitespace)","target_class":"","rationale":""},"second":0.21504492566602065},"protected static appendIterable(StringBuilder sb, Iterator\u003c?\u003e iter)":{"first":{"method_name":"appendIterable","method_signature":"protected static appendIterable(StringBuilder sb, Iterator\u003c?\u003e iter)","target_class":"","rationale":""},"second":0.21703938998267572},"public hasNext()":{"first":{"method_name":"hasNext","method_signature":"public hasNext()","target_class":"","rationale":""},"second":0.2210631942576057},"public next()":{"first":{"method_name":"next","method_signature":"public next()","target_class":"","rationale":""},"second":0.25797304785680947},"protected isNext(String expected, boolean ignoreLeadingAndTrailingWhitespace)":{"first":{"method_name":"isNext","method_signature":"protected isNext(String expected, boolean ignoreLeadingAndTrailingWhitespace)","target_class":"","rationale":""},"second":0.26094873516898154},"protected static escape(String value)":{"first":{"method_name":"escape","method_signature":"protected static escape(String value)","target_class":"","rationale":""},"second":0.28065245258570803},"private consumeNextToken()":{"first":{"method_name":"consumeNextToken","method_signature":"private consumeNextToken()","target_class":"","rationale":""},"second":0.31069943609930306},"protected static alignMapKeysWithSchema(Schema mapSchema, Map\u003cObject, Object\u003e input)":{"first":{"method_name":"alignMapKeysWithSchema","method_signature":"protected static alignMapKeysWithSchema(Schema mapSchema, Map\u003cObject, Object\u003e input)","target_class":"","rationale":""},"second":0.393441736486571},"protected static canParseSingleTokenLiteral(Parser parser, boolean embedded, String tokenLiteral)":{"first":{"method_name":"canParseSingleTokenLiteral","method_signature":"protected static canParseSingleTokenLiteral(Parser parser, boolean embedded, String tokenLiteral)","target_class":"","rationale":""},"second":0.4115110701400712}},"llmMethodPriority":{"priority_method_names":["position","canConsumeNextToken","rewindTo","mark","next","canConsume","canConsume","appendIterable","hasNext","next","isNext","escape","consumeNextToken","alignMapKeysWithSchema","canParseSingleTokenLiteral"],"llm_response_time":2873},"targetClassMap":{"position":{"target_classes":[],"target_classes_sorted_by_llm":[],"llm_response_time":381,"similarity_computation_time":0,"similarity_metric":"tfidf"},"canConsumeNextToken":{"target_classes":[],"target_classes_sorted_by_llm":[],"llm_response_time":373,"similarity_computation_time":0,"similarity_metric":"tfidf"},"rewindTo":{"target_classes":[],"target_classes_sorted_by_llm":[],"llm_response_time":400,"similarity_computation_time":0,"similarity_metric":"tfidf"}}}
{"id":"110bcbf9-4b48-4bac-a7cb-a6adab6e082c","methodCount":3,"hostFunctionTelemetryData":{"hostFunctionSize":39,"lineStart":109,"lineEnd":147,"bodyLineStart":109,"language":"java","filePath":"/Users/fraol/Documents/MM-assist-dataset/kafka/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/RestClientTest.java","sourceCode":"@RunWith(Parameterized.class)\n    public static class RequestFailureParameterizedTest {\n\n        @Rule\n        public MockitoRule initRule \u003d MockitoJUnit.rule().strictness(Strictness.STRICT_STUBS);\n\n        @Mock\n        private HttpClient httpClient;\n\n        @Parameterized.Parameter\n        public Throwable requestException;\n        \n        @Parameterized.Parameters\n        public static Collection\u003cObject[]\u003e requestExceptions() {\n            return Arrays.asList(new Object[][]{\n                    {new InterruptedException()},\n                    {new ExecutionException(null)},\n                    {new TimeoutException()}\n            });\n        }\n\n        private static Request buildThrowingMockRequest(Throwable t) throws ExecutionException, InterruptedException, TimeoutException {\n            Request req \u003d mock(Request.class);\n            when(req.header(anyString(), anyString())).thenReturn(req);\n            when(req.send()).thenThrow(t);\n            return req;\n        }\n\n        @Test\n        public void testFailureDuringRequestCausesInternalServerError() throws Exception {\n            Request request \u003d buildThrowingMockRequest(requestException);\n            when(httpClient.newRequest(anyString())).thenReturn(request);\n            ConnectRestException e \u003d assertThrows(ConnectRestException.class, () -\u003e httpRequest(\n                    httpClient, MOCK_URL, TEST_METHOD, TEST_TYPE, TEST_SIGNATURE_ALGORITHM\n            ));\n            assertIsInternalServerError(e);\n            assertEquals(requestException, e.getCause());\n        }\n    }","methodCount":3},"candidatesTelemetryData":{"numberOfSuggestions":6,"candidates":[{"lineStart":120,"lineEnd":127,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method requestExceptions to class ClientMetricsTestUtils","description":"move method requestExceptions to PsiClass:ClientMetricsTestUtils\nRationale: The requestExceptions() method generates test data for exceptions, which is closely related to testing metrics and client interactions. Moving it to ClientMetricsTestUtils aligns with the Single Responsibility Principle by consolidating testing utilities in one place, enhancing cohesion. This move would improve the organization of test-related utilities, but care must be taken to ensure that it does not clutter the utility class with unrelated methods.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":120,"lineEnd":127,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method requestExceptions to class ControllerMetricsTestUtils","description":"move method requestExceptions to PsiClass:ControllerMetricsTestUtils\nRationale: Similar to ClientMetricsTestUtils, ControllerMetricsTestUtils is focused on testing metrics and behaviors related to controllers. The requestExceptions() method can be seen as a utility for generating exceptions during testing, which fits well within this context. This move would enhance the separation of concerns, but it may require careful documentation to clarify its purpose within the testing framework.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":120,"lineEnd":127,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method requestExceptions to class RequestUtils","description":"move method requestExceptions to PsiClass:RequestUtils\nRationale: RequestUtils deals with various request-related functionalities, and generating exceptions for requests can be considered part of this utility. Moving requestExceptions() here would centralize request-related utilities, adhering to the Single Responsibility Principle. However, it may dilute the focus of RequestUtils if it becomes too broad.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":129,"lineEnd":134,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method buildThrowingMockRequest to class RestClientTest","description":"move method buildThrowingMockRequest to PsiClass:RestClientTest\nRationale: The buildThrowingMockRequest() method is designed to create mock requests for testing purposes, which aligns closely with the responsibilities of RestClientTest. Moving it here adheres to the Single Responsibility Principle, as it centralizes mock request creation within the context of REST client testing. This enhances cohesion and makes the method more reusable in various test scenarios. However, it may increase the size of RestClientTest if not managed properly.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":129,"lineEnd":134,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method buildThrowingMockRequest to class InternalRequestSignatureTest","description":"move method buildThrowingMockRequest to PsiClass:InternalRequestSignatureTest\nRationale: This class is focused on testing internal request signatures, which may involve creating mock requests. The buildThrowingMockRequest() method could be beneficial for simulating error scenarios in signature validation tests. This move aligns with the Open/Closed Principle, as it allows InternalRequestSignatureTest to extend its capabilities without modifying existing code. A potential drawback is that it may introduce dependencies on mocking frameworks that could clutter the test logic.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":129,"lineEnd":134,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method buildThrowingMockRequest to class TestSslUtils","description":"move method buildThrowingMockRequest to PsiClass:TestSslUtils\nRationale: TestSslUtils deals with SSL-related testing utilities, which may require mock requests for testing secure connections. The buildThrowingMockRequest() method can be useful for simulating failures in SSL contexts. This move supports the Interface Segregation Principle by allowing TestSslUtils to focus on SSL-specific testing utilities. However, it may dilute the class\u0027s focus if not carefully integrated.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false}]},"iterationData":[{"iteration_num":1,"suggested_move_methods":[{"method_name":"buildThrowingMockRequest","method_signature":"private static Request buildThrowingMockRequest(Throwable t) throws ExecutionException, InterruptedException, TimeoutException","target_class":"RequestTestUtils","rationale":"The method buildThrowingMockRequest() is a utility function for creating mock requests that throw exceptions. Moving it to a separate utility class like RequestTestUtils would enhance code organization and reusability across different test classes."}],"llm_response_time":1926},{"iteration_num":2,"suggested_move_methods":[{"method_name":"buildThrowingMockRequest","method_signature":"private static Request buildThrowingMockRequest(Throwable t) throws ExecutionException, InterruptedException, TimeoutException","target_class":"RequestTestUtils","rationale":"The method buildThrowingMockRequest() is a utility method for creating mock requests that throw exceptions. It should be moved to a separate utility class (e.g., RequestTestUtils) to promote code reuse and separation of concerns."}],"llm_response_time":1763},{"iteration_num":3,"suggested_move_methods":[{"method_name":"buildThrowingMockRequest","method_signature":"private static Request buildThrowingMockRequest(Throwable t) throws ExecutionException, InterruptedException, TimeoutException","target_class":"RequestTestUtils","rationale":"The method buildThrowingMockRequest() is a utility method for creating mock requests that throw exceptions. It does not belong to the test class itself but rather to a utility class that can be reused across different tests."}],"llm_response_time":1614},{"iteration_num":-1,"suggested_move_methods":[{"method_name":"requestExceptions","method_signature":"@Parameterized.Parameters\n        public static requestExceptions()","target_class":"","rationale":""},{"method_name":"buildThrowingMockRequest","method_signature":"private static buildThrowingMockRequest(Throwable t)","target_class":"","rationale":""}],"llm_response_time":0},{"iteration_num":-2,"suggested_move_methods":[{"method_name":"requestExceptions","method_signature":"@Parameterized.Parameters\n        public static requestExceptions()","target_class":"","rationale":""},{"method_name":"buildThrowingMockRequest","method_signature":"private static buildThrowingMockRequest(Throwable t)","target_class":"","rationale":""}],"llm_response_time":0}],"methodCompatibilityScores":{"@Parameterized.Parameters\n        public static requestExceptions()":{"first":{"method_name":"requestExceptions","method_signature":"@Parameterized.Parameters\n        public static requestExceptions()","target_class":"","rationale":""},"second":0.4331199495949317},"private static buildThrowingMockRequest(Throwable t)":{"first":{"method_name":"buildThrowingMockRequest","method_signature":"private static buildThrowingMockRequest(Throwable t)","target_class":"","rationale":""},"second":0.5264116895511091}},"llmMethodPriority":{"priority_method_names":["requestExceptions","buildThrowingMockRequest"],"llm_response_time":936},"targetClassMap":{"requestExceptions":{"target_classes":[{"class_name":"RestClientTest","similarity_score":0.289266402408709},{"class_name":"SSLUtils","similarity_score":0.1895590242079552},{"class_name":"WorkerTestUtils","similarity_score":0.24692057669693723},{"class_name":"PluginUtils","similarity_score":0.23116796322380606},{"class_name":"RetryUtil","similarity_score":0.13745713485327285},{"class_name":"TestUtils","similarity_score":0.16974745388928425},{"class_name":"SinkUtils","similarity_score":0.19381748474093957},{"class_name":"SchemaUtil","similarity_score":0.31719576756473444},{"class_name":"ConcurrencyUtils","similarity_score":0.12832838666786217},{"class_name":"ConnectorUtils","similarity_score":0.09052544090947708},{"class_name":"ConnectUtils","similarity_score":0.17596243769573466},{"class_name":"MirrorUtils","similarity_score":0.20064505326831172},{"class_name":"MockitoUtils","similarity_score":0.08063808629864941},{"class_name":"OffsetUtils","similarity_score":0.1354502079141182},{"class_name":"RemoteClusterUtils","similarity_score":0.22199079402972974},{"class_name":"AuthorizerUtils","similarity_score":0.39909573231462303},{"class_name":"ControllerMetricsTestUtils","similarity_score":0.2933651867845827},{"class_name":"ControllerRequestContextUtil","similarity_score":0.36992354344781053},{"class_name":"ControlRecordUtils","similarity_score":0.3728729238375568},{"class_name":"SystemTestUtil","similarity_score":0.1511790402900343},{"class_name":"ApiUtils","similarity_score":0.24230718024741293},{"class_name":"FutureUtils","similarity_score":0.13701116417546072},{"class_name":"RequestTestUtils","similarity_score":0.2367246286672597},{"class_name":"RequestUtils","similarity_score":0.30159285836566985},{"class_name":"IntegrationTestUtils","similarity_score":0.2130413416792438},{"class_name":"FetchUtils","similarity_score":0.04508602731264772},{"class_name":"TaskAssignmentUtils","similarity_score":0.18533518132521376},{"class_name":"TaskAssignmentUtilsTest","similarity_score":0.14292422563920232},{"class_name":"InternalConfig","similarity_score":0.3284358621940117},{"class_name":"TaskBuilder","similarity_score":0.3192096537552621},{"class_name":"InternalQueryResultUtil","similarity_score":0.20608974540621472},{"class_name":"InternalRequestSignature","similarity_score":0.24712819443801454},{"class_name":"InternalRequestSignatureTest","similarity_score":0.21569091171488114},{"class_name":"AssignmentTestUtil","similarity_score":0.26868256351012515},{"class_name":"AssignmentTestUtils","similarity_score":0.2304263786930126},{"class_name":"AssignorBenchmarkUtils","similarity_score":0.1979131233319198},{"class_name":"ClaimValidationUtils","similarity_score":0.06853028307062774},{"class_name":"AdminClientTestUtils","similarity_score":0.22448762138853914},{"class_name":"AdminUtils","similarity_score":0.0850072076850968},{"class_name":"RestClient","similarity_score":0.17665147576828188},{"class_name":"TestCloseable","similarity_score":0.21709653634194542},{"class_name":"CsvUtils","similarity_score":0.33881720949396205},{"class_name":"ByteUtils","similarity_score":0.1634502743822472},{"class_name":"ByteUtilsBenchmark","similarity_score":0.25654882047912597},{"class_name":"Tests","similarity_score":0.17075957849167392},{"class_name":"TestSslUtils","similarity_score":0.2376752268221882},{"class_name":"TestUtil","similarity_score":0.2573957808351148},{"class_name":"GraphGraceSearchUtil","similarity_score":0.24553683128543735},{"class_name":"ClientMetricsTestUtils","similarity_score":0.3123257596742155}],"target_classes_sorted_by_llm":["ClientMetricsTestUtils","ControllerMetricsTestUtils","RequestUtils","TaskBuilder","InternalConfig","CsvUtils","SchemaUtil","ControlRecordUtils","AuthorizerUtils","ControllerRequestContextUtil"],"llm_response_time":13427,"similarity_computation_time":103,"similarity_metric":"tfidf"},"buildThrowingMockRequest":{"target_classes":[{"class_name":"RestClientTest","similarity_score":0.4447802580066538},{"class_name":"SSLUtils","similarity_score":0.25527421250882987},{"class_name":"PluginUtils","similarity_score":0.2875754670956345},{"class_name":"WorkerTestUtils","similarity_score":0.20470544319294587},{"class_name":"SinkUtils","similarity_score":0.2422120283277993},{"class_name":"RetryUtil","similarity_score":0.18739525994838171},{"class_name":"TestUtils","similarity_score":0.23341161070712593},{"class_name":"SchemaUtil","similarity_score":0.32432465244948644},{"class_name":"ConnectorUtils","similarity_score":0.1388399617665651},{"class_name":"ConnectUtils","similarity_score":0.16855395554646643},{"class_name":"ConcurrencyUtils","similarity_score":0.10805737826496586},{"class_name":"RemoteClusterUtils","similarity_score":0.32852364678849993},{"class_name":"OffsetUtils","similarity_score":0.1530727721620895},{"class_name":"MirrorUtils","similarity_score":0.23517710278144396},{"class_name":"MockitoUtils","similarity_score":0.06976572680885752},{"class_name":"ControllerMetricsTestUtils","similarity_score":0.3352477197892781},{"class_name":"ControllerRequestContextUtil","similarity_score":0.28654353798196575},{"class_name":"ControlRecordUtils","similarity_score":0.3532198181153885},{"class_name":"StateManagerUtil","similarity_score":0.20790318585778775},{"class_name":"SystemTestUtil","similarity_score":0.20288197955774712},{"class_name":"FutureUtils","similarity_score":0.15282594678438124},{"class_name":"ExceptionUtils","similarity_score":0.308287726505694},{"class_name":"RequestTestUtils","similarity_score":0.22919300341873178},{"class_name":"RequestUtils","similarity_score":0.3721719838017242},{"class_name":"IntegrationTestUtils","similarity_score":0.24880512368141164},{"class_name":"FetchUtils","similarity_score":0.05378254348272378},{"class_name":"InternalConfig","similarity_score":0.28784345572855},{"class_name":"TaskAssignmentUtils","similarity_score":0.2687129800399253},{"class_name":"TaskAssignmentUtilsTest","similarity_score":0.20319919458084573},{"class_name":"TaskBuilder","similarity_score":0.3263838001607887},{"class_name":"InternalQueryResultUtil","similarity_score":0.16209350304978645},{"class_name":"InternalRequestSignature","similarity_score":0.3237492359474084},{"class_name":"InternalRequestSignatureTest","similarity_score":0.3657711769715194},{"class_name":"StoreQueryUtils","similarity_score":0.33108178329918997},{"class_name":"ClaimValidationUtils","similarity_score":0.07831406782704563},{"class_name":"AuthorizerUtils","similarity_score":0.3200512122912777},{"class_name":"StreamsConfigUtils","similarity_score":0.32091420381105534},{"class_name":"RestClient","similarity_score":0.3035981220833518},{"class_name":"SmokeTestUtil","similarity_score":0.3551034153472527},{"class_name":"TestCloseable","similarity_score":0.3551611510464143},{"class_name":"StreamsTestUtils","similarity_score":0.34768346934492433},{"class_name":"StreamStreamJoinUtil","similarity_score":0.17498008634707726},{"class_name":"CsvUtils","similarity_score":0.36589450375591576},{"class_name":"Tests","similarity_score":0.3071620073956433},{"class_name":"TestSslUtils","similarity_score":0.3522630440190927},{"class_name":"TestUtil","similarity_score":0.19738550848793068},{"class_name":"GraphGraceSearchUtil","similarity_score":0.32200558505538823},{"class_name":"ThreadUtils","similarity_score":0.22297064625396115},{"class_name":"ClientMetricsTestUtils","similarity_score":0.2919727474407247}],"target_classes_sorted_by_llm":["RestClientTest","InternalRequestSignatureTest","TestSslUtils","SmokeTestUtil","RequestUtils","ControlRecordUtils","CsvUtils","StreamsTestUtils","TestCloseable","ControllerMetricsTestUtils"],"llm_response_time":12493,"similarity_computation_time":35,"similarity_metric":"tfidf"}}}
{"id":"20dd2306-e0fe-4d33-818f-bce8cc9d12bc","methodCount":51,"hostFunctionTelemetryData":{"hostFunctionSize":1976,"lineStart":59,"lineEnd":2034,"bodyLineStart":59,"language":"java","filePath":"/Users/fraol/Documents/MM-assist-dataset/kafka/raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientSnapshotTest.java","sourceCode":"public final class KafkaRaftClientSnapshotTest {\n    @Test\n    public void testLatestSnapshotId() throws Exception {\n        int localId \u003d 0;\n        int leaderId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, leaderId);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(3, 1);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .withEmptySnapshot(snapshotId)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        assertEquals(Optional.of(snapshotId), context.client.latestSnapshotId());\n    }\n\n    @Test\n    public void testLatestSnapshotIdMissing() throws Exception {\n        int localId \u003d 0;\n        int leaderId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, leaderId);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(3, 1);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        assertEquals(Optional.empty(), context.client.latestSnapshotId());\n    }\n\n    @ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public void testLeaderListenerNotified(boolean entireLog) throws Exception {\n        int localId \u003d 0;\n        int otherNodeId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, otherNodeId);\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(3, 1);\n\n        RaftClientTestContext.Builder contextBuilder \u003d new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .withEmptySnapshot(snapshotId);\n\n        if (!entireLog) {\n            contextBuilder.deleteBeforeSnapshot(snapshotId);\n        }\n\n        RaftClientTestContext context \u003d contextBuilder.build();\n\n        context.becomeLeader();\n        int epoch \u003d context.currentEpoch();\n\n        // Advance the highWatermark\n        long localLogEndOffset \u003d context.log.endOffset().offset;\n        context.deliverRequest(context.fetchRequest(epoch, otherNodeId, localLogEndOffset, epoch, 0));\n        context.pollUntilResponse();\n        context.assertSentFetchPartitionResponse(Errors.NONE, epoch, OptionalInt.of(localId));\n        assertEquals(localLogEndOffset, context.client.highWatermark().getAsLong());\n\n        // Check that listener was notified of the new snapshot\n        try (SnapshotReader\u003cString\u003e snapshot \u003d context.listener.drainHandledSnapshot().get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            SnapshotWriterReaderTest.assertSnapshot(Collections.emptyList(), snapshot);\n        }\n    }\n\n    @ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public void testFollowerListenerNotified(boolean entireLog) throws Exception {\n        int localId \u003d 0;\n        int leaderId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, leaderId);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(3, 1);\n\n        RaftClientTestContext.Builder contextBuilder \u003d new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .withEmptySnapshot(snapshotId)\n            .withElectedLeader(epoch, leaderId);\n\n        if (!entireLog) {\n            contextBuilder.deleteBeforeSnapshot(snapshotId);\n        }\n\n        RaftClientTestContext context \u003d contextBuilder.build();\n\n        // Advance the highWatermark\n        long localLogEndOffset \u003d context.log.endOffset().offset;\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, localLogEndOffset, snapshotId.epoch());\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            context.fetchResponse(epoch, leaderId, MemoryRecords.EMPTY, localLogEndOffset, Errors.NONE)\n        );\n\n        context.pollUntilRequest();\n        context.assertSentFetchRequest(epoch, localLogEndOffset, snapshotId.epoch());\n\n        // Check that listener was notified of the new snapshot\n        try (SnapshotReader\u003cString\u003e snapshot \u003d context.listener.drainHandledSnapshot().get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            SnapshotWriterReaderTest.assertSnapshot(Collections.emptyList(), snapshot);\n        }\n    }\n\n    @ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public void testSecondListenerNotified(boolean entireLog) throws Exception {\n        int localId \u003d 0;\n        int leaderId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, leaderId);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(3, 1);\n\n        RaftClientTestContext.Builder contextBuilder \u003d new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .withEmptySnapshot(snapshotId)\n            .withElectedLeader(epoch, leaderId);\n\n        if (!entireLog) {\n            contextBuilder.deleteBeforeSnapshot(snapshotId);\n        }\n\n        RaftClientTestContext context \u003d contextBuilder.build();\n\n        // Advance the highWatermark\n        long localLogEndOffset \u003d context.log.endOffset().offset;\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, localLogEndOffset, snapshotId.epoch());\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            context.fetchResponse(epoch, leaderId, MemoryRecords.EMPTY, localLogEndOffset, Errors.NONE)\n        );\n\n        context.pollUntilRequest();\n        context.assertSentFetchRequest(epoch, localLogEndOffset, snapshotId.epoch());\n\n        RaftClientTestContext.MockListener secondListener \u003d new RaftClientTestContext.MockListener(OptionalInt.of(localId));\n        context.client.register(secondListener);\n        context.client.poll();\n\n        // Check that the second listener was notified of the new snapshot\n        try (SnapshotReader\u003cString\u003e snapshot \u003d secondListener.drainHandledSnapshot().get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            SnapshotWriterReaderTest.assertSnapshot(Collections.emptyList(), snapshot);\n        }\n    }\n\n    @Test\n    public void testListenerRenotified() throws Exception {\n        int localId \u003d 0;\n        int otherNodeId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, otherNodeId);\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(3, 1);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"g\", \"h\", \"i\"))\n            .withEmptySnapshot(snapshotId)\n            .deleteBeforeSnapshot(snapshotId)\n            .build();\n\n        context.becomeLeader();\n        int epoch \u003d context.currentEpoch();\n\n        // Stop the listener from reading commit batches\n        context.listener.updateReadCommit(false);\n\n        // Advance the highWatermark\n        long localLogEndOffset \u003d context.log.endOffset().offset;\n        context.deliverRequest(context.fetchRequest(epoch, otherNodeId, localLogEndOffset, epoch, 0));\n        context.pollUntilResponse();\n        context.assertSentFetchPartitionResponse(Errors.NONE, epoch, OptionalInt.of(localId));\n        assertEquals(localLogEndOffset, context.client.highWatermark().getAsLong());\n\n        // Check that listener was notified of the new snapshot\n        try (SnapshotReader\u003cString\u003e snapshot \u003d context.listener.drainHandledSnapshot().get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            SnapshotWriterReaderTest.assertSnapshot(Collections.emptyList(), snapshot);\n        }\n\n        // Generate a new snapshot\n        OffsetAndEpoch secondSnapshotId \u003d new OffsetAndEpoch(localLogEndOffset, epoch);\n        try (SnapshotWriter\u003cString\u003e snapshot \u003d context.client.createSnapshot(secondSnapshotId, 0).get()) {\n            assertEquals(secondSnapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n        context.log.deleteBeforeSnapshot(secondSnapshotId);\n        context.client.poll();\n\n        // Resume the listener from reading commit batches\n        context.listener.updateReadCommit(true);\n\n        context.client.poll();\n        // Check that listener was notified of the second snapshot\n        try (SnapshotReader\u003cString\u003e snapshot \u003d context.listener.drainHandledSnapshot().get()) {\n            assertEquals(secondSnapshotId, snapshot.snapshotId());\n            SnapshotWriterReaderTest.assertSnapshot(Collections.emptyList(), snapshot);\n        }\n    }\n\n    @Test\n    public void testLeaderImmediatelySendsSnapshotId() throws Exception {\n        int localId \u003d 0;\n        int otherNodeId \u003d 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, otherNodeId);\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(3, 4);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .withUnknownLeader(snapshotId.epoch())\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"g\", \"h\", \"i\"))\n            .withEmptySnapshot(snapshotId)\n            .deleteBeforeSnapshot(snapshotId)\n            .build();\n\n        context.becomeLeader();\n        int epoch \u003d context.currentEpoch();\n\n        // Send a fetch request for an end offset and epoch which has been snapshotted\n        context.deliverRequest(context.fetchRequest(epoch, otherNodeId, 6, 2, 500));\n        context.client.poll();\n\n        // Expect that the leader replies immediately with a snapshot id\n        FetchResponseData.PartitionData partitionResponse \u003d context.assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(localId, partitionResponse.currentLeader().leaderId());\n        assertEquals(snapshotId.epoch(), partitionResponse.snapshotId().epoch());\n        assertEquals(snapshotId.offset(), partitionResponse.snapshotId().endOffset());\n    }\n\n    @Test\n    public void testFetchRequestOffsetLessThanLogStart() throws Exception {\n        int localId \u003d 0;\n        int otherNodeId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, otherNodeId);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .withAppendLingerMs(1)\n            .build();\n\n        context.becomeLeader();\n        int epoch \u003d context.currentEpoch();\n\n        List\u003cString\u003e appendRecords \u003d Arrays.asList(\"a\", \"b\", \"c\");\n        context.client.scheduleAppend(epoch, appendRecords);\n        context.time.sleep(context.appendLingerMs());\n        context.client.poll();\n\n        long localLogEndOffset \u003d context.log.endOffset().offset;\n        assertTrue(\n            appendRecords.size() \u003c\u003d localLogEndOffset,\n            String.format(\"Record length \u003d %s, log end offset \u003d %s\", appendRecords.size(), localLogEndOffset)\n        );\n\n        // Advance the highWatermark\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(localLogEndOffset, epoch);\n        try (SnapshotWriter\u003cString\u003e snapshot \u003d context.client.createSnapshot(snapshotId, 0).get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n        context.log.deleteBeforeSnapshot(snapshotId);\n        context.client.poll();\n\n        // Send Fetch request less than start offset\n        context.deliverRequest(context.fetchRequest(epoch, otherNodeId, snapshotId.offset() - 2, snapshotId.epoch(), 0));\n        context.pollUntilResponse();\n        FetchResponseData.PartitionData partitionResponse \u003d context.assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(localId, partitionResponse.currentLeader().leaderId());\n        assertEquals(snapshotId.epoch(), partitionResponse.snapshotId().epoch());\n        assertEquals(snapshotId.offset(), partitionResponse.snapshotId().endOffset());\n    }\n\n    @Test\n    public void testFetchRequestOffsetAtZero() throws Exception {\n        // When the follower sends a FETCH request at offset 0, reply with snapshot id if it exists\n        int localId \u003d 0;\n        int otherNodeId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, otherNodeId);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .withAppendLingerMs(1)\n            .build();\n\n        context.becomeLeader();\n        int epoch \u003d context.currentEpoch();\n\n        List\u003cString\u003e appendRecords \u003d Arrays.asList(\"a\", \"b\", \"c\");\n        context.client.scheduleAppend(epoch, appendRecords);\n        context.time.sleep(context.appendLingerMs());\n        context.client.poll();\n\n        long localLogEndOffset \u003d context.log.endOffset().offset;\n        assertTrue(\n            appendRecords.size() \u003c\u003d localLogEndOffset,\n            String.format(\"Record length \u003d %s, log end offset \u003d %s\", appendRecords.size(), localLogEndOffset)\n        );\n\n        // Advance the highWatermark\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        // Generate a snapshot at the LEO\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(localLogEndOffset, epoch);\n        try (SnapshotWriter\u003cString\u003e snapshot \u003d context.client.createSnapshot(snapshotId, 0).get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n\n        // Send Fetch request for offset 0\n        context.deliverRequest(context.fetchRequest(epoch, otherNodeId, 0, 0, 0));\n        context.pollUntilResponse();\n        FetchResponseData.PartitionData partitionResponse \u003d context.assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(localId, partitionResponse.currentLeader().leaderId());\n        assertEquals(snapshotId.epoch(), partitionResponse.snapshotId().epoch());\n        assertEquals(snapshotId.offset(), partitionResponse.snapshotId().endOffset());\n    }\n\n    @Test\n    public void testFetchRequestWithLargerLastFetchedEpoch() throws Exception {\n        int localId \u003d 0;\n        int otherNodeId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, otherNodeId);\n\n        OffsetAndEpoch oldestSnapshotId \u003d new OffsetAndEpoch(3, 2);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .withAppendLingerMs(1)\n            .build();\n\n        context.becomeLeader();\n        int epoch \u003d context.currentEpoch();\n        assertEquals(oldestSnapshotId.epoch() + 1, epoch);\n\n        // Advance the highWatermark\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        // Create a snapshot at the high watermark\n        try (SnapshotWriter\u003cString\u003e snapshot \u003d context.client.createSnapshot(oldestSnapshotId, 0).get()) {\n            assertEquals(oldestSnapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n        context.client.poll();\n\n        context.client.scheduleAppend(epoch, Arrays.asList(\"g\", \"h\", \"i\"));\n        context.time.sleep(context.appendLingerMs());\n        context.client.poll();\n\n        // It is an invalid request to send an last fetched epoch greater than the current epoch\n        context.deliverRequest(context.fetchRequest(epoch, otherNodeId, oldestSnapshotId.offset() + 1, epoch + 1, 0));\n        context.pollUntilResponse();\n        context.assertSentFetchPartitionResponse(Errors.INVALID_REQUEST, epoch, OptionalInt.of(localId));\n    }\n\n    @Test\n    public void testFetchRequestTruncateToLogStart() throws Exception {\n        int localId \u003d 0;\n        int otherNodeId \u003d localId + 1;\n        int syncNodeId \u003d otherNodeId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, otherNodeId, syncNodeId);\n\n        OffsetAndEpoch oldestSnapshotId \u003d new OffsetAndEpoch(3, 2);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(oldestSnapshotId.epoch() + 2, Arrays.asList(\"d\", \"e\", \"f\"))\n            .withAppendLingerMs(1)\n            .build();\n\n        context.becomeLeader();\n        int epoch \u003d context.currentEpoch();\n        assertEquals(oldestSnapshotId.epoch() + 2 + 1, epoch);\n\n        // Advance the highWatermark\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        // Create a snapshot at the high watermark\n        try (SnapshotWriter\u003cString\u003e snapshot \u003d context.client.createSnapshot(oldestSnapshotId, 0).get()) {\n            assertEquals(oldestSnapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n        context.client.poll();\n\n        // This should truncate to the old snapshot\n        context.deliverRequest(\n            context.fetchRequest(epoch, otherNodeId, oldestSnapshotId.offset() + 1, oldestSnapshotId.epoch() + 1, 0)\n        );\n        context.pollUntilResponse();\n        FetchResponseData.PartitionData partitionResponse \u003d context.assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(localId, partitionResponse.currentLeader().leaderId());\n        assertEquals(oldestSnapshotId.epoch(), partitionResponse.divergingEpoch().epoch());\n        assertEquals(oldestSnapshotId.offset(), partitionResponse.divergingEpoch().endOffset());\n    }\n\n    @Test\n    public void testFetchRequestAtLogStartOffsetWithValidEpoch() throws Exception {\n        int localId \u003d 0;\n        int otherNodeId \u003d localId + 1;\n        int syncNodeId \u003d otherNodeId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, otherNodeId, syncNodeId);\n\n        OffsetAndEpoch oldestSnapshotId \u003d new OffsetAndEpoch(3, 2);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .appendToLog(oldestSnapshotId.epoch() + 2, Arrays.asList(\"g\", \"h\", \"i\"))\n            .withAppendLingerMs(1)\n            .build();\n\n        context.becomeLeader();\n        int epoch \u003d context.currentEpoch();\n        assertEquals(oldestSnapshotId.epoch() + 2 + 1, epoch);\n\n        // Advance the highWatermark\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        // Create a snapshot at the high watermark\n        try (SnapshotWriter\u003cString\u003e snapshot \u003d context.client.createSnapshot(oldestSnapshotId, 0).get()) {\n            assertEquals(oldestSnapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n        context.client.poll();\n\n        // Send fetch request at log start offset with valid last fetched epoch\n        context.deliverRequest(\n            context.fetchRequest(epoch, otherNodeId, oldestSnapshotId.offset(), oldestSnapshotId.epoch(), 0)\n        );\n        context.pollUntilResponse();\n        context.assertSentFetchPartitionResponse(Errors.NONE, epoch, OptionalInt.of(localId));\n    }\n\n    @Test\n    public void testFetchRequestAtLogStartOffsetWithInvalidEpoch() throws Exception {\n        int localId \u003d 0;\n        int otherNodeId \u003d localId + 1;\n        int syncNodeId \u003d otherNodeId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, otherNodeId, syncNodeId);\n\n        OffsetAndEpoch oldestSnapshotId \u003d new OffsetAndEpoch(3, 2);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .appendToLog(oldestSnapshotId.epoch() + 2, Arrays.asList(\"g\", \"h\", \"i\"))\n            .withAppendLingerMs(1)\n            .build();\n\n        context.becomeLeader();\n        int epoch \u003d context.currentEpoch();\n        assertEquals(oldestSnapshotId.epoch() + 2 + 1, epoch);\n\n        // Advance the highWatermark\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        // Create a snapshot at the high watermark\n        try (SnapshotWriter\u003cString\u003e snapshot \u003d context.client.createSnapshot(oldestSnapshotId, 0).get()) {\n            assertEquals(oldestSnapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n        context.log.deleteBeforeSnapshot(oldestSnapshotId);\n        context.client.poll();\n\n        // Send fetch with log start offset and invalid last fetched epoch\n        context.deliverRequest(\n            context.fetchRequest(epoch, otherNodeId, oldestSnapshotId.offset(), oldestSnapshotId.epoch() + 1, 0)\n        );\n        context.pollUntilResponse();\n        FetchResponseData.PartitionData partitionResponse \u003d context.assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(localId, partitionResponse.currentLeader().leaderId());\n        assertEquals(oldestSnapshotId.epoch(), partitionResponse.snapshotId().epoch());\n        assertEquals(oldestSnapshotId.offset(), partitionResponse.snapshotId().endOffset());\n    }\n\n    @Test\n    public void testFetchRequestWithLastFetchedEpochLessThanOldestSnapshot() throws Exception {\n        int localId \u003d 0;\n        int otherNodeId \u003d localId + 1;\n        int syncNodeId \u003d otherNodeId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, otherNodeId, syncNodeId);\n\n        OffsetAndEpoch oldestSnapshotId \u003d new OffsetAndEpoch(3, 2);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .appendToLog(oldestSnapshotId.epoch() + 2, Arrays.asList(\"g\", \"h\", \"i\"))\n            .withAppendLingerMs(1)\n            .build();\n\n        context.becomeLeader();\n        int epoch \u003d context.currentEpoch();\n        assertEquals(oldestSnapshotId.epoch() + 2 + 1, epoch);\n\n        // Advance the highWatermark\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        // Create a snapshot at the high watermark\n        try (SnapshotWriter\u003cString\u003e snapshot \u003d context.client.createSnapshot(oldestSnapshotId, 0).get()) {\n            assertEquals(oldestSnapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n        context.client.poll();\n\n        // Send a epoch less than the oldest snapshot\n        context.deliverRequest(\n            context.fetchRequest(\n                epoch,\n                otherNodeId,\n                context.log.endOffset().offset,\n                oldestSnapshotId.epoch() - 1,\n                0\n            )\n        );\n        context.pollUntilResponse();\n        FetchResponseData.PartitionData partitionResponse \u003d context.assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(localId, partitionResponse.currentLeader().leaderId());\n        assertEquals(oldestSnapshotId.epoch(), partitionResponse.snapshotId().epoch());\n        assertEquals(oldestSnapshotId.offset(), partitionResponse.snapshotId().endOffset());\n    }\n\n    @Test\n    public void testFetchSnapshotRequestMissingSnapshot() throws Exception {\n        int localId \u003d 0;\n        int epoch \u003d 2;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, localId + 1);\n\n        RaftClientTestContext context \u003d RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch,\n                new OffsetAndEpoch(0, 0),\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response \u003d context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n        assertEquals(Errors.SNAPSHOT_NOT_FOUND, Errors.forCode(response.errorCode()));\n    }\n\n    @Test\n    public void testFetchSnapshotRequestUnknownPartition() throws Exception {\n        int localId \u003d 0;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, localId + 1);\n        int epoch \u003d 2;\n        TopicPartition topicPartition \u003d new TopicPartition(\"unknown\", 0);\n\n        RaftClientTestContext context \u003d RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                topicPartition,\n                epoch,\n                new OffsetAndEpoch(0, 0),\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response \u003d context.assertSentFetchSnapshotResponse(topicPartition).get();\n        assertEquals(Errors.UNKNOWN_TOPIC_OR_PARTITION, Errors.forCode(response.errorCode()));\n    }\n\n    @Test\n    public void testFetchSnapshotRequestAsLeader() throws Exception {\n        int localId \u003d 0;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, localId + 1);\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(1, 1);\n        List\u003cString\u003e records \u003d Arrays.asList(\"foo\", \"bar\");\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Collections.singletonList(\"a\"))\n            .build();\n\n        context.becomeLeader();\n        int epoch \u003d context.currentEpoch();\n\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        try (SnapshotWriter\u003cString\u003e snapshot \u003d context.client.createSnapshot(snapshotId, 0).get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            snapshot.append(records);\n            snapshot.freeze();\n        }\n\n        RawSnapshotReader snapshot \u003d context.log.readSnapshot(snapshotId).get();\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response \u003d context\n            .assertSentFetchSnapshotResponse(context.metadataPartition)\n            .get();\n\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n        assertEquals(snapshot.sizeInBytes(), response.size());\n        assertEquals(0, response.position());\n        assertEquals(snapshot.sizeInBytes(), response.unalignedRecords().sizeInBytes());\n\n        UnalignedMemoryRecords memoryRecords \u003d (UnalignedMemoryRecords) snapshot.slice(0, Math.toIntExact(snapshot.sizeInBytes()));\n\n        assertEquals(memoryRecords.buffer(), ((UnalignedMemoryRecords) response.unalignedRecords()).buffer());\n    }\n\n    @Test\n    public void testLeaderShouldResignLeadershipIfNotGetFetchSnapshotRequestFromMajorityVoters() throws Exception {\n        int localId \u003d 0;\n        int voter1 \u003d 1;\n        int voter2 \u003d 2;\n        int observerId3 \u003d 3;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, voter1, voter2);\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(1, 1);\n        List\u003cString\u003e records \u003d Arrays.asList(\"foo\", \"bar\");\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n                .appendToLog(snapshotId.epoch(), Collections.singletonList(\"a\"))\n                .build();\n\n        int resignLeadershipTimeout \u003d context.checkQuorumTimeoutMs;\n        context.becomeLeader();\n        int epoch \u003d context.currentEpoch();\n\n        FetchSnapshotRequestData voter1FetchSnapshotRequest \u003d fetchSnapshotRequest(\n                context.clusterId.toString(),\n                voter1,\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                0\n        );\n\n        FetchSnapshotRequestData voter2FetchSnapshotRequest \u003d fetchSnapshotRequest(\n                context.clusterId.toString(),\n                voter2,\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                0\n        );\n\n        FetchSnapshotRequestData observerFetchSnapshotRequest \u003d fetchSnapshotRequest(\n                context.clusterId.toString(),\n                observerId3,\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                0\n        );\n\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n        try (SnapshotWriter\u003cString\u003e snapshot \u003d context.client.createSnapshot(snapshotId, 0).get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            snapshot.append(records);\n            snapshot.freeze();\n        }\n\n        // fetch timeout is not expired, the leader should not get resigned\n        context.time.sleep(resignLeadershipTimeout / 2);\n        context.client.poll();\n        assertFalse(context.client.quorum().isResigned());\n\n        // voter1 sends fetchSnapshotRequest, the fetch timer should be reset\n        context.deliverRequest(voter1FetchSnapshotRequest);\n        context.client.poll();\n        context.assertSentFetchSnapshotResponse(context.metadataPartition);\n\n        // Since the fetch timer is reset, the leader should not get resigned\n        context.time.sleep(resignLeadershipTimeout / 2);\n        context.client.poll();\n        assertFalse(context.client.quorum().isResigned());\n\n        // voter2 sends fetchSnapshotRequest, the fetch timer should be reset\n        context.deliverRequest(voter2FetchSnapshotRequest);\n        context.client.poll();\n        context.assertSentFetchSnapshotResponse(context.metadataPartition);\n\n        // Since the fetch timer is reset, the leader should not get resigned\n        context.time.sleep(resignLeadershipTimeout / 2);\n        context.client.poll();\n        assertFalse(context.client.quorum().isResigned());\n\n        // An observer sends fetchSnapshotRequest, but the fetch timer should not be reset.\n        context.deliverRequest(observerFetchSnapshotRequest);\n        context.client.poll();\n        context.assertSentFetchSnapshotResponse(context.metadataPartition);\n\n        // After this sleep, the fetch timeout should expire since we don\u0027t receive fetch request from the majority voters within fetchTimeoutMs\n        context.time.sleep(resignLeadershipTimeout / 2);\n        context.client.poll();\n        assertTrue(context.client.quorum().isResigned());\n    }\n\n    @Test\n    public void testPartialFetchSnapshotRequestAsLeader() throws Exception {\n        int localId \u003d 0;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, localId + 1);\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(2, 1);\n        List\u003cString\u003e records \u003d Arrays.asList(\"foo\", \"bar\");\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), records)\n            .build();\n\n        context.becomeLeader();\n        int epoch \u003d context.currentEpoch();\n\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        try (SnapshotWriter\u003cString\u003e snapshot \u003d context.client.createSnapshot(snapshotId, 0).get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            snapshot.append(records);\n            snapshot.freeze();\n        }\n\n        RawSnapshotReader snapshot \u003d context.log.readSnapshot(snapshotId).get();\n        // Fetch half of the snapshot\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Math.toIntExact(snapshot.sizeInBytes() / 2),\n                0\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response \u003d context\n            .assertSentFetchSnapshotResponse(context.metadataPartition)\n            .get();\n\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n        assertEquals(snapshot.sizeInBytes(), response.size());\n        assertEquals(0, response.position());\n        assertEquals(snapshot.sizeInBytes() / 2, response.unalignedRecords().sizeInBytes());\n\n        UnalignedMemoryRecords memoryRecords \u003d (UnalignedMemoryRecords) snapshot.slice(0, Math.toIntExact(snapshot.sizeInBytes()));\n        ByteBuffer snapshotBuffer \u003d memoryRecords.buffer();\n\n        ByteBuffer responseBuffer \u003d ByteBuffer.allocate(Math.toIntExact(snapshot.sizeInBytes()));\n        responseBuffer.put(((UnalignedMemoryRecords) response.unalignedRecords()).buffer());\n\n        ByteBuffer expectedBytes \u003d snapshotBuffer.duplicate();\n        expectedBytes.limit(Math.toIntExact(snapshot.sizeInBytes() / 2));\n\n        assertEquals(expectedBytes, responseBuffer.duplicate().flip());\n\n        // Fetch the remainder of the snapshot\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                responseBuffer.position()\n            )\n        );\n\n        context.client.poll();\n\n        response \u003d context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n        assertEquals(snapshot.sizeInBytes(), response.size());\n        assertEquals(responseBuffer.position(), response.position());\n        assertEquals(snapshot.sizeInBytes() - (snapshot.sizeInBytes() / 2), response.unalignedRecords().sizeInBytes());\n\n        responseBuffer.put(((UnalignedMemoryRecords) response.unalignedRecords()).buffer());\n        assertEquals(snapshotBuffer, responseBuffer.flip());\n    }\n\n    @Test\n    public void testFetchSnapshotRequestAsFollower() throws IOException {\n        int localId \u003d 0;\n        int leaderId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, leaderId);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(0, 0);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response \u003d context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n        assertEquals(Errors.NOT_LEADER_OR_FOLLOWER, Errors.forCode(response.errorCode()));\n        assertEquals(epoch, response.currentLeader().leaderEpoch());\n        assertEquals(leaderId, response.currentLeader().leaderId());\n    }\n\n    @Test\n    public void testFetchSnapshotRequestWithInvalidPosition() throws Exception {\n        int localId \u003d 0;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, localId + 1);\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(1, 1);\n        List\u003cString\u003e records \u003d Arrays.asList(\"foo\", \"bar\");\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Collections.singletonList(\"a\"))\n            .build();\n\n        context.becomeLeader();\n        int epoch \u003d context.currentEpoch();\n\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        try (SnapshotWriter\u003cString\u003e snapshot \u003d context.client.createSnapshot(snapshotId, 0).get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            snapshot.append(records);\n            snapshot.freeze();\n        }\n\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                -1\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response \u003d context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n        assertEquals(Errors.POSITION_OUT_OF_RANGE, Errors.forCode(response.errorCode()));\n        assertEquals(epoch, response.currentLeader().leaderEpoch());\n        assertEquals(localId, response.currentLeader().leaderId());\n\n        RawSnapshotReader snapshot \u003d context.log.readSnapshot(snapshotId).get();\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                snapshot.sizeInBytes()\n            )\n        );\n\n        context.client.poll();\n\n        response \u003d context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n        assertEquals(Errors.POSITION_OUT_OF_RANGE, Errors.forCode(response.errorCode()));\n        assertEquals(epoch, response.currentLeader().leaderEpoch());\n        assertEquals(localId, response.currentLeader().leaderId());\n    }\n\n    @Test\n    public void testFetchSnapshotRequestWithOlderEpoch() throws Exception {\n        int localId \u003d 0;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, localId + 1);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(0, 0);\n\n        RaftClientTestContext context \u003d RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch - 1,\n                snapshotId,\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response \u003d context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n        assertEquals(Errors.FENCED_LEADER_EPOCH, Errors.forCode(response.errorCode()));\n        assertEquals(epoch, response.currentLeader().leaderEpoch());\n        assertEquals(localId, response.currentLeader().leaderId());\n    }\n\n    @Test\n    public void testFetchSnapshotRequestWithNewerEpoch() throws Exception {\n        int localId \u003d 0;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, localId + 1);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(0, 0);\n\n        RaftClientTestContext context \u003d RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch + 1,\n                snapshotId,\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response \u003d context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n        assertEquals(Errors.UNKNOWN_LEADER_EPOCH, Errors.forCode(response.errorCode()));\n        assertEquals(epoch, response.currentLeader().leaderEpoch());\n        assertEquals(localId, response.currentLeader().leaderId());\n    }\n\n    @Test\n    public void testFetchResponseWithInvalidSnapshotId() throws Exception {\n        int localId \u003d 0;\n        int leaderId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, leaderId);\n        int epoch \u003d 2;\n        OffsetAndEpoch invalidEpoch \u003d new OffsetAndEpoch(100L, -1);\n        OffsetAndEpoch invalidEndOffset \u003d new OffsetAndEpoch(-1L, 1);\n        int slept \u003d 0;\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, invalidEpoch, 200L)\n        );\n\n        // Handle the invalid response\n        context.client.poll();\n\n        // Expect another fetch request after backoff has expired\n        context.time.sleep(context.retryBackoffMs);\n        slept +\u003d context.retryBackoffMs;\n\n        context.pollUntilRequest();\n        fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, invalidEndOffset, 200L)\n        );\n\n        // Handle the invalid response\n        context.client.poll();\n\n        // Expect another fetch request after backoff has expired\n        context.time.sleep(context.retryBackoffMs);\n        slept +\u003d context.retryBackoffMs;\n\n        context.pollUntilRequest();\n        fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        // Fetch timer is not reset; sleeping for remainder should transition to candidate\n        context.time.sleep(context.fetchTimeoutMs - slept);\n\n        context.pollUntilRequest();\n\n        context.assertSentVoteRequest(epoch + 1, 0, 0L, 1);\n        context.assertVotedCandidate(epoch + 1, localId);\n    }\n\n    @Test\n    public void testFetchResponseWithSnapshotId() throws Exception {\n        int localId \u003d 0;\n        int leaderId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, leaderId);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound snapshotRequest \u003d context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request \u003d assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        List\u003cString\u003e records \u003d Arrays.asList(\"foo\", \"bar\");\n        MemorySnapshotWriter memorySnapshot \u003d new MemorySnapshotWriter(snapshotId);\n        try (SnapshotWriter\u003cString\u003e snapshotWriter \u003d snapshotWriter(context, memorySnapshot)) {\n            snapshotWriter.append(records);\n            snapshotWriter.freeze();\n        }\n\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            fetchSnapshotResponse(\n                context.metadataPartition,\n                epoch,\n                leaderId,\n                snapshotId,\n                memorySnapshot.buffer().remaining(),\n                0L,\n                memorySnapshot.buffer().slice()\n            )\n        );\n\n        context.pollUntilRequest();\n        fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, snapshotId.offset(), snapshotId.epoch());\n\n        // Check that the snapshot was written to the log\n        RawSnapshotReader snapshot \u003d context.log.readSnapshot(snapshotId).get();\n        assertEquals(memorySnapshot.buffer().remaining(), snapshot.sizeInBytes());\n        SnapshotWriterReaderTest.assertSnapshot(Collections.singletonList(records), snapshot);\n\n        // Check that listener was notified of the new snapshot\n        try (SnapshotReader\u003cString\u003e reader \u003d context.listener.drainHandledSnapshot().get()) {\n            assertEquals(snapshotId, reader.snapshotId());\n            SnapshotWriterReaderTest.assertSnapshot(Collections.singletonList(records), reader);\n        }\n    }\n\n    @Test\n    public void testFetchSnapshotResponsePartialData() throws Exception {\n        int localId \u003d 0;\n        int leaderId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, leaderId);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound snapshotRequest \u003d context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request \u003d assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        List\u003cString\u003e records \u003d Arrays.asList(\"foo\", \"bar\");\n        MemorySnapshotWriter memorySnapshot \u003d new MemorySnapshotWriter(snapshotId);\n        try (SnapshotWriter\u003cString\u003e snapshotWriter \u003d snapshotWriter(context, memorySnapshot)) {\n            snapshotWriter.append(records);\n            snapshotWriter.freeze();\n        }\n\n        ByteBuffer sendingBuffer \u003d memorySnapshot.buffer().slice();\n        sendingBuffer.limit(sendingBuffer.limit() / 2);\n\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            fetchSnapshotResponse(\n                context.metadataPartition,\n                epoch,\n                leaderId,\n                snapshotId,\n                memorySnapshot.buffer().remaining(),\n                0L,\n                sendingBuffer\n            )\n        );\n\n        context.pollUntilRequest();\n        snapshotRequest \u003d context.assertSentFetchSnapshotRequest();\n        request \u003d assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(sendingBuffer.limit(), request.position());\n\n        sendingBuffer \u003d memorySnapshot.buffer().slice();\n        sendingBuffer.position(Math.toIntExact(request.position()));\n\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            fetchSnapshotResponse(\n                context.metadataPartition,\n                epoch,\n                leaderId,\n                snapshotId,\n                memorySnapshot.buffer().remaining(),\n                request.position(),\n                sendingBuffer\n            )\n        );\n\n        context.pollUntilRequest();\n        fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, snapshotId.offset(), snapshotId.epoch());\n\n        // Check that the snapshot was written to the log\n        RawSnapshotReader snapshot \u003d context.log.readSnapshot(snapshotId).get();\n        assertEquals(memorySnapshot.buffer().remaining(), snapshot.sizeInBytes());\n        SnapshotWriterReaderTest.assertSnapshot(Collections.singletonList(records), snapshot);\n\n        // Check that listener was notified of the new snapshot\n        try (SnapshotReader\u003cString\u003e reader \u003d context.listener.drainHandledSnapshot().get()) {\n            assertEquals(snapshotId, reader.snapshotId());\n            SnapshotWriterReaderTest.assertSnapshot(Collections.singletonList(records), reader);\n        }\n    }\n\n    @Test\n    public void testFetchSnapshotResponseMissingSnapshot() throws Exception {\n        int localId \u003d 0;\n        int leaderId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, leaderId);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound snapshotRequest \u003d context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request \u003d assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        // Reply with a snapshot not found error\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            FetchSnapshotResponse.singleton(\n                context.metadataPartition,\n                responsePartitionSnapshot -\u003e {\n                    responsePartitionSnapshot\n                        .currentLeader()\n                        .setLeaderEpoch(epoch)\n                        .setLeaderId(leaderId);\n\n                    return responsePartitionSnapshot\n                        .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code());\n                }\n            )\n        );\n\n        context.pollUntilRequest();\n        fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n    }\n\n    @Test\n    public void testFetchSnapshotResponseFromNewerEpochNotLeader() throws Exception {\n        int localId \u003d 0;\n        int firstLeaderId \u003d localId + 1;\n        int secondLeaderId \u003d firstLeaderId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, firstLeaderId, secondLeaderId);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, firstLeaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, firstLeaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound snapshotRequest \u003d context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request \u003d assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        // Reply with new leader response\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            FetchSnapshotResponse.singleton(\n                context.metadataPartition,\n                responsePartitionSnapshot -\u003e {\n                    responsePartitionSnapshot\n                        .currentLeader()\n                        .setLeaderEpoch(epoch + 1)\n                        .setLeaderId(secondLeaderId);\n\n                    return responsePartitionSnapshot\n                        .setErrorCode(Errors.FENCED_LEADER_EPOCH.code());\n                }\n            )\n        );\n\n        context.pollUntilRequest();\n        fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch + 1, 0L, 0);\n    }\n\n    @Test\n    public void testFetchSnapshotResponseFromNewerEpochLeader() throws Exception {\n        int localId \u003d 0;\n        int leaderId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, leaderId);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound snapshotRequest \u003d context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request \u003d assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        // Reply with new leader epoch\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            FetchSnapshotResponse.singleton(\n                context.metadataPartition,\n                responsePartitionSnapshot -\u003e {\n                    responsePartitionSnapshot\n                        .currentLeader()\n                        .setLeaderEpoch(epoch + 1)\n                        .setLeaderId(leaderId);\n\n                    return responsePartitionSnapshot\n                        .setErrorCode(Errors.FENCED_LEADER_EPOCH.code());\n                }\n            )\n        );\n\n        context.pollUntilRequest();\n        fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch + 1, 0L, 0);\n    }\n\n    @Test\n    public void testFetchSnapshotResponseFromOlderEpoch() throws Exception {\n        int localId \u003d 0;\n        int leaderId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, leaderId);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound snapshotRequest \u003d context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request \u003d assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        // Reply with unknown leader epoch\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            FetchSnapshotResponse.singleton(\n                context.metadataPartition,\n                responsePartitionSnapshot -\u003e {\n                    responsePartitionSnapshot\n                        .currentLeader()\n                        .setLeaderEpoch(epoch - 1)\n                        .setLeaderId(leaderId + 1);\n\n                    return responsePartitionSnapshot\n                        .setErrorCode(Errors.UNKNOWN_LEADER_EPOCH.code());\n                }\n            )\n        );\n\n        context.pollUntilRequest();\n\n        // Follower should resend the fetch snapshot request\n        snapshotRequest \u003d context.assertSentFetchSnapshotRequest();\n        request \u003d assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n    }\n\n    @Test\n    public void testFetchSnapshotResponseWithInvalidId() throws Exception {\n        int localId \u003d 0;\n        int leaderId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, leaderId);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound snapshotRequest \u003d context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request \u003d assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        // Reply with an invalid snapshot id endOffset\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            FetchSnapshotResponse.singleton(\n                context.metadataPartition,\n                responsePartitionSnapshot -\u003e {\n                    responsePartitionSnapshot\n                        .currentLeader()\n                        .setLeaderEpoch(epoch)\n                        .setLeaderId(leaderId);\n\n                    responsePartitionSnapshot\n                        .snapshotId()\n                        .setEndOffset(-1)\n                        .setEpoch(snapshotId.epoch());\n\n                    return responsePartitionSnapshot;\n                }\n            )\n        );\n\n        context.pollUntilRequest();\n\n        // Follower should send a fetch request\n        fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n\n        snapshotRequest \u003d context.assertSentFetchSnapshotRequest();\n        request \u003d assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        // Reply with an invalid snapshot id epoch\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            FetchSnapshotResponse.singleton(\n                context.metadataPartition,\n                responsePartitionSnapshot -\u003e {\n                    responsePartitionSnapshot\n                        .currentLeader()\n                        .setLeaderEpoch(epoch)\n                        .setLeaderId(leaderId);\n\n                    responsePartitionSnapshot\n                        .snapshotId()\n                        .setEndOffset(snapshotId.offset())\n                        .setEpoch(-1);\n\n                    return responsePartitionSnapshot;\n                }\n            )\n        );\n\n        context.pollUntilRequest();\n\n        // Follower should send a fetch request\n        fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n    }\n\n    @Test\n    public void testFetchSnapshotResponseToNotFollower() throws Exception {\n        int localId \u003d 0;\n        int leaderId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, leaderId);\n        int epoch \u003d 2;\n        OffsetAndEpoch snapshotId \u003d new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest \u003d context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n\n        RaftRequest.Outbound snapshotRequest \u003d context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request \u003d assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        // Sleeping for fetch timeout should transition to candidate\n        context.time.sleep(context.fetchTimeoutMs);\n\n        context.pollUntilRequest();\n\n        context.assertSentVoteRequest(epoch + 1, 0, 0L, 1);\n        context.assertVotedCandidate(epoch + 1, localId);\n\n        // Send the response late\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            FetchSnapshotResponse.singleton(\n                context.metadataPartition,\n                responsePartitionSnapshot -\u003e {\n                    responsePartitionSnapshot\n                        .currentLeader()\n                        .setLeaderEpoch(epoch)\n                        .setLeaderId(leaderId);\n\n                    responsePartitionSnapshot\n                        .snapshotId()\n                        .setEndOffset(snapshotId.offset())\n                        .setEpoch(snapshotId.epoch());\n\n                    return responsePartitionSnapshot;\n                }\n            )\n        );\n\n        // Assert that the response is ignored and the replicas stays as a candidate\n        context.client.poll();\n        context.assertVotedCandidate(epoch + 1, localId);\n    }\n\n    @Test\n    public void testFetchSnapshotRequestClusterIdValidation() throws Exception {\n        int localId \u003d 0;\n        int otherNodeId \u003d 1;\n        int epoch \u003d 5;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, otherNodeId);\n\n        RaftClientTestContext context \u003d RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n\n        // null cluster id is accepted\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.clusterId.toString(),\n                otherNodeId,\n                context.metadataPartition,\n                epoch,\n                new OffsetAndEpoch(0, 0),\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n        context.pollUntilResponse();\n        context.assertSentFetchSnapshotResponse(context.metadataPartition);\n\n        // null cluster id is accepted\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                null,\n                otherNodeId,\n                context.metadataPartition,\n                epoch,\n                new OffsetAndEpoch(0, 0),\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n        context.pollUntilResponse();\n        context.assertSentFetchSnapshotResponse(context.metadataPartition);\n\n        // empty cluster id is rejected\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                \"\",\n                otherNodeId,\n                context.metadataPartition,\n                epoch,\n                new OffsetAndEpoch(0, 0),\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n        context.pollUntilResponse();\n        context.assertSentFetchSnapshotResponse(Errors.INCONSISTENT_CLUSTER_ID);\n\n        // invalid cluster id is rejected\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                \"invalid-uuid\",\n                otherNodeId,\n                context.metadataPartition,\n                epoch,\n                new OffsetAndEpoch(0, 0),\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n        context.pollUntilResponse();\n        context.assertSentFetchSnapshotResponse(Errors.INCONSISTENT_CLUSTER_ID);\n    }\n\n    @Test\n    public void testCreateSnapshotAsLeaderWithInvalidSnapshotId() throws Exception {\n        int localId \u003d 0;\n        int otherNodeId \u003d localId + 1;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, otherNodeId);\n        int epoch \u003d 2;\n\n        List\u003cString\u003e appendRecords \u003d Arrays.asList(\"a\", \"b\", \"c\");\n        OffsetAndEpoch invalidSnapshotId1 \u003d new OffsetAndEpoch(4, epoch);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n                .appendToLog(epoch, appendRecords)\n                .withAppendLingerMs(1)\n                .build();\n\n        context.becomeLeader();\n        int currentEpoch \u003d context.currentEpoch();\n\n        // When leader creating snapshot:\n        // 1.1 high watermark cannot be empty\n        assertEquals(OptionalLong.empty(), context.client.highWatermark());\n        assertThrows(IllegalArgumentException.class, () -\u003e context.client.createSnapshot(invalidSnapshotId1, 0));\n\n        // 1.2 high watermark must larger than or equal to the snapshotId\u0027s endOffset\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n        // append some more records to make the LEO \u003e high watermark\n        List\u003cString\u003e newRecords \u003d Arrays.asList(\"d\", \"e\", \"f\");\n        context.client.scheduleAppend(currentEpoch, newRecords);\n        context.time.sleep(context.appendLingerMs());\n        context.client.poll();\n        assertEquals(context.log.endOffset().offset, context.client.highWatermark().getAsLong() + newRecords.size());\n\n        OffsetAndEpoch invalidSnapshotId2 \u003d new OffsetAndEpoch(context.client.highWatermark().getAsLong() + 2, currentEpoch);\n        assertThrows(IllegalArgumentException.class, () -\u003e context.client.createSnapshot(invalidSnapshotId2, 0));\n\n        // 2 the quorum epoch must larger than or equal to the snapshotId\u0027s epoch\n        OffsetAndEpoch invalidSnapshotId3 \u003d new OffsetAndEpoch(context.client.highWatermark().getAsLong(), currentEpoch + 1);\n        assertThrows(IllegalArgumentException.class, () -\u003e context.client.createSnapshot(invalidSnapshotId3, 0));\n\n        // 3 the snapshotId should be validated against endOffsetForEpoch\n        OffsetAndEpoch endOffsetForEpoch \u003d context.log.endOffsetForEpoch(epoch);\n        assertEquals(epoch, endOffsetForEpoch.epoch());\n        OffsetAndEpoch invalidSnapshotId4 \u003d new OffsetAndEpoch(endOffsetForEpoch.offset() + 2, epoch);\n        assertThrows(IllegalArgumentException.class, () -\u003e context.client.createSnapshot(invalidSnapshotId4, 0));\n    }\n\n    @Test\n    public void testCreateSnapshotAsFollowerWithInvalidSnapshotId() throws Exception {\n        int localId \u003d 0;\n        int leaderId \u003d 1;\n        int otherFollowerId \u003d 2;\n        int epoch \u003d 5;\n        Set\u003cInteger\u003e voters \u003d Utils.mkSet(localId, leaderId, otherFollowerId);\n\n        RaftClientTestContext context \u003d new RaftClientTestContext.Builder(localId, voters)\n                .withElectedLeader(epoch, leaderId)\n                .build();\n        context.assertElectedLeader(epoch, leaderId);\n\n        // When follower creating snapshot:\n        // 1) The high watermark cannot be empty\n        assertEquals(OptionalLong.empty(), context.client.highWatermark());\n        OffsetAndEpoch invalidSnapshotId1 \u003d new OffsetAndEpoch(1, 0);\n        assertThrows(IllegalArgumentException.class, () -\u003e context.client.createSnapshot(invalidSnapshotId1, 0));\n\n        // Poll for our first fetch request\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest \u003d context.assertSentFetchRequest();\n        assertTrue(voters.contains(fetchRequest.destination().id()));\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        // The response does not advance the high watermark\n        List\u003cString\u003e records1 \u003d Arrays.asList(\"a\", \"b\", \"c\");\n        MemoryRecords batch1 \u003d context.buildBatch(0L, 3, records1);\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            context.fetchResponse(epoch, leaderId, batch1, 0L, Errors.NONE)\n        );\n        context.client.poll();\n\n        // 2) The high watermark must be larger than or equal to the snapshotId\u0027s endOffset\n        int currentEpoch \u003d context.currentEpoch();\n        OffsetAndEpoch invalidSnapshotId2 \u003d new OffsetAndEpoch(context.client.highWatermark().getAsLong() + 1, currentEpoch);\n        assertThrows(IllegalArgumentException.class, () -\u003e context.client.createSnapshot(invalidSnapshotId2, 0));\n\n        // 3) The quorum epoch must be larger than or equal to the snapshotId\u0027s epoch\n        OffsetAndEpoch invalidSnapshotId3 \u003d new OffsetAndEpoch(context.client.highWatermark().getAsLong() + 1, currentEpoch + 1);\n        assertThrows(IllegalArgumentException.class, () -\u003e context.client.createSnapshot(invalidSnapshotId3, 0));\n\n        // The high watermark advances to be larger than log.endOffsetForEpoch(3), to test the case 3\n        context.pollUntilRequest();\n        fetchRequest \u003d context.assertSentFetchRequest();\n        assertTrue(voters.contains(fetchRequest.destination().id()));\n        context.assertFetchRequestData(fetchRequest, epoch, 3L, 3);\n\n        List\u003cString\u003e records2 \u003d Arrays.asList(\"d\", \"e\", \"f\");\n        MemoryRecords batch2 \u003d context.buildBatch(3L, 4, records2);\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            context.fetchResponse(epoch, leaderId, batch2, 6L, Errors.NONE)\n        );\n        context.client.poll();\n        assertEquals(6L, context.client.highWatermark().getAsLong());\n\n        // 4) The snapshotId should be validated against endOffsetForEpoch\n        OffsetAndEpoch endOffsetForEpoch \u003d context.log.endOffsetForEpoch(3);\n        assertEquals(3, endOffsetForEpoch.epoch());\n        OffsetAndEpoch invalidSnapshotId4 \u003d new OffsetAndEpoch(endOffsetForEpoch.offset() + 1, epoch);\n        assertThrows(IllegalArgumentException.class, () -\u003e context.client.createSnapshot(invalidSnapshotId4, 0));\n    }\n\n    private static FetchSnapshotRequestData fetchSnapshotRequest(\n            TopicPartition topicPartition,\n            int epoch,\n            OffsetAndEpoch offsetAndEpoch,\n            int maxBytes,\n            long position\n    ) {\n        return fetchSnapshotRequest(null, -1, topicPartition, epoch, offsetAndEpoch, maxBytes, position);\n    }\n\n    private static FetchSnapshotRequestData fetchSnapshotRequest(\n        String clusterId,\n        int replicaId,\n        TopicPartition topicPartition,\n        int epoch,\n        OffsetAndEpoch offsetAndEpoch,\n        int maxBytes,\n        long position\n    ) {\n        FetchSnapshotRequestData.SnapshotId snapshotId \u003d new FetchSnapshotRequestData.SnapshotId()\n            .setEndOffset(offsetAndEpoch.offset())\n            .setEpoch(offsetAndEpoch.epoch());\n\n        FetchSnapshotRequestData request \u003d FetchSnapshotRequest.singleton(\n            clusterId,\n            replicaId,\n            topicPartition,\n            snapshotPartition -\u003e snapshotPartition\n                .setCurrentLeaderEpoch(epoch)\n                .setSnapshotId(snapshotId)\n                .setPosition(position)\n        );\n\n        return request.setMaxBytes(maxBytes);\n    }\n\n    private static FetchSnapshotResponseData fetchSnapshotResponse(\n        TopicPartition topicPartition,\n        int leaderEpoch,\n        int leaderId,\n        OffsetAndEpoch snapshotId,\n        long size,\n        long position,\n        ByteBuffer buffer\n    ) {\n        return FetchSnapshotResponse.singleton(\n            topicPartition,\n            partitionSnapshot -\u003e {\n                partitionSnapshot.currentLeader()\n                    .setLeaderEpoch(leaderEpoch)\n                    .setLeaderId(leaderId);\n\n                partitionSnapshot.snapshotId()\n                    .setEndOffset(snapshotId.offset())\n                    .setEpoch(snapshotId.epoch());\n\n                return partitionSnapshot\n                    .setSize(size)\n                    .setPosition(position)\n                    .setUnalignedRecords(MemoryRecords.readableRecords(buffer.slice()));\n            }\n        );\n    }\n\n    private static FetchResponseData snapshotFetchResponse(\n        TopicPartition topicPartition,\n        Uuid topicId,\n        int epoch,\n        int leaderId,\n        OffsetAndEpoch snapshotId,\n        long highWatermark\n    ) {\n        return RaftUtil.singletonFetchResponse(topicPartition, topicId, Errors.NONE, partitionData -\u003e {\n            partitionData.setHighWatermark(highWatermark);\n\n            partitionData.currentLeader()\n                .setLeaderEpoch(epoch)\n                .setLeaderId(leaderId);\n\n            partitionData.snapshotId()\n                .setEpoch(snapshotId.epoch())\n                .setEndOffset(snapshotId.offset());\n        });\n    }\n\n    private static Optional\u003cFetchSnapshotRequestData.PartitionSnapshot\u003e assertFetchSnapshotRequest(\n        RaftRequest.Outbound request,\n        TopicPartition topicPartition,\n        int replicaId,\n        int maxBytes\n    ) {\n        assertInstanceOf(FetchSnapshotRequestData.class, request.data());\n\n        FetchSnapshotRequestData data \u003d (FetchSnapshotRequestData) request.data();\n\n        assertEquals(replicaId, data.replicaId());\n        assertEquals(maxBytes, data.maxBytes());\n\n        return FetchSnapshotRequest.forTopicPartition(data, topicPartition);\n    }\n\n    private static SnapshotWriter\u003cString\u003e snapshotWriter(RaftClientTestContext context, RawSnapshotWriter snapshot) {\n        return new RecordsSnapshotWriter.Builder()\n            .setTime(context.time)\n            .setRawSnapshotWriter(snapshot)\n            .build(new StringSerde());\n    }\n\n    private static final class MemorySnapshotWriter implements RawSnapshotWriter {\n        private final OffsetAndEpoch snapshotId;\n        private final AtomicLong frozenPosition;\n        private ByteBuffer data;\n\n        public MemorySnapshotWriter(OffsetAndEpoch snapshotId) {\n            this.snapshotId \u003d snapshotId;\n            this.data \u003d ByteBuffer.allocate(0);\n            this.frozenPosition \u003d new AtomicLong(-1L);\n        }\n\n        @Override\n        public OffsetAndEpoch snapshotId() {\n            return snapshotId;\n        }\n\n        @Override\n        public long sizeInBytes() {\n            long position \u003d frozenPosition.get();\n            return (position \u003c 0) ? data.position() : position;\n        }\n\n        @Override\n        public void append(UnalignedMemoryRecords records) {\n            if (isFrozen()) {\n                throw new RuntimeException(\"Snapshot is already frozen \" + snapshotId);\n            }\n            append(records.buffer());\n        }\n\n        @Override\n        public void append(MemoryRecords records) {\n            if (isFrozen()) {\n                throw new RuntimeException(\"Snapshot is already frozen \" + snapshotId);\n            }\n            append(records.buffer());\n        }\n\n        private void append(ByteBuffer buffer) {\n            if (!(data.remaining() \u003e\u003d buffer.remaining())) {\n                ByteBuffer old \u003d data;\n                old.flip();\n\n                int newSize \u003d Math.max(data.capacity() * 2, data.capacity() + buffer.remaining());\n                data \u003d ByteBuffer.allocate(newSize);\n\n                data.put(old);\n            }\n            data.put(buffer);\n        }\n\n        @Override\n        public boolean isFrozen() {\n            return frozenPosition.get() \u003e\u003d 0;\n        }\n\n        @Override\n        public void freeze() {\n            if (!frozenPosition.compareAndSet(-1L, data.position())) {\n                throw new RuntimeException(\"Snapshot is already frozen \" + snapshotId);\n            }\n            data.flip();\n        }\n\n        @Override\n        public void close() {}\n\n        public ByteBuffer buffer() {\n            return data;\n        }\n    }\n}","methodCount":51},"candidatesTelemetryData":{"numberOfSuggestions":6,"candidates":[{"lineStart":1955,"lineEnd":1960,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method snapshotWriter to class StreamsTestUtils","description":"move method snapshotWriter to PsiClass:StreamsTestUtils\nRationale: The snapshotWriter() method is related to creating a snapshot writer for stream processing, which aligns well with the utility functions provided by StreamsTestUtils. Moving it here adheres to the Single Responsibility Principle, as it centralizes stream-related utilities. This enhances cohesion and reusability for testing stream components. However, care should be taken to ensure that it doesn\u0027t clutter the utility class with too many unrelated methods.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":1955,"lineEnd":1960,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method snapshotWriter to class ToolsTestUtils","description":"move method snapshotWriter to PsiClass:ToolsTestUtils\nRationale: The snapshotWriter() method can be considered a utility for testing purposes, similar to the methods in ToolsTestUtils. This move would keep testing utilities together, improving organization. It aligns with the Open/Closed Principle, as it allows for extension without modifying existing code. However, it may introduce a broader scope of responsibilities to ToolsTestUtils, which should be monitored.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":1955,"lineEnd":1960,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method snapshotWriter to class TaskBuilder","description":"move method snapshotWriter to PsiClass:TaskBuilder\nRationale: If the snapshotWriter() method is used in the context of task management, it could be relevant to TaskBuilder. This move would enhance the encapsulation of task-related functionalities. However, the connection between snapshot writing and task management should be clearly defined to avoid confusion.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":1854,"lineEnd":1862,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method fetchSnapshotRequest to class RequestUtils","description":"move method fetchSnapshotRequest to PsiClass:RequestUtils\nRationale: The fetchSnapshotRequest() method is related to creating request data, which aligns with the responsibilities of RequestUtils. Moving it here adheres to the Single Responsibility Principle by consolidating request-related methods, improving cohesion. This enhances the overall design by centralizing request logic, making it easier to maintain. However, care should be taken to ensure that existing dependencies on the method are updated accordingly.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":1854,"lineEnd":1862,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method fetchSnapshotRequest to class ControlRecordUtils","description":"move method fetchSnapshotRequest to PsiClass:ControlRecordUtils\nRationale: The method fetchSnapshotRequest() appears to be related to snapshot data, which is closely tied to control records. Moving it to ControlRecordUtils would enhance cohesion as this class already deals with deserializing and validating control records. This aligns with the Open/Closed Principle, allowing for future extensions related to snapshot handling without modifying existing code. A potential drawback is that it may increase the complexity of ControlRecordUtils if it becomes too overloaded with responsibilities.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":1854,"lineEnd":1862,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method fetchSnapshotRequest to class MessageUtil","description":"move method fetchSnapshotRequest to PsiClass:MessageUtil\nRationale: Given that fetchSnapshotRequest() may involve message serialization or transformation, moving it to MessageUtil would be appropriate. This aligns with the Interface Segregation Principle by keeping message-related functionalities together. It improves the design by making message handling more cohesive. However, it is essential to ensure that the method\u0027s dependencies on other classes are minimal to avoid tight coupling.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false}]},"iterationData":[{"iteration_num":1,"suggested_move_methods":[{"method_name":"fetchSnapshotRequest","method_signature":"private static FetchSnapshotRequestData fetchSnapshotRequest(TopicPartition topicPartition, int epoch, OffsetAndEpoch offsetAndEpoch, int maxBytes, long position)","target_class":"SnapshotRequestBuilder","rationale":"This method is responsible for creating a fetch snapshot request, which is a distinct concern from the test class. Moving it to a dedicated builder class would improve separation of concerns."},{"method_name":"fetchSnapshotRequest","method_signature":"private static FetchSnapshotRequestData fetchSnapshotRequest(String clusterId, int replicaId, TopicPartition topicPartition, int epoch, OffsetAndEpoch offsetAndEpoch, int maxBytes, long position)","target_class":"SnapshotRequestBuilder","rationale":"Similar to the previous method, this method constructs a fetch snapshot request and should be moved to a dedicated builder class to enhance code organization."},{"method_name":"fetchSnapshotResponse","method_signature":"private static FetchSnapshotResponseData fetchSnapshotResponse(TopicPartition topicPartition, int leaderEpoch, int leaderId, OffsetAndEpoch snapshotId, long size, long position, ByteBuffer buffer)","target_class":"SnapshotResponseBuilder","rationale":"This method constructs a fetch snapshot response, which is a distinct concern from the test class. Moving it to a dedicated response builder class would improve code clarity and maintainability."},{"method_name":"snapshotFetchResponse","method_signature":"private static FetchResponseData snapshotFetchResponse(TopicPartition topicPartition, Uuid topicId, int epoch, int leaderId, OffsetAndEpoch snapshotId, long highWatermark)","target_class":"SnapshotResponseBuilder","rationale":"This method creates a snapshot fetch response and should be moved to a dedicated response builder class to maintain separation of concerns and improve code organization."},{"method_name":"assertFetchSnapshotRequest","method_signature":"private static Optional\u003cFetchSnapshotRequestData.PartitionSnapshot\u003e assertFetchSnapshotRequest(RaftRequest.Outbound request, TopicPartition topicPartition, int replicaId, int maxBytes)","target_class":"SnapshotRequestValidator","rationale":"This method validates a fetch snapshot request and should be moved to a dedicated validator class to enhance code organization and clarity."},{"method_name":"snapshotWriter","method_signature":"private static SnapshotWriter\u003cString\u003e snapshotWriter(RaftClientTestContext context, RawSnapshotWriter snapshot)","target_class":"SnapshotWriterFactory","rationale":"This method creates a snapshot writer and should be moved to a dedicated factory class to improve separation of concerns and code organization."}],"llm_response_time":10693},{"iteration_num":2,"suggested_move_methods":[{"method_name":"fetchSnapshotRequest","method_signature":"private static FetchSnapshotRequestData fetchSnapshotRequest(TopicPartition topicPartition, int epoch, OffsetAndEpoch offsetAndEpoch, int maxBytes, long position)","target_class":"SnapshotRequestBuilder","rationale":"This method is responsible for creating a fetch snapshot request, which is a utility function that can be better organized in a dedicated class for building snapshot requests."},{"method_name":"fetchSnapshotRequest","method_signature":"private static FetchSnapshotRequestData fetchSnapshotRequest(String clusterId, int replicaId, TopicPartition topicPartition, int epoch, OffsetAndEpoch offsetAndEpoch, int maxBytes, long position)","target_class":"SnapshotRequestBuilder","rationale":"Similar to the previous method, this method constructs a fetch snapshot request and should be moved to a dedicated class that handles the creation of snapshot requests."},{"method_name":"fetchSnapshotResponse","method_signature":"private static FetchSnapshotResponseData fetchSnapshotResponse(TopicPartition topicPartition, int leaderEpoch, int leaderId, OffsetAndEpoch snapshotId, long size, long position, ByteBuffer buffer)","target_class":"SnapshotResponseBuilder","rationale":"This method constructs a fetch snapshot response and should be moved to a dedicated class that handles the creation of snapshot responses."},{"method_name":"snapshotFetchResponse","method_signature":"private static FetchResponseData snapshotFetchResponse(TopicPartition topicPartition, Uuid topicId, int epoch, int leaderId, OffsetAndEpoch snapshotId, long highWatermark)","target_class":"SnapshotResponseBuilder","rationale":"This method is responsible for creating a snapshot fetch response and should be moved to a dedicated class that handles snapshot responses."},{"method_name":"assertFetchSnapshotRequest","method_signature":"private static Optional\u003cFetchSnapshotRequestData.PartitionSnapshot\u003e assertFetchSnapshotRequest(RaftRequest.Outbound request, TopicPartition topicPartition, int replicaId, int maxBytes)","target_class":"SnapshotRequestValidator","rationale":"This method validates the fetch snapshot request and should be moved to a dedicated class that handles validation of snapshot requests."},{"method_name":"snapshotWriter","method_signature":"private static SnapshotWriter\u003cString\u003e snapshotWriter(RaftClientTestContext context, RawSnapshotWriter snapshot)","target_class":"SnapshotWriterFactory","rationale":"This method creates a snapshot writer and should be moved to a dedicated factory class that handles the creation of snapshot writers."}],"llm_response_time":9396},{"iteration_num":3,"suggested_move_methods":[{"method_name":"fetchSnapshotRequest","method_signature":"private static FetchSnapshotRequestData fetchSnapshotRequest(TopicPartition topicPartition, int epoch, OffsetAndEpoch offsetAndEpoch, int maxBytes, long position)","target_class":"FetchSnapshotRequestBuilder","rationale":"This method is responsible for creating a fetch snapshot request, which is a distinct operation that could be encapsulated in a dedicated builder class for better separation of concerns."},{"method_name":"fetchSnapshotRequest","method_signature":"private static FetchSnapshotRequestData fetchSnapshotRequest(String clusterId, int replicaId, TopicPartition topicPartition, int epoch, OffsetAndEpoch offsetAndEpoch, int maxBytes, long position)","target_class":"FetchSnapshotRequestBuilder","rationale":"Similar to the previous method, this method constructs a fetch snapshot request and should be part of a dedicated builder class to enhance code organization and maintainability."},{"method_name":"fetchSnapshotResponse","method_signature":"private static FetchSnapshotResponseData fetchSnapshotResponse(TopicPartition topicPartition, int leaderEpoch, int leaderId, OffsetAndEpoch snapshotId, long size, long position, ByteBuffer buffer)","target_class":"FetchSnapshotResponseBuilder","rationale":"This method constructs a fetch snapshot response and would benefit from being part of a dedicated builder class that handles response creation, improving clarity and separation of concerns."},{"method_name":"snapshotFetchResponse","method_signature":"private static FetchResponseData snapshotFetchResponse(TopicPartition topicPartition, Uuid topicId, int epoch, int leaderId, OffsetAndEpoch snapshotId, long highWatermark)","target_class":"SnapshotFetchResponseBuilder","rationale":"This method creates a snapshot fetch response and should be moved to a dedicated builder class to encapsulate the logic related to snapshot responses, enhancing code organization."},{"method_name":"assertFetchSnapshotRequest","method_signature":"private static Optional\u003cFetchSnapshotRequestData.PartitionSnapshot\u003e assertFetchSnapshotRequest(RaftRequest.Outbound request, TopicPartition topicPartition, int replicaId, int maxBytes)","target_class":"FetchSnapshotRequestValidator","rationale":"This method validates the fetch snapshot request and should be part of a dedicated validator class to separate validation logic from the test class, improving maintainability."},{"method_name":"snapshotWriter","method_signature":"private static SnapshotWriter\u003cString\u003e snapshotWriter(RaftClientTestContext context, RawSnapshotWriter snapshot)","target_class":"SnapshotWriterFactory","rationale":"This method creates a snapshot writer and should be moved to a factory class that specializes in creating various types of snapshot writers, enhancing code organization."}],"llm_response_time":11046},{"iteration_num":-1,"suggested_move_methods":[{"method_name":"testLeaderListenerNotified","method_signature":"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testLeaderListenerNotified(boolean entireLog)","target_class":"","rationale":""},{"method_name":"testFollowerListenerNotified","method_signature":"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testFollowerListenerNotified(boolean entireLog)","target_class":"","rationale":""},{"method_name":"testSecondListenerNotified","method_signature":"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testSecondListenerNotified(boolean entireLog)","target_class":"","rationale":""},{"method_name":"fetchSnapshotRequest","method_signature":"private static fetchSnapshotRequest(\n            TopicPartition topicPartition,\n            int epoch,\n            OffsetAndEpoch offsetAndEpoch,\n            int maxBytes,\n            long position\n    )","target_class":"","rationale":""},{"method_name":"fetchSnapshotRequest","method_signature":"private static fetchSnapshotRequest(\n        String clusterId,\n        int replicaId,\n        TopicPartition topicPartition,\n        int epoch,\n        OffsetAndEpoch offsetAndEpoch,\n        int maxBytes,\n        long position\n    )","target_class":"","rationale":""},{"method_name":"fetchSnapshotResponse","method_signature":"private static fetchSnapshotResponse(\n        TopicPartition topicPartition,\n        int leaderEpoch,\n        int leaderId,\n        OffsetAndEpoch snapshotId,\n        long size,\n        long position,\n        ByteBuffer buffer\n    )","target_class":"","rationale":""},{"method_name":"snapshotFetchResponse","method_signature":"private static snapshotFetchResponse(\n        TopicPartition topicPartition,\n        Uuid topicId,\n        int epoch,\n        int leaderId,\n        OffsetAndEpoch snapshotId,\n        long highWatermark\n    )","target_class":"","rationale":""},{"method_name":"assertFetchSnapshotRequest","method_signature":"private static assertFetchSnapshotRequest(\n        RaftRequest.Outbound request,\n        TopicPartition topicPartition,\n        int replicaId,\n        int maxBytes\n    )","target_class":"","rationale":""},{"method_name":"snapshotWriter","method_signature":"private static snapshotWriter(RaftClientTestContext context, RawSnapshotWriter snapshot)","target_class":"","rationale":""},{"method_name":"append","method_signature":"private append(ByteBuffer buffer)","target_class":"","rationale":""}],"llm_response_time":0},{"iteration_num":-2,"suggested_move_methods":[{"method_name":"append","method_signature":"private append(ByteBuffer buffer)","target_class":"","rationale":""},{"method_name":"snapshotWriter","method_signature":"private static snapshotWriter(RaftClientTestContext context, RawSnapshotWriter snapshot)","target_class":"","rationale":""},{"method_name":"fetchSnapshotRequest","method_signature":"private static fetchSnapshotRequest(\n            TopicPartition topicPartition,\n            int epoch,\n            OffsetAndEpoch offsetAndEpoch,\n            int maxBytes,\n            long position\n    )","target_class":"","rationale":""},{"method_name":"snapshotFetchResponse","method_signature":"private static snapshotFetchResponse(\n        TopicPartition topicPartition,\n        Uuid topicId,\n        int epoch,\n        int leaderId,\n        OffsetAndEpoch snapshotId,\n        long highWatermark\n    )","target_class":"","rationale":""},{"method_name":"fetchSnapshotRequest","method_signature":"private static fetchSnapshotRequest(\n        String clusterId,\n        int replicaId,\n        TopicPartition topicPartition,\n        int epoch,\n        OffsetAndEpoch offsetAndEpoch,\n        int maxBytes,\n        long position\n    )","target_class":"","rationale":""},{"method_name":"fetchSnapshotResponse","method_signature":"private static fetchSnapshotResponse(\n        TopicPartition topicPartition,\n        int leaderEpoch,\n        int leaderId,\n        OffsetAndEpoch snapshotId,\n        long size,\n        long position,\n        ByteBuffer buffer\n    )","target_class":"","rationale":""},{"method_name":"assertFetchSnapshotRequest","method_signature":"private static assertFetchSnapshotRequest(\n        RaftRequest.Outbound request,\n        TopicPartition topicPartition,\n        int replicaId,\n        int maxBytes\n    )","target_class":"","rationale":""},{"method_name":"testSecondListenerNotified","method_signature":"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testSecondListenerNotified(boolean entireLog)","target_class":"","rationale":""},{"method_name":"testLeaderListenerNotified","method_signature":"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testLeaderListenerNotified(boolean entireLog)","target_class":"","rationale":""},{"method_name":"testFollowerListenerNotified","method_signature":"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testFollowerListenerNotified(boolean entireLog)","target_class":"","rationale":""}],"llm_response_time":0}],"methodCompatibilityScores":{"private append(ByteBuffer buffer)":{"first":{"method_name":"append","method_signature":"private append(ByteBuffer buffer)","target_class":"","rationale":""},"second":0.3120759465632404},"private static snapshotWriter(RaftClientTestContext context, RawSnapshotWriter snapshot)":{"first":{"method_name":"snapshotWriter","method_signature":"private static snapshotWriter(RaftClientTestContext context, RawSnapshotWriter snapshot)","target_class":"","rationale":""},"second":0.41250005915593746},"private static fetchSnapshotRequest(\n            TopicPartition topicPartition,\n            int epoch,\n            OffsetAndEpoch offsetAndEpoch,\n            int maxBytes,\n            long position\n    )":{"first":{"method_name":"fetchSnapshotRequest","method_signature":"private static fetchSnapshotRequest(\n            TopicPartition topicPartition,\n            int epoch,\n            OffsetAndEpoch offsetAndEpoch,\n            int maxBytes,\n            long position\n    )","target_class":"","rationale":""},"second":0.6237488176465389},"private static snapshotFetchResponse(\n        TopicPartition topicPartition,\n        Uuid topicId,\n        int epoch,\n        int leaderId,\n        OffsetAndEpoch snapshotId,\n        long highWatermark\n    )":{"first":{"method_name":"snapshotFetchResponse","method_signature":"private static snapshotFetchResponse(\n        TopicPartition topicPartition,\n        Uuid topicId,\n        int epoch,\n        int leaderId,\n        OffsetAndEpoch snapshotId,\n        long highWatermark\n    )","target_class":"","rationale":""},"second":0.6712537814889239},"private static fetchSnapshotRequest(\n        String clusterId,\n        int replicaId,\n        TopicPartition topicPartition,\n        int epoch,\n        OffsetAndEpoch offsetAndEpoch,\n        int maxBytes,\n        long position\n    )":{"first":{"method_name":"fetchSnapshotRequest","method_signature":"private static fetchSnapshotRequest(\n        String clusterId,\n        int replicaId,\n        TopicPartition topicPartition,\n        int epoch,\n        OffsetAndEpoch offsetAndEpoch,\n        int maxBytes,\n        long position\n    )","target_class":"","rationale":""},"second":0.6800846594341448},"private static fetchSnapshotResponse(\n        TopicPartition topicPartition,\n        int leaderEpoch,\n        int leaderId,\n        OffsetAndEpoch snapshotId,\n        long size,\n        long position,\n        ByteBuffer buffer\n    )":{"first":{"method_name":"fetchSnapshotResponse","method_signature":"private static fetchSnapshotResponse(\n        TopicPartition topicPartition,\n        int leaderEpoch,\n        int leaderId,\n        OffsetAndEpoch snapshotId,\n        long size,\n        long position,\n        ByteBuffer buffer\n    )","target_class":"","rationale":""},"second":0.6826140774229446},"private static assertFetchSnapshotRequest(\n        RaftRequest.Outbound request,\n        TopicPartition topicPartition,\n        int replicaId,\n        int maxBytes\n    )":{"first":{"method_name":"assertFetchSnapshotRequest","method_signature":"private static assertFetchSnapshotRequest(\n        RaftRequest.Outbound request,\n        TopicPartition topicPartition,\n        int replicaId,\n        int maxBytes\n    )","target_class":"","rationale":""},"second":0.7028488528710449},"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testSecondListenerNotified(boolean entireLog)":{"first":{"method_name":"testSecondListenerNotified","method_signature":"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testSecondListenerNotified(boolean entireLog)","target_class":"","rationale":""},"second":0.8779336263971838},"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testLeaderListenerNotified(boolean entireLog)":{"first":{"method_name":"testLeaderListenerNotified","method_signature":"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testLeaderListenerNotified(boolean entireLog)","target_class":"","rationale":""},"second":0.8791010452907714},"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testFollowerListenerNotified(boolean entireLog)":{"first":{"method_name":"testFollowerListenerNotified","method_signature":"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testFollowerListenerNotified(boolean entireLog)","target_class":"","rationale":""},"second":0.890418807568558}},"llmMethodPriority":{"priority_method_names":["append","snapshotWriter","fetchSnapshotRequest","snapshotFetchResponse","fetchSnapshotRequest","fetchSnapshotResponse","assertFetchSnapshotRequest","testSecondListenerNotified","testLeaderListenerNotified","testFollowerListenerNotified"],"llm_response_time":3892},"targetClassMap":{"append":{"target_classes":[],"target_classes_sorted_by_llm":[],"llm_response_time":370,"similarity_computation_time":0,"similarity_metric":"tfidf"},"snapshotWriter":{"target_classes":[{"class_name":"RaftUtil","similarity_score":0.2854020680960567},{"class_name":"ToolsTestUtils","similarity_score":0.3751424877046732},{"class_name":"ToolsUtils","similarity_score":0.3031847710324455},{"class_name":"StateManagerUtil","similarity_score":0.2219096760961717},{"class_name":"SystemTestUtil","similarity_score":0.2152430458539858},{"class_name":"SchemaUtil","similarity_score":0.34027232683581016},{"class_name":"ThreadUtils","similarity_score":0.26892643710023856},{"class_name":"LogFileUtils","similarity_score":0.10709830970288935},{"class_name":"ScramCredentialUtils","similarity_score":0.3411913900230344},{"class_name":"TopologyMetadataBuilder","similarity_score":0.35918485695793173},{"class_name":"TieredStorageTestUtils","similarity_score":0.14999400035997598},{"class_name":"MetricsTestUtils","similarity_score":0.3327052315253509},{"class_name":"MetricsUtils","similarity_score":0.19043751565387806},{"class_name":"RequestTestUtils","similarity_score":0.2486589650756355},{"class_name":"RequestUtils","similarity_score":0.3491282676376715},{"class_name":"NetworkClientUtils","similarity_score":0.19852534707899597},{"class_name":"TaskAssignmentUtils","similarity_score":0.2536470162920501},{"class_name":"NetworkTestUtils","similarity_score":0.33296780450533353},{"class_name":"TaskAssignmentUtilsTest","similarity_score":0.20498506205771055},{"class_name":"TaskBuilder","similarity_score":0.37208784861724203},{"class_name":"SinkUtils","similarity_score":0.25819888974716115},{"class_name":"MirrorUtils","similarity_score":0.27772541005579665},{"class_name":"StoreQueryUtils","similarity_score":0.32637456026770134},{"class_name":"PluginUtils","similarity_score":0.2925584652634134},{"class_name":"SecurityUtils","similarity_score":0.42050503372731207},{"class_name":"SSLUtils","similarity_score":0.22173003507924643},{"class_name":"MockitoUtils","similarity_score":0.07437051466359121},{"class_name":"StreamsConfigUtils","similarity_score":0.3961107997648847},{"class_name":"SmokeTestUtil","similarity_score":0.3487313685396747},{"class_name":"TestCloseable","similarity_score":0.3383764840919798},{"class_name":"StreamsTestUtils","similarity_score":0.3941051724776672},{"class_name":"StreamStreamJoinUtil","similarity_score":0.19378331984971012},{"class_name":"Util","similarity_score":0.32186315478935906},{"class_name":"RetryUtil","similarity_score":0.16480531510839336},{"class_name":"Utils","similarity_score":0.23583357363557664},{"class_name":"UtilsTest","similarity_score":0.24849526674957323},{"class_name":"TestSslUtils","similarity_score":0.32417762108472364},{"class_name":"StandbyTaskAssignmentUtils","similarity_score":0.2774711218553872},{"class_name":"StandbyTaskAssignmentUtilsTest","similarity_score":0.2864459496157732},{"class_name":"TestUtil","similarity_score":0.3086066999241838},{"class_name":"TestUtils","similarity_score":0.3087251123189294},{"class_name":"FutureUtils","similarity_score":0.16427061504961218},{"class_name":"ProcessorContextUtils","similarity_score":0.3400600872554179},{"class_name":"IntegrationTestUtils","similarity_score":0.25425590933042574},{"class_name":"FetchUtils","similarity_score":0.04504687313477795},{"class_name":"ProducerTestUtils","similarity_score":0.2716072381275556},{"class_name":"InternalQueryResultUtil","similarity_score":0.2470929244126543},{"class_name":"AdminClientTestUtils","similarity_score":0.29330579793633},{"class_name":"AdminUtils","similarity_score":0.12884234199893185},{"class_name":"OAuthBearerScopeUtils","similarity_score":0.17054776006016165}],"target_classes_sorted_by_llm":["StreamsTestUtils","ToolsTestUtils","TaskBuilder","SchemaUtil","SmokeTestUtil","RequestUtils","TopologyMetadataBuilder","ScramCredentialUtils","StreamsConfigUtils","SecurityUtils"],"llm_response_time":19079,"similarity_computation_time":64,"similarity_metric":"tfidf"},"fetchSnapshotRequest":{"target_classes":[{"class_name":"RaftUtil","similarity_score":0.2185067700740813},{"class_name":"GraphGraceSearchUtil","similarity_score":0.22933195885153773},{"class_name":"SystemTestUtil","similarity_score":0.15319037979302844},{"class_name":"FutureUtils","similarity_score":0.1009701885353091},{"class_name":"LogFileUtils","similarity_score":0.07920026810467563},{"class_name":"RequestTestUtils","similarity_score":0.2207690804065308},{"class_name":"RequestUtils","similarity_score":0.261345270458263},{"class_name":"IntegrationTestUtils","similarity_score":0.18960093743339193},{"class_name":"FetchUtils","similarity_score":0.04264014327112208},{"class_name":"NetworkClientUtils","similarity_score":0.13164633276182638},{"class_name":"NetworkTestUtils","similarity_score":0.272331441934157},{"class_name":"TaskAssignmentUtils","similarity_score":0.18212464753114177},{"class_name":"TaskAssignmentUtilsTest","similarity_score":0.13318132296578225},{"class_name":"TaskBuilder","similarity_score":0.2300135339093155},{"class_name":"SinkUtils","similarity_score":0.15129773723028805},{"class_name":"ControllerMetricsTestUtils","similarity_score":0.25180366967663975},{"class_name":"ControllerRequestContextUtil","similarity_score":0.2271788710939271},{"class_name":"InternalQueryResultUtil","similarity_score":0.14136295385768194},{"class_name":"ControlRecordUtils","similarity_score":0.24003556345874155},{"class_name":"TestCloseable","similarity_score":0.21998486556185612},{"class_name":"SmokeTestUtil","similarity_score":0.2569005539240373},{"class_name":"RetryUtil","similarity_score":0.13371447673510323},{"class_name":"TestSslUtils","similarity_score":0.18501021519295105},{"class_name":"TestUtil","similarity_score":0.18257418583505539},{"class_name":"TestUtils","similarity_score":0.2160486910084376},{"class_name":"CommandLineUtils","similarity_score":0.21376809793323848},{"class_name":"CommandLineUtilsTest","similarity_score":0.10059565406206353},{"class_name":"CommandUtils","similarity_score":0.1730933273916592},{"class_name":"ClaimValidationUtils","similarity_score":0.053919697168025894},{"class_name":"ExceptionUtils","similarity_score":0.21253757099690063},{"class_name":"ConfigUtils","similarity_score":0.12122803833037532},{"class_name":"ThreadUtils","similarity_score":0.15657364440559265},{"class_name":"ClientMetricsTestUtils","similarity_score":0.2097817260387595},{"class_name":"CsvUtils","similarity_score":0.28700423092949096},{"class_name":"ClientTelemetryUtils","similarity_score":0.2613827051218073},{"class_name":"ClientUtils","similarity_score":0.23385091047739825},{"class_name":"TieredStorageTestUtils","similarity_score":0.10749932131461074},{"class_name":"ConnectorUtils","similarity_score":0.12230643125527943},{"class_name":"ConnectUtils","similarity_score":0.12762777149654544},{"class_name":"AuthorizerUtils","similarity_score":0.285462665862998},{"class_name":"AdminClientTestUtils","similarity_score":0.14465031406937198},{"class_name":"AdminUtils","similarity_score":0.14432207214070225},{"class_name":"SSLUtils","similarity_score":0.14242102273672957},{"class_name":"OAuthBearerScopeUtils","similarity_score":0.10482848367219183},{"class_name":"OAuthBearerValidationUtils","similarity_score":0.10089725124219273},{"class_name":"OAuthBearerValidationUtilsTest","similarity_score":0.18843022288032024},{"class_name":"ConsumerGroupCommandTestUtils","similarity_score":0.16467326516511843},{"class_name":"ConcurrencyUtils","similarity_score":0.0917899150453113},{"class_name":"ConsumerProtocolUtils","similarity_score":0.252838805955433},{"class_name":"MessageUtil","similarity_score":0.2516900364204447}],"target_classes_sorted_by_llm":["RequestUtils","ControlRecordUtils","MessageUtil","ConsumerProtocolUtils","NetworkTestUtils","ClientTelemetryUtils","AuthorizerUtils","CsvUtils","SmokeTestUtil","ControllerMetricsTestUtils"],"llm_response_time":13904,"similarity_computation_time":32,"similarity_metric":"tfidf"}}}
{"id":"4561db0b-3f9a-4027-9952-82da4979e233","methodCount":81,"hostFunctionTelemetryData":{"hostFunctionSize":1374,"lineStart":67,"lineEnd":1440,"bodyLineStart":67,"language":"java","filePath":"/Users/fraol/Documents/MM-assist-dataset/kafka/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/consumer/ConsumerGroup.java","sourceCode":"/**\n * A Consumer Group. All the metadata in this class are backed by\n * records in the __consumer_offsets partitions.\n */\npublic class ConsumerGroup implements Group {\n\n    public enum ConsumerGroupState {\n        EMPTY(\"Empty\"),\n        ASSIGNING(\"Assigning\"),\n        RECONCILING(\"Reconciling\"),\n        STABLE(\"Stable\"),\n        DEAD(\"Dead\");\n\n        private final String name;\n\n        private final String lowerCaseName;\n\n        ConsumerGroupState(String name) {\n            this.name \u003d name;\n            this.lowerCaseName \u003d name.toLowerCase(Locale.ROOT);\n        }\n\n        @Override\n        public String toString() {\n            return name;\n        }\n\n        public String toLowerCaseString() {\n            return lowerCaseName;\n        }\n    }\n\n    public static class DeadlineAndEpoch {\n        static final DeadlineAndEpoch EMPTY \u003d new DeadlineAndEpoch(0L, 0);\n\n        public final long deadlineMs;\n        public final int epoch;\n\n        DeadlineAndEpoch(long deadlineMs, int epoch) {\n            this.deadlineMs \u003d deadlineMs;\n            this.epoch \u003d epoch;\n        }\n    }\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The group id.\n     */\n    private final String groupId;\n\n    /**\n     * The group state.\n     */\n    private final TimelineObject\u003cConsumerGroupState\u003e state;\n\n    /**\n     * The group epoch. The epoch is incremented whenever the subscriptions\n     * are updated and it will trigger the computation of a new assignment\n     * for the group.\n     */\n    private final TimelineInteger groupEpoch;\n\n    /**\n     * The group members.\n     */\n    private final TimelineHashMap\u003cString, ConsumerGroupMember\u003e members;\n\n    /**\n     * The static group members.\n     */\n    private final TimelineHashMap\u003cString, String\u003e staticMembers;\n\n    /**\n     * The number of members supporting each server assignor name.\n     */\n    private final TimelineHashMap\u003cString, Integer\u003e serverAssignors;\n\n    /**\n     * The number of subscribers per topic.\n     */\n    private final TimelineHashMap\u003cString, Integer\u003e subscribedTopicNames;\n\n    /**\n     * The metadata associated with each subscribed topic name.\n     */\n    private final TimelineHashMap\u003cString, TopicMetadata\u003e subscribedTopicMetadata;\n\n    /**\n     * The consumer group\u0027s subscription type.\n     * This value is set to Homogeneous by default.\n     */\n    private final TimelineObject\u003cSubscriptionType\u003e subscriptionType;\n\n    /**\n     * The target assignment epoch. An assignment epoch smaller than the group epoch\n     * means that a new assignment is required. The assignment epoch is updated when\n     * a new assignment is installed.\n     */\n    private final TimelineInteger targetAssignmentEpoch;\n\n    /**\n     * The target assignment per member id.\n     */\n    private final TimelineHashMap\u003cString, Assignment\u003e targetAssignment;\n\n    /**\n     * Reverse lookup map representing topic partitions with\n     * their current member assignments.\n     */\n    private final TimelineHashMap\u003cUuid, TimelineHashMap\u003cInteger, String\u003e\u003e invertedTargetAssignment;\n\n    /**\n     * The current partition epoch maps each topic-partitions to their current epoch where\n     * the epoch is the epoch of their owners. When a member revokes a partition, it removes\n     * its epochs from this map. When a member gets a partition, it adds its epochs to this map.\n     */\n    private final TimelineHashMap\u003cUuid, TimelineHashMap\u003cInteger, Integer\u003e\u003e currentPartitionEpoch;\n\n    /**\n     * The coordinator metrics.\n     */\n    private final GroupCoordinatorMetricsShard metrics;\n\n    /**\n     * The metadata refresh deadline. It consists of a timestamp in milliseconds together with\n     * the group epoch at the time of setting it. The metadata refresh time is considered as a\n     * soft state (read that it is not stored in a timeline data structure). It is like this\n     * because it is not persisted to the log. The group epoch is here to ensure that the\n     * metadata refresh deadline is invalidated if the group epoch does not correspond to\n     * the current group epoch. This can happen if the metadata refresh deadline is updated\n     * after having refreshed the metadata but the write operation failed. In this case, the\n     * time is not automatically rolled back.\n     */\n    private DeadlineAndEpoch metadataRefreshDeadline \u003d DeadlineAndEpoch.EMPTY;\n\n    /**\n     * The number of members that use the classic protocol.\n     */\n    private final TimelineInteger numClassicProtocolMembers;\n\n    /**\n     * Map of protocol names to the number of members that use classic protocol and support them.\n     */\n    private final TimelineHashMap\u003cString, Integer\u003e classicProtocolMembersSupportedProtocols;\n\n    public ConsumerGroup(\n        SnapshotRegistry snapshotRegistry,\n        String groupId,\n        GroupCoordinatorMetricsShard metrics\n    ) {\n        this.snapshotRegistry \u003d Objects.requireNonNull(snapshotRegistry);\n        this.groupId \u003d Objects.requireNonNull(groupId);\n        this.state \u003d new TimelineObject\u003c\u003e(snapshotRegistry, EMPTY);\n        this.groupEpoch \u003d new TimelineInteger(snapshotRegistry);\n        this.members \u003d new TimelineHashMap\u003c\u003e(snapshotRegistry, 0);\n        this.staticMembers \u003d new TimelineHashMap\u003c\u003e(snapshotRegistry, 0);\n        this.serverAssignors \u003d new TimelineHashMap\u003c\u003e(snapshotRegistry, 0);\n        this.subscribedTopicNames \u003d new TimelineHashMap\u003c\u003e(snapshotRegistry, 0);\n        this.subscribedTopicMetadata \u003d new TimelineHashMap\u003c\u003e(snapshotRegistry, 0);\n        this.subscriptionType \u003d new TimelineObject\u003c\u003e(snapshotRegistry, HOMOGENEOUS);\n        this.targetAssignmentEpoch \u003d new TimelineInteger(snapshotRegistry);\n        this.targetAssignment \u003d new TimelineHashMap\u003c\u003e(snapshotRegistry, 0);\n        this.invertedTargetAssignment \u003d new TimelineHashMap\u003c\u003e(snapshotRegistry, 0);\n        this.currentPartitionEpoch \u003d new TimelineHashMap\u003c\u003e(snapshotRegistry, 0);\n        this.metrics \u003d Objects.requireNonNull(metrics);\n        this.numClassicProtocolMembers \u003d new TimelineInteger(snapshotRegistry);\n        this.classicProtocolMembersSupportedProtocols \u003d new TimelineHashMap\u003c\u003e(snapshotRegistry, 0);\n    }\n\n    /**\n     * @return The group type (Consumer).\n     */\n    @Override\n    public GroupType type() {\n        return GroupType.CONSUMER;\n    }\n\n    /**\n     * @return The current state as a String.\n     */\n    @Override\n    public String stateAsString() {\n        return state.get().toString();\n    }\n\n    /**\n     * @return The current state as a String with given committedOffset.\n     */\n    public String stateAsString(long committedOffset) {\n        return state.get(committedOffset).toString();\n    }\n\n    /**\n     * @return the group formatted as a list group response based on the committed offset.\n     */\n    public ListGroupsResponseData.ListedGroup asListedGroup(long committedOffset) {\n        return new ListGroupsResponseData.ListedGroup()\n            .setGroupId(groupId)\n            .setProtocolType(ConsumerProtocol.PROTOCOL_TYPE)\n            .setGroupState(state.get(committedOffset).toString())\n            .setGroupType(type().toString());\n    }\n\n    /**\n     * @return The group id.\n     */\n    @Override\n    public String groupId() {\n        return groupId;\n    }\n\n    /**\n     * @return The current state.\n     */\n    public ConsumerGroupState state() {\n        return state.get();\n    }\n\n    /**\n     * @return The current state based on committed offset.\n     */\n    public ConsumerGroupState state(long committedOffset) {\n        return state.get(committedOffset);\n    }\n\n    /**\n     * @return The group epoch.\n     */\n    public int groupEpoch() {\n        return groupEpoch.get();\n    }\n\n    /**\n     * Sets the group epoch.\n     *\n     * @param groupEpoch The new group epoch.\n     */\n    public void setGroupEpoch(int groupEpoch) {\n        this.groupEpoch.set(groupEpoch);\n        maybeUpdateGroupState();\n    }\n\n    /**\n     * @return The target assignment epoch.\n     */\n    public int assignmentEpoch() {\n        return targetAssignmentEpoch.get();\n    }\n\n    /**\n     * Sets the assignment epoch.\n     *\n     * @param targetAssignmentEpoch The new assignment epoch.\n     */\n    public void setTargetAssignmentEpoch(int targetAssignmentEpoch) {\n        this.targetAssignmentEpoch.set(targetAssignmentEpoch);\n        maybeUpdateGroupState();\n    }\n\n    /**\n     * Sets the number of members using the classic protocol.\n     *\n     * @param numClassicProtocolMembers The new NumClassicProtocolMembers.\n     */\n    public void setNumClassicProtocolMembers(int numClassicProtocolMembers) {\n        this.numClassicProtocolMembers.set(numClassicProtocolMembers);\n    }\n\n    /**\n     * Get member id of a static member that matches the given group\n     * instance id.\n     *\n     * @param groupInstanceId The group instance id.\n     *\n     * @return The member id corresponding to the given instance id or null if it does not exist\n     */\n    public String staticMemberId(String groupInstanceId) {\n        return staticMembers.get(groupInstanceId);\n    }\n\n    /**\n     * Gets or creates a new member but without adding it to the group. Adding a member\n     * is done via the {@link ConsumerGroup#updateMember(ConsumerGroupMember)} method.\n     *\n     * @param memberId          The member id.\n     * @param createIfNotExists Booleans indicating whether the member must be\n     *                          created if it does not exist.\n     *\n     * @return A ConsumerGroupMember.\n     */\n    public ConsumerGroupMember getOrMaybeCreateMember(\n        String memberId,\n        boolean createIfNotExists\n    ) {\n        ConsumerGroupMember member \u003d members.get(memberId);\n        if (member !\u003d null) return member;\n\n        if (!createIfNotExists) {\n            throw new UnknownMemberIdException(\n                String.format(\"Member %s is not a member of group %s.\", memberId, groupId)\n            );\n        }\n\n        return new ConsumerGroupMember.Builder(memberId).build();\n    }\n\n    /**\n     * Gets a static member.\n     *\n     * @param instanceId The group instance id.\n     *\n     * @return The member corresponding to the given instance id or null if it does not exist\n     */\n    public ConsumerGroupMember staticMember(String instanceId) {\n        String existingMemberId \u003d staticMemberId(instanceId);\n        return existingMemberId \u003d\u003d null ? null : getOrMaybeCreateMember(existingMemberId, false);\n    }\n\n    /**\n     * Adds or updates the member.\n     *\n     * @param newMember The new member state.\n     */\n    public void updateMember(ConsumerGroupMember newMember) {\n        if (newMember \u003d\u003d null) {\n            throw new IllegalArgumentException(\"newMember cannot be null.\");\n        }\n        ConsumerGroupMember oldMember \u003d members.put(newMember.memberId(), newMember);\n        maybeUpdateSubscribedTopicNamesAndGroupSubscriptionType(oldMember, newMember);\n        maybeUpdateServerAssignors(oldMember, newMember);\n        maybeUpdatePartitionEpoch(oldMember, newMember);\n        updateStaticMember(newMember);\n        maybeUpdateGroupState();\n        maybeUpdateNumClassicProtocolMembers(oldMember, newMember);\n        maybeUpdateClassicProtocolMembersSupportedProtocols(oldMember, newMember);\n    }\n\n    /**\n     * Updates the member id stored against the instance id if the member is a static member.\n     *\n     * @param newMember The new member state.\n     */\n    private void updateStaticMember(ConsumerGroupMember newMember) {\n        if (newMember.instanceId() !\u003d null) {\n            staticMembers.put(newMember.instanceId(), newMember.memberId());\n        }\n    }\n\n    /**\n     * Remove the member from the group.\n     *\n     * @param memberId The member id to remove.\n     */\n    public void removeMember(String memberId) {\n        ConsumerGroupMember oldMember \u003d members.remove(memberId);\n        maybeUpdateSubscribedTopicNamesAndGroupSubscriptionType(oldMember, null);\n        maybeUpdateServerAssignors(oldMember, null);\n        maybeRemovePartitionEpoch(oldMember);\n        removeStaticMember(oldMember);\n        maybeUpdateGroupState();\n        maybeUpdateNumClassicProtocolMembers(oldMember, null);\n        maybeUpdateClassicProtocolMembersSupportedProtocols(oldMember, null);\n    }\n\n    /**\n     * Remove the static member mapping if the removed member is static.\n     *\n     * @param oldMember The member to remove.\n     */\n    private void removeStaticMember(ConsumerGroupMember oldMember) {\n        if (oldMember.instanceId() !\u003d null) {\n            staticMembers.remove(oldMember.instanceId());\n        }\n    }\n\n    /**\n     * Returns true if the member exists.\n     *\n     * @param memberId The member id.\n     *\n     * @return A boolean indicating whether the member exists or not.\n     */\n    public boolean hasMember(String memberId) {\n        return members.containsKey(memberId);\n    }\n\n    /**\n     * @return The number of members.\n     */\n    public int numMembers() {\n        return members.size();\n    }\n\n    /**\n     * @return The number of members that use the classic protocol.\n     */\n    public int numClassicProtocolMembers() {\n        return numClassicProtocolMembers.get();\n    }\n\n    /**\n     * @return The map of the protocol name and the number of members using the classic protocol that support it.\n     */\n    public Map\u003cString, Integer\u003e classicMembersSupportedProtocols() {\n        return Collections.unmodifiableMap(classicProtocolMembersSupportedProtocols);\n    }\n\n    /**\n     * @return An immutable Map containing all the members keyed by their id.\n     */\n    public Map\u003cString, ConsumerGroupMember\u003e members() {\n        return Collections.unmodifiableMap(members);\n    }\n\n    /**\n     * @return An immutable Map containing all the static members keyed by instance id.\n     */\n    public Map\u003cString, String\u003e staticMembers() {\n        return Collections.unmodifiableMap(staticMembers);\n    }\n\n    /**\n     * @return An immutable map containing all the subscribed topic names\n     *         with the subscribers counts per topic.\n     */\n    public Map\u003cString, Integer\u003e subscribedTopicNames() {\n        return Collections.unmodifiableMap(subscribedTopicNames);\n    }\n\n    /**\n     * Returns true if the consumer group is actively subscribed to the topic.\n     *\n     * @param topic  The topic name.\n     *\n     * @return Whether the group is subscribed to the topic.\n     */\n    @Override\n    public boolean isSubscribedToTopic(String topic) {\n        return subscribedTopicNames.containsKey(topic);\n    }\n\n    /**\n     * @return The group\u0027s subscription type.\n     */\n    public SubscriptionType subscriptionType() {\n        return subscriptionType.get();\n    }\n\n    /**\n     * Returns the target assignment of the member.\n     *\n     * @return The ConsumerGroupMemberAssignment or an EMPTY one if it does not\n     *         exist.\n     */\n    public Assignment targetAssignment(String memberId) {\n        return targetAssignment.getOrDefault(memberId, Assignment.EMPTY);\n    }\n\n    /**\n     * @return An immutable map containing all the topic partitions\n     *         with their current member assignments.\n     */\n    public Map\u003cUuid, Map\u003cInteger, String\u003e\u003e invertedTargetAssignment() {\n        return Collections.unmodifiableMap(invertedTargetAssignment);\n    }\n\n    /**\n     * Updates the target assignment of a member.\n     *\n     * @param memberId              The member id.\n     * @param newTargetAssignment   The new target assignment.\n     */\n    public void updateTargetAssignment(String memberId, Assignment newTargetAssignment) {\n        updateInvertedTargetAssignment(\n            memberId,\n            targetAssignment.getOrDefault(memberId, new Assignment(Collections.emptyMap())),\n            newTargetAssignment\n        );\n        targetAssignment.put(memberId, newTargetAssignment);\n    }\n\n    /**\n     * Updates the reverse lookup map of the target assignment.\n     *\n     * @param memberId              The member Id.\n     * @param oldTargetAssignment   The old target assignment.\n     * @param newTargetAssignment   The new target assignment.\n     */\n    private void updateInvertedTargetAssignment(\n        String memberId,\n        Assignment oldTargetAssignment,\n        Assignment newTargetAssignment\n    ) {\n        // Combine keys from both old and new assignments.\n        Set\u003cUuid\u003e allTopicIds \u003d new HashSet\u003c\u003e();\n        allTopicIds.addAll(oldTargetAssignment.partitions().keySet());\n        allTopicIds.addAll(newTargetAssignment.partitions().keySet());\n\n        for (Uuid topicId : allTopicIds) {\n            Set\u003cInteger\u003e oldPartitions \u003d oldTargetAssignment.partitions().getOrDefault(topicId, Collections.emptySet());\n            Set\u003cInteger\u003e newPartitions \u003d newTargetAssignment.partitions().getOrDefault(topicId, Collections.emptySet());\n\n            TimelineHashMap\u003cInteger, String\u003e topicPartitionAssignment \u003d invertedTargetAssignment.computeIfAbsent(\n                topicId, k -\u003e new TimelineHashMap\u003c\u003e(snapshotRegistry, Math.max(oldPartitions.size(), newPartitions.size()))\n            );\n\n            // Remove partitions that aren\u0027t present in the new assignment only if the partition is currently\n            // still assigned to the member in question.\n            // If p0 was moved from A to B, and the target assignment map was updated for B first, we don\u0027t want to\n            // remove the key p0 from the inverted map and undo the action when A eventually tries to update its assignment.\n            for (Integer partition : oldPartitions) {\n                if (!newPartitions.contains(partition) \u0026\u0026 memberId.equals(topicPartitionAssignment.get(partition))) {\n                    topicPartitionAssignment.remove(partition);\n                }\n            }\n\n            // Add partitions that are in the new assignment but not in the old assignment.\n            for (Integer partition : newPartitions) {\n                if (!oldPartitions.contains(partition)) {\n                    topicPartitionAssignment.put(partition, memberId);\n                }\n            }\n\n            if (topicPartitionAssignment.isEmpty()) {\n                invertedTargetAssignment.remove(topicId);\n            } else {\n                invertedTargetAssignment.put(topicId, topicPartitionAssignment);\n            }\n        }\n    }\n\n    /**\n     * Removes the target assignment of a member.\n     *\n     * @param memberId The member id.\n     */\n    public void removeTargetAssignment(String memberId) {\n        updateInvertedTargetAssignment(\n            memberId,\n            targetAssignment.getOrDefault(memberId, Assignment.EMPTY),\n            Assignment.EMPTY\n        );\n        targetAssignment.remove(memberId);\n    }\n\n    /**\n     * @return An immutable Map containing all the target assignment keyed by member id.\n     */\n    public Map\u003cString, Assignment\u003e targetAssignment() {\n        return Collections.unmodifiableMap(targetAssignment);\n    }\n\n    /**\n     * Returns the current epoch of a partition or -1 if the partition\n     * does not have one.\n     *\n     * @param topicId       The topic id.\n     * @param partitionId   The partition id.\n     *\n     * @return The epoch or -1.\n     */\n    public int currentPartitionEpoch(\n        Uuid topicId, int partitionId\n    ) {\n        Map\u003cInteger, Integer\u003e partitions \u003d currentPartitionEpoch.get(topicId);\n        if (partitions \u003d\u003d null) {\n            return -1;\n        } else {\n            return partitions.getOrDefault(partitionId, -1);\n        }\n    }\n\n    /**\n     * Compute the preferred (server side) assignor for the group while\n     * taking into account the updated member. The computation relies\n     * on {{@link ConsumerGroup#serverAssignors}} persisted structure\n     * but it does not update it.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     *\n     * @return An Optional containing the preferred assignor.\n     */\n    public Optional\u003cString\u003e computePreferredServerAssignor(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        // Copy the current count and update it.\n        Map\u003cString, Integer\u003e counts \u003d new HashMap\u003c\u003e(this.serverAssignors);\n        maybeUpdateServerAssignors(counts, oldMember, newMember);\n\n        return counts.entrySet().stream()\n            .max(Map.Entry.comparingByValue())\n            .map(Map.Entry::getKey);\n    }\n\n    /**\n     * @return The preferred assignor for the group.\n     */\n    public Optional\u003cString\u003e preferredServerAssignor() {\n        return preferredServerAssignor(Long.MAX_VALUE);\n    }\n\n    /**\n     * @return The preferred assignor for the group with given offset.\n     */\n    public Optional\u003cString\u003e preferredServerAssignor(long committedOffset) {\n        return serverAssignors.entrySet(committedOffset).stream()\n            .max(Map.Entry.comparingByValue())\n            .map(Map.Entry::getKey);\n    }\n\n    /**\n     * @return An immutable Map of subscription metadata for\n     *         each topic that the consumer group is subscribed to.\n     */\n    public Map\u003cString, TopicMetadata\u003e subscriptionMetadata() {\n        return Collections.unmodifiableMap(subscribedTopicMetadata);\n    }\n\n    /**\n     * Updates the subscription metadata. This replaces the previous one.\n     *\n     * @param subscriptionMetadata The new subscription metadata.\n     */\n    public void setSubscriptionMetadata(\n        Map\u003cString, TopicMetadata\u003e subscriptionMetadata\n    ) {\n        this.subscribedTopicMetadata.clear();\n        this.subscribedTopicMetadata.putAll(subscriptionMetadata);\n    }\n\n    /**\n     * Computes the subscription metadata based on the current subscription info.\n     *\n     * @param subscribedTopicNames      Map of topic names to the number of subscribers.\n     * @param topicsImage               The current metadata for all available topics.\n     * @param clusterImage              The current metadata for the Kafka cluster.\n     *\n     * @return An immutable map of subscription metadata for each topic that the consumer group is subscribed to.\n     */\n    public Map\u003cString, TopicMetadata\u003e computeSubscriptionMetadata(\n        Map\u003cString, Integer\u003e subscribedTopicNames,\n        TopicsImage topicsImage,\n        ClusterImage clusterImage\n    ) {\n        // Create the topic metadata for each subscribed topic.\n        Map\u003cString, TopicMetadata\u003e newSubscriptionMetadata \u003d new HashMap\u003c\u003e(subscribedTopicNames.size());\n\n        subscribedTopicNames.forEach((topicName, count) -\u003e {\n            TopicImage topicImage \u003d topicsImage.getTopic(topicName);\n            if (topicImage !\u003d null) {\n                Map\u003cInteger, Set\u003cString\u003e\u003e partitionRacks \u003d new HashMap\u003c\u003e();\n                topicImage.partitions().forEach((partition, partitionRegistration) -\u003e {\n                    Set\u003cString\u003e racks \u003d new HashSet\u003c\u003e();\n                    for (int replica : partitionRegistration.replicas) {\n                        Optional\u003cString\u003e rackOptional \u003d clusterImage.broker(replica).rack();\n                        // Only add the rack if it is available for the broker/replica.\n                        rackOptional.ifPresent(racks::add);\n                    }\n                    // If rack information is unavailable for all replicas of this partition,\n                    // no corresponding entry will be stored for it in the map.\n                    if (!racks.isEmpty())\n                        partitionRacks.put(partition, racks);\n                });\n\n                newSubscriptionMetadata.put(topicName, new TopicMetadata(\n                    topicImage.id(),\n                    topicImage.name(),\n                    topicImage.partitions().size(),\n                    partitionRacks)\n                );\n            }\n        });\n\n        return Collections.unmodifiableMap(newSubscriptionMetadata);\n    }\n\n    /**\n     * Updates the metadata refresh deadline.\n     *\n     * @param deadlineMs The deadline in milliseconds.\n     * @param groupEpoch The associated group epoch.\n     */\n    public void setMetadataRefreshDeadline(\n        long deadlineMs,\n        int groupEpoch\n    ) {\n        this.metadataRefreshDeadline \u003d new DeadlineAndEpoch(deadlineMs, groupEpoch);\n    }\n\n    /**\n     * Requests a metadata refresh.\n     */\n    public void requestMetadataRefresh() {\n        this.metadataRefreshDeadline \u003d DeadlineAndEpoch.EMPTY;\n    }\n\n    /**\n     * Checks if a metadata refresh is required. A refresh is required in two cases:\n     * 1) The deadline is smaller or equal to the current time;\n     * 2) The group epoch associated with the deadline is larger than\n     *    the current group epoch. This means that the operations which updated\n     *    the deadline failed.\n     *\n     * @param currentTimeMs The current time in milliseconds.\n     * @return A boolean indicating whether a refresh is required or not.\n     */\n    public boolean hasMetadataExpired(long currentTimeMs) {\n        return currentTimeMs \u003e\u003d metadataRefreshDeadline.deadlineMs || groupEpoch() \u003c metadataRefreshDeadline.epoch;\n    }\n\n    /**\n     * @return The metadata refresh deadline.\n     */\n    public DeadlineAndEpoch metadataRefreshDeadline() {\n        return metadataRefreshDeadline;\n    }\n\n    /**\n     * Validates the OffsetCommit request.\n     *\n     * @param memberId          The member id.\n     * @param groupInstanceId   The group instance id.\n     * @param memberEpoch       The member epoch.\n     * @param isTransactional   Whether the offset commit is transactional or not. It has no\n     *                          impact when a consumer group is used.\n     * @param apiVersion        The api version.\n     * @throws UnknownMemberIdException     If the member is not found.\n     * @throws StaleMemberEpochException    If the member uses the consumer protocol and the provided\n     *                                      member epoch doesn\u0027t match the actual member epoch.\n     * @throws IllegalGenerationException   If the member uses the classic protocol and the provided\n     *                                      generation id is not equal to the member epoch.\n     */\n    @Override\n    public void validateOffsetCommit(\n        String memberId,\n        String groupInstanceId,\n        int memberEpoch,\n        boolean isTransactional,\n        short apiVersion\n    ) throws UnknownMemberIdException, StaleMemberEpochException, IllegalGenerationException {\n        // When the member epoch is -1, the request comes from either the admin client\n        // or a consumer which does not use the group management facility. In this case,\n        // the request can commit offsets if the group is empty.\n        if (memberEpoch \u003c 0 \u0026\u0026 members().isEmpty()) return;\n\n        final ConsumerGroupMember member \u003d getOrMaybeCreateMember(memberId, false);\n\n        // If the commit is not transactional and the member uses the new consumer protocol (KIP-848),\n        // the member should be using the OffsetCommit API version \u003e\u003d 9.\n        if (!isTransactional \u0026\u0026 !member.useClassicProtocol() \u0026\u0026 apiVersion \u003c 9) {\n            throw new UnsupportedVersionException(\"OffsetCommit version 9 or above must be used \" +\n                \"by members using the consumer group protocol\");\n        }\n\n        validateMemberEpoch(memberEpoch, member.memberEpoch(), member.useClassicProtocol());\n    }\n\n    /**\n     * Validates the OffsetFetch request.\n     *\n     * @param memberId              The member id for consumer groups.\n     * @param memberEpoch           The member epoch for consumer groups.\n     * @param lastCommittedOffset   The last committed offsets in the timeline.\n     * @throws UnknownMemberIdException     If the member is not found.\n     * @throws StaleMemberEpochException    If the member uses the consumer protocol and the provided\n     *                                      member epoch doesn\u0027t match the actual member epoch.\n     * @throws IllegalGenerationException   If the member uses the classic protocol and the provided\n     *                                      generation id is not equal to the member epoch.\n     */\n    @Override\n    public void validateOffsetFetch(\n        String memberId,\n        int memberEpoch,\n        long lastCommittedOffset\n    ) throws UnknownMemberIdException, StaleMemberEpochException, IllegalGenerationException {\n        // When the member id is null and the member epoch is -1, the request either comes\n        // from the admin client or from a client which does not provide them. In this case,\n        // the fetch request is accepted.\n        if (memberId \u003d\u003d null \u0026\u0026 memberEpoch \u003c 0) return;\n\n        final ConsumerGroupMember member \u003d members.get(memberId, lastCommittedOffset);\n        if (member \u003d\u003d null) {\n            throw new UnknownMemberIdException(String.format(\"Member %s is not a member of group %s.\",\n                memberId, groupId));\n        }\n        validateMemberEpoch(memberEpoch, member.memberEpoch(), member.useClassicProtocol());\n    }\n\n    /**\n     * Validates the OffsetDelete request.\n     */\n    @Override\n    public void validateOffsetDelete() {}\n\n    /**\n     * Validates the DeleteGroups request.\n     */\n    @Override\n    public void validateDeleteGroup() throws ApiException {\n        if (state() !\u003d ConsumerGroupState.EMPTY) {\n            throw Errors.NON_EMPTY_GROUP.exception();\n        }\n    }\n\n    /**\n     * Populates the list of records with tombstone(s) for deleting the group.\n     *\n     * @param records The list of records.\n     */\n    @Override\n    public void createGroupTombstoneRecords(List\u003cCoordinatorRecord\u003e records) {\n        members().forEach((memberId, member) -\u003e\n            records.add(CoordinatorRecordHelpers.newCurrentAssignmentTombstoneRecord(groupId(), memberId))\n        );\n\n        members().forEach((memberId, member) -\u003e\n            records.add(CoordinatorRecordHelpers.newTargetAssignmentTombstoneRecord(groupId(), memberId))\n        );\n        records.add(CoordinatorRecordHelpers.newTargetAssignmentEpochTombstoneRecord(groupId()));\n\n        members().forEach((memberId, member) -\u003e\n            records.add(CoordinatorRecordHelpers.newMemberSubscriptionTombstoneRecord(groupId(), memberId))\n        );\n\n        records.add(CoordinatorRecordHelpers.newGroupSubscriptionMetadataTombstoneRecord(groupId()));\n        records.add(CoordinatorRecordHelpers.newGroupEpochTombstoneRecord(groupId()));\n    }\n\n    @Override\n    public boolean isEmpty() {\n        return state() \u003d\u003d ConsumerGroupState.EMPTY;\n    }\n\n    /**\n     * See {@link org.apache.kafka.coordinator.group.OffsetExpirationCondition}\n     *\n     * @return The offset expiration condition for the group or Empty if no such condition exists.\n     */\n    @Override\n    public Optional\u003cOffsetExpirationCondition\u003e offsetExpirationCondition() {\n        return Optional.of(new OffsetExpirationConditionImpl(offsetAndMetadata -\u003e offsetAndMetadata.commitTimestampMs));\n    }\n\n    @Override\n    public boolean isInStates(Set\u003cString\u003e statesFilter, long committedOffset) {\n        return statesFilter.contains(state.get(committedOffset).toLowerCaseString());\n    }\n\n    /**\n     * Throws an exception if the received member epoch does not match the expected member epoch.\n     *\n     * @param receivedMemberEpoch   The received member epoch or generation id.\n     * @param expectedMemberEpoch   The expected member epoch.\n     * @param useClassicProtocol    The boolean indicating whether the checked member uses the classic protocol.\n     * @throws StaleMemberEpochException    if the member with unmatched member epoch uses the consumer protocol.\n     * @throws IllegalGenerationException   if the member with unmatched generation id uses the classic protocol.\n     */\n    private void validateMemberEpoch(\n        int receivedMemberEpoch,\n        int expectedMemberEpoch,\n        boolean useClassicProtocol\n    ) throws StaleMemberEpochException, IllegalGenerationException {\n        if (receivedMemberEpoch !\u003d expectedMemberEpoch) {\n            if (useClassicProtocol) {\n                throw new IllegalGenerationException(String.format(\"The received generation id %d does not match \" +\n                    \"the expected member epoch %d.\", receivedMemberEpoch, expectedMemberEpoch));\n            } else {\n                throw new StaleMemberEpochException(String.format(\"The received member epoch %d does not match \"\n                    + \"the expected member epoch %d.\", receivedMemberEpoch, expectedMemberEpoch));\n            }\n        }\n    }\n\n    /**\n     * Updates the current state of the group.\n     */\n    private void maybeUpdateGroupState() {\n        ConsumerGroupState previousState \u003d state.get();\n        ConsumerGroupState newState \u003d STABLE;\n        if (members.isEmpty()) {\n            newState \u003d EMPTY;\n        } else if (groupEpoch.get() \u003e targetAssignmentEpoch.get()) {\n            newState \u003d ASSIGNING;\n        } else {\n            for (ConsumerGroupMember member : members.values()) {\n                if (!member.isReconciledTo(targetAssignmentEpoch.get())) {\n                    newState \u003d RECONCILING;\n                    break;\n                }\n            }\n        }\n\n        state.set(newState);\n        metrics.onConsumerGroupStateTransition(previousState, newState);\n    }\n\n    /**\n     * Updates the server assignors count.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdateServerAssignors(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        maybeUpdateServerAssignors(serverAssignors, oldMember, newMember);\n    }\n\n    /**\n     * Updates the server assignors count.\n     *\n     * @param serverAssignorCount   The count to update.\n     * @param oldMember             The old member.\n     * @param newMember             The new member.\n     */\n    private static void maybeUpdateServerAssignors(\n        Map\u003cString, Integer\u003e serverAssignorCount,\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        if (oldMember !\u003d null) {\n            oldMember.serverAssignorName().ifPresent(name -\u003e\n                serverAssignorCount.compute(name, ConsumerGroup::decValue)\n            );\n        }\n        if (newMember !\u003d null) {\n            newMember.serverAssignorName().ifPresent(name -\u003e\n                serverAssignorCount.compute(name, ConsumerGroup::incValue)\n            );\n        }\n    }\n\n    /**\n     * Updates the number of the members that use the classic protocol.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdateNumClassicProtocolMembers(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        int delta \u003d 0;\n        if (oldMember !\u003d null \u0026\u0026 oldMember.useClassicProtocol()) {\n            delta--;\n        }\n        if (newMember !\u003d null \u0026\u0026 newMember.useClassicProtocol()) {\n            delta++;\n        }\n        setNumClassicProtocolMembers(numClassicProtocolMembers() + delta);\n    }\n\n    /**\n     * Updates the supported protocol count of the members that use the classic protocol.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdateClassicProtocolMembersSupportedProtocols(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        if (oldMember !\u003d null) {\n            oldMember.supportedClassicProtocols().ifPresent(protocols -\u003e\n                protocols.forEach(protocol -\u003e\n                    classicProtocolMembersSupportedProtocols.compute(protocol.name(), ConsumerGroup::decValue)\n                )\n            );\n        }\n        if (newMember !\u003d null) {\n            newMember.supportedClassicProtocols().ifPresent(protocols -\u003e\n                protocols.forEach(protocol -\u003e\n                    classicProtocolMembersSupportedProtocols.compute(protocol.name(), ConsumerGroup::incValue)\n                )\n            );\n        }\n    }\n\n    /**\n     * Updates the subscribed topic names count.\n     * The subscription type is updated as a consequence.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdateSubscribedTopicNamesAndGroupSubscriptionType(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        maybeUpdateSubscribedTopicNames(subscribedTopicNames, oldMember, newMember);\n        subscriptionType.set(subscriptionType(subscribedTopicNames, members.size()));\n    }\n\n    /**\n     * Updates the subscription count.\n     *\n     * @param subscribedTopicCount  The map to update.\n     * @param oldMember             The old member.\n     * @param newMember             The new member.\n     */\n    private static void maybeUpdateSubscribedTopicNames(\n        Map\u003cString, Integer\u003e subscribedTopicCount,\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        if (oldMember !\u003d null) {\n            oldMember.subscribedTopicNames().forEach(topicName -\u003e\n                subscribedTopicCount.compute(topicName, ConsumerGroup::decValue)\n            );\n        }\n\n        if (newMember !\u003d null) {\n            newMember.subscribedTopicNames().forEach(topicName -\u003e\n                subscribedTopicCount.compute(topicName, ConsumerGroup::incValue)\n            );\n        }\n    }\n\n    /**\n     * Updates the subscription count.\n     *\n     * @param oldMember             The old member.\n     * @param newMember             The new member.\n     *\n     * @return Copy of the map of topics to the count of number of subscribers.\n     */\n    public Map\u003cString, Integer\u003e computeSubscribedTopicNames(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        Map\u003cString, Integer\u003e subscribedTopicNames \u003d new HashMap\u003c\u003e(this.subscribedTopicNames);\n        maybeUpdateSubscribedTopicNames(\n            subscribedTopicNames,\n            oldMember,\n            newMember\n        );\n        return subscribedTopicNames;\n    }\n\n    /**\n     * Updates the subscription count with a set of members removed.\n     *\n     * @param removedMembers        The set of removed members.\n     *\n     * @return Copy of the map of topics to the count of number of subscribers.\n     */\n    public Map\u003cString, Integer\u003e computeSubscribedTopicNames(\n        Set\u003cConsumerGroupMember\u003e removedMembers\n    ) {\n        Map\u003cString, Integer\u003e subscribedTopicNames \u003d new HashMap\u003c\u003e(this.subscribedTopicNames);\n        if (removedMembers !\u003d null) {\n            removedMembers.forEach(removedMember -\u003e\n                maybeUpdateSubscribedTopicNames(\n                    subscribedTopicNames,\n                    removedMember,\n                    null\n                )\n            );\n        }\n        return subscribedTopicNames;\n    }\n\n    /**\n     * Compute the subscription type of the consumer group.\n     *\n     * @param subscribedTopicNames      A map of topic names to the count of members subscribed to each topic.\n     *\n     * @return {@link SubscriptionType#HOMOGENEOUS} if all members are subscribed to exactly the same topics;\n     *         otherwise, {@link SubscriptionType#HETEROGENEOUS}.\n     */\n    public static SubscriptionType subscriptionType(\n        Map\u003cString, Integer\u003e subscribedTopicNames,\n        int numberOfMembers\n    ) {\n        if (subscribedTopicNames.isEmpty()) {\n            return HOMOGENEOUS;\n        }\n\n        for (int subscriberCount : subscribedTopicNames.values()) {\n            if (subscriberCount !\u003d numberOfMembers) {\n                return HETEROGENEOUS;\n            }\n        }\n        return HOMOGENEOUS;\n    }\n\n    /**\n     * Updates the partition epochs based on the old and the new member.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdatePartitionEpoch(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        maybeRemovePartitionEpoch(oldMember);\n        addPartitionEpochs(newMember.assignedPartitions(), newMember.memberEpoch());\n        addPartitionEpochs(newMember.partitionsPendingRevocation(), newMember.memberEpoch());\n    }\n\n    /**\n     * Removes the partition epochs for the provided member.\n     *\n     * @param oldMember The old member.\n     */\n    private void maybeRemovePartitionEpoch(\n        ConsumerGroupMember oldMember\n    ) {\n        if (oldMember !\u003d null) {\n            removePartitionEpochs(oldMember.assignedPartitions(), oldMember.memberEpoch());\n            removePartitionEpochs(oldMember.partitionsPendingRevocation(), oldMember.memberEpoch());\n        }\n    }\n\n    /**\n     * Removes the partition epochs based on the provided assignment.\n     *\n     * @param assignment    The assignment.\n     * @param expectedEpoch The expected epoch.\n     * @throws IllegalStateException if the epoch does not match the expected one.\n     * package-private for testing.\n     */\n    void removePartitionEpochs(\n        Map\u003cUuid, Set\u003cInteger\u003e\u003e assignment,\n        int expectedEpoch\n    ) {\n        assignment.forEach((topicId, assignedPartitions) -\u003e {\n            currentPartitionEpoch.compute(topicId, (__, partitionsOrNull) -\u003e {\n                if (partitionsOrNull !\u003d null) {\n                    assignedPartitions.forEach(partitionId -\u003e {\n                        Integer prevValue \u003d partitionsOrNull.remove(partitionId);\n                        if (prevValue !\u003d expectedEpoch) {\n                            throw new IllegalStateException(\n                                String.format(\"Cannot remove the epoch %d from %s-%s because the partition is \" +\n                                    \"still owned at a different epoch %d\", expectedEpoch, topicId, partitionId, prevValue));\n                        }\n                    });\n                    if (partitionsOrNull.isEmpty()) {\n                        return null;\n                    } else {\n                        return partitionsOrNull;\n                    }\n                } else {\n                    throw new IllegalStateException(\n                        String.format(\"Cannot remove the epoch %d from %s because it does not have any epoch\",\n                            expectedEpoch, topicId));\n                }\n            });\n        });\n    }\n\n    /**\n     * Adds the partitions epoch based on the provided assignment.\n     *\n     * @param assignment    The assignment.\n     * @param epoch         The new epoch.\n     * @throws IllegalStateException if the partition already has an epoch assigned.\n     * package-private for testing.\n     */\n    void addPartitionEpochs(\n        Map\u003cUuid, Set\u003cInteger\u003e\u003e assignment,\n        int epoch\n    ) {\n        assignment.forEach((topicId, assignedPartitions) -\u003e {\n            currentPartitionEpoch.compute(topicId, (__, partitionsOrNull) -\u003e {\n                if (partitionsOrNull \u003d\u003d null) {\n                    partitionsOrNull \u003d new TimelineHashMap\u003c\u003e(snapshotRegistry, assignedPartitions.size());\n                }\n                for (Integer partitionId : assignedPartitions) {\n                    Integer prevValue \u003d partitionsOrNull.put(partitionId, epoch);\n                    if (prevValue !\u003d null) {\n                        throw new IllegalStateException(\n                            String.format(\"Cannot set the epoch of %s-%s to %d because the partition is \" +\n                                \"still owned at epoch %d\", topicId, partitionId, epoch, prevValue));\n                    }\n                }\n                return partitionsOrNull;\n            });\n        });\n    }\n\n    /**\n     * Decrements value by 1; returns null when reaching zero. This helper is\n     * meant to be used with Map#compute.\n     */\n    private static Integer decValue(String key, Integer value) {\n        if (value \u003d\u003d null) return null;\n        return value \u003d\u003d 1 ? null : value - 1;\n    }\n\n    /**\n     * Increments value by 1; This helper is meant to be used with Map#compute.\n     */\n    private static Integer incValue(String key, Integer value) {\n        return value \u003d\u003d null ? 1 : value + 1;\n    }\n\n    public ConsumerGroupDescribeResponseData.DescribedGroup asDescribedGroup(\n        long committedOffset,\n        String defaultAssignor,\n        TopicsImage topicsImage\n    ) {\n        ConsumerGroupDescribeResponseData.DescribedGroup describedGroup \u003d new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(groupId)\n            .setAssignorName(preferredServerAssignor(committedOffset).orElse(defaultAssignor))\n            .setGroupEpoch(groupEpoch.get(committedOffset))\n            .setGroupState(state.get(committedOffset).toString())\n            .setAssignmentEpoch(targetAssignmentEpoch.get(committedOffset));\n        members.entrySet(committedOffset).forEach(\n            entry -\u003e describedGroup.members().add(\n                entry.getValue().asConsumerGroupDescribeMember(\n                    targetAssignment.get(entry.getValue().memberId(), committedOffset),\n                    topicsImage\n                )\n            )\n        );\n        return describedGroup;\n    }\n\n    /**\n     * Create a new consumer group according to the given classic group.\n     *\n     * @param snapshotRegistry  The SnapshotRegistry.\n     * @param metrics           The GroupCoordinatorMetricsShard.\n     * @param classicGroup      The converted classic group.\n     * @param topicsImage       The TopicsImage for topic id and topic name conversion.\n     * @return  The created ConsumerGruop.\n     */\n    public static ConsumerGroup fromClassicGroup(\n        SnapshotRegistry snapshotRegistry,\n        GroupCoordinatorMetricsShard metrics,\n        ClassicGroup classicGroup,\n        TopicsImage topicsImage\n    ) {\n        String groupId \u003d classicGroup.groupId();\n        ConsumerGroup consumerGroup \u003d new ConsumerGroup(snapshotRegistry, groupId, metrics);\n        consumerGroup.setGroupEpoch(classicGroup.generationId());\n        consumerGroup.setTargetAssignmentEpoch(classicGroup.generationId());\n\n        classicGroup.allMembers().forEach(classicGroupMember -\u003e {\n            Map\u003cUuid, Set\u003cInteger\u003e\u003e assignedPartitions \u003d toTopicPartitionMap(\n                ConsumerProtocol.deserializeConsumerProtocolAssignment(\n                    ByteBuffer.wrap(classicGroupMember.assignment())\n                ),\n                topicsImage\n            );\n            ConsumerProtocolSubscription subscription \u003d ConsumerProtocol.deserializeConsumerProtocolSubscription(\n                ByteBuffer.wrap(classicGroupMember.metadata(classicGroup.protocolName().get()))\n            );\n\n            // The target assignment and the assigned partitions of each member are set based on the last\n            // assignment of the classic group. All the members are put in the Stable state. If the classic\n            // group was in Preparing Rebalance or Completing Rebalance states, the classic members are\n            // asked to rejoin the group to re-trigger a rebalance or collect their assignments.\n            ConsumerGroupMember newMember \u003d new ConsumerGroupMember.Builder(classicGroupMember.memberId())\n                .setMemberEpoch(classicGroup.generationId())\n                .setState(MemberState.STABLE)\n                .setPreviousMemberEpoch(classicGroup.generationId())\n                .setInstanceId(classicGroupMember.groupInstanceId().orElse(null))\n                .setRackId(toOptional(subscription.rackId()).orElse(null))\n                .setRebalanceTimeoutMs(classicGroupMember.rebalanceTimeoutMs())\n                .setClientId(classicGroupMember.clientId())\n                .setClientHost(classicGroupMember.clientHost())\n                .setSubscribedTopicNames(subscription.topics())\n                .setAssignedPartitions(assignedPartitions)\n                .setClassicMemberMetadata(\n                    new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                        .setSessionTimeoutMs(classicGroupMember.sessionTimeoutMs())\n                        .setSupportedProtocols(ConsumerGroupMember.classicProtocolListFromJoinRequestProtocolCollection(\n                            classicGroupMember.supportedProtocols()\n                        ))\n                )\n                .build();\n            consumerGroup.updateTargetAssignment(newMember.memberId(), new Assignment(assignedPartitions));\n            consumerGroup.updateMember(newMember);\n        });\n\n        return consumerGroup;\n    }\n\n    /**\n     * Populate the record list with the records needed to create the given consumer group.\n     *\n     * @param records The list to which the new records are added.\n     */\n    public void createConsumerGroupRecords(\n        List\u003cCoordinatorRecord\u003e records\n    ) {\n        members().forEach((__, consumerGroupMember) -\u003e\n            records.add(CoordinatorRecordHelpers.newMemberSubscriptionRecord(groupId(), consumerGroupMember))\n        );\n\n        records.add(CoordinatorRecordHelpers.newGroupEpochRecord(groupId(), groupEpoch()));\n\n        members().forEach((consumerGroupMemberId, consumerGroupMember) -\u003e\n            records.add(CoordinatorRecordHelpers.newTargetAssignmentRecord(\n                groupId(),\n                consumerGroupMemberId,\n                targetAssignment(consumerGroupMemberId).partitions()\n            ))\n        );\n\n        records.add(CoordinatorRecordHelpers.newTargetAssignmentEpochRecord(groupId(), groupEpoch()));\n\n        members().forEach((__, consumerGroupMember) -\u003e\n            records.add(CoordinatorRecordHelpers.newCurrentAssignmentRecord(groupId(), consumerGroupMember))\n        );\n    }\n\n    /**\n     * Checks whether at least one of the given protocols can be supported. A\n     * protocol can be supported if it is supported by all members that use the\n     * classic protocol.\n     *\n     * @param memberProtocolType  The member protocol type.\n     * @param memberProtocols     The set of protocol names.\n     *\n     * @return A boolean based on the condition mentioned above.\n     */\n    public boolean supportsClassicProtocols(String memberProtocolType, Set\u003cString\u003e memberProtocols) {\n        if (ConsumerProtocol.PROTOCOL_TYPE.equals(memberProtocolType)) {\n            if (isEmpty()) {\n                return !memberProtocols.isEmpty();\n            } else {\n                return memberProtocols.stream().anyMatch(\n                    name -\u003e classicProtocolMembersSupportedProtocols.getOrDefault(name, 0) \u003d\u003d numClassicProtocolMembers()\n                );\n            }\n        }\n        return false;\n    }\n\n    /**\n     * Checks whether all the members use the classic protocol except the given member.\n     *\n     * @param memberId The member to remove.\n     * @return A boolean indicating whether all the members use the classic protocol.\n     */\n    public boolean allMembersUseClassicProtocolExcept(String memberId) {\n        return numClassicProtocolMembers() \u003d\u003d members().size() - 1 \u0026\u0026\n            !getOrMaybeCreateMember(memberId, false).useClassicProtocol();\n    }\n\n    /**\n     * Checks whether the member has any unreleased partition.\n     *\n     * @param member The member to check.\n     * @return A boolean indicating whether the member has partitions in the target\n     *         assignment that hasn\u0027t been revoked by other members.\n     */\n    public boolean waitingOnUnreleasedPartition(ConsumerGroupMember member) {\n        if (member.state() \u003d\u003d MemberState.UNRELEASED_PARTITIONS) {\n            for (Map.Entry\u003cUuid, Set\u003cInteger\u003e\u003e entry : targetAssignment().get(member.memberId()).partitions().entrySet()) {\n                Uuid topicId \u003d entry.getKey();\n                Set\u003cInteger\u003e assignedPartitions \u003d member.assignedPartitions().getOrDefault(topicId, Collections.emptySet());\n\n                for (int partition : entry.getValue()) {\n                    if (!assignedPartitions.contains(partition) \u0026\u0026 currentPartitionEpoch(topicId, partition) !\u003d -1) {\n                        return true;\n                    }\n                }\n            }\n        }\n        return false;\n    }\n}","methodCount":81},"candidatesTelemetryData":{"numberOfSuggestions":5,"candidates":[{"lineStart":666,"lineEnd":671,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method preferredServerAssignor to class GroupCoordinatorMetricsShard","description":"Move method preferredServerAssignor to org.apache.kafka.coordinator.group.metrics.GroupCoordinatorMetricsShard\nRationale: The method could also be relevant to metrics tracking within the group coordination context. Placing it in GroupCoordinatorMetricsShard would allow for better tracking of server assignments as part of overall group metrics. This adheres to the Single Responsibility Principle, but care must be taken to ensure that the method does not become too tightly coupled with metrics that are unrelated to server assignment.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":666,"lineEnd":671,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method preferredServerAssignor to class SnapshotRegistry","description":"Move method preferredServerAssignor to org.apache.kafka.timeline.SnapshotRegistry\nRationale: Given that the preferred server assignor might need to access or modify snapshots, moving it to SnapshotRegistry could provide a more centralized location for related operations. This aligns with the Interface Segregation Principle, as it allows SnapshotRegistry to handle specific responsibilities related to server assignment logic. However, this could lead to a bloated class if not managed carefully.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":666,"lineEnd":671,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method preferredServerAssignor to class DeadlineAndEpoch","description":"Move method preferredServerAssignor to org.apache.kafka.coordinator.group.consumer.ConsumerGroup.DeadlineAndEpoch\nRationale: If the preferred server assignor is influenced by deadlines or epochs in the system, relocating it to DeadlineAndEpoch could be appropriate. This class encapsulates concepts of time and state, which may be relevant for determining the preferred assignor. This aligns with the Open/Closed Principle, allowing for future extensions related to time-based assignments. However, if the method\u0027s logic is not time-sensitive, this could introduce confusion.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":673,"lineEnd":680,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method preferredServerAssignor to class DeadlineAndEpoch","description":"Move method preferredServerAssignor to org.apache.kafka.coordinator.group.consumer.ConsumerGroup.DeadlineAndEpoch\nRationale: The preferredServerAssignor() method is focused on determining the best server assignor based on a committed offset, which may relate to the timing and state of deadlines and epochs in a distributed system. Moving this method to DeadlineAndEpoch can enhance the cohesion of the class by associating it with the logic that may depend on the timing of deadlines. This aligns with the Single Responsibility Principle, as DeadlineAndEpoch can encapsulate both the timing aspects and the logic for determining the preferred assignor. However, one consideration is that the current class\u0027s context and dependencies on serverAssignors may need to be addressed, ensuring that the method can still function correctly after relocation.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":281,"lineEnd":286,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method state to class DeadlineAndEpoch","description":"Move method state to org.apache.kafka.coordinator.group.consumer.ConsumerGroup.DeadlineAndEpoch\nRationale: The state() method is responsible for retrieving the current state, which suggests it is closely related to the lifecycle or status of an entity. The DeadlineAndEpoch class appears to manage time-related aspects (deadlineMs and epoch), which could imply that it also maintains or reflects a state. Moving the state() method to DeadlineAndEpoch aligns with the Single Responsibility Principle, as it consolidates state management within a class that likely deals with time-sensitive operations. This enhances cohesion by ensuring that all state-related logic is encapsulated within a single class. However, one consideration is whether DeadlineAndEpoch has the necessary context to manage or understand the state being returned, which could lead to tight coupling if not handled properly.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false}]},"iterationData":[{"iteration_num":1,"suggested_move_methods":[{"method_name":"validateOffsetCommit","method_signature":"public void validateOffsetCommit(String memberId, String groupInstanceId, int memberEpoch, boolean isTransactional, short apiVersion)","target_class":"OffsetValidator","rationale":"The method is responsible for validating offset commit requests, which is a distinct responsibility that could be encapsulated in a separate class dedicated to validation logic."},{"method_name":"validateOffsetFetch","method_signature":"public void validateOffsetFetch(String memberId, int memberEpoch, long lastCommittedOffset)","target_class":"OffsetValidator","rationale":"Similar to validateOffsetCommit, this method handles validation for offset fetch requests and should be moved to a dedicated OffsetValidator class."},{"method_name":"validateOffsetDelete","method_signature":"public void validateOffsetDelete()","target_class":"OffsetValidator","rationale":"This method validates offset deletion requests and should be part of a dedicated OffsetValidator class to separate concerns."},{"method_name":"validateDeleteGroup","method_signature":"public void validateDeleteGroup() throws ApiException","target_class":"GroupValidator","rationale":"This method is responsible for validating group deletion requests and should be moved to a dedicated GroupValidator class."},{"method_name":"createGroupTombstoneRecords","method_signature":"public void createGroupTombstoneRecords(List\u003cCoordinatorRecord\u003e records)","target_class":"TombstoneRecordCreator","rationale":"This method creates tombstone records for group deletion, which is a specific functionality that can be encapsulated in a separate TombstoneRecordCreator class."},{"method_name":"createConsumerGroupRecords","method_signature":"public void createConsumerGroupRecords(List\u003cCoordinatorRecord\u003e records)","target_class":"ConsumerGroupRecordCreator","rationale":"This method is responsible for creating records related to consumer groups and should be moved to a dedicated ConsumerGroupRecordCreator class."},{"method_name":"supportsClassicProtocols","method_signature":"public boolean supportsClassicProtocols(String memberProtocolType, Set\u003cString\u003e memberProtocols)","target_class":"ProtocolSupportChecker","rationale":"This method checks if classic protocols are supported, which is a specific concern that can be handled by a separate ProtocolSupportChecker class."},{"method_name":"allMembersUseClassicProtocolExcept","method_signature":"public boolean allMembersUseClassicProtocolExcept(String memberId)","target_class":"ProtocolSupportChecker","rationale":"This method checks the protocol usage among members and should be part of a dedicated ProtocolSupportChecker class."},{"method_name":"waitingOnUnreleasedPartition","method_signature":"public boolean waitingOnUnreleasedPartition(ConsumerGroupMember member)","target_class":"PartitionManager","rationale":"This method checks if a member is waiting on unreleased partitions, which is a specific responsibility that can be encapsulated in a PartitionManager class."}],"llm_response_time":10564},{"iteration_num":2,"suggested_move_methods":[{"method_name":"validateOffsetCommit","method_signature":"public void validateOffsetCommit(String memberId, String groupInstanceId, int memberEpoch, boolean isTransactional, short apiVersion)","target_class":"OffsetCommitValidator","rationale":"The method validates offset commit requests and could be better organized in a dedicated class focused on validation logic, improving separation of concerns."},{"method_name":"validateOffsetFetch","method_signature":"public void validateOffsetFetch(String memberId, int memberEpoch, long lastCommittedOffset)","target_class":"OffsetFetchValidator","rationale":"Similar to validateOffsetCommit, this method handles validation for offset fetch requests and should be moved to a dedicated validator class."},{"method_name":"validateOffsetDelete","method_signature":"public void validateOffsetDelete()","target_class":"OffsetDeleteValidator","rationale":"This method validates offset delete requests and would fit better in a dedicated class for managing offset deletion logic."},{"method_name":"validateDeleteGroup","method_signature":"public void validateDeleteGroup()","target_class":"GroupDeletionValidator","rationale":"This method checks the conditions for deleting a group and should be moved to a class that handles group deletion validation."},{"method_name":"createGroupTombstoneRecords","method_signature":"public void createGroupTombstoneRecords(List\u003cCoordinatorRecord\u003e records)","target_class":"TombstoneRecordCreator","rationale":"This method is responsible for creating tombstone records for group deletion and should be encapsulated in a dedicated class for record creation."},{"method_name":"createConsumerGroupRecords","method_signature":"public void createConsumerGroupRecords(List\u003cCoordinatorRecord\u003e records)","target_class":"ConsumerGroupRecordCreator","rationale":"This method creates records for a consumer group and would benefit from being in a separate class focused on consumer group record creation."},{"method_name":"supportsClassicProtocols","method_signature":"public boolean supportsClassicProtocols(String memberProtocolType, Set\u003cString\u003e memberProtocols)","target_class":"ProtocolSupportChecker","rationale":"This method checks protocol support and should be moved to a class dedicated to managing protocol support logic."},{"method_name":"waitingOnUnreleasedPartition","method_signature":"public boolean waitingOnUnreleasedPartition(ConsumerGroupMember member)","target_class":"PartitionReleaseChecker","rationale":"This method checks if a member is waiting on unreleased partitions and should be moved to a class that handles partition release logic."}],"llm_response_time":9538},{"iteration_num":3,"suggested_move_methods":[{"method_name":"validateOffsetCommit","method_signature":"public void validateOffsetCommit(String memberId, String groupInstanceId, int memberEpoch, boolean isTransactional, short apiVersion)","target_class":"OffsetValidator","rationale":"The method is responsible for validating offset commit requests, which is a distinct concern that can be encapsulated in a separate class dedicated to validation logic."},{"method_name":"validateOffsetFetch","method_signature":"public void validateOffsetFetch(String memberId, int memberEpoch, long lastCommittedOffset)","target_class":"OffsetValidator","rationale":"Similar to validateOffsetCommit, this method handles validation for offset fetch requests and should be part of a dedicated validation class."},{"method_name":"validateOffsetDelete","method_signature":"public void validateOffsetDelete()","target_class":"OffsetValidator","rationale":"This method is also related to validation of offset deletion and should be included in the OffsetValidator class for better separation of concerns."},{"method_name":"validateDeleteGroup","method_signature":"public void validateDeleteGroup() throws ApiException","target_class":"GroupValidator","rationale":"This method validates the deletion of a group, which is a specific validation task that can be handled by a separate GroupValidator class."},{"method_name":"createGroupTombstoneRecords","method_signature":"public void createGroupTombstoneRecords(List\u003cCoordinatorRecord\u003e records)","target_class":"TombstoneRecordCreator","rationale":"The method is responsible for creating tombstone records for group deletion, which can be encapsulated in a dedicated class focused on record creation."},{"method_name":"createConsumerGroupRecords","method_signature":"public void createConsumerGroupRecords(List\u003cCoordinatorRecord\u003e records)","target_class":"ConsumerGroupRecordCreator","rationale":"This method creates records for a consumer group, which can be better managed in a dedicated class that handles consumer group record creation."},{"method_name":"supportsClassicProtocols","method_signature":"public boolean supportsClassicProtocols(String memberProtocolType, Set\u003cString\u003e memberProtocols)","target_class":"ProtocolSupportChecker","rationale":"This method checks for protocol support, which is a specific concern that can be encapsulated in a class dedicated to protocol support validation."},{"method_name":"waitingOnUnreleasedPartition","method_signature":"public boolean waitingOnUnreleasedPartition(ConsumerGroupMember member)","target_class":"PartitionManager","rationale":"This method checks if a member is waiting on unreleased partitions, which is a specific concern that can be managed by a dedicated PartitionManager class."}],"llm_response_time":9641},{"iteration_num":-1,"suggested_move_methods":[{"method_name":"stateAsString","method_signature":"public stateAsString(long committedOffset)","target_class":"","rationale":""},{"method_name":"asListedGroup","method_signature":"public asListedGroup(long committedOffset)","target_class":"","rationale":""},{"method_name":"state","method_signature":"public state()","target_class":"","rationale":""},{"method_name":"state","method_signature":"public state(long committedOffset)","target_class":"","rationale":""},{"method_name":"groupEpoch","method_signature":"public groupEpoch()","target_class":"","rationale":""},{"method_name":"assignmentEpoch","method_signature":"public assignmentEpoch()","target_class":"","rationale":""},{"method_name":"staticMemberId","method_signature":"public staticMemberId(String groupInstanceId)","target_class":"","rationale":""},{"method_name":"getOrMaybeCreateMember","method_signature":"public getOrMaybeCreateMember(\n        String memberId,\n        boolean createIfNotExists\n    )","target_class":"","rationale":""},{"method_name":"staticMember","method_signature":"public staticMember(String instanceId)","target_class":"","rationale":""},{"method_name":"updateMember","method_signature":"public updateMember(ConsumerGroupMember newMember)","target_class":"","rationale":""},{"method_name":"updateStaticMember","method_signature":"private updateStaticMember(ConsumerGroupMember newMember)","target_class":"","rationale":""},{"method_name":"removeMember","method_signature":"public removeMember(String memberId)","target_class":"","rationale":""},{"method_name":"removeStaticMember","method_signature":"private removeStaticMember(ConsumerGroupMember oldMember)","target_class":"","rationale":""},{"method_name":"hasMember","method_signature":"public hasMember(String memberId)","target_class":"","rationale":""},{"method_name":"numMembers","method_signature":"public numMembers()","target_class":"","rationale":""},{"method_name":"numClassicProtocolMembers","method_signature":"public numClassicProtocolMembers()","target_class":"","rationale":""},{"method_name":"classicMembersSupportedProtocols","method_signature":"public classicMembersSupportedProtocols()","target_class":"","rationale":""},{"method_name":"members","method_signature":"public members()","target_class":"","rationale":""},{"method_name":"staticMembers","method_signature":"public staticMembers()","target_class":"","rationale":""},{"method_name":"subscribedTopicNames","method_signature":"public subscribedTopicNames()","target_class":"","rationale":""},{"method_name":"subscriptionType","method_signature":"public subscriptionType()","target_class":"","rationale":""},{"method_name":"targetAssignment","method_signature":"public targetAssignment(String memberId)","target_class":"","rationale":""},{"method_name":"invertedTargetAssignment","method_signature":"public invertedTargetAssignment()","target_class":"","rationale":""},{"method_name":"updateTargetAssignment","method_signature":"public updateTargetAssignment(String memberId, Assignment newTargetAssignment)","target_class":"","rationale":""},{"method_name":"updateInvertedTargetAssignment","method_signature":"private updateInvertedTargetAssignment(\n        String memberId,\n        Assignment oldTargetAssignment,\n        Assignment newTargetAssignment\n    )","target_class":"","rationale":""},{"method_name":"removeTargetAssignment","method_signature":"public removeTargetAssignment(String memberId)","target_class":"","rationale":""},{"method_name":"targetAssignment","method_signature":"public targetAssignment()","target_class":"","rationale":""},{"method_name":"currentPartitionEpoch","method_signature":"public currentPartitionEpoch(\n        Uuid topicId, int partitionId\n    )","target_class":"","rationale":""},{"method_name":"computePreferredServerAssignor","method_signature":"public computePreferredServerAssignor(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )","target_class":"","rationale":""},{"method_name":"preferredServerAssignor","method_signature":"public preferredServerAssignor()","target_class":"","rationale":""},{"method_name":"preferredServerAssignor","method_signature":"public preferredServerAssignor(long committedOffset)","target_class":"","rationale":""},{"method_name":"subscriptionMetadata","method_signature":"public subscriptionMetadata()","target_class":"","rationale":""},{"method_name":"computeSubscriptionMetadata","method_signature":"public computeSubscriptionMetadata(\n        Map\u003cString, Integer\u003e subscribedTopicNames,\n        TopicsImage topicsImage,\n        ClusterImage clusterImage\n    )","target_class":"","rationale":""},{"method_name":"hasMetadataExpired","method_signature":"public hasMetadataExpired(long currentTimeMs)","target_class":"","rationale":""},{"method_name":"validateMemberEpoch","method_signature":"private validateMemberEpoch(\n        int receivedMemberEpoch,\n        int expectedMemberEpoch,\n        boolean useClassicProtocol\n    )","target_class":"","rationale":""},{"method_name":"maybeUpdateGroupState","method_signature":"private maybeUpdateGroupState()","target_class":"","rationale":""},{"method_name":"maybeUpdateServerAssignors","method_signature":"private maybeUpdateServerAssignors(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )","target_class":"","rationale":""},{"method_name":"maybeUpdateServerAssignors","method_signature":"private static maybeUpdateServerAssignors(\n        Map\u003cString, Integer\u003e serverAssignorCount,\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )","target_class":"","rationale":""},{"method_name":"maybeUpdateNumClassicProtocolMembers","method_signature":"private maybeUpdateNumClassicProtocolMembers(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )","target_class":"","rationale":""},{"method_name":"maybeUpdateClassicProtocolMembersSupportedProtocols","method_signature":"private maybeUpdateClassicProtocolMembersSupportedProtocols(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )","target_class":"","rationale":""},{"method_name":"maybeUpdateSubscribedTopicNamesAndGroupSubscriptionType","method_signature":"private maybeUpdateSubscribedTopicNamesAndGroupSubscriptionType(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )","target_class":"","rationale":""},{"method_name":"maybeUpdateSubscribedTopicNames","method_signature":"private static maybeUpdateSubscribedTopicNames(\n        Map\u003cString, Integer\u003e subscribedTopicCount,\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )","target_class":"","rationale":""},{"method_name":"computeSubscribedTopicNames","method_signature":"public computeSubscribedTopicNames(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )","target_class":"","rationale":""},{"method_name":"computeSubscribedTopicNames","method_signature":"public computeSubscribedTopicNames(\n        Set\u003cConsumerGroupMember\u003e removedMembers\n    )","target_class":"","rationale":""},{"method_name":"subscriptionType","method_signature":"public static subscriptionType(\n        Map\u003cString, Integer\u003e subscribedTopicNames,\n        int numberOfMembers\n    )","target_class":"","rationale":""},{"method_name":"maybeUpdatePartitionEpoch","method_signature":"private maybeUpdatePartitionEpoch(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )","target_class":"","rationale":""},{"method_name":"maybeRemovePartitionEpoch","method_signature":"private maybeRemovePartitionEpoch(\n        ConsumerGroupMember oldMember\n    )","target_class":"","rationale":""},{"method_name":"removePartitionEpochs","method_signature":" removePartitionEpochs(\n        Map\u003cUuid, Set\u003cInteger\u003e\u003e assignment,\n        int expectedEpoch\n    )","target_class":"","rationale":""},{"method_name":"addPartitionEpochs","method_signature":" addPartitionEpochs(\n        Map\u003cUuid, Set\u003cInteger\u003e\u003e assignment,\n        int epoch\n    )","target_class":"","rationale":""},{"method_name":"decValue","method_signature":"private static decValue(String key, Integer value)","target_class":"","rationale":""},{"method_name":"incValue","method_signature":"private static incValue(String key, Integer value)","target_class":"","rationale":""},{"method_name":"asDescribedGroup","method_signature":"public asDescribedGroup(\n        long committedOffset,\n        String defaultAssignor,\n        TopicsImage topicsImage\n    )","target_class":"","rationale":""},{"method_name":"fromClassicGroup","method_signature":"public static fromClassicGroup(\n        SnapshotRegistry snapshotRegistry,\n        GroupCoordinatorMetricsShard metrics,\n        ClassicGroup classicGroup,\n        TopicsImage topicsImage\n    )","target_class":"","rationale":""},{"method_name":"createConsumerGroupRecords","method_signature":"public createConsumerGroupRecords(\n        List\u003cCoordinatorRecord\u003e records\n    )","target_class":"","rationale":""},{"method_name":"supportsClassicProtocols","method_signature":"public supportsClassicProtocols(String memberProtocolType, Set\u003cString\u003e memberProtocols)","target_class":"","rationale":""},{"method_name":"allMembersUseClassicProtocolExcept","method_signature":"public allMembersUseClassicProtocolExcept(String memberId)","target_class":"","rationale":""},{"method_name":"waitingOnUnreleasedPartition","method_signature":"public waitingOnUnreleasedPartition(ConsumerGroupMember member)","target_class":"","rationale":""}],"llm_response_time":0},{"iteration_num":-2,"suggested_move_methods":[{"method_name":"decValue","method_signature":"private static decValue(String key, Integer value)","target_class":"","rationale":""},{"method_name":"incValue","method_signature":"private static incValue(String key, Integer value)","target_class":"","rationale":""},{"method_name":"preferredServerAssignor","method_signature":"public preferredServerAssignor()","target_class":"","rationale":""},{"method_name":"invertedTargetAssignment","method_signature":"public invertedTargetAssignment()","target_class":"","rationale":""},{"method_name":"targetAssignment","method_signature":"public targetAssignment()","target_class":"","rationale":""},{"method_name":"state","method_signature":"public state()","target_class":"","rationale":""},{"method_name":"preferredServerAssignor","method_signature":"public preferredServerAssignor(long committedOffset)","target_class":"","rationale":""},{"method_name":"maybeUpdateServerAssignors","method_signature":"private maybeUpdateServerAssignors(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )","target_class":"","rationale":""},{"method_name":"stateAsString","method_signature":"public stateAsString(long committedOffset)","target_class":"","rationale":""},{"method_name":"subscriptionType","method_signature":"public subscriptionType()","target_class":"","rationale":""},{"method_name":"assignmentEpoch","method_signature":"public assignmentEpoch()","target_class":"","rationale":""},{"method_name":"computePreferredServerAssignor","method_signature":"public computePreferredServerAssignor(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )","target_class":"","rationale":""},{"method_name":"removeTargetAssignment","method_signature":"public removeTargetAssignment(String memberId)","target_class":"","rationale":""},{"method_name":"state","method_signature":"public state(long committedOffset)","target_class":"","rationale":""},{"method_name":"targetAssignment","method_signature":"public targetAssignment(String memberId)","target_class":"","rationale":""}],"llm_response_time":0}],"methodCompatibilityScores":{"private static decValue(String key, Integer value)":{"first":{"method_name":"decValue","method_signature":"private static decValue(String key, Integer value)","target_class":"","rationale":""},"second":0.16111954560041794},"private static incValue(String key, Integer value)":{"first":{"method_name":"incValue","method_signature":"private static incValue(String key, Integer value)","target_class":"","rationale":""},"second":0.16494688523694195},"public preferredServerAssignor()":{"first":{"method_name":"preferredServerAssignor","method_signature":"public preferredServerAssignor()","target_class":"","rationale":""},"second":0.1839156850365568},"public invertedTargetAssignment()":{"first":{"method_name":"invertedTargetAssignment","method_signature":"public invertedTargetAssignment()","target_class":"","rationale":""},"second":0.22114685330522021},"public targetAssignment()":{"first":{"method_name":"targetAssignment","method_signature":"public targetAssignment()","target_class":"","rationale":""},"second":0.2421554199356214},"public state()":{"first":{"method_name":"state","method_signature":"public state()","target_class":"","rationale":""},"second":0.2495610220124828},"public preferredServerAssignor(long committedOffset)":{"first":{"method_name":"preferredServerAssignor","method_signature":"public preferredServerAssignor(long committedOffset)","target_class":"","rationale":""},"second":0.270906265258285},"private maybeUpdateServerAssignors(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )":{"first":{"method_name":"maybeUpdateServerAssignors","method_signature":"private maybeUpdateServerAssignors(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )","target_class":"","rationale":""},"second":0.2969992965019352},"public stateAsString(long committedOffset)":{"first":{"method_name":"stateAsString","method_signature":"public stateAsString(long committedOffset)","target_class":"","rationale":""},"second":0.3009025396090453},"public subscriptionType()":{"first":{"method_name":"subscriptionType","method_signature":"public subscriptionType()","target_class":"","rationale":""},"second":0.30267015695133664},"public assignmentEpoch()":{"first":{"method_name":"assignmentEpoch","method_signature":"public assignmentEpoch()","target_class":"","rationale":""},"second":0.3115767753972712},"public computePreferredServerAssignor(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )":{"first":{"method_name":"computePreferredServerAssignor","method_signature":"public computePreferredServerAssignor(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    )","target_class":"","rationale":""},"second":0.329875174041037},"public removeTargetAssignment(String memberId)":{"first":{"method_name":"removeTargetAssignment","method_signature":"public removeTargetAssignment(String memberId)","target_class":"","rationale":""},"second":0.3356454194404492},"public state(long committedOffset)":{"first":{"method_name":"state","method_signature":"public state(long committedOffset)","target_class":"","rationale":""},"second":0.33619756674146534},"public targetAssignment(String memberId)":{"first":{"method_name":"targetAssignment","method_signature":"public targetAssignment(String memberId)","target_class":"","rationale":""},"second":0.3414265516136657}},"llmMethodPriority":{"priority_method_names":["preferredServerAssignor","preferredServerAssignor","state","state","assignmentEpoch","targetAssignment","targetAssignment","invertedTargetAssignment","subscriptionType","removeTargetAssignment","stateAsString","decValue","incValue","maybeUpdateServerAssignors","computePreferredServerAssignor"],"llm_response_time":2840},"targetClassMap":{"preferredServerAssignor":{"target_classes":[{"class_name":"DeadlineAndEpoch","similarity_score":0.1723280873710658}],"target_classes_sorted_by_llm":["DeadlineAndEpoch"],"llm_response_time":5046,"similarity_computation_time":1,"similarity_metric":"tfidf"},"state":{"target_classes":[{"class_name":"DeadlineAndEpoch","similarity_score":0.23028309323591914}],"target_classes_sorted_by_llm":["DeadlineAndEpoch"],"llm_response_time":3133,"similarity_computation_time":0,"similarity_metric":"tfidf"}}}
{"id":"ad161ae6-c425-4550-bdec-b61ebc9bb00b","methodCount":15,"hostFunctionTelemetryData":{"hostFunctionSize":395,"lineStart":70,"lineEnd":464,"bodyLineStart":70,"language":"java","filePath":"/Users/fraol/Documents/MM-assist-dataset/kafka/server/src/test/java/org/apache/kafka/server/AssignmentsManagerTest.java","sourceCode":"public class AssignmentsManagerTest {\n\n    private static final Uuid TOPIC_1 \u003d Uuid.fromString(\"88rnFIqYSZykX4ZSKv81bg\");\n    private static final Uuid TOPIC_2 \u003d Uuid.fromString(\"VKCnzHdhR5uDQc1shqBYrQ\");\n    private static final Uuid DIR_1 \u003d Uuid.fromString(\"cbgD8WdLQCyzLrFIMBhv3w\");\n    private static final Uuid DIR_2 \u003d Uuid.fromString(\"zO0bDc0vSuam7Db9iH7rYQ\");\n    private static final Uuid DIR_3 \u003d Uuid.fromString(\"CGBWbrFkRkeJQy6Aryzq2Q\");\n\n    private MockTime time;\n    private NodeToControllerChannelManager channelManager;\n    private AssignmentsManager manager;\n\n    @BeforeEach\n    public void setup() {\n        time \u003d new MockTime();\n        channelManager \u003d mock(NodeToControllerChannelManager.class);\n        Map\u003cUuid, String\u003e topicNames \u003d new HashMap\u003c\u003e();\n        topicNames.put(TOPIC_1, \"TOPIC_1\");\n        topicNames.put(TOPIC_2, \"TOPIC_2\");\n        Map\u003cUuid, String\u003e dirPaths \u003d new HashMap\u003c\u003e();\n        dirPaths.put(DIR_1, \"DIR_1\");\n        dirPaths.put(DIR_2, \"DIR_2\");\n        dirPaths.put(DIR_3, \"DIR_3\");\n        manager \u003d new AssignmentsManager(time, channelManager, 8, () -\u003e 100L,\n                id -\u003e Optional.ofNullable(dirPaths.get(id)), id -\u003e Optional.ofNullable(topicNames.get(id)));\n    }\n\n    @AfterEach\n    void tearDown() throws InterruptedException {\n        manager.close();\n    }\n\n    AssignReplicasToDirsRequestData normalize(AssignReplicasToDirsRequestData request) {\n        request \u003d request.duplicate();\n        request.directories().sort(Comparator.comparing(\n            AssignReplicasToDirsRequestData.DirectoryData::id));\n        for (AssignReplicasToDirsRequestData.DirectoryData directory : request.directories()) {\n            directory.topics().sort(Comparator.comparing(\n                AssignReplicasToDirsRequestData.TopicData::topicId));\n            for (AssignReplicasToDirsRequestData.TopicData topic : directory.topics()) {\n                topic.partitions().sort(Comparator.comparing(\n                    AssignReplicasToDirsRequestData.PartitionData::partitionIndex));\n            }\n        }\n        return request;\n    }\n\n\n    void assertRequestEquals(\n        AssignReplicasToDirsRequestData expected,\n        AssignReplicasToDirsRequestData actual\n    ) {\n        assertEquals(normalize(expected), normalize(actual));\n    }\n\n    @Test\n    void testBuildRequestData() {\n        Map\u003cTopicIdPartition, Uuid\u003e assignment \u003d new HashMap\u003cTopicIdPartition, Uuid\u003e() {{\n                put(new TopicIdPartition(TOPIC_1, 1), DIR_1);\n                put(new TopicIdPartition(TOPIC_1, 2), DIR_2);\n                put(new TopicIdPartition(TOPIC_1, 3), DIR_3);\n                put(new TopicIdPartition(TOPIC_1, 4), DIR_1);\n                put(new TopicIdPartition(TOPIC_2, 5), DIR_2);\n            }};\n        AssignReplicasToDirsRequestData built \u003d AssignmentsManager.buildRequestData(8, 100L, assignment);\n        AssignReplicasToDirsRequestData expected \u003d new AssignReplicasToDirsRequestData()\n            .setBrokerId(8)\n            .setBrokerEpoch(100L)\n            .setDirectories(Arrays.asList(\n                new AssignReplicasToDirsRequestData.DirectoryData()\n                    .setId(DIR_2)\n                    .setTopics(Arrays.asList(\n                        new AssignReplicasToDirsRequestData.TopicData()\n                            .setTopicId(TOPIC_1)\n                            .setPartitions(Collections.singletonList(\n                                    new AssignReplicasToDirsRequestData.PartitionData()\n                                            .setPartitionIndex(2))),\n                new AssignReplicasToDirsRequestData.TopicData()\n                    .setTopicId(TOPIC_2)\n                    .setPartitions(Collections.singletonList(\n                            new AssignReplicasToDirsRequestData.PartitionData()\n                                    .setPartitionIndex(5))))),\n            new AssignReplicasToDirsRequestData.DirectoryData()\n                .setId(DIR_3)\n                .setTopics(Collections.singletonList(\n                    new AssignReplicasToDirsRequestData.TopicData()\n                        .setTopicId(TOPIC_1)\n                        .setPartitions(Collections.singletonList(\n                            new AssignReplicasToDirsRequestData.PartitionData()\n                                    .setPartitionIndex(3))))),\n            new AssignReplicasToDirsRequestData.DirectoryData()\n                .setId(DIR_1)\n                .setTopics(Collections.singletonList(\n                    new AssignReplicasToDirsRequestData.TopicData()\n                        .setTopicId(TOPIC_1)\n                        .setPartitions(Arrays.asList(\n                            new AssignReplicasToDirsRequestData.PartitionData()\n                                .setPartitionIndex(4),\n                            new AssignReplicasToDirsRequestData.PartitionData()\n                                .setPartitionIndex(1)))))));\n        assertRequestEquals(expected, built);\n    }\n\n    @Test\n    public void testAssignmentAggregation() throws InterruptedException {\n        CountDownLatch readyToAssert \u003d new CountDownLatch(1);\n        doAnswer(invocation -\u003e {\n            readyToAssert.countDown();\n            return null;\n        }).when(channelManager).sendRequest(any(AssignReplicasToDirsRequest.Builder.class),\n            any(ControllerRequestCompletionHandler.class));\n\n        manager.onAssignment(new TopicIdPartition(TOPIC_1, 1), DIR_1, \"testAssignmentAggregation\", () -\u003e { });\n        manager.onAssignment(new TopicIdPartition(TOPIC_1, 2), DIR_2, \"testAssignmentAggregation\", () -\u003e { });\n        manager.onAssignment(new TopicIdPartition(TOPIC_1, 3), DIR_3, \"testAssignmentAggregation\", () -\u003e { });\n        manager.onAssignment(new TopicIdPartition(TOPIC_1, 4), DIR_1, \"testAssignmentAggregation\", () -\u003e { });\n        manager.onAssignment(new TopicIdPartition(TOPIC_2, 5), DIR_2, \"testAssignmentAggregation\", () -\u003e { });\n        TestUtils.waitForCondition(() -\u003e {\n            time.sleep(100);\n            manager.wakeup();\n            return readyToAssert.await(1, TimeUnit.MILLISECONDS);\n        }, \"Timed out waiting for AssignReplicasToDirsRequest to be sent.\");\n\n        ArgumentCaptor\u003cAssignReplicasToDirsRequest.Builder\u003e captor \u003d\n            ArgumentCaptor.forClass(AssignReplicasToDirsRequest.Builder.class);\n        verify(channelManager, times(1)).start();\n        verify(channelManager).sendRequest(captor.capture(), any(ControllerRequestCompletionHandler.class));\n        verify(channelManager, atMostOnce()).shutdown();\n        verifyNoMoreInteractions(channelManager);\n        assertEquals(1, captor.getAllValues().size());\n        AssignReplicasToDirsRequestData actual \u003d captor.getValue().build().data();\n        AssignReplicasToDirsRequestData expected \u003d buildRequestData(\n            8, 100L, new HashMap\u003cTopicIdPartition, Uuid\u003e() {{\n                    put(new TopicIdPartition(TOPIC_1, 1), DIR_1);\n                    put(new TopicIdPartition(TOPIC_1, 2), DIR_2);\n                    put(new TopicIdPartition(TOPIC_1, 3), DIR_3);\n                    put(new TopicIdPartition(TOPIC_1, 4), DIR_1);\n                    put(new TopicIdPartition(TOPIC_2, 5), DIR_2);\n                }}\n        );\n        assertRequestEquals(expected, actual);\n    }\n\n    @Test\n    void testRequeuesFailedAssignmentPropagations() throws InterruptedException {\n        CountDownLatch readyToAssert \u003d new CountDownLatch(5);\n        doAnswer(invocation -\u003e {\n            readyToAssert.countDown();\n            if (readyToAssert.getCount() \u003d\u003d 4) {\n                invocation.getArgument(1, ControllerRequestCompletionHandler.class).onTimeout();\n                manager.onAssignment(new TopicIdPartition(TOPIC_1, 2), DIR_3, \"testRequeuesFailedAssignmentPropagations\", () -\u003e { });\n            }\n            if (readyToAssert.getCount() \u003d\u003d 3) {\n                invocation.getArgument(1, ControllerRequestCompletionHandler.class).onComplete(\n                    new ClientResponse(null, null, null, 0L, 0L, false, false,\n                        new UnsupportedVersionException(\"test unsupported version exception\"), null, null));\n\n                // duplicate should be ignored\n                manager.onAssignment(new TopicIdPartition(TOPIC_1, 2), DIR_3, \"testRequeuesFailedAssignmentPropagations\", () -\u003e { });\n\n                manager.onAssignment(new TopicIdPartition(TOPIC_1, 3),\n                     Uuid.fromString(\"xHLCnG54R9W3lZxTPnpk1Q\"), \"testRequeuesFailedAssignmentPropagations\", () -\u003e { });\n            }\n            if (readyToAssert.getCount() \u003d\u003d 2) {\n                invocation.getArgument(1, ControllerRequestCompletionHandler.class).onComplete(\n                        new ClientResponse(null, null, null, 0L, 0L, false, false, null,\n                                new AuthenticationException(\"test authentication exception\"), null)\n                );\n\n                // duplicate should be ignored\n                manager.onAssignment(new TopicIdPartition(TOPIC_1, 3),\n                     Uuid.fromString(\"xHLCnG54R9W3lZxTPnpk1Q\"), \"testRequeuesFailedAssignmentPropagations\", () -\u003e { });\n\n                manager.onAssignment(new TopicIdPartition(TOPIC_1, 4),\n                     Uuid.fromString(\"RCYu1A0CTa6eEIpuKDOfxw\"), \"testRequeuesFailedAssignmentPropagations\", () -\u003e { });\n            }\n            if (readyToAssert.getCount() \u003d\u003d 1) {\n                invocation.getArgument(1, ControllerRequestCompletionHandler.class).onComplete(\n                    new ClientResponse(null, null, null, 0L, 0L, false, false, null, null,\n                        new AssignReplicasToDirsResponse(new AssignReplicasToDirsResponseData()\n                            .setErrorCode(Errors.NOT_CONTROLLER.code())\n                            .setThrottleTimeMs(0))));\n            }\n            return null;\n        }).when(channelManager).sendRequest(any(AssignReplicasToDirsRequest.Builder.class),\n            any(ControllerRequestCompletionHandler.class));\n\n        manager.onAssignment(new TopicIdPartition(TOPIC_1, 1), DIR_1, \"testRequeuesFailedAssignmentPropagations\", () -\u003e { });\n        TestUtils.waitForCondition(() -\u003e {\n            time.sleep(TimeUnit.SECONDS.toMillis(1));\n            manager.wakeup();\n            return readyToAssert.await(1, TimeUnit.MILLISECONDS);\n        }, \"Timed out waiting for AssignReplicasToDirsRequest to be sent.\");\n\n        ArgumentCaptor\u003cAssignReplicasToDirsRequest.Builder\u003e captor \u003d\n            ArgumentCaptor.forClass(AssignReplicasToDirsRequest.Builder.class);\n        verify(channelManager, times(1)).start();\n        verify(channelManager, times(5)).sendRequest(captor.capture(),\n            any(ControllerRequestCompletionHandler.class));\n        verify(channelManager, atMostOnce()).shutdown();\n        verifyNoMoreInteractions(channelManager);\n        assertEquals(5, captor.getAllValues().size());\n        assertRequestEquals(buildRequestData(\n            8, 100L, new HashMap\u003cTopicIdPartition, Uuid\u003e() {{\n                    put(new TopicIdPartition(TOPIC_1, 1), DIR_1);\n                }}\n        ), captor.getAllValues().get(0).build().data());\n        assertRequestEquals(buildRequestData(\n            8, 100L, new HashMap\u003cTopicIdPartition, Uuid\u003e() {{\n                    put(new TopicIdPartition(TOPIC_1, 1), DIR_1);\n                    put(new TopicIdPartition(TOPIC_1, 2), DIR_3);\n                }}\n        ), captor.getAllValues().get(1).build().data());\n        assertRequestEquals(buildRequestData(\n            8, 100L, new HashMap\u003cTopicIdPartition, Uuid\u003e() {{\n                    put(new TopicIdPartition(TOPIC_1, 1), DIR_1);\n                    put(new TopicIdPartition(TOPIC_1, 2), DIR_3);\n                    put(new TopicIdPartition(TOPIC_1, 3), Uuid.fromString(\"xHLCnG54R9W3lZxTPnpk1Q\"));\n                    put(new TopicIdPartition(TOPIC_1, 4), Uuid.fromString(\"RCYu1A0CTa6eEIpuKDOfxw\"));\n                }}\n        ), captor.getAllValues().get(4).build().data());\n    }\n\n    @Timeout(30)\n    @Test\n    void testOnCompletion() throws Exception {\n        CountDownLatch readyToAssert \u003d new CountDownLatch(300);\n        doAnswer(invocation -\u003e {\n            AssignReplicasToDirsRequestData request \u003d invocation.getArgument(0, AssignReplicasToDirsRequest.Builder.class).build().data();\n            ControllerRequestCompletionHandler completionHandler \u003d invocation.getArgument(1, ControllerRequestCompletionHandler.class);\n            completionHandler.onComplete(buildSuccessfulResponse(request));\n\n            return null;\n        }).when(channelManager).sendRequest(any(AssignReplicasToDirsRequest.Builder.class),\n                any(ControllerRequestCompletionHandler.class));\n\n        for (int i \u003d 0; i \u003c 300; i++) {\n            manager.onAssignment(new TopicIdPartition(TOPIC_1, i % 5), DIR_1, \"testOnCompletion\", readyToAssert::countDown);\n        }\n\n        TestUtils.waitForCondition(() -\u003e {\n            time.sleep(TimeUnit.SECONDS.toMillis(1));\n            manager.wakeup();\n            return readyToAssert.await(1, TimeUnit.MILLISECONDS);\n        }, \"Timed out waiting for AssignReplicasToDirsRequest to be sent.\");\n    }\n\n    private static ClientResponse buildSuccessfulResponse(AssignReplicasToDirsRequestData request) {\n        return buildResponse(request, topicIdPartition -\u003e Errors.NONE);\n    }\n\n    private static ClientResponse buildResponse(AssignReplicasToDirsRequestData request,\n                                                Function\u003cTopicIdPartition, Errors\u003e perPartitionError) {\n        Map\u003cUuid, Map\u003cTopicIdPartition, Errors\u003e\u003e errors \u003d new HashMap\u003c\u003e();\n        for (AssignReplicasToDirsRequestData.DirectoryData directory : request.directories()) {\n            for (AssignReplicasToDirsRequestData.TopicData topic : directory.topics()) {\n                for (AssignReplicasToDirsRequestData.PartitionData partition : topic.partitions()) {\n                    TopicIdPartition topicIdPartition \u003d new TopicIdPartition(topic.topicId(), partition.partitionIndex());\n                    Errors error \u003d perPartitionError.apply(topicIdPartition);\n                    if (error \u003d\u003d null) {\n                        error \u003d Errors.NONE;\n                    }\n                    errors.computeIfAbsent(directory.id(), d -\u003e new HashMap\u003c\u003e()).put(topicIdPartition, error);\n                }\n            }\n        }\n        AssignReplicasToDirsResponseData responseData \u003d AssignmentsHelper.buildResponseData(Errors.NONE.code(), 0, errors);\n        return new ClientResponse(null, null, null,\n                0L, 0L, false, false, null, null,\n                new AssignReplicasToDirsResponse(responseData));\n    }\n\n    @Test\n    public void testAssignmentCompaction() throws Exception {\n        // Delay the first controller response to force assignment compaction logic\n        CompletableFuture\u003cRunnable\u003e completionFuture \u003d new CompletableFuture\u003c\u003e();\n        doAnswer(invocation -\u003e {\n            AssignReplicasToDirsRequestData request \u003d invocation.getArgument(0, AssignReplicasToDirsRequest.Builder.class).build().data();\n            ControllerRequestCompletionHandler completionHandler \u003d invocation.getArgument(1, ControllerRequestCompletionHandler.class);\n            ClientResponse response \u003d buildSuccessfulResponse(request);\n            Runnable completion \u003d () -\u003e completionHandler.onComplete(response);\n            if (completionFuture.isDone()) completion.run();\n            else completionFuture.complete(completion);\n            return null;\n        }).when(channelManager).sendRequest(any(AssignReplicasToDirsRequest.Builder.class),\n                any(ControllerRequestCompletionHandler.class));\n\n        CountDownLatch remainingInvocations \u003d new CountDownLatch(20);\n        Runnable onComplete \u003d () -\u003e {\n            assertTrue(completionFuture.isDone(), \"Premature invocation\");\n            assertTrue(remainingInvocations.getCount() \u003e 0, \"Extra invocation\");\n            remainingInvocations.countDown();\n        };\n        Uuid[] dirs \u003d {DIR_1, DIR_2, DIR_3};\n        for (int i \u003d 0; i \u003c remainingInvocations.getCount(); i++) {\n            time.sleep(100);\n            manager.onAssignment(new TopicIdPartition(TOPIC_1, 0), dirs[i % 3], \"testAssignmentCompaction\", onComplete);\n        }\n        activeWait(completionFuture::isDone);\n        completionFuture.get().run();\n        activeWait(() -\u003e remainingInvocations.getCount() \u003d\u003d 0);\n    }\n\n    void activeWait(Supplier\u003cBoolean\u003e predicate) throws InterruptedException {\n        TestUtils.waitForCondition(() -\u003e {\n            boolean conditionSatisfied \u003d predicate.get();\n            if (!conditionSatisfied) {\n                time.sleep(100);\n                manager.wakeup();\n            }\n            return conditionSatisfied;\n        }, TestUtils.DEFAULT_MAX_WAIT_MS, 50, null);\n    }\n\n    static Metric findMetric(String name) {\n        for (Map.Entry\u003cMetricName, Metric\u003e entry : KafkaYammerMetrics.defaultRegistry().allMetrics().entrySet()) {\n            MetricName metricName \u003d entry.getKey();\n            if (AssignmentsManager.class.getSimpleName().equals(metricName.getType()) \u0026\u0026 metricName.getName().equals(name)) {\n                return entry.getValue();\n            }\n        }\n        throw new IllegalArgumentException(\"metric named \" + name + \" not found\");\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    @Test\n    void testQueuedReplicaToDirAssignmentsMetric() throws Exception {\n        CountDownLatch readyToAssert \u003d new CountDownLatch(1);\n        doAnswer(invocation -\u003e {\n            readyToAssert.countDown();\n            return null;\n        }).when(channelManager).sendRequest(any(AssignReplicasToDirsRequest.Builder.class), any(ControllerRequestCompletionHandler.class));\n\n        Gauge\u003cInteger\u003e queuedReplicaToDirAssignments \u003d (Gauge\u003cInteger\u003e) findMetric(AssignmentsManager.QUEUE_REPLICA_TO_DIR_ASSIGNMENTS_METRIC_NAME);\n        assertEquals(0, queuedReplicaToDirAssignments.value());\n\n        for (int i \u003d 0; i \u003c 4; i++) {\n            manager.onAssignment(new TopicIdPartition(TOPIC_1, i), DIR_1, \"testQueuedReplicaToDirAssignmentsMetric\", () -\u003e { });\n        }\n        TestUtils.waitForCondition(() -\u003e {\n            time.sleep(100);\n            return readyToAssert.await(1, TimeUnit.MILLISECONDS);\n        }, \"Timed out waiting for AssignReplicasToDirsRequest to be sent.\");\n        assertEquals(4, queuedReplicaToDirAssignments.value());\n\n        for (int i \u003d 4; i \u003c 8; i++) {\n            manager.onAssignment(new TopicIdPartition(TOPIC_1, i), DIR_1, \"testQueuedReplicaToDirAssignmentsMetric\", () -\u003e { });\n        }\n        TestUtils.retryOnExceptionWithTimeout(5_000, () -\u003e assertEquals(8, queuedReplicaToDirAssignments.value()));\n    }\n\n    // AssignmentsManager retries to propagate assignments (via AssignReplicasToDirsRequest) after failures.\n    // When an assignment fails to propagate with NOT_LEADER_OR_FOLLOWER, AssignmentsManager should conclude\n    // that the broker has been removed as a replica for the partition, and stop trying to propagate it.\n    @Test\n    void testDropsOldAssignments() throws InterruptedException {\n        TopicIdPartition tp1 \u003d new TopicIdPartition(TOPIC_1, 1), tp2 \u003d new TopicIdPartition(TOPIC_1, 2);\n        List\u003cAssignReplicasToDirsRequestData\u003e requests \u003d new ArrayList\u003c\u003e();\n        CountDownLatch readyToAssert \u003d new CountDownLatch(2);\n        doAnswer(invocation -\u003e {\n            AssignReplicasToDirsRequestData request \u003d invocation.getArgument(0, AssignReplicasToDirsRequest.Builder.class).build().data();\n            ControllerRequestCompletionHandler completionHandler \u003d invocation.getArgument(1, ControllerRequestCompletionHandler.class);\n            if (readyToAssert.getCount() \u003d\u003d 2) {\n                // First request, reply with a partition-level NOT_LEADER_OR_FOLLOWER error and queue a different assignment\n                completionHandler.onComplete(buildResponse(request, topicIdPartition -\u003e Errors.NOT_LEADER_OR_FOLLOWER));\n                manager.onAssignment(tp2, DIR_1, \"testDropsOldAssignments-second\");\n            }\n            if (readyToAssert.getCount() \u003d\u003d 1) {\n                // Second request, reply with success\n                completionHandler.onComplete(buildSuccessfulResponse(request));\n            }\n            requests.add(request);\n            readyToAssert.countDown();\n            return null;\n        }).when(channelManager).sendRequest(any(), any());\n\n        manager.onAssignment(tp1, DIR_1, \"testDropsOldAssignments-first\");\n        TestUtils.waitForCondition(() -\u003e {\n            time.sleep(TimeUnit.SECONDS.toMillis(1));\n            manager.wakeup();\n            return readyToAssert.await(1, TimeUnit.MILLISECONDS);\n        }, \"Timed out waiting for AssignReplicasToDirsRequest to be sent.\");\n\n        assertEquals(Arrays.asList(\n                buildRequestData(8, 100, new HashMap\u003cTopicIdPartition, Uuid\u003e() {{\n                        put(tp1, DIR_1);\n                    }}),\n                // Even though the controller replied with NOT_LEADER_OR_FOLLOWER, the second request does not include\n                // partition 1, meaning AssignmentManager dropped (no longer retries) the assignment.\n                buildRequestData(8, 100, new HashMap\u003cTopicIdPartition, Uuid\u003e() {{\n                        put(tp2, DIR_1);\n                    }})\n        ), requests);\n    }\n}","methodCount":15},"candidatesTelemetryData":{"numberOfSuggestions":5,"candidates":[{"lineStart":96,"lineEnd":99,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method tearDown to class AssignmentsManager","description":"Move method tearDown to org.apache.kafka.server.AssignmentsManager\nRationale: The tearDown() method is responsible for closing the manager, which suggests a lifecycle management responsibility. Since the AssignmentsManager class contains a close() method, it indicates that it is designed to manage its own resources, making it a suitable target for this method. Moving tearDown() to AssignmentsManager adheres to the Single Responsibility Principle, as it consolidates resource management within the class that owns the resources. This enhances cohesion and makes the code easier to maintain. However, care must be taken to ensure that the context in which tearDown() is called remains valid after the move, particularly in relation to the manager\u0027s lifecycle.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":96,"lineEnd":99,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method tearDown to class NodeToControllerChannelManager","description":"Move method tearDown to org.apache.kafka.server.NodeToControllerChannelManager\nRationale: Although the NodeToControllerChannelManager does not directly relate to the closing of the manager, it is another potential target due to its role in managing communication channels. If the manager being closed is related to channel management, this could be a logical place for the tearDown() method. This move could help in organizing the shutdown processes related to communication, adhering to the Interface Segregation Principle by keeping related functionalities together. However, this class currently lacks a close() method, which may lead to confusion about the responsibilities of the class and could introduce unnecessary coupling if the method does not align well with its existing responsibilities.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":117,"lineEnd":122,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method assertRequestEquals to class AssignmentsManager","description":"Move method assertRequestEquals to org.apache.kafka.server.AssignmentsManager\nRationale: The assertRequestEquals() method is primarily concerned with comparing two instances of AssignReplicasToDirsRequestData, which is likely related to the assignment logic handled by AssignmentsManager. Moving this method to AssignmentsManager adheres to the Single Responsibility Principle, as it centralizes request validation logic within a class that manages assignments. This enhances cohesion by keeping related functionalities together, making the codebase easier to maintain and understand. However, care must be taken to ensure that the method does not introduce unnecessary dependencies on the state of AssignmentsManager, which could complicate its usage.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":117,"lineEnd":122,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method assertRequestEquals to class NodeToControllerChannelManager","description":"Move method assertRequestEquals to org.apache.kafka.server.NodeToControllerChannelManager\nRationale: While NodeToControllerChannelManager primarily handles communication with the controller, it may also benefit from having request validation methods like assertRequestEquals() to ensure that requests sent are valid. This could improve the robustness of the communication layer. However, this move may dilute the focus of NodeToControllerChannelManager, as it is not directly responsible for assignment logic. It could lead to a violation of the Single Responsibility Principle if the class becomes overloaded with unrelated responsibilities. Therefore, while it is a potential target, it is less ideal than AssignmentsManager.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":372,"lineEnd":381,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method activeWait to class NodeToControllerChannelManager","description":"Move method activeWait to org.apache.kafka.server.NodeToControllerChannelManager\nRationale: The activeWait() method is primarily concerned with managing the state of an operation based on a condition, which aligns closely with the responsibilities of the NodeToControllerChannelManager class. This class is likely involved in coordinating communication and managing the lifecycle of operations, making it a suitable candidate for handling waiting logic. Moving the method here adheres to the Single Responsibility Principle, as it centralizes the waiting logic within the context of channel management. Additionally, this move enhances cohesion by keeping related functionality together, improving maintainability. However, care should be taken to ensure that the method\u0027s dependencies, such as the \u0027manager\u0027 reference and the \u0027time\u0027 object, are appropriately managed within the new context to avoid introducing tight coupling.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false}]},"iterationData":[{"iteration_num":1,"suggested_move_methods":[{"method_name":"normalize","method_signature":"AssignReplicasToDirsRequestData normalize(AssignReplicasToDirsRequestData request)","target_class":"AssignmentsHelper","rationale":"The normalize method is responsible for sorting the directories, topics, and partitions within the AssignReplicasToDirsRequestData. This functionality aligns more closely with data manipulation and utility operations, which would be better suited in a helper class like AssignmentsHelper."},{"method_name":"assertRequestEquals","method_signature":"void assertRequestEquals(AssignReplicasToDirsRequestData expected, AssignReplicasToDirsRequestData actual)","target_class":"AssertionsHelper","rationale":"The assertRequestEquals method is a utility method for comparing two AssignReplicasToDirsRequestData objects. Moving it to a dedicated AssertionsHelper class would promote better separation of concerns and enhance code reusability for assertion-related operations."},{"method_name":"activeWait","method_signature":"void activeWait(Supplier\u003cBoolean\u003e predicate)","target_class":"TestUtils","rationale":"The activeWait method is a utility method that waits for a condition to be satisfied. This type of functionality is typically found in utility classes like TestUtils, which can provide various testing-related helper methods."},{"method_name":"buildSuccessfulResponse","method_signature":"private static ClientResponse buildSuccessfulResponse(AssignReplicasToDirsRequestData request)","target_class":"ResponseBuilder","rationale":"The buildSuccessfulResponse method constructs a successful ClientResponse based on the request data. This method should be moved to a ResponseBuilder class that centralizes response creation logic, improving maintainability and clarity."},{"method_name":"buildResponse","method_signature":"private static ClientResponse buildResponse(AssignReplicasToDirsRequestData request, Function\u003cTopicIdPartition, Errors\u003e perPartitionError)","target_class":"ResponseBuilder","rationale":"Similar to buildSuccessfulResponse, the buildResponse method is responsible for constructing ClientResponse objects. It should be moved to a ResponseBuilder class to consolidate response-related logic and enhance code organization."}],"llm_response_time":7395},{"iteration_num":2,"suggested_move_methods":[{"method_name":"normalize","method_signature":"AssignReplicasToDirsRequestData normalize(AssignReplicasToDirsRequestData request)","target_class":"AssignmentsHelper","rationale":"The normalize method is primarily focused on organizing the structure of AssignReplicasToDirsRequestData. Moving it to AssignmentsHelper would better encapsulate the functionality related to request data manipulation."},{"method_name":"assertRequestEquals","method_signature":"void assertRequestEquals(AssignReplicasToDirsRequestData expected, AssignReplicasToDirsRequestData actual)","target_class":"AssertionsHelper","rationale":"The assertRequestEquals method is a utility for comparing two request data objects. Moving it to a dedicated AssertionsHelper class would promote code reuse and separation of concerns for testing utilities."},{"method_name":"activeWait","method_signature":"void activeWait(Supplier\u003cBoolean\u003e predicate)","target_class":"TestUtils","rationale":"The activeWait method is a utility function for waiting on conditions, which aligns with the responsibilities of TestUtils. Moving it there would centralize waiting logic for tests."},{"method_name":"buildSuccessfulResponse","method_signature":"private static ClientResponse buildSuccessfulResponse(AssignReplicasToDirsRequestData request)","target_class":"ResponseBuilder","rationale":"The buildSuccessfulResponse method is responsible for constructing a successful ClientResponse based on request data. Moving it to a ResponseBuilder class would clarify its purpose and improve code organization."},{"method_name":"buildResponse","method_signature":"private static ClientResponse buildResponse(AssignReplicasToDirsRequestData request, Function\u003cTopicIdPartition, Errors\u003e perPartitionError)","target_class":"ResponseBuilder","rationale":"Similar to buildSuccessfulResponse, the buildResponse method constructs ClientResponse objects. It should be moved to ResponseBuilder to consolidate response-building logic."}],"llm_response_time":6632},{"iteration_num":3,"suggested_move_methods":[{"method_name":"normalize","method_signature":"AssignReplicasToDirsRequestData normalize(AssignReplicasToDirsRequestData request)","target_class":"AssignmentsHelper","rationale":"The normalize method is primarily focused on organizing and sorting the data structure of AssignReplicasToDirsRequestData, which aligns more closely with utility functions that could be encapsulated in a helper class."},{"method_name":"assertRequestEquals","method_signature":"void assertRequestEquals(AssignReplicasToDirsRequestData expected, AssignReplicasToDirsRequestData actual)","target_class":"AssertionsHelper","rationale":"The assertRequestEquals method is a utility for comparing two request data objects, which could be better placed in a dedicated assertions helper class to promote separation of concerns."},{"method_name":"activeWait","method_signature":"void activeWait(Supplier\u003cBoolean\u003e predicate)","target_class":"TestUtils","rationale":"The activeWait method is a utility function that can be reused across different tests, making it a good candidate for inclusion in a utility class like TestUtils."},{"method_name":"buildSuccessfulResponse","method_signature":"private static ClientResponse buildSuccessfulResponse(AssignReplicasToDirsRequestData request)","target_class":"ResponseBuilder","rationale":"The buildSuccessfulResponse method is focused on constructing a response object, which would fit well in a dedicated response building class, enhancing code organization."},{"method_name":"buildResponse","method_signature":"private static ClientResponse buildResponse(AssignReplicasToDirsRequestData request, Function\u003cTopicIdPartition, Errors\u003e perPartitionError)","target_class":"ResponseBuilder","rationale":"Similar to buildSuccessfulResponse, the buildResponse method is responsible for creating response objects and should be moved to a ResponseBuilder class for better encapsulation."}],"llm_response_time":6544},{"iteration_num":-1,"suggested_move_methods":[{"method_name":"setup","method_signature":"@BeforeEach\n    public setup()","target_class":"","rationale":""},{"method_name":"tearDown","method_signature":"@AfterEach tearDown()","target_class":"","rationale":""},{"method_name":"normalize","method_signature":" normalize(AssignReplicasToDirsRequestData request)","target_class":"","rationale":""},{"method_name":"assertRequestEquals","method_signature":" assertRequestEquals(\n        AssignReplicasToDirsRequestData expected,\n        AssignReplicasToDirsRequestData actual\n    )","target_class":"","rationale":""},{"method_name":"buildSuccessfulResponse","method_signature":"private static buildSuccessfulResponse(AssignReplicasToDirsRequestData request)","target_class":"","rationale":""},{"method_name":"buildResponse","method_signature":"private static buildResponse(AssignReplicasToDirsRequestData request,\n                                                Function\u003cTopicIdPartition, Errors\u003e perPartitionError)","target_class":"","rationale":""},{"method_name":"activeWait","method_signature":" activeWait(Supplier\u003cBoolean\u003e predicate)","target_class":"","rationale":""},{"method_name":"findMetric","method_signature":"static findMetric(String name)","target_class":"","rationale":""}],"llm_response_time":0},{"iteration_num":-2,"suggested_move_methods":[{"method_name":"tearDown","method_signature":"@AfterEach tearDown()","target_class":"","rationale":""},{"method_name":"assertRequestEquals","method_signature":" assertRequestEquals(\n        AssignReplicasToDirsRequestData expected,\n        AssignReplicasToDirsRequestData actual\n    )","target_class":"","rationale":""},{"method_name":"activeWait","method_signature":" activeWait(Supplier\u003cBoolean\u003e predicate)","target_class":"","rationale":""},{"method_name":"buildSuccessfulResponse","method_signature":"private static buildSuccessfulResponse(AssignReplicasToDirsRequestData request)","target_class":"","rationale":""},{"method_name":"findMetric","method_signature":"static findMetric(String name)","target_class":"","rationale":""},{"method_name":"normalize","method_signature":" normalize(AssignReplicasToDirsRequestData request)","target_class":"","rationale":""},{"method_name":"buildResponse","method_signature":"private static buildResponse(AssignReplicasToDirsRequestData request,\n                                                Function\u003cTopicIdPartition, Errors\u003e perPartitionError)","target_class":"","rationale":""},{"method_name":"setup","method_signature":"@BeforeEach\n    public setup()","target_class":"","rationale":""}],"llm_response_time":0}],"methodCompatibilityScores":{"@AfterEach tearDown()":{"first":{"method_name":"tearDown","method_signature":"@AfterEach tearDown()","target_class":"","rationale":""},"second":0.24264214662988598}," assertRequestEquals(\n        AssignReplicasToDirsRequestData expected,\n        AssignReplicasToDirsRequestData actual\n    )":{"first":{"method_name":"assertRequestEquals","method_signature":" assertRequestEquals(\n        AssignReplicasToDirsRequestData expected,\n        AssignReplicasToDirsRequestData actual\n    )","target_class":"","rationale":""},"second":0.3069083330941286}," activeWait(Supplier\u003cBoolean\u003e predicate)":{"first":{"method_name":"activeWait","method_signature":" activeWait(Supplier\u003cBoolean\u003e predicate)","target_class":"","rationale":""},"second":0.34899033452268896},"private static buildSuccessfulResponse(AssignReplicasToDirsRequestData request)":{"first":{"method_name":"buildSuccessfulResponse","method_signature":"private static buildSuccessfulResponse(AssignReplicasToDirsRequestData request)","target_class":"","rationale":""},"second":0.3962498073269859},"static findMetric(String name)":{"first":{"method_name":"findMetric","method_signature":"static findMetric(String name)","target_class":"","rationale":""},"second":0.42020595558698054}," normalize(AssignReplicasToDirsRequestData request)":{"first":{"method_name":"normalize","method_signature":" normalize(AssignReplicasToDirsRequestData request)","target_class":"","rationale":""},"second":0.5083744968248176},"private static buildResponse(AssignReplicasToDirsRequestData request,\n                                                Function\u003cTopicIdPartition, Errors\u003e perPartitionError)":{"first":{"method_name":"buildResponse","method_signature":"private static buildResponse(AssignReplicasToDirsRequestData request,\n                                                Function\u003cTopicIdPartition, Errors\u003e perPartitionError)","target_class":"","rationale":""},"second":0.6612683786603846},"@BeforeEach\n    public setup()":{"first":{"method_name":"setup","method_signature":"@BeforeEach\n    public setup()","target_class":"","rationale":""},"second":0.7187920046612672}},"llmMethodPriority":{"priority_method_names":["tearDown","assertRequestEquals","activeWait","buildSuccessfulResponse","findMetric","normalize","buildResponse","setup"],"llm_response_time":1985},"targetClassMap":{"tearDown":{"target_classes":[{"class_name":"NodeToControllerChannelManager","similarity_score":0.34668762264076824},{"class_name":"AssignmentsManager","similarity_score":0.38703832299674}],"target_classes_sorted_by_llm":["AssignmentsManager","NodeToControllerChannelManager"],"llm_response_time":6375,"similarity_computation_time":4,"similarity_metric":"tfidf"},"assertRequestEquals":{"target_classes":[{"class_name":"NodeToControllerChannelManager","similarity_score":0.2719641466102106},{"class_name":"AssignmentsManager","similarity_score":0.3036178402259392}],"target_classes_sorted_by_llm":["AssignmentsManager","NodeToControllerChannelManager"],"llm_response_time":5385,"similarity_computation_time":1,"similarity_metric":"tfidf"},"activeWait":{"target_classes":[{"class_name":"NodeToControllerChannelManager","similarity_score":0.269069117598525}],"target_classes_sorted_by_llm":["NodeToControllerChannelManager"],"llm_response_time":1863,"similarity_computation_time":1,"similarity_metric":"tfidf"}}}
{"id":"9dc0565c-6c7c-4661-9758-543b748c5141","methodCount":15,"hostFunctionTelemetryData":{"hostFunctionSize":198,"lineStart":74,"lineEnd":271,"bodyLineStart":74,"language":"java","filePath":"/Users/fraol/Documents/MM-assist-dataset/kafka/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkThreadTest.java","sourceCode":"public class ConsumerNetworkThreadTest {\n    private final Time time;\n    private final BlockingQueue\u003cApplicationEvent\u003e applicationEventsQueue;\n    private final ApplicationEventProcessor applicationEventProcessor;\n    private final OffsetsRequestManager offsetsRequestManager;\n    private final HeartbeatRequestManager heartbeatRequestManager;\n    private final CoordinatorRequestManager coordinatorRequestManager;\n    private final ConsumerNetworkThread consumerNetworkThread;\n    private final NetworkClientDelegate networkClientDelegate;\n    private final RequestManagers requestManagers;\n    private final CompletableEventReaper applicationEventReaper;\n\n    ConsumerNetworkThreadTest() {\n        this.networkClientDelegate \u003d mock(NetworkClientDelegate.class);\n        this.requestManagers \u003d mock(RequestManagers.class);\n        this.offsetsRequestManager \u003d mock(OffsetsRequestManager.class);\n        this.heartbeatRequestManager \u003d mock(HeartbeatRequestManager.class);\n        this.coordinatorRequestManager \u003d mock(CoordinatorRequestManager.class);\n        this.applicationEventProcessor \u003d mock(ApplicationEventProcessor.class);\n        this.applicationEventReaper \u003d mock(CompletableEventReaper.class);\n        this.time \u003d new MockTime();\n        this.applicationEventsQueue \u003d new LinkedBlockingQueue\u003c\u003e();\n        LogContext logContext \u003d new LogContext();\n\n        this.consumerNetworkThread \u003d new ConsumerNetworkThread(\n                logContext,\n                time,\n                applicationEventsQueue,\n                applicationEventReaper,\n                () -\u003e applicationEventProcessor,\n                () -\u003e networkClientDelegate,\n                () -\u003e requestManagers\n        );\n    }\n\n    @BeforeEach\n    public void setup() {\n        consumerNetworkThread.initializeResources();\n    }\n\n    @AfterEach\n    public void tearDown() {\n        if (consumerNetworkThread !\u003d null)\n            consumerNetworkThread.close();\n    }\n\n    @Test\n    public void testEnsureCloseStopsRunningThread() {\n        assertTrue(consumerNetworkThread.isRunning(),\n            \"ConsumerNetworkThread should start running when created\");\n\n        consumerNetworkThread.close();\n        assertFalse(consumerNetworkThread.isRunning(),\n            \"close() should make consumerNetworkThread.running false by calling closeInternal(Duration timeout)\");\n    }\n\n    @ParameterizedTest\n    @ValueSource(longs \u003d {ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS - 1, ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS, ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS + 1})\n    public void testConsumerNetworkThreadPollTimeComputations(long exampleTime) {\n        List\u003cOptional\u003c? extends RequestManager\u003e\u003e list \u003d new ArrayList\u003c\u003e();\n        list.add(Optional.of(coordinatorRequestManager));\n        list.add(Optional.of(heartbeatRequestManager));\n\n        when(requestManagers.entries()).thenReturn(list);\n\n        NetworkClientDelegate.PollResult pollResult \u003d new NetworkClientDelegate.PollResult(exampleTime);\n        NetworkClientDelegate.PollResult pollResult1 \u003d new NetworkClientDelegate.PollResult(exampleTime + 100);\n\n        long t \u003d time.milliseconds();\n        when(coordinatorRequestManager.poll(t)).thenReturn(pollResult);\n        when(coordinatorRequestManager.maximumTimeToWait(t)).thenReturn(exampleTime);\n        when(heartbeatRequestManager.poll(t)).thenReturn(pollResult1);\n        when(heartbeatRequestManager.maximumTimeToWait(t)).thenReturn(exampleTime + 100);\n        when(networkClientDelegate.addAll(pollResult)).thenReturn(pollResult.timeUntilNextPollMs);\n        when(networkClientDelegate.addAll(pollResult1)).thenReturn(pollResult1.timeUntilNextPollMs);\n        consumerNetworkThread.runOnce();\n\n        verify(networkClientDelegate).poll(Math.min(exampleTime, ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS), time.milliseconds());\n        assertEquals(consumerNetworkThread.maximumTimeToWait(), exampleTime);\n    }\n\n    @Test\n    public void testStartupAndTearDown() throws InterruptedException {\n        consumerNetworkThread.start();\n        TestCondition isStarted \u003d consumerNetworkThread::isRunning;\n        TestCondition isClosed \u003d () -\u003e !(consumerNetworkThread.isRunning() || consumerNetworkThread.isAlive());\n\n        // There\u0027s a nonzero amount of time between starting the thread and having it\n        // begin to execute our code. Wait for a bit before checking...\n        TestUtils.waitForCondition(isStarted,\n                \"The consumer network thread did not start within \" + DEFAULT_MAX_WAIT_MS + \" ms\");\n\n        consumerNetworkThread.close(Duration.ofMillis(DEFAULT_MAX_WAIT_MS));\n\n        TestUtils.waitForCondition(isClosed,\n                \"The consumer network thread did not stop within \" + DEFAULT_MAX_WAIT_MS + \" ms\");\n    }\n\n    @Test\n    public void testRequestsTransferFromManagersToClientOnThreadRun() {\n        List\u003cOptional\u003c? extends RequestManager\u003e\u003e list \u003d new ArrayList\u003c\u003e();\n        list.add(Optional.of(coordinatorRequestManager));\n        list.add(Optional.of(heartbeatRequestManager));\n        list.add(Optional.of(offsetsRequestManager));\n\n        when(requestManagers.entries()).thenReturn(list);\n        when(coordinatorRequestManager.poll(anyLong())).thenReturn(mock(NetworkClientDelegate.PollResult.class));\n        consumerNetworkThread.runOnce();\n        requestManagers.entries().forEach(rmo -\u003e rmo.ifPresent(rm -\u003e verify(rm).poll(anyLong())));\n        requestManagers.entries().forEach(rmo -\u003e rmo.ifPresent(rm -\u003e verify(rm).maximumTimeToWait(anyLong())));\n        verify(networkClientDelegate).addAll(any(NetworkClientDelegate.PollResult.class));\n        verify(networkClientDelegate).poll(anyLong(), anyLong());\n    }\n\n    @ParameterizedTest\n    @MethodSource(\"applicationEvents\")\n    public void testApplicationEventIsProcessed(ApplicationEvent e) {\n        applicationEventsQueue.add(e);\n        consumerNetworkThread.runOnce();\n\n        if (e instanceof CompletableEvent)\n            verify(applicationEventReaper).add((CompletableEvent\u003c?\u003e) e);\n\n        verify(applicationEventProcessor).process(any(e.getClass()));\n        assertTrue(applicationEventsQueue.isEmpty());\n    }\n\n    @ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public void testListOffsetsEventIsProcessed(boolean requireTimestamp) {\n        Map\u003cTopicPartition, Long\u003e timestamps \u003d Collections.singletonMap(new TopicPartition(\"topic1\", 1), 5L);\n        ApplicationEvent e \u003d new ListOffsetsEvent(timestamps, calculateDeadlineMs(time, 100), requireTimestamp);\n        applicationEventsQueue.add(e);\n        consumerNetworkThread.runOnce();\n        verify(applicationEventProcessor).process(any(ListOffsetsEvent.class));\n        assertTrue(applicationEventsQueue.isEmpty());\n    }\n\n    @Test\n    public void testResetPositionsProcessFailureIsIgnored() {\n        doThrow(new NullPointerException()).when(offsetsRequestManager).resetPositionsIfNeeded();\n\n        ResetPositionsEvent event \u003d new ResetPositionsEvent(calculateDeadlineMs(time, 100));\n        applicationEventsQueue.add(event);\n        assertDoesNotThrow(() -\u003e consumerNetworkThread.runOnce());\n\n        verify(applicationEventProcessor).process(any(ResetPositionsEvent.class));\n    }\n\n    @Test\n    public void testMaximumTimeToWait() {\n        final int defaultHeartbeatIntervalMs \u003d 1000;\n        // Initial value before runOnce has been called\n        assertEquals(ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS, consumerNetworkThread.maximumTimeToWait());\n\n        when(requestManagers.entries()).thenReturn(Collections.singletonList(Optional.of(heartbeatRequestManager)));\n        when(heartbeatRequestManager.maximumTimeToWait(time.milliseconds())).thenReturn((long) defaultHeartbeatIntervalMs);\n\n        consumerNetworkThread.runOnce();\n        // After runOnce has been called, it takes the default heartbeat interval from the heartbeat request manager\n        assertEquals(defaultHeartbeatIntervalMs, consumerNetworkThread.maximumTimeToWait());\n    }\n\n    @Test\n    public void testCleanupInvokesReaper() {\n        LinkedList\u003cNetworkClientDelegate.UnsentRequest\u003e queue \u003d new LinkedList\u003c\u003e();\n        when(networkClientDelegate.unsentRequests()).thenReturn(queue);\n        consumerNetworkThread.cleanup();\n        verify(applicationEventReaper).reap(applicationEventsQueue);\n    }\n\n    @Test\n    public void testRunOnceInvokesReaper() {\n        consumerNetworkThread.runOnce();\n        verify(applicationEventReaper).reap(any(Long.class));\n    }\n\n    @Test\n    public void testSendUnsentRequests() {\n        when(networkClientDelegate.hasAnyPendingRequests()).thenReturn(true).thenReturn(true).thenReturn(false);\n        consumerNetworkThread.cleanup();\n        verify(networkClientDelegate, times(2)).poll(anyLong(), anyLong());\n    }\n\n    private static Stream\u003cArguments\u003e applicationEvents() {\n        Map\u003cTopicPartition, OffsetAndMetadata\u003e offset \u003d new HashMap\u003c\u003e();\n        final long currentTimeMs \u003d 12345;\n        return Stream.of(\n                Arguments.of(new PollEvent(100)),\n                Arguments.of(new NewTopicsMetadataUpdateRequestEvent()),\n                Arguments.of(new AsyncCommitEvent(new HashMap\u003c\u003e())),\n                Arguments.of(new SyncCommitEvent(new HashMap\u003c\u003e(), 500)),\n                Arguments.of(new ResetPositionsEvent(500)),\n                Arguments.of(new ValidatePositionsEvent(500)),\n                Arguments.of(new TopicMetadataEvent(\"topic\", Long.MAX_VALUE)),\n                Arguments.of(new AssignmentChangeEvent(offset, currentTimeMs)));\n    }\n}","methodCount":15},"candidatesTelemetryData":{"numberOfSuggestions":7,"candidates":[{"lineStart":108,"lineEnd":111,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method setup to class ConsumerNetworkThread","description":"Move method setup to org.apache.kafka.clients.consumer.internals.ConsumerNetworkThread\nRationale: The setup() method is responsible for initializing resources related to the consumer network, which is a core function of the ConsumerNetworkThread class. Moving this method to ConsumerNetworkThread adheres to the Single Responsibility Principle, as it consolidates resource initialization within the class that directly manages those resources. This enhances cohesion, making the class more self-contained and easier to maintain. However, care should be taken to ensure that any dependencies or context required for initialization are properly managed within the class.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":108,"lineEnd":111,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method setup to class Time","description":"Move method setup to org.apache.kafka.common.utils.Time\nRationale: While the Time class does not directly relate to resource initialization, it could be considered if the setup() method involves timing-related functionality (e.g., initializing resources based on time constraints). However, this is a less suitable option since the Time class is more focused on time manipulation rather than resource management. Moving the method here could violate the Single Responsibility Principle, as it would mix concerns of time management with resource initialization. Therefore, this option is less optimal but could be considered if there are specific timing functionalities that need to be integrated.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":257,"lineEnd":269,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method applicationEvents to class AssignmentTestUtils","description":"move method applicationEvents to PsiClass:AssignmentTestUtils\nRationale: The applicationEvents() method generates various event instances that are likely used in testing scenarios related to assignment and event handling. Moving it to AssignmentTestUtils aligns with the Single Responsibility Principle, as this class already deals with assignment-related utilities. This enhances cohesion and makes the method more reusable in tests. However, care should be taken to ensure that the method\u0027s static nature does not conflict with instance-based testing utilities.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":257,"lineEnd":269,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method applicationEvents to class CompletedFetchTest","description":"move method applicationEvents to PsiClass:CompletedFetchTest\nRationale: The applicationEvents() method creates event instances that may be relevant for testing fetch operations. Relocating it to CompletedFetchTest would allow for better organization of test-related utilities, adhering to the Open/Closed Principle by making it easier to extend tests without modifying existing code. However, this may lead to a bloated test class if not managed properly.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":257,"lineEnd":269,"refactoringType":"MyMoveStaticMethodRefactoring","refactoringInfo":"Move Static method applicationEvents to class OffsetFetcherUtils","description":"move method applicationEvents to PsiClass:OffsetFetcherUtils\nRationale: Given that applicationEvents() generates events related to offsets and metadata, moving it to OffsetFetcherUtils could provide a more centralized location for offset-related utilities. This aligns with the Interface Segregation Principle by keeping offset-related methods together. However, it may dilute the focus of OffsetFetcherUtils if it becomes too broad.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":113,"lineEnd":117,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method tearDown to class ConsumerNetworkThread","description":"Move method tearDown to org.apache.kafka.clients.consumer.internals.ConsumerNetworkThread\nRationale: The tearDown() method is responsible for closing the consumerNetworkThread, which is directly related to the lifecycle management of the ConsumerNetworkThread instance. Moving this method to ConsumerNetworkThread adheres to the Single Responsibility Principle, as it centralizes the resource management logic within the class that owns the resource. This enhances cohesion, making the ConsumerNetworkThread class more self-sufficient in managing its lifecycle. However, care should be taken to ensure that any dependencies on the current class\u0027s state are properly managed, as this could lead to tighter coupling if not handled correctly.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":113,"lineEnd":117,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method tearDown to class Time","description":"Move method tearDown to org.apache.kafka.common.utils.Time\nRationale: While the Time class primarily deals with time-related functionalities, the tearDown() method does not logically fit within its responsibilities. Moving it here would not align with the Single Responsibility Principle, as it would mix resource management with time functionalities. However, if the Time class were to manage various system resources, it could be considered. The drawback of this move is that it could dilute the purpose of the Time class, leading to confusion about its responsibilities and potentially violating the Interface Segregation Principle by forcing it to handle unrelated concerns.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false}]},"iterationData":[{"iteration_num":1,"suggested_move_methods":[{"method_name":"testEnsureCloseStopsRunningThread","method_signature":"public void testEnsureCloseStopsRunningThread()","target_class":"ConsumerNetworkThread","rationale":"This test method directly verifies the behavior of the ConsumerNetworkThread\u0027s close method. It should be part of the ConsumerNetworkThread class to ensure that the thread\u0027s lifecycle management is encapsulated within the class itself."},{"method_name":"testConsumerNetworkThreadPollTimeComputations","method_signature":"public void testConsumerNetworkThreadPollTimeComputations(long exampleTime)","target_class":"ConsumerNetworkThread","rationale":"This method tests the polling time computations of the ConsumerNetworkThread. It should be moved to the ConsumerNetworkThread class to keep related functionality and tests together."},{"method_name":"testRequestsTransferFromManagersToClientOnThreadRun","method_signature":"public void testRequestsTransferFromManagersToClientOnThreadRun()","target_class":"ConsumerNetworkThread","rationale":"This test method verifies the interaction between the ConsumerNetworkThread and its associated request managers. It should be part of the ConsumerNetworkThread class to maintain cohesion."},{"method_name":"testApplicationEventIsProcessed","method_signature":"public void testApplicationEventIsProcessed(ApplicationEvent e)","target_class":"ApplicationEventProcessor","rationale":"This method tests the processing of application events, which is a responsibility of the ApplicationEventProcessor. It should be moved to that class to better reflect its purpose."},{"method_name":"testResetPositionsProcessFailureIsIgnored","method_signature":"public void testResetPositionsProcessFailureIsIgnored()","target_class":"OffsetsRequestManager","rationale":"This test method focuses on the behavior of the offsets request manager when processing reset positions. It should be moved to the OffsetsRequestManager class to encapsulate its specific behavior."},{"method_name":"testMaximumTimeToWait","method_signature":"public void testMaximumTimeToWait()","target_class":"ConsumerNetworkThread","rationale":"This method tests the maximum time to wait functionality of the ConsumerNetworkThread. It should be part of the ConsumerNetworkThread class to keep related functionality together."},{"method_name":"testCleanupInvokesReaper","method_signature":"public void testCleanupInvokesReaper()","target_class":"ConsumerNetworkThread","rationale":"This test method verifies that the cleanup process invokes the reaper. It should be part of the ConsumerNetworkThread class to ensure that cleanup behavior is encapsulated within the class."},{"method_name":"testRunOnceInvokesReaper","method_signature":"public void testRunOnceInvokesReaper()","target_class":"ConsumerNetworkThread","rationale":"This method tests that the runOnce method invokes the reaper. It should be part of the ConsumerNetworkThread class to maintain cohesion."},{"method_name":"testSendUnsentRequests","method_signature":"public void testSendUnsentRequests()","target_class":"ConsumerNetworkThread","rationale":"This method tests the behavior of sending unsent requests. It should be part of the ConsumerNetworkThread class to ensure that request handling is encapsulated within the class."}],"llm_response_time":10556},{"iteration_num":2,"suggested_move_methods":[{"method_name":"testEnsureCloseStopsRunningThread","method_signature":"public void testEnsureCloseStopsRunningThread()","target_class":"ConsumerNetworkThread","rationale":"This test method directly verifies the behavior of the ConsumerNetworkThread class, specifically its close functionality. Moving it to the ConsumerNetworkThread class would help in keeping the tests closer to the implementation they are verifying."},{"method_name":"testConsumerNetworkThreadPollTimeComputations","method_signature":"public void testConsumerNetworkThreadPollTimeComputations(long exampleTime)","target_class":"ConsumerNetworkThread","rationale":"This test method focuses on the polling time computations of the ConsumerNetworkThread. It should be moved to the ConsumerNetworkThread class to maintain a clear relationship between the tests and the class functionality."},{"method_name":"testRequestsTransferFromManagersToClientOnThreadRun","method_signature":"public void testRequestsTransferFromManagersToClientOnThreadRun()","target_class":"ConsumerNetworkThread","rationale":"This test method is specifically validating the interaction between the ConsumerNetworkThread and its request managers. It should be moved to the ConsumerNetworkThread class to better encapsulate related functionality."},{"method_name":"testApplicationEventIsProcessed","method_signature":"public void testApplicationEventIsProcessed(ApplicationEvent e)","target_class":"ApplicationEventProcessor","rationale":"This test method is focused on how application events are processed, which is the responsibility of the ApplicationEventProcessor. Moving it there would align the test with the class that handles this functionality."},{"method_name":"testResetPositionsProcessFailureIsIgnored","method_signature":"public void testResetPositionsProcessFailureIsIgnored()","target_class":"OffsetsRequestManager","rationale":"This test method is specifically testing the behavior of the offsets request manager when an exception occurs. It should be moved to the OffsetsRequestManager class to better reflect its purpose."},{"method_name":"testMaximumTimeToWait","method_signature":"public void testMaximumTimeToWait()","target_class":"ConsumerNetworkThread","rationale":"This test method is directly related to the maximum time to wait functionality of the ConsumerNetworkThread. It should be moved to the ConsumerNetworkThread class to maintain clarity and organization."},{"method_name":"testCleanupInvokesReaper","method_signature":"public void testCleanupInvokesReaper()","target_class":"ConsumerNetworkThread","rationale":"This test method verifies the cleanup process of the ConsumerNetworkThread and its interaction with the application event reaper. It should be moved to the ConsumerNetworkThread class for better encapsulation."},{"method_name":"testRunOnceInvokesReaper","method_signature":"public void testRunOnceInvokesReaper()","target_class":"ConsumerNetworkThread","rationale":"This test method checks the invocation of the reaper during the runOnce method of the ConsumerNetworkThread. It should be moved to the ConsumerNetworkThread class to keep related tests together."},{"method_name":"testSendUnsentRequests","method_signature":"public void testSendUnsentRequests()","target_class":"ConsumerNetworkThread","rationale":"This test method is focused on the behavior of sending unsent requests within the ConsumerNetworkThread. It should be moved to the ConsumerNetworkThread class to maintain a clear relationship with the functionality being tested."}],"llm_response_time":11063},{"iteration_num":3,"suggested_move_methods":[{"method_name":"testEnsureCloseStopsRunningThread","method_signature":"public void testEnsureCloseStopsRunningThread()","target_class":"ConsumerNetworkThread","rationale":"This method tests the behavior of the ConsumerNetworkThread class when it is closed. It should be moved to the ConsumerNetworkThread class to encapsulate its behavior and ensure that tests related to its functionality are located within the class itself."},{"method_name":"testConsumerNetworkThreadPollTimeComputations","method_signature":"public void testConsumerNetworkThreadPollTimeComputations(long exampleTime)","target_class":"ConsumerNetworkThread","rationale":"This method tests the polling time computations of the ConsumerNetworkThread. It should be moved to the ConsumerNetworkThread class to keep related functionality and tests together."},{"method_name":"testStartupAndTearDown","method_signature":"public void testStartupAndTearDown() throws InterruptedException","target_class":"ConsumerNetworkThread","rationale":"This method tests the startup and teardown process of the ConsumerNetworkThread. It should be moved to the ConsumerNetworkThread class to ensure that tests related to its lifecycle are encapsulated within the class."},{"method_name":"testRequestsTransferFromManagersToClientOnThreadRun","method_signature":"public void testRequestsTransferFromManagersToClientOnThreadRun()","target_class":"ConsumerNetworkThread","rationale":"This method tests the transfer of requests from managers to the client during the thread\u0027s run. It should be moved to the ConsumerNetworkThread class to maintain cohesion between the class and its tests."},{"method_name":"testApplicationEventIsProcessed","method_signature":"public void testApplicationEventIsProcessed(ApplicationEvent e)","target_class":"ConsumerNetworkThread","rationale":"This method tests the processing of application events within the ConsumerNetworkThread. It should be moved to the ConsumerNetworkThread class to keep related functionality and tests together."},{"method_name":"testListOffsetsEventIsProcessed","method_signature":"public void testListOffsetsEventIsProcessed(boolean requireTimestamp)","target_class":"ConsumerNetworkThread","rationale":"This method tests the processing of ListOffsets events. It should be moved to the ConsumerNetworkThread class to ensure that event processing tests are encapsulated within the class."},{"method_name":"testResetPositionsProcessFailureIsIgnored","method_signature":"public void testResetPositionsProcessFailureIsIgnored()","target_class":"ConsumerNetworkThread","rationale":"This method tests the behavior of the ConsumerNetworkThread when a reset positions process fails. It should be moved to the ConsumerNetworkThread class to maintain cohesion between the class and its tests."},{"method_name":"testMaximumTimeToWait","method_signature":"public void testMaximumTimeToWait()","target_class":"ConsumerNetworkThread","rationale":"This method tests the maximum time to wait functionality of the ConsumerNetworkThread. It should be moved to the ConsumerNetworkThread class to keep related functionality and tests together."},{"method_name":"testCleanupInvokesReaper","method_signature":"public void testCleanupInvokesReaper()","target_class":"ConsumerNetworkThread","rationale":"This method tests the cleanup process of the ConsumerNetworkThread and its interaction with the reaper. It should be moved to the ConsumerNetworkThread class to ensure that cleanup-related tests are encapsulated within the class."},{"method_name":"testRunOnceInvokesReaper","method_signature":"public void testRunOnceInvokesReaper()","target_class":"ConsumerNetworkThread","rationale":"This method tests the invocation of the reaper during the runOnce method of the ConsumerNetworkThread. It should be moved to the ConsumerNetworkThread class to maintain cohesion between the class and its tests."},{"method_name":"testSendUnsentRequests","method_signature":"public void testSendUnsentRequests()","target_class":"ConsumerNetworkThread","rationale":"This method tests the sending of unsent requests within the ConsumerNetworkThread. It should be moved to the ConsumerNetworkThread class to keep related functionality and tests together."}],"llm_response_time":13727},{"iteration_num":-1,"suggested_move_methods":[{"method_name":"setup","method_signature":"@BeforeEach\n    public setup()","target_class":"","rationale":""},{"method_name":"tearDown","method_signature":"@AfterEach\n    public tearDown()","target_class":"","rationale":""},{"method_name":"testConsumerNetworkThreadPollTimeComputations","method_signature":"@ParameterizedTest\n    @ValueSource(longs \u003d {ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS - 1, ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS, ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS + 1})\n    public testConsumerNetworkThreadPollTimeComputations(long exampleTime)","target_class":"","rationale":""},{"method_name":"testApplicationEventIsProcessed","method_signature":"@ParameterizedTest\n    @MethodSource(\"applicationEvents\")\n    public testApplicationEventIsProcessed(ApplicationEvent e)","target_class":"","rationale":""},{"method_name":"testListOffsetsEventIsProcessed","method_signature":"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testListOffsetsEventIsProcessed(boolean requireTimestamp)","target_class":"","rationale":""},{"method_name":"applicationEvents","method_signature":"private static applicationEvents()","target_class":"","rationale":""}],"llm_response_time":0},{"iteration_num":-2,"suggested_move_methods":[{"method_name":"setup","method_signature":"@BeforeEach\n    public setup()","target_class":"","rationale":""},{"method_name":"applicationEvents","method_signature":"private static applicationEvents()","target_class":"","rationale":""},{"method_name":"tearDown","method_signature":"@AfterEach\n    public tearDown()","target_class":"","rationale":""},{"method_name":"testApplicationEventIsProcessed","method_signature":"@ParameterizedTest\n    @MethodSource(\"applicationEvents\")\n    public testApplicationEventIsProcessed(ApplicationEvent e)","target_class":"","rationale":""},{"method_name":"testListOffsetsEventIsProcessed","method_signature":"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testListOffsetsEventIsProcessed(boolean requireTimestamp)","target_class":"","rationale":""},{"method_name":"testConsumerNetworkThreadPollTimeComputations","method_signature":"@ParameterizedTest\n    @ValueSource(longs \u003d {ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS - 1, ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS, ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS + 1})\n    public testConsumerNetworkThreadPollTimeComputations(long exampleTime)","target_class":"","rationale":""}],"llm_response_time":0}],"methodCompatibilityScores":{"@BeforeEach\n    public setup()":{"first":{"method_name":"setup","method_signature":"@BeforeEach\n    public setup()","target_class":"","rationale":""},"second":0.4899024438788099},"private static applicationEvents()":{"first":{"method_name":"applicationEvents","method_signature":"private static applicationEvents()","target_class":"","rationale":""},"second":0.4941906464139908},"@AfterEach\n    public tearDown()":{"first":{"method_name":"tearDown","method_signature":"@AfterEach\n    public tearDown()","target_class":"","rationale":""},"second":0.5370120319494175},"@ParameterizedTest\n    @MethodSource(\"applicationEvents\")\n    public testApplicationEventIsProcessed(ApplicationEvent e)":{"first":{"method_name":"testApplicationEventIsProcessed","method_signature":"@ParameterizedTest\n    @MethodSource(\"applicationEvents\")\n    public testApplicationEventIsProcessed(ApplicationEvent e)","target_class":"","rationale":""},"second":0.6811899182837046},"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testListOffsetsEventIsProcessed(boolean requireTimestamp)":{"first":{"method_name":"testListOffsetsEventIsProcessed","method_signature":"@ParameterizedTest\n    @ValueSource(booleans \u003d {true, false})\n    public testListOffsetsEventIsProcessed(boolean requireTimestamp)","target_class":"","rationale":""},"second":0.7300478380072795},"@ParameterizedTest\n    @ValueSource(longs \u003d {ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS - 1, ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS, ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS + 1})\n    public testConsumerNetworkThreadPollTimeComputations(long exampleTime)":{"first":{"method_name":"testConsumerNetworkThreadPollTimeComputations","method_signature":"@ParameterizedTest\n    @ValueSource(longs \u003d {ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS - 1, ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS, ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS + 1})\n    public testConsumerNetworkThreadPollTimeComputations(long exampleTime)","target_class":"","rationale":""},"second":0.737329760455618}},"llmMethodPriority":{"priority_method_names":["setup","applicationEvents","tearDown","testApplicationEventIsProcessed","testListOffsetsEventIsProcessed","testConsumerNetworkThreadPollTimeComputations"],"llm_response_time":1438},"targetClassMap":{"setup":{"target_classes":[{"class_name":"Time","similarity_score":0.13719630611540523},{"class_name":"ConsumerNetworkThread","similarity_score":0.23987480118821103}],"target_classes_sorted_by_llm":["ConsumerNetworkThread","Time"],"llm_response_time":4558,"similarity_computation_time":3,"similarity_metric":"tfidf"},"applicationEvents":{"target_classes":[{"class_name":"FetchUtils","similarity_score":0.025482359571881278},{"class_name":"ConsumerUtils","similarity_score":0.24884899630250318},{"class_name":"OffsetFetcherUtils","similarity_score":0.2643175338305979},{"class_name":"OffsetsForLeaderEpochUtils","similarity_score":0.23387491037547753},{"class_name":"ProducerTestUtils","similarity_score":0.1554733542042308},{"class_name":"ClientTelemetryUtils","similarity_score":0.22780048021120808},{"class_name":"ClientUtils","similarity_score":0.21690775770677548},{"class_name":"TaskAssignmentUtilsTest","similarity_score":0.22065490022869094},{"class_name":"InternalQueryResultUtil","similarity_score":0.12288076127325037},{"class_name":"AdminClientTestUtils","similarity_score":0.1617359876781498},{"class_name":"CsvUtils","similarity_score":0.23237900077244503},{"class_name":"GraphGraceSearchUtil","similarity_score":0.23790168754125599},{"class_name":"AssignmentTestUtils","similarity_score":0.2765507021565337},{"class_name":"ConsumerGroupCommandTestUtils","similarity_score":0.1779594611984412},{"class_name":"ConsumerProtocolUtils","similarity_score":0.21494519448743638},{"class_name":"ConsumerRecordUtil","similarity_score":0.13718237135667424},{"class_name":"StandbyTaskAssignmentUtils","similarity_score":0.17618112900581342},{"class_name":"StandbyTaskAssignmentUtilsTest","similarity_score":0.22063250497179882},{"class_name":"StateManagerUtil","similarity_score":0.16164966686237187},{"class_name":"ExceptionUtils","similarity_score":0.20957556296649676},{"class_name":"NetworkClientUtils","similarity_score":0.10798355319767},{"class_name":"StoreQueryUtils","similarity_score":0.25152010930227814},{"class_name":"WrappingNullableUtils","similarity_score":0.20850395827599474},{"class_name":"StreamStreamJoinUtil","similarity_score":0.11975337775498705},{"class_name":"ProcessorContextUtils","similarity_score":0.17811762496977412},{"class_name":"MetricsUtils","similarity_score":0.09160521000760305},{"class_name":"RackUtils","similarity_score":0.2320341166307735},{"class_name":"CommandLineUtils","similarity_score":0.14828226354314467},{"class_name":"CommandLineUtilsTest","similarity_score":0.14396535707343733},{"class_name":"CommandUtils","similarity_score":0.19136963970964108},{"class_name":"AuthorizerUtils","similarity_score":0.15164139783836436},{"class_name":"ControllerMetricsTestUtils","similarity_score":0.22572221244968038},{"class_name":"CommitRequestManager","similarity_score":0.20682008432640023},{"class_name":"CommitRequestManagerTest","similarity_score":0.2367531020895609},{"class_name":"ControllerRequestContextUtil","similarity_score":0.1846408665940713},{"class_name":"ControlRecordUtils","similarity_score":0.22314240099168053},{"class_name":"ClaimValidationUtils","similarity_score":0.04687008339045181},{"class_name":"SystemTestUtil","similarity_score":0.16021033402800036},{"class_name":"FutureUtils","similarity_score":0.09503743569649013},{"class_name":"ClientMetricsTestUtils","similarity_score":0.20750658262930366},{"class_name":"RequestFuture","similarity_score":0.13103671320539137},{"class_name":"RequestFutureTest","similarity_score":0.2025374871452388},{"class_name":"RequestManagers","similarity_score":0.24265984339267566},{"class_name":"RequestTestUtils","similarity_score":0.18775331298958148},{"class_name":"RequestUtils","similarity_score":0.1964891358463238},{"class_name":"IntegrationTestUtils","similarity_score":0.19602316413842202},{"class_name":"CompletedFetch","similarity_score":0.20948880454153365},{"class_name":"CompletedFetchTest","similarity_score":0.2571521661459982},{"class_name":"TaskAssignmentUtils","similarity_score":0.23038343524793395},{"class_name":"TaskBuilder","similarity_score":0.21907588389669605}],"target_classes_sorted_by_llm":["AssignmentTestUtils","CompletedFetchTest","OffsetFetcherUtils","CommitRequestManagerTest","ConsumerUtils","OffsetsForLeaderEpochUtils","StoreQueryUtils","RequestManagers","GraphGraceSearchUtil","CsvUtils"],"llm_response_time":17546,"similarity_computation_time":113,"similarity_metric":"tfidf"},"tearDown":{"target_classes":[{"class_name":"Time","similarity_score":0.12507976106435625},{"class_name":"ConsumerNetworkThread","similarity_score":0.2100225669704419}],"target_classes_sorted_by_llm":["ConsumerNetworkThread","Time"],"llm_response_time":6121,"similarity_computation_time":1,"similarity_metric":"tfidf"}}}
{"id":"d627d6f6-8e79-4831-b775-26bc39615c5e","methodCount":81,"hostFunctionTelemetryData":{"hostFunctionSize":1244,"lineStart":72,"lineEnd":1315,"bodyLineStart":72,"language":"java","filePath":"/Users/fraol/Documents/MM-assist-dataset/kafka/clients/src/main/java/org/apache/kafka/clients/consumer/internals/CommitRequestManager.java","sourceCode":"public class CommitRequestManager implements RequestManager, MemberStateListener {\n    private final Time time;\n    private final SubscriptionState subscriptions;\n    private final LogContext logContext;\n    private final Logger log;\n    private final Optional\u003cAutoCommitState\u003e autoCommitState;\n    private final CoordinatorRequestManager coordinatorRequestManager;\n    private final OffsetCommitCallbackInvoker offsetCommitCallbackInvoker;\n    private final OffsetCommitMetricsManager metricsManager;\n    private final long retryBackoffMs;\n    private final String groupId;\n    private final Optional\u003cString\u003e groupInstanceId;\n    private final long retryBackoffMaxMs;\n    // For testing only\n    private final OptionalDouble jitter;\n    private final boolean throwOnFetchStableOffsetUnsupported;\n    final PendingRequests pendingRequests;\n    private boolean closing \u003d false;\n\n    /**\n     * Last member epoch sent in a commit request. Empty if no epoch was included in the last\n     * request. Used for logging.\n     */\n    private Optional\u003cInteger\u003e lastEpochSentOnCommit;\n\n    /**\n     *  Latest member ID and epoch received via the {@link #onMemberEpochUpdated(Optional, Optional)},\n     *  to be included in the OffsetFetch and OffsetCommit requests if present. This will have\n     *  the latest values received from the broker, or empty of the member is not part of the\n     *  group anymore.\n     */\n    private final MemberInfo memberInfo;\n\n    public CommitRequestManager(\n            final Time time,\n            final LogContext logContext,\n            final SubscriptionState subscriptions,\n            final ConsumerConfig config,\n            final CoordinatorRequestManager coordinatorRequestManager,\n            final OffsetCommitCallbackInvoker offsetCommitCallbackInvoker,\n            final String groupId,\n            final Optional\u003cString\u003e groupInstanceId,\n            final Metrics metrics) {\n        this(time,\n            logContext,\n            subscriptions,\n            config,\n            coordinatorRequestManager,\n            offsetCommitCallbackInvoker,\n            groupId,\n            groupInstanceId,\n            config.getLong(ConsumerConfig.RETRY_BACKOFF_MS_CONFIG),\n            config.getLong(ConsumerConfig.RETRY_BACKOFF_MAX_MS_CONFIG),\n            OptionalDouble.empty(),\n            metrics);\n    }\n\n    // Visible for testing\n    CommitRequestManager(\n        final Time time,\n        final LogContext logContext,\n        final SubscriptionState subscriptions,\n        final ConsumerConfig config,\n        final CoordinatorRequestManager coordinatorRequestManager,\n        final OffsetCommitCallbackInvoker offsetCommitCallbackInvoker,\n        final String groupId,\n        final Optional\u003cString\u003e groupInstanceId,\n        final long retryBackoffMs,\n        final long retryBackoffMaxMs,\n        final OptionalDouble jitter,\n        final Metrics metrics) {\n        Objects.requireNonNull(coordinatorRequestManager, \"Coordinator is needed upon committing offsets\");\n        this.time \u003d time;\n        this.logContext \u003d logContext;\n        this.log \u003d logContext.logger(getClass());\n        this.pendingRequests \u003d new PendingRequests();\n        if (config.getBoolean(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG)) {\n            final long autoCommitInterval \u003d\n                Integer.toUnsignedLong(config.getInt(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG));\n            this.autoCommitState \u003d Optional.of(new AutoCommitState(time, autoCommitInterval, logContext));\n        } else {\n            this.autoCommitState \u003d Optional.empty();\n        }\n        this.coordinatorRequestManager \u003d coordinatorRequestManager;\n        this.groupId \u003d groupId;\n        this.groupInstanceId \u003d groupInstanceId;\n        this.subscriptions \u003d subscriptions;\n        this.retryBackoffMs \u003d retryBackoffMs;\n        this.retryBackoffMaxMs \u003d retryBackoffMaxMs;\n        this.jitter \u003d jitter;\n        this.throwOnFetchStableOffsetUnsupported \u003d config.getBoolean(THROW_ON_FETCH_STABLE_OFFSET_UNSUPPORTED);\n        this.memberInfo \u003d new MemberInfo();\n        this.metricsManager \u003d new OffsetCommitMetricsManager(metrics);\n        this.offsetCommitCallbackInvoker \u003d offsetCommitCallbackInvoker;\n        this.lastEpochSentOnCommit \u003d Optional.empty();\n    }\n\n    /**\n     * Poll for the {@link OffsetFetchRequest} and {@link OffsetCommitRequest} request if there\u0027s any. The function will\n     * also try to autocommit the offsets, if feature is enabled.\n     */\n    @Override\n    public NetworkClientDelegate.PollResult poll(final long currentTimeMs) {\n        // poll only when the coordinator node is known.\n        if (!coordinatorRequestManager.coordinator().isPresent())\n            return EMPTY;\n\n        if (closing) {\n            return drainPendingOffsetCommitRequests();\n        }\n\n        maybeAutoCommitAsync();\n        if (!pendingRequests.hasUnsentRequests())\n            return EMPTY;\n\n        List\u003cNetworkClientDelegate.UnsentRequest\u003e requests \u003d pendingRequests.drain(currentTimeMs);\n        // min of the remainingBackoffMs of all the request that are still backing off\n        final long timeUntilNextPoll \u003d Math.min(\n            findMinTime(unsentOffsetCommitRequests(), currentTimeMs),\n            findMinTime(unsentOffsetFetchRequests(), currentTimeMs));\n        return new NetworkClientDelegate.PollResult(timeUntilNextPoll, requests);\n    }\n\n    @Override\n    public void signalClose() {\n        closing \u003d true;\n    }\n\n    /**\n     * Returns the delay for which the application thread can safely wait before it should be responsive\n     * to results from the request managers. For example, the subscription state can change when heartbeats\n     * are sent, so blocking for longer than the heartbeat interval might mean the application thread is not\n     * responsive to changes.\n     */\n    @Override\n    public long maximumTimeToWait(long currentTimeMs) {\n        return autoCommitState.map(ac -\u003e ac.remainingMs(currentTimeMs)).orElse(Long.MAX_VALUE);\n    }\n\n    private static long findMinTime(final Collection\u003c? extends RequestState\u003e requests, final long currentTimeMs) {\n        return requests.stream()\n            .mapToLong(request -\u003e request.remainingBackoffMs(currentTimeMs))\n            .min()\n            .orElse(Long.MAX_VALUE);\n    }\n\n    private KafkaException maybeWrapAsTimeoutException(Throwable t) {\n        if (t instanceof TimeoutException)\n            return (TimeoutException) t;\n        else\n            return new TimeoutException(t);\n    }\n\n    /**\n     * Generate a request to commit consumed offsets. Add the request to the queue of pending\n     * requests to be sent out on the next call to {@link #poll(long)}. If there are empty\n     * offsets to commit, no request will be generated and a completed future will be returned.\n     *\n     * @param requestState Commit request\n     * @return Future containing the offsets that were committed, or an error if the request\n     * failed.\n     */\n    private CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e requestAutoCommit(final OffsetCommitRequestState requestState) {\n        AutoCommitState autocommit \u003d autoCommitState.get();\n        CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e result;\n        if (requestState.offsets.isEmpty()) {\n            result \u003d CompletableFuture.completedFuture(Collections.emptyMap());\n        } else {\n            autocommit.setInflightCommitStatus(true);\n            OffsetCommitRequestState request \u003d pendingRequests.addOffsetCommitRequest(requestState);\n            result \u003d request.future;\n            result.whenComplete(autoCommitCallback(request.offsets));\n        }\n        return result;\n    }\n\n    /**\n     * If auto-commit is enabled, and the auto-commit interval has expired, this will generate and\n     * enqueue a request to commit all consumed offsets, and will reset the auto-commit timer to the\n     * interval. The request will be sent on the next call to {@link #poll(long)}.\n     * \u003cp/\u003e\n     * If the request completes with a retriable error, this will reset the auto-commit timer with\n     * the exponential backoff. If it fails with a non-retriable error, no action is taken, so\n     * the next commit will be generated when the interval expires.\n     * \u003cp/\u003e\n     * This will not generate a new commit request if a previous one hasn\u0027t received a response.\n     * In that case, the next auto-commit request will be sent on the next call to poll, after a\n     * response for the in-flight is received.\n     */\n    public void maybeAutoCommitAsync() {\n        if (autoCommitEnabled() \u0026\u0026 autoCommitState.get().shouldAutoCommit()) {\n            OffsetCommitRequestState requestState \u003d createOffsetCommitRequest(\n                subscriptions.allConsumed(),\n                Long.MAX_VALUE);\n            CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e result \u003d requestAutoCommit(requestState);\n            // Reset timer to the interval (even if no request was generated), but ensure that if\n            // the request completes with a retriable error, the timer is reset to send the next\n            // auto-commit after the backoff expires.\n            resetAutoCommitTimer();\n            maybeResetTimerWithBackoff(result);\n        }\n    }\n\n    /**\n     * Reset auto-commit timer to retry with backoff if the future failed with a RetriableCommitFailedException.\n     */\n    private void maybeResetTimerWithBackoff(final CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e result) {\n        result.whenComplete((offsets, error) -\u003e {\n            if (error !\u003d null) {\n                if (error instanceof RetriableCommitFailedException) {\n                    log.debug(\"Asynchronous auto-commit of offsets {} failed due to retriable error.\", offsets, error);\n                    resetAutoCommitTimer(retryBackoffMs);\n                } else {\n                    log.debug(\"Asynchronous auto-commit of offsets {} failed: {}\", offsets, error.getMessage());\n                }\n            } else {\n                log.debug(\"Completed asynchronous auto-commit of offsets {}\", offsets);\n            }\n        });\n    }\n\n    /**\n     * Commit consumed offsets if auto-commit is enabled, regardless of the auto-commit interval.\n     * This is used for committing offsets before revoking partitions. This will retry committing\n     * the latest offsets until the request succeeds, fails with a fatal error, or the timeout\n     * expires. Note that:\n     * \u003cul\u003e\n     *     \u003cli\u003eConsiders {@link Errors#STALE_MEMBER_EPOCH} as a retriable error, and will retry it\n     *     including the latest member ID and epoch received from the broker.\u003c/li\u003e\n     *     \u003cli\u003eConsiders {@link Errors#UNKNOWN_TOPIC_OR_PARTITION} as a fatal error, and will not\n     *     retry it although the error extends RetriableException. The reason is that if a topic\n     *     or partition is deleted, revocation would not finish in time since the auto commit would keep retrying.\u003c/li\u003e\n     * \u003c/ul\u003e\n     *\n     * Also note that this will generate a commit request even if there is another one in-flight,\n     * generated by the auto-commit on the interval logic, to ensure that the latest offsets are\n     * committed before revoking partitions.\n     *\n     * @return Future that will complete when the offsets are successfully committed. It will\n     * complete exceptionally if the commit fails with a non-retriable error, or if the retry\n     * timeout expires.\n     */\n    public CompletableFuture\u003cVoid\u003e maybeAutoCommitSyncBeforeRevocation(final long deadlineMs) {\n        if (!autoCommitEnabled()) {\n            return CompletableFuture.completedFuture(null);\n        }\n\n        CompletableFuture\u003cVoid\u003e result \u003d new CompletableFuture\u003c\u003e();\n        OffsetCommitRequestState requestState \u003d\n            createOffsetCommitRequest(subscriptions.allConsumed(), deadlineMs);\n        autoCommitSyncBeforeRevocationWithRetries(requestState, result);\n        return result;\n    }\n\n    private void autoCommitSyncBeforeRevocationWithRetries(OffsetCommitRequestState requestAttempt,\n                                                           CompletableFuture\u003cVoid\u003e result) {\n        CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e commitAttempt \u003d requestAutoCommit(requestAttempt);\n        commitAttempt.whenComplete((committedOffsets, error) -\u003e {\n            if (error \u003d\u003d null) {\n                result.complete(null);\n            } else {\n                if (error instanceof RetriableException || isStaleEpochErrorAndValidEpochAvailable(error)) {\n                    if (requestAttempt.isExpired()) {\n                        log.debug(\"Auto-commit sync before revocation timed out and won\u0027t be retried anymore\");\n                        result.completeExceptionally(maybeWrapAsTimeoutException(error));\n                    } else if (error instanceof UnknownTopicOrPartitionException) {\n                        log.debug(\"Auto-commit sync before revocation failed because topic or partition were deleted\");\n                        result.completeExceptionally(error);\n                    } else {\n                        // Make sure the auto-commit is retried with the latest offsets\n                        log.debug(\"Member {} will retry auto-commit of latest offsets after receiving retriable error {}\",\n                            memberInfo.memberId.orElse(\"undefined\"),\n                            error.getMessage());\n                        requestAttempt.offsets \u003d subscriptions.allConsumed();\n                        requestAttempt.resetFuture();\n                        autoCommitSyncBeforeRevocationWithRetries(requestAttempt, result);\n                    }\n                } else {\n                    log.debug(\"Auto-commit sync before revocation failed with non-retriable error\", error);\n                    result.completeExceptionally(error);\n                }\n            }\n        });\n    }\n\n    /**\n     * Clear the inflight auto-commit flag and log auto-commit completion status.\n     */\n    private BiConsumer\u003c? super Map\u003cTopicPartition, OffsetAndMetadata\u003e, ? super Throwable\u003e autoCommitCallback(final Map\u003cTopicPartition, OffsetAndMetadata\u003e allConsumedOffsets) {\n        return (response, throwable) -\u003e {\n            autoCommitState.ifPresent(autoCommitState -\u003e autoCommitState.setInflightCommitStatus(false));\n            if (throwable \u003d\u003d null) {\n                offsetCommitCallbackInvoker.enqueueInterceptorInvocation(allConsumedOffsets);\n                log.debug(\"Completed auto-commit of offsets {}\", allConsumedOffsets);\n            } else if (throwable instanceof RetriableCommitFailedException) {\n                log.debug(\"Auto-commit of offsets {} failed due to retriable error: {}\",\n                        allConsumedOffsets, throwable.getMessage());\n            } else {\n                log.warn(\"Auto-commit of offsets {} failed\", allConsumedOffsets, throwable);\n            }\n        };\n    }\n\n    /**\n     * Generate a request to commit offsets without retrying, even if it fails with a retriable\n     * error. The generated request will be added to the queue to be sent on the next call to\n     * {@link #poll(long)}.\n     *\n     * @param offsets Offsets to commit per partition.\n     * @return Future that will complete when a response is received, successfully or\n     * exceptionally depending on the response. If the request fails with a retriable error, the\n     * future will be completed with a {@link RetriableCommitFailedException}.\n     */\n    public CompletableFuture\u003cVoid\u003e commitAsync(final Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets) {\n        if (offsets.isEmpty()) {\n            log.debug(\"Skipping commit of empty offsets\");\n            return CompletableFuture.completedFuture(null);\n        }\n        OffsetCommitRequestState commitRequest \u003d createOffsetCommitRequest(offsets, Long.MAX_VALUE);\n        pendingRequests.addOffsetCommitRequest(commitRequest);\n\n        CompletableFuture\u003cVoid\u003e asyncCommitResult \u003d new CompletableFuture\u003c\u003e();\n        commitRequest.future.whenComplete((committedOffsets, error) -\u003e {\n            if (error !\u003d null) {\n                asyncCommitResult.completeExceptionally(commitAsyncExceptionForError(error));\n            } else {\n                asyncCommitResult.complete(null);\n            }\n        });\n        return asyncCommitResult;\n    }\n\n    /**\n     * Commit offsets, retrying on expected retriable errors while the retry timeout hasn\u0027t expired.\n     *\n     * @param offsets               Offsets to commit\n     * @param deadlineMs            Time until which the request will be retried if it fails with\n     *                              an expected retriable error.\n     * @return Future that will complete when a successful response\n     */\n    public CompletableFuture\u003cVoid\u003e commitSync(final Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets,\n                                              final long deadlineMs) {\n        CompletableFuture\u003cVoid\u003e result \u003d new CompletableFuture\u003c\u003e();\n        OffsetCommitRequestState requestState \u003d createOffsetCommitRequest(offsets, deadlineMs);\n        commitSyncWithRetries(requestState, result);\n        return result;\n    }\n\n    private OffsetCommitRequestState createOffsetCommitRequest(final Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets,\n                                                               final long deadlineMs) {\n        return jitter.isPresent() ?\n            new OffsetCommitRequestState(\n                offsets,\n                groupId,\n                groupInstanceId,\n                deadlineMs,\n                retryBackoffMs,\n                retryBackoffMaxMs,\n                jitter.getAsDouble(),\n                memberInfo) :\n            new OffsetCommitRequestState(\n                offsets,\n                groupId,\n                groupInstanceId,\n                deadlineMs,\n                retryBackoffMs,\n                retryBackoffMaxMs,\n                memberInfo);\n    }\n\n    private void commitSyncWithRetries(OffsetCommitRequestState requestAttempt,\n                                       CompletableFuture\u003cVoid\u003e result) {\n        pendingRequests.addOffsetCommitRequest(requestAttempt);\n\n        // Retry the same commit request while it fails with RetriableException and the retry\n        // timeout hasn\u0027t expired.\n        requestAttempt.future.whenComplete((res, error) -\u003e {\n            if (error \u003d\u003d null) {\n                result.complete(null);\n            } else {\n                if (error instanceof RetriableException) {\n                    if (requestAttempt.isExpired()) {\n                        log.info(\"OffsetCommit timeout expired so it won\u0027t be retried anymore\");\n                        result.completeExceptionally(maybeWrapAsTimeoutException(error));\n                    } else {\n                        requestAttempt.resetFuture();\n                        commitSyncWithRetries(requestAttempt, result);\n                    }\n                } else {\n                    result.completeExceptionally(commitSyncExceptionForError(error));\n                }\n            }\n        });\n    }\n\n    private Throwable commitSyncExceptionForError(Throwable error) {\n        if (error instanceof StaleMemberEpochException) {\n            return new CommitFailedException(\"OffsetCommit failed with stale member epoch.\"\n                + Errors.STALE_MEMBER_EPOCH.message());\n        }\n        return error;\n    }\n\n    private Throwable commitAsyncExceptionForError(Throwable error) {\n        if (error instanceof RetriableException) {\n            return new RetriableCommitFailedException(error);\n        }\n        return error;\n    }\n\n    /**\n     * Enqueue a request to fetch committed offsets, that will be sent on the next call to {@link #poll(long)}.\n     *\n     * @param partitions       Partitions to fetch offsets for.\n     * @param deadlineMs       Time until which the request should be retried if it fails\n     *                         with expected retriable errors.\n     * @return Future that will complete when a successful response is received, or the request\n     * fails and cannot be retried. Note that the request is retried whenever it fails with\n     * retriable expected error and the retry time hasn\u0027t expired.\n     */\n    public CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e fetchOffsets(\n        final Set\u003cTopicPartition\u003e partitions,\n        final long deadlineMs) {\n        if (partitions.isEmpty()) {\n            return CompletableFuture.completedFuture(Collections.emptyMap());\n        }\n        CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e result \u003d new CompletableFuture\u003c\u003e();\n        OffsetFetchRequestState request \u003d createOffsetFetchRequest(partitions, deadlineMs);\n        fetchOffsetsWithRetries(request, result);\n        return result;\n    }\n\n    // Visible for testing\n    OffsetFetchRequestState createOffsetFetchRequest(final Set\u003cTopicPartition\u003e partitions,\n                                                             final long deadlineMs) {\n        return jitter.isPresent() ?\n            new OffsetFetchRequestState(\n                partitions,\n                retryBackoffMs,\n                retryBackoffMaxMs,\n                deadlineMs,\n                jitter.getAsDouble(),\n                memberInfo) :\n            new OffsetFetchRequestState(\n                partitions,\n                retryBackoffMs,\n                retryBackoffMaxMs,\n                deadlineMs,\n                memberInfo);\n    }\n\n    private void fetchOffsetsWithRetries(final OffsetFetchRequestState fetchRequest,\n                                         final CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e result) {\n        CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e currentResult \u003d pendingRequests.addOffsetFetchRequest(fetchRequest);\n\n        // Retry the same fetch request while it fails with RetriableException and the retry timeout hasn\u0027t expired.\n        currentResult.whenComplete((res, error) -\u003e {\n            boolean inflightRemoved \u003d pendingRequests.inflightOffsetFetches.remove(fetchRequest);\n            if (!inflightRemoved) {\n                log.warn(\"A duplicated, inflight, request was identified, but unable to find it in the \" +\n                    \"outbound buffer:\" + fetchRequest);\n            }\n            if (error \u003d\u003d null) {\n                result.complete(res);\n            } else {\n                if (error instanceof RetriableException || isStaleEpochErrorAndValidEpochAvailable(error)) {\n                    if (fetchRequest.isExpired()) {\n                        log.debug(\"OffsetFetch request for {} timed out and won\u0027t be retried anymore\", fetchRequest.requestedPartitions);\n                        result.completeExceptionally(maybeWrapAsTimeoutException(error));\n                    } else {\n                        fetchRequest.resetFuture();\n                        fetchOffsetsWithRetries(fetchRequest, result);\n                    }\n                } else\n                    result.completeExceptionally(error);\n            }\n        });\n    }\n\n    private boolean isStaleEpochErrorAndValidEpochAvailable(Throwable error) {\n        return error instanceof StaleMemberEpochException \u0026\u0026 memberInfo.memberEpoch.isPresent();\n    }\n\n    public void updateAutoCommitTimer(final long currentTimeMs) {\n        this.autoCommitState.ifPresent(t -\u003e t.updateTimer(currentTimeMs));\n    }\n\n    // Visible for testing\n    Queue\u003cOffsetCommitRequestState\u003e unsentOffsetCommitRequests() {\n        return pendingRequests.unsentOffsetCommits;\n    }\n\n    private List\u003cOffsetFetchRequestState\u003e unsentOffsetFetchRequests() {\n        return pendingRequests.unsentOffsetFetches;\n    }\n\n    private void handleCoordinatorDisconnect(Throwable exception, long currentTimeMs) {\n        if (exception instanceof DisconnectException) {\n            coordinatorRequestManager.markCoordinatorUnknown(exception.getMessage(), currentTimeMs);\n        }\n    }\n\n    /**\n     * Update latest member ID and epoch used by the member.\n     *\n     * @param memberEpoch New member epoch received. To be included in the new request.\n     * @param memberId Current member ID. To be included in the new request.\n     */\n    @Override\n    public void onMemberEpochUpdated(Optional\u003cInteger\u003e memberEpoch, Optional\u003cString\u003e memberId) {\n        if (!memberEpoch.isPresent() \u0026\u0026 memberInfo.memberEpoch.isPresent()) {\n            log.info(\"Member {} won\u0027t include member id and epoch in following offset \" +\n                \"commit/fetch requests because it has left the group.\", memberInfo.memberId.orElse(\"unknown\"));\n        }\n        memberInfo.memberId \u003d memberId;\n        memberInfo.memberEpoch \u003d memberEpoch;\n    }\n\n    /**\n     * @return True if auto-commit is enabled as defined in the config {@link ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG}\n     */\n    public boolean autoCommitEnabled() {\n        return autoCommitState.isPresent();\n    }\n\n    /**\n     * Reset the auto-commit timer to the auto-commit interval, so that the next auto-commit is\n     * sent out on the interval starting from now. If auto-commit is not enabled this will\n     * perform no action.\n     */\n    public void resetAutoCommitTimer() {\n        autoCommitState.ifPresent(AutoCommitState::resetTimer);\n    }\n\n    /**\n     * Reset the auto-commit timer to the provided time (backoff), so that the next auto-commit is\n     * sent out then. If auto-commit is not enabled this will perform no action.\n     */\n    public void resetAutoCommitTimer(long retryBackoffMs) {\n        autoCommitState.ifPresent(s -\u003e s.resetTimer(retryBackoffMs));\n    }\n\n    /**\n     * Drains the inflight offsetCommits during shutdown because we want to make sure all pending commits are sent\n     * before closing.\n     */\n    public NetworkClientDelegate.PollResult drainPendingOffsetCommitRequests() {\n        if (pendingRequests.unsentOffsetCommits.isEmpty())\n            return EMPTY;\n        List\u003cNetworkClientDelegate.UnsentRequest\u003e requests \u003d pendingRequests.drainPendingCommits();\n        return new NetworkClientDelegate.PollResult(Long.MAX_VALUE, requests);\n    }\n\n    private class OffsetCommitRequestState extends RetriableRequestState {\n        private Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets;\n        private final String groupId;\n        private final Optional\u003cString\u003e groupInstanceId;\n\n        /**\n         * Future containing the offsets that were committed. It completes when a response is\n         * received for the commit request.\n         */\n        private CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e future;\n\n        OffsetCommitRequestState(final Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets,\n                                 final String groupId,\n                                 final Optional\u003cString\u003e groupInstanceId,\n                                 final long deadlineMs,\n                                 final long retryBackoffMs,\n                                 final long retryBackoffMaxMs,\n                                 final MemberInfo memberInfo) {\n            super(logContext, CommitRequestManager.class.getSimpleName(), retryBackoffMs,\n                retryBackoffMaxMs, memberInfo, deadlineTimer(time, deadlineMs));\n            this.offsets \u003d offsets;\n            this.groupId \u003d groupId;\n            this.groupInstanceId \u003d groupInstanceId;\n            this.future \u003d new CompletableFuture\u003c\u003e();\n        }\n\n        // Visible for testing\n        OffsetCommitRequestState(final Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets,\n                                 final String groupId,\n                                 final Optional\u003cString\u003e groupInstanceId,\n                                 final long deadlineMs,\n                                 final long retryBackoffMs,\n                                 final long retryBackoffMaxMs,\n                                 final double jitter,\n                                 final MemberInfo memberInfo) {\n            super(logContext, CommitRequestManager.class.getSimpleName(), retryBackoffMs, 2,\n                retryBackoffMaxMs, jitter, memberInfo, deadlineTimer(time, deadlineMs));\n            this.offsets \u003d offsets;\n            this.groupId \u003d groupId;\n            this.groupInstanceId \u003d groupInstanceId;\n            this.future \u003d new CompletableFuture\u003c\u003e();\n        }\n\n        public NetworkClientDelegate.UnsentRequest toUnsentRequest() {\n            Map\u003cString, OffsetCommitRequestData.OffsetCommitRequestTopic\u003e requestTopicDataMap \u003d new HashMap\u003c\u003e();\n            for (Map.Entry\u003cTopicPartition, OffsetAndMetadata\u003e entry : offsets.entrySet()) {\n                TopicPartition topicPartition \u003d entry.getKey();\n                OffsetAndMetadata offsetAndMetadata \u003d entry.getValue();\n\n                OffsetCommitRequestData.OffsetCommitRequestTopic topic \u003d requestTopicDataMap\n                    .getOrDefault(topicPartition.topic(),\n                        new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                            .setName(topicPartition.topic())\n                    );\n\n                topic.partitions().add(new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                    .setPartitionIndex(topicPartition.partition())\n                    .setCommittedOffset(offsetAndMetadata.offset())\n                    .setCommittedLeaderEpoch(offsetAndMetadata.leaderEpoch().orElse(RecordBatch.NO_PARTITION_LEADER_EPOCH))\n                    .setCommittedMetadata(offsetAndMetadata.metadata())\n                );\n                requestTopicDataMap.put(topicPartition.topic(), topic);\n            }\n\n            OffsetCommitRequestData data \u003d new OffsetCommitRequestData()\n                    .setGroupId(this.groupId)\n                    .setGroupInstanceId(groupInstanceId.orElse(null))\n                    .setTopics(new ArrayList\u003c\u003e(requestTopicDataMap.values()));\n            if (memberInfo.memberId.isPresent()) {\n                data \u003d data.setMemberId(memberInfo.memberId.get());\n            }\n            if (memberInfo.memberEpoch.isPresent()) {\n                data \u003d data.setGenerationIdOrMemberEpoch(memberInfo.memberEpoch.get());\n                lastEpochSentOnCommit \u003d memberInfo.memberEpoch;\n            } else {\n                lastEpochSentOnCommit \u003d Optional.empty();\n            }\n\n            OffsetCommitRequest.Builder builder \u003d new OffsetCommitRequest.Builder(data);\n\n            return buildRequestWithResponseHandling(builder);\n        }\n\n        /**\n         * Handle OffsetCommitResponse. This will complete the request future successfully if no\n         * errors are found in the response. If the response contains errors, this will:\n         *   - handle expected errors and fail the future with specific exceptions depending on the error\n         *   - fail the future with a non-recoverable KafkaException for all unexpected errors (even if retriable)\n         */\n        @Override\n        public void onResponse(final ClientResponse response) {\n            metricsManager.recordRequestLatency(response.requestLatencyMs());\n            long currentTimeMs \u003d response.receivedTimeMs();\n            OffsetCommitResponse commitResponse \u003d (OffsetCommitResponse) response.responseBody();\n            Set\u003cString\u003e unauthorizedTopics \u003d new HashSet\u003c\u003e();\n            for (OffsetCommitResponseData.OffsetCommitResponseTopic topic : commitResponse.data().topics()) {\n                for (OffsetCommitResponseData.OffsetCommitResponsePartition partition : topic.partitions()) {\n                    TopicPartition tp \u003d new TopicPartition(topic.name(), partition.partitionIndex());\n\n                    Errors error \u003d Errors.forCode(partition.errorCode());\n                    if (error \u003d\u003d Errors.NONE) {\n                        OffsetAndMetadata offsetAndMetadata \u003d offsets.get(tp);\n                        long offset \u003d offsetAndMetadata.offset();\n                        log.debug(\"OffsetCommit completed successfully for offset {} partition {}\", offset, tp);\n                        continue;\n                    }\n\n                    onFailedAttempt(currentTimeMs);\n                    if (error \u003d\u003d Errors.GROUP_AUTHORIZATION_FAILED) {\n                        future.completeExceptionally(GroupAuthorizationException.forGroupId(groupId));\n                        return;\n                    } else if (error \u003d\u003d Errors.COORDINATOR_NOT_AVAILABLE ||\n                        error \u003d\u003d Errors.NOT_COORDINATOR ||\n                        error \u003d\u003d Errors.REQUEST_TIMED_OUT) {\n                        coordinatorRequestManager.markCoordinatorUnknown(error.message(), currentTimeMs);\n                        future.completeExceptionally(error.exception());\n                        return;\n                    } else if (error \u003d\u003d Errors.FENCED_INSTANCE_ID) {\n                        String fencedError \u003d \"OffsetCommit failed due to group instance id fenced: \" + groupInstanceId;\n                        log.error(fencedError);\n                        future.completeExceptionally(new CommitFailedException(fencedError));\n                        return;\n                    } else if (error \u003d\u003d Errors.OFFSET_METADATA_TOO_LARGE ||\n                        error \u003d\u003d Errors.INVALID_COMMIT_OFFSET_SIZE) {\n                        future.completeExceptionally(error.exception());\n                        return;\n                    } else if (error \u003d\u003d Errors.COORDINATOR_LOAD_IN_PROGRESS ||\n                        error \u003d\u003d Errors.UNKNOWN_TOPIC_OR_PARTITION) {\n                        // just retry\n                        future.completeExceptionally(error.exception());\n                        return;\n                    } else if (error \u003d\u003d Errors.UNKNOWN_MEMBER_ID) {\n                        log.error(\"OffsetCommit failed with {}\", error);\n                        future.completeExceptionally(new CommitFailedException(\"OffsetCommit \" +\n                            \"failed with unknown member ID. \" + error.message()));\n                        return;\n                    } else if (error \u003d\u003d Errors.STALE_MEMBER_EPOCH) {\n                        log.error(\"OffsetCommit failed for member {} with stale member epoch error. Last epoch sent: {}\",\n                            memberInfo.memberId.orElse(\"undefined\"),\n                            lastEpochSentOnCommit.isPresent() ? lastEpochSentOnCommit.get() : \"undefined\");\n                        future.completeExceptionally(error.exception());\n                        return;\n                    } else if (error \u003d\u003d Errors.TOPIC_AUTHORIZATION_FAILED) {\n                        // Collect all unauthorized topics before failing\n                        unauthorizedTopics.add(tp.topic());\n                    } else {\n                        // Fail with a non-retriable KafkaException for all unexpected errors\n                        // (even if they are retriable)\n                        future.completeExceptionally(new KafkaException(\"Unexpected error in commit: \" + error.message()));\n                        return;\n                    }\n                }\n            }\n\n            if (!unauthorizedTopics.isEmpty()) {\n                log.error(\"OffsetCommit failed due to not authorized to commit to topics {}\", unauthorizedTopics);\n                future.completeExceptionally(new TopicAuthorizationException(unauthorizedTopics));\n            } else {\n                future.complete(null);\n            }\n        }\n\n        @Override\n        String requestDescription() {\n            return \"OffsetCommit request for offsets \" + offsets;\n        }\n\n        @Override\n        CompletableFuture\u003c?\u003e future() {\n            return future;\n        }\n\n        void resetFuture() {\n            future \u003d new CompletableFuture\u003c\u003e();\n        }\n\n        @Override\n        void removeRequest() {\n            if (!unsentOffsetCommitRequests().remove(this)) {\n                log.warn(\"OffsetCommit request to remove not found in the outbound buffer: {}\", this);\n            }\n        }\n    }\n\n    // Visible for testing\n    Optional\u003cInteger\u003e lastEpochSentOnCommit() {\n        return lastEpochSentOnCommit;\n    }\n\n    /**\n     * Represents a request that can be retried or aborted, based on member ID and epoch\n     * information.\n     */\n    abstract class RetriableRequestState extends TimedRequestState {\n\n        /**\n         * Member info (ID and epoch) to be included in the request if present.\n         */\n        final MemberInfo memberInfo;\n\n        RetriableRequestState(LogContext logContext, String owner, long retryBackoffMs,\n                              long retryBackoffMaxMs, MemberInfo memberInfo, Timer timer) {\n            super(logContext, owner, retryBackoffMs, retryBackoffMaxMs, timer);\n            this.memberInfo \u003d memberInfo;\n        }\n\n        // Visible for testing\n        RetriableRequestState(LogContext logContext, String owner, long retryBackoffMs, int retryBackoffExpBase,\n                              long retryBackoffMaxMs, double jitter, MemberInfo memberInfo, Timer timer) {\n            super(logContext, owner, retryBackoffMs, retryBackoffExpBase, retryBackoffMaxMs, jitter, timer);\n            this.memberInfo \u003d memberInfo;\n        }\n\n        /**\n         * @return String containing the request name and arguments, to be used for logging\n         * purposes.\n         */\n        abstract String requestDescription();\n\n        /**\n         * @return Future that will complete with the request response or failure.\n         */\n        abstract CompletableFuture\u003c?\u003e future();\n\n        /**\n         * Complete the request future with a TimeoutException if the request has been sent out\n         * at least once and the timeout has been reached.\n         */\n        void maybeExpire() {\n            if (numAttempts \u003e 0 \u0026\u0026 isExpired()) {\n                removeRequest();\n                future().completeExceptionally(new TimeoutException(requestDescription() +\n                    \" could not complete before timeout expired.\"));\n            }\n        }\n\n        /**\n         * Build request with the given builder, including response handling logic.\n         */\n        NetworkClientDelegate.UnsentRequest buildRequestWithResponseHandling(final AbstractRequest.Builder\u003c?\u003e builder) {\n            NetworkClientDelegate.UnsentRequest request \u003d new NetworkClientDelegate.UnsentRequest(\n                builder,\n                coordinatorRequestManager.coordinator()\n            );\n            request.whenComplete(\n                (response, throwable) -\u003e {\n                    long completionTimeMs \u003d request.handler().completionTimeMs();\n                    handleClientResponse(response, throwable, completionTimeMs);\n                });\n            return request;\n        }\n\n        private void handleClientResponse(final ClientResponse response,\n                                          final Throwable error,\n                                          final long requestCompletionTimeMs) {\n            try {\n                if (error \u003d\u003d null) {\n                    onResponse(response);\n                } else {\n                    log.debug(\"{} completed with error\", requestDescription(), error);\n                    onFailedAttempt(requestCompletionTimeMs);\n                    handleCoordinatorDisconnect(error, requestCompletionTimeMs);\n                    future().completeExceptionally(error);\n                }\n            } catch (Throwable t) {\n                log.error(\"Unexpected error handling response for {}\", requestDescription(), t);\n                future().completeExceptionally(t);\n            }\n        }\n\n        @Override\n        public String toStringBase() {\n            return super.toStringBase() + \", \" + memberInfo;\n        }\n\n        abstract void onResponse(final ClientResponse response);\n\n        abstract void removeRequest();\n    }\n\n    class OffsetFetchRequestState extends RetriableRequestState {\n\n        /**\n         * Partitions to get committed offsets for.\n         */\n        public final Set\u003cTopicPartition\u003e requestedPartitions;\n\n        /**\n         * Future with the result of the request. This can be reset using {@link #resetFuture()}\n         * to get a new result when the request is retried.\n         */\n        private CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e future;\n\n        public OffsetFetchRequestState(final Set\u003cTopicPartition\u003e partitions,\n                                       final long retryBackoffMs,\n                                       final long retryBackoffMaxMs,\n                                       final long deadlineMs,\n                                       final MemberInfo memberInfo) {\n            super(logContext, CommitRequestManager.class.getSimpleName(), retryBackoffMs,\n                retryBackoffMaxMs, memberInfo, deadlineTimer(time, deadlineMs));\n            this.requestedPartitions \u003d partitions;\n            this.future \u003d new CompletableFuture\u003c\u003e();\n        }\n\n        public OffsetFetchRequestState(final Set\u003cTopicPartition\u003e partitions,\n                                       final long retryBackoffMs,\n                                       final long retryBackoffMaxMs,\n                                       final long deadlineMs,\n                                       final double jitter,\n                                       final MemberInfo memberInfo) {\n            super(logContext, CommitRequestManager.class.getSimpleName(), retryBackoffMs, 2,\n                retryBackoffMaxMs, jitter, memberInfo, deadlineTimer(time, deadlineMs));\n            this.requestedPartitions \u003d partitions;\n            this.future \u003d new CompletableFuture\u003c\u003e();\n        }\n\n        public boolean sameRequest(final OffsetFetchRequestState request) {\n            return requestedPartitions.equals(request.requestedPartitions);\n        }\n\n        public NetworkClientDelegate.UnsentRequest toUnsentRequest() {\n\n            OffsetFetchRequest.Builder builder;\n            if (memberInfo.memberId.isPresent() \u0026\u0026 memberInfo.memberEpoch.isPresent()) {\n                builder \u003d new OffsetFetchRequest.Builder(\n                        groupId,\n                        memberInfo.memberId.get(),\n                        memberInfo.memberEpoch.get(),\n                        true,\n                        new ArrayList\u003c\u003e(this.requestedPartitions),\n                        throwOnFetchStableOffsetUnsupported);\n            } else {\n                // Building request without passing member ID/epoch to leave the logic to choose\n                // default values when not present on the request builder.\n                builder \u003d new OffsetFetchRequest.Builder(\n                        groupId,\n                        true,\n                        new ArrayList\u003c\u003e(this.requestedPartitions),\n                        throwOnFetchStableOffsetUnsupported);\n            }\n            return buildRequestWithResponseHandling(builder);\n        }\n\n        /**\n         * Handle OffsetFetch response, including successful and failed.\n         */\n        @Override\n        void onResponse(final ClientResponse response) {\n            long currentTimeMs \u003d response.receivedTimeMs();\n            OffsetFetchResponse fetchResponse \u003d (OffsetFetchResponse) response.responseBody();\n            Errors responseError \u003d fetchResponse.groupLevelError(groupId);\n            if (responseError !\u003d Errors.NONE) {\n                onFailure(currentTimeMs, responseError);\n                return;\n            }\n            onSuccess(currentTimeMs, fetchResponse);\n        }\n\n        /**\n         * Handle failed responses. This will retry if the error is retriable, or complete the\n         * result future exceptionally in the case of non-recoverable or unexpected errors.\n         */\n        private void onFailure(final long currentTimeMs,\n                               final Errors responseError) {\n            log.debug(\"Offset fetch failed: {}\", responseError.message());\n            onFailedAttempt(currentTimeMs);\n            if (responseError \u003d\u003d COORDINATOR_LOAD_IN_PROGRESS) {\n                future.completeExceptionally(responseError.exception());\n            } else if (responseError \u003d\u003d Errors.UNKNOWN_MEMBER_ID) {\n                log.error(\"OffsetFetch failed with {} because the member is not part of the group\" +\n                    \" anymore.\", responseError);\n                future.completeExceptionally(responseError.exception());\n            } else if (responseError \u003d\u003d Errors.STALE_MEMBER_EPOCH) {\n                log.error(\"OffsetFetch failed with {} and the consumer is not part \" +\n                    \"of the group anymore (it probably left the group, got fenced\" +\n                    \" or failed). The request cannot be retried and will fail.\", responseError);\n                future.completeExceptionally(responseError.exception());\n            } else if (responseError \u003d\u003d Errors.NOT_COORDINATOR || responseError \u003d\u003d Errors.COORDINATOR_NOT_AVAILABLE) {\n                // Re-discover the coordinator and retry\n                coordinatorRequestManager.markCoordinatorUnknown(\"error response \" + responseError.name(), currentTimeMs);\n                future.completeExceptionally(responseError.exception());\n            } else if (responseError \u003d\u003d Errors.GROUP_AUTHORIZATION_FAILED) {\n                future.completeExceptionally(GroupAuthorizationException.forGroupId(groupId));\n            } else {\n                // Fail with a non-retriable KafkaException for all unexpected errors (even if\n                // they are retriable)\n                future.completeExceptionally(new KafkaException(\"Unexpected error in fetch offset response: \" + responseError.message()));\n            }\n        }\n\n        @Override\n        String requestDescription() {\n            return \"OffsetFetch request for partitions \" + requestedPartitions;\n        }\n\n        @Override\n        CompletableFuture\u003c?\u003e future() {\n            return future;\n        }\n\n        void resetFuture() {\n            future \u003d new CompletableFuture\u003c\u003e();\n        }\n\n        @Override\n        void removeRequest() {\n            if (!unsentOffsetFetchRequests().remove(this)) {\n                log.warn(\"OffsetFetch request to remove not found in the outbound buffer: {}\", this);\n            }\n        }\n\n        /**\n         * Handle OffsetFetch response that has no group level errors. This will look for\n         * partition level errors and fail the future accordingly, also recording a failed request\n         * attempt. If no partition level errors are found, this will complete the future with the\n         * offsets contained in the response, and record a successful request attempt.\n         */\n        private void onSuccess(final long currentTimeMs,\n                               final OffsetFetchResponse response) {\n            Set\u003cString\u003e unauthorizedTopics \u003d null;\n            Map\u003cTopicPartition, OffsetFetchResponse.PartitionData\u003e responseData \u003d\n                    response.partitionDataMap(groupId);\n            Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets \u003d new HashMap\u003c\u003e(responseData.size());\n            Set\u003cTopicPartition\u003e unstableTxnOffsetTopicPartitions \u003d new HashSet\u003c\u003e();\n            for (Map.Entry\u003cTopicPartition, OffsetFetchResponse.PartitionData\u003e entry : responseData.entrySet()) {\n                TopicPartition tp \u003d entry.getKey();\n                OffsetFetchResponse.PartitionData partitionData \u003d entry.getValue();\n                if (partitionData.hasError()) {\n                    onFailedAttempt(currentTimeMs);\n                    Errors error \u003d partitionData.error;\n                    log.debug(\"Failed to fetch offset for partition {}: {}\", tp, error.message());\n\n                    if (error \u003d\u003d Errors.UNKNOWN_TOPIC_OR_PARTITION) {\n                        future.completeExceptionally(new KafkaException(\"Topic or Partition \" + tp + \" does not exist\"));\n                        return;\n                    } else if (error \u003d\u003d Errors.TOPIC_AUTHORIZATION_FAILED) {\n                        if (unauthorizedTopics \u003d\u003d null) {\n                            unauthorizedTopics \u003d new HashSet\u003c\u003e();\n                        }\n                        unauthorizedTopics.add(tp.topic());\n                    } else if (error \u003d\u003d Errors.UNSTABLE_OFFSET_COMMIT) {\n                        unstableTxnOffsetTopicPartitions.add(tp);\n                    } else {\n                        // Fail with a non-retriable KafkaException for all unexpected partition\n                        // errors (even if they are retriable)\n                        future.completeExceptionally(new KafkaException(\"Unexpected error in fetch offset \" +\n                                \"response for partition \" + tp + \": \" + error.message()));\n                        return;\n                    }\n                } else if (partitionData.offset \u003e\u003d 0) {\n                    // record the position with the offset (-1 indicates no committed offset to fetch);\n                    // if there\u0027s no committed offset, record as null\n                    offsets.put(tp, new OffsetAndMetadata(partitionData.offset, partitionData.leaderEpoch, partitionData.metadata));\n                } else {\n                    log.info(\"Found no committed offset for partition {}\", tp);\n                    offsets.put(tp, null);\n                }\n            }\n\n            if (unauthorizedTopics !\u003d null) {\n                future.completeExceptionally(new TopicAuthorizationException(unauthorizedTopics));\n            } else if (!unstableTxnOffsetTopicPartitions.isEmpty()) {\n                // TODO: Optimization question: Do we need to retry all partitions upon a single partition error?\n                log.info(\"The following partitions still have unstable offsets \" +\n                        \"which are not cleared on the broker side: {}\" +\n                        \", this could be either \" +\n                        \"transactional offsets waiting for completion, or \" +\n                        \"normal offsets waiting for replication after appending to local log\", unstableTxnOffsetTopicPartitions);\n                future.completeExceptionally(new UnstableOffsetCommitException(\"There are \" +\n                    \"unstable offsets for the requested topic partitions\"));\n            } else {\n                onSuccessfulAttempt(currentTimeMs);\n                future.complete(offsets);\n            }\n        }\n\n        private void chainFuture(\n            final CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e otherFuture) {\n            this.future.whenComplete((r, t) -\u003e {\n                if (t !\u003d null) {\n                    otherFuture.completeExceptionally(t);\n                } else {\n                    otherFuture.complete(r);\n                }\n            });\n        }\n\n        @Override\n        public String toStringBase() {\n            return super.toStringBase() +\n                    \", requestedPartitions\u003d\" + requestedPartitions;\n        }\n    }\n\n    /**\n     * \u003cp\u003eThis is used to stage the unsent {@link OffsetCommitRequestState} and {@link OffsetFetchRequestState}.\n     * \u003cli\u003eunsentOffsetCommits holds the offset commit requests that have not been sent out\u003c/\u003e\n     * \u003cli\u003eunsentOffsetFetches holds the offset fetch requests that have not been sent out\u003c/li\u003e\n     * \u003cli\u003einflightOffsetFetches holds the offset fetch requests that have been sent out but not completed\u003c/\u003e.\n     * \u003cp\u003e\n     * {@code addOffsetFetchRequest} dedupes the requests to avoid sending the same requests.\n     */\n\n    class PendingRequests {\n        // Queue is used to ensure the sequence of commit\n        Queue\u003cOffsetCommitRequestState\u003e unsentOffsetCommits \u003d new LinkedList\u003c\u003e();\n        List\u003cOffsetFetchRequestState\u003e unsentOffsetFetches \u003d new ArrayList\u003c\u003e();\n        List\u003cOffsetFetchRequestState\u003e inflightOffsetFetches \u003d new ArrayList\u003c\u003e();\n\n        // Visible for testing\n        boolean hasUnsentRequests() {\n            return !unsentOffsetCommits.isEmpty() || !unsentOffsetFetches.isEmpty();\n        }\n\n        /**\n         * Add a commit request to the queue, so that it\u0027s sent out on the next call to\n         * {@link #poll(long)}. This is used from all commits (sync, async, auto-commit).\n         */\n        OffsetCommitRequestState addOffsetCommitRequest(OffsetCommitRequestState request) {\n            log.debug(\"Enqueuing OffsetCommit request for offsets: {}\", request.offsets);\n            unsentOffsetCommits.add(request);\n            return request;\n        }\n\n        /**\n         * \u003cp\u003eAdding an offset fetch request to the outgoing buffer.  If the same request was made, we chain the future\n         * to the existing one.\n         *\n         * \u003cp\u003eIf the request is new, it invokes a callback to remove itself from the {@code inflightOffsetFetches}\n         * upon completion.\n         */\n        private CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e addOffsetFetchRequest(final OffsetFetchRequestState request) {\n            Optional\u003cOffsetFetchRequestState\u003e dupe \u003d\n                    unsentOffsetFetches.stream().filter(r -\u003e r.sameRequest(request)).findAny();\n            Optional\u003cOffsetFetchRequestState\u003e inflight \u003d\n                    inflightOffsetFetches.stream().filter(r -\u003e r.sameRequest(request)).findAny();\n\n            if (dupe.isPresent() || inflight.isPresent()) {\n                log.debug(\"Duplicated unsent offset fetch request found for partitions: {}\", request.requestedPartitions);\n                dupe.orElseGet(inflight::get).chainFuture(request.future);\n            } else {\n                log.debug(\"Enqueuing offset fetch request for partitions: {}\", request.requestedPartitions);\n                this.unsentOffsetFetches.add(request);\n            }\n            return request.future;\n        }\n\n        /**\n         * Clear {@code unsentOffsetCommits} and moves all the sendable request in {@code\n         * unsentOffsetFetches} to the {@code inflightOffsetFetches} to bookkeep all the inflight\n         * requests. Note: Sendable requests are determined by their timer as we are expecting\n         * backoff on failed attempt. See {@link RequestState}.\n         */\n        List\u003cNetworkClientDelegate.UnsentRequest\u003e drain(final long currentTimeMs) {\n            // not ready to sent request\n            List\u003cOffsetCommitRequestState\u003e unreadyCommitRequests \u003d unsentOffsetCommits.stream()\n                .filter(request -\u003e !request.canSendRequest(currentTimeMs))\n                .collect(Collectors.toList());\n\n            failAndRemoveExpiredCommitRequests();\n\n            // Add all unsent offset commit requests to the unsentRequests list\n            List\u003cNetworkClientDelegate.UnsentRequest\u003e unsentRequests \u003d unsentOffsetCommits.stream()\n                .filter(request -\u003e request.canSendRequest(currentTimeMs))\n                .peek(request -\u003e request.onSendAttempt(currentTimeMs))\n                .map(OffsetCommitRequestState::toUnsentRequest)\n                .collect(Collectors.toCollection(ArrayList::new));\n\n            // Partition the unsent offset fetch requests into sendable and non-sendable lists\n            Map\u003cBoolean, List\u003cOffsetFetchRequestState\u003e\u003e partitionedBySendability \u003d\n                    unsentOffsetFetches.stream()\n                            .collect(Collectors.partitioningBy(request -\u003e request.canSendRequest(currentTimeMs)));\n\n            failAndRemoveExpiredFetchRequests();\n\n            // Add all sendable offset fetch requests to the unsentRequests list and to the inflightOffsetFetches list\n            for (OffsetFetchRequestState request : partitionedBySendability.get(true)) {\n                request.onSendAttempt(currentTimeMs);\n                unsentRequests.add(request.toUnsentRequest());\n                inflightOffsetFetches.add(request);\n            }\n\n            // Clear the unsent offset commit and fetch lists and add all non-sendable offset fetch requests to the unsentOffsetFetches list\n            clearAll();\n            unsentOffsetFetches.addAll(partitionedBySendability.get(false));\n            unsentOffsetCommits.addAll(unreadyCommitRequests);\n\n            return Collections.unmodifiableList(unsentRequests);\n        }\n\n        /**\n         * Find the unsent commit requests that have expired, remove them and complete their\n         * futures with a TimeoutException.\n         */\n        private void failAndRemoveExpiredCommitRequests() {\n            Queue\u003cOffsetCommitRequestState\u003e requestsToPurge \u003d new LinkedList\u003c\u003e(unsentOffsetCommits);\n            requestsToPurge.forEach(RetriableRequestState::maybeExpire);\n        }\n\n        /**\n         * Find the unsent fetch requests that have expired, remove them and complete their\n         * futures with a TimeoutException.\n         */\n        private void failAndRemoveExpiredFetchRequests() {\n            Queue\u003cOffsetFetchRequestState\u003e requestsToPurge \u003d new LinkedList\u003c\u003e(unsentOffsetFetches);\n            requestsToPurge.forEach(RetriableRequestState::maybeExpire);\n        }\n\n        private void clearAll() {\n            unsentOffsetCommits.clear();\n            unsentOffsetFetches.clear();\n        }\n\n        private List\u003cNetworkClientDelegate.UnsentRequest\u003e drainPendingCommits() {\n            List\u003cNetworkClientDelegate.UnsentRequest\u003e res \u003d unsentOffsetCommits.stream()\n                .map(OffsetCommitRequestState::toUnsentRequest)\n                .collect(Collectors.toCollection(ArrayList::new));\n            clearAll();\n            return res;\n        }\n    }\n\n    /**\n     * Encapsulates the state of auto-committing and manages the auto-commit timer.\n     */\n    private static class AutoCommitState {\n        private final Timer timer;\n        private final long autoCommitInterval;\n        private boolean hasInflightCommit;\n\n        private final Logger log;\n\n        public AutoCommitState(\n                final Time time,\n                final long autoCommitInterval,\n                final LogContext logContext) {\n            this.autoCommitInterval \u003d autoCommitInterval;\n            this.timer \u003d time.timer(autoCommitInterval);\n            this.hasInflightCommit \u003d false;\n            this.log \u003d logContext.logger(getClass());\n        }\n\n        public boolean shouldAutoCommit() {\n            if (!this.timer.isExpired()) {\n                return false;\n            }\n            if (this.hasInflightCommit) {\n                log.trace(\"Skipping auto-commit on the interval because a previous one is still in-flight.\");\n                return false;\n            }\n            return true;\n        }\n\n        public void resetTimer() {\n            this.timer.reset(autoCommitInterval);\n        }\n\n        public void resetTimer(long retryBackoffMs) {\n            this.timer.reset(retryBackoffMs);\n        }\n\n        public long remainingMs(final long currentTimeMs) {\n            this.timer.update(currentTimeMs);\n            return this.timer.remainingMs();\n        }\n\n        public void updateTimer(final long currentTimeMs) {\n            this.timer.update(currentTimeMs);\n        }\n\n        public void setInflightCommitStatus(final boolean inflightCommitStatus) {\n            this.hasInflightCommit \u003d inflightCommitStatus;\n        }\n    }\n\n    static class MemberInfo {\n        Optional\u003cString\u003e memberId;\n        Optional\u003cInteger\u003e memberEpoch;\n\n        MemberInfo() {\n            this.memberId \u003d Optional.empty();\n            this.memberEpoch \u003d Optional.empty();\n        }\n\n        @Override\n        public String toString() {\n            return \"memberId\u003d\" + memberId.orElse(\"undefined\") +\n                    \", memberEpoch\u003d\" + (memberEpoch.isPresent() ? memberEpoch.get() : \"undefined\");\n        }\n    }\n}","methodCount":81},"candidatesTelemetryData":{"numberOfSuggestions":4,"candidates":[{"lineStart":217,"lineEnd":222,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method maybeWrapAsTimeoutException to class CoordinatorRequestManager","description":"Move method maybeWrapAsTimeoutException to org.apache.kafka.clients.consumer.internals.CoordinatorRequestManager\nRationale: The maybeWrapAsTimeoutException() method deals with exceptions that may arise during request handling, which is closely related to the responsibilities of the CoordinatorRequestManager. Moving this method here adheres to the Single Responsibility Principle, as it centralizes exception handling related to coordinator requests. This enhances cohesion by keeping related functionalities together. However, care must be taken to ensure that the method\u0027s context is appropriately managed in the new class.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":217,"lineEnd":222,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method maybeWrapAsTimeoutException to class PendingRequests","description":"Move method maybeWrapAsTimeoutException to org.apache.kafka.clients.consumer.internals.CommitRequestManager.PendingRequests\nRationale: The method is relevant to managing requests and their potential timeouts, which aligns well with the PendingRequests class. By relocating it here, we can encapsulate timeout exception handling within the context of pending requests, improving the class\u0027s cohesion. This move supports the Open/Closed Principle, allowing for easier extension of request handling without modifying existing code. However, the method\u0027s integration must be carefully considered to avoid introducing tight coupling.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":217,"lineEnd":222,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method maybeWrapAsTimeoutException to class OffsetCommitCallbackInvoker","description":"Move method maybeWrapAsTimeoutException to org.apache.kafka.clients.consumer.internals.OffsetCommitCallbackInvoker\nRationale: The maybeWrapAsTimeoutException() method can be useful in the context of callback invocations, especially when dealing with timeouts during offset commits. Moving it here would enhance the class\u0027s ability to handle exceptions effectively, aligning with the Single Responsibility Principle. This would also improve the overall design by making the callback invoker more robust. However, it may introduce additional dependencies that need to be managed.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false},{"lineStart":1285,"lineEnd":1288,"refactoringType":"MyMoveInstanceMethodRefactoring","refactoringInfo":"Move method remainingMs to class Timer","description":"Move method remainingMs to org.apache.kafka.common.utils.Timer\nRationale: The remainingMs() method directly interacts with the timer\u0027s state by updating it and then retrieving the remaining time. This functionality is inherently tied to the Timer class, which is responsible for managing time-related operations. Moving this method to Timer adheres to the Single Responsibility Principle, as it consolidates all timer-related logic within the Timer class, enhancing cohesion. Additionally, this aligns with the Open/Closed Principle, as the Timer class can be extended or modified without affecting other classes. A potential drawback is that if the current class has specific context or state that influences the timer\u0027s behavior, this context may need to be managed or passed to the Timer class, which could complicate the design.","couldCreateRefObject":true,"applied":false,"startedRefactoringFlow":false,"undone":false}]},"iterationData":[{"iteration_num":1,"suggested_move_methods":[{"method_name":"createOffsetCommitRequest","method_signature":"private OffsetCommitRequestState createOffsetCommitRequest(final Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets, final long deadlineMs)","target_class":"OffsetCommitRequestState","rationale":"This method is responsible for creating instances of OffsetCommitRequestState, which is a separate class. Moving it to OffsetCommitRequestState will encapsulate the creation logic within the class it constructs."},{"method_name":"createOffsetFetchRequest","method_signature":"OffsetFetchRequestState createOffsetFetchRequest(final Set\u003cTopicPartition\u003e partitions, final long deadlineMs)","target_class":"OffsetFetchRequestState","rationale":"Similar to createOffsetCommitRequest, this method creates instances of OffsetFetchRequestState. It should be moved to that class to maintain cohesion and encapsulate the request creation logic."},{"method_name":"commitSyncWithRetries","method_signature":"private void commitSyncWithRetries(OffsetCommitRequestState requestAttempt, CompletableFuture\u003cVoid\u003e result)","target_class":"OffsetCommitRequestState","rationale":"This method handles the logic for committing offsets with retries, which is directly related to the OffsetCommitRequestState. Moving it will enhance the single responsibility principle."},{"method_name":"fetchOffsetsWithRetries","method_signature":"private void fetchOffsetsWithRetries(final OffsetFetchRequestState fetchRequest, final CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e result)","target_class":"OffsetFetchRequestState","rationale":"This method manages the fetching of offsets with retries, which is closely tied to the OffsetFetchRequestState. Moving it will improve the organization of related functionality."},{"method_name":"autoCommitSyncBeforeRevocationWithRetries","method_signature":"private void autoCommitSyncBeforeRevocationWithRetries(OffsetCommitRequestState requestAttempt, CompletableFuture\u003cVoid\u003e result)","target_class":"OffsetCommitRequestState","rationale":"This method is responsible for handling the synchronization of auto-commit before revocation, which is specific to the OffsetCommitRequestState. Moving it will enhance the clarity of responsibilities."}],"llm_response_time":11356},{"iteration_num":2,"suggested_move_methods":[{"method_name":"createOffsetCommitRequest","method_signature":"private OffsetCommitRequestState createOffsetCommitRequest(final Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets, final long deadlineMs)","target_class":"OffsetCommitRequestState","rationale":"The method is responsible for creating an instance of OffsetCommitRequestState, which indicates that it is closely related to that class. Moving it there would enhance cohesion."},{"method_name":"createOffsetFetchRequest","method_signature":"OffsetFetchRequestState createOffsetFetchRequest(final Set\u003cTopicPartition\u003e partitions, final long deadlineMs)","target_class":"OffsetFetchRequestState","rationale":"This method constructs an OffsetFetchRequestState, which suggests it belongs in that class to improve encapsulation and maintainability."},{"method_name":"autoCommitCallback","method_signature":"private BiConsumer\u003c? super Map\u003cTopicPartition, OffsetAndMetadata\u003e, ? super Throwable\u003e autoCommitCallback(final Map\u003cTopicPartition, OffsetAndMetadata\u003e allConsumedOffsets)","target_class":"OffsetCommitRequestState","rationale":"The callback is specifically tied to the completion of offset commit requests, making it more appropriate to reside in the OffsetCommitRequestState class."},{"method_name":"commitSyncWithRetries","method_signature":"private void commitSyncWithRetries(OffsetCommitRequestState requestAttempt, CompletableFuture\u003cVoid\u003e result)","target_class":"OffsetCommitRequestState","rationale":"This method handles the logic for committing offsets synchronously with retries, which is directly related to the state of an OffsetCommitRequestState."},{"method_name":"fetchOffsetsWithRetries","method_signature":"private void fetchOffsetsWithRetries(final OffsetFetchRequestState fetchRequest, final CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e result)","target_class":"OffsetFetchRequestState","rationale":"This method manages the retry logic for fetching offsets, which is inherently tied to the OffsetFetchRequestState, making it a better fit there."}],"llm_response_time":8326},{"iteration_num":3,"suggested_move_methods":[{"method_name":"createOffsetCommitRequest","method_signature":"private OffsetCommitRequestState createOffsetCommitRequest(final Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets, final long deadlineMs)","target_class":"OffsetCommitRequestState","rationale":"The method creates an instance of OffsetCommitRequestState and is closely related to its functionality. Moving it to OffsetCommitRequestState would encapsulate the creation logic within the class that it constructs."},{"method_name":"createOffsetFetchRequest","method_signature":"OffsetFetchRequestState createOffsetFetchRequest(final Set\u003cTopicPartition\u003e partitions, final long deadlineMs)","target_class":"OffsetFetchRequestState","rationale":"Similar to createOffsetCommitRequest, this method constructs an OffsetFetchRequestState. Moving it to OffsetFetchRequestState would enhance cohesion by keeping the creation logic within the class it pertains to."},{"method_name":"maybeWrapAsTimeoutException","method_signature":"private KafkaException maybeWrapAsTimeoutException(Throwable t)","target_class":"RetriableRequestState","rationale":"This method is responsible for wrapping exceptions related to request timeouts, which is a concern of retriable requests. Moving it to RetriableRequestState would centralize timeout handling logic."},{"method_name":"commitSyncWithRetries","method_signature":"private void commitSyncWithRetries(OffsetCommitRequestState requestAttempt, CompletableFuture\u003cVoid\u003e result)","target_class":"OffsetCommitRequestState","rationale":"The method handles the retry logic for committing offsets, which is specific to the OffsetCommitRequestState\u0027s lifecycle. Moving it would enhance the encapsulation of commit-related logic."},{"method_name":"fetchOffsetsWithRetries","method_signature":"private void fetchOffsetsWithRetries(final OffsetFetchRequestState fetchRequest, final CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e result)","target_class":"OffsetFetchRequestState","rationale":"This method manages the retry logic for fetching offsets, which is specific to the OffsetFetchRequestState. Moving it would improve the organization of related functionality."},{"method_name":"onResponse","method_signature":"void onResponse(final ClientResponse response)","target_class":"OffsetCommitRequestState","rationale":"The method handles the response for offset commit requests, which is a core responsibility of OffsetCommitRequestState. Moving it would align the response handling with the request state it pertains to."},{"method_name":"onResponse","method_signature":"void onResponse(final ClientResponse response)","target_class":"OffsetFetchRequestState","rationale":"This method processes the response for offset fetch requests, which is a primary responsibility of OffsetFetchRequestState. Moving it would enhance the clarity of the class\u0027s purpose."}],"llm_response_time":9564},{"iteration_num":-1,"suggested_move_methods":[{"method_name":"findMinTime","method_signature":"private static findMinTime(final Collection\u003c? extends RequestState\u003e requests, final long currentTimeMs)","target_class":"","rationale":""},{"method_name":"maybeWrapAsTimeoutException","method_signature":"private maybeWrapAsTimeoutException(Throwable t)","target_class":"","rationale":""},{"method_name":"requestAutoCommit","method_signature":"private requestAutoCommit(final OffsetCommitRequestState requestState)","target_class":"","rationale":""},{"method_name":"maybeAutoCommitAsync","method_signature":"public maybeAutoCommitAsync()","target_class":"","rationale":""},{"method_name":"maybeResetTimerWithBackoff","method_signature":"private maybeResetTimerWithBackoff(final CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e result)","target_class":"","rationale":""},{"method_name":"maybeAutoCommitSyncBeforeRevocation","method_signature":"public maybeAutoCommitSyncBeforeRevocation(final long deadlineMs)","target_class":"","rationale":""},{"method_name":"autoCommitSyncBeforeRevocationWithRetries","method_signature":"private autoCommitSyncBeforeRevocationWithRetries(OffsetCommitRequestState requestAttempt,\n                                                           CompletableFuture\u003cVoid\u003e result)","target_class":"","rationale":""},{"method_name":"autoCommitCallback","method_signature":"private autoCommitCallback(final Map\u003cTopicPartition, OffsetAndMetadata\u003e allConsumedOffsets)","target_class":"","rationale":""},{"method_name":"commitAsync","method_signature":"public commitAsync(final Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets)","target_class":"","rationale":""},{"method_name":"commitSync","method_signature":"public commitSync(final Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets,\n                                              final long deadlineMs)","target_class":"","rationale":""},{"method_name":"createOffsetCommitRequest","method_signature":"private createOffsetCommitRequest(final Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets,\n                                                               final long deadlineMs)","target_class":"","rationale":""},{"method_name":"commitSyncWithRetries","method_signature":"private commitSyncWithRetries(OffsetCommitRequestState requestAttempt,\n                                       CompletableFuture\u003cVoid\u003e result)","target_class":"","rationale":""},{"method_name":"commitSyncExceptionForError","method_signature":"private commitSyncExceptionForError(Throwable error)","target_class":"","rationale":""},{"method_name":"commitAsyncExceptionForError","method_signature":"private commitAsyncExceptionForError(Throwable error)","target_class":"","rationale":""},{"method_name":"fetchOffsets","method_signature":"public fetchOffsets(\n        final Set\u003cTopicPartition\u003e partitions,\n        final long deadlineMs)","target_class":"","rationale":""},{"method_name":"createOffsetFetchRequest","method_signature":" createOffsetFetchRequest(final Set\u003cTopicPartition\u003e partitions,\n                                                             final long deadlineMs)","target_class":"","rationale":""},{"method_name":"fetchOffsetsWithRetries","method_signature":"private fetchOffsetsWithRetries(final OffsetFetchRequestState fetchRequest,\n                                         final CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e result)","target_class":"","rationale":""},{"method_name":"isStaleEpochErrorAndValidEpochAvailable","method_signature":"private isStaleEpochErrorAndValidEpochAvailable(Throwable error)","target_class":"","rationale":""},{"method_name":"updateAutoCommitTimer","method_signature":"public updateAutoCommitTimer(final long currentTimeMs)","target_class":"","rationale":""},{"method_name":"handleCoordinatorDisconnect","method_signature":"private handleCoordinatorDisconnect(Throwable exception, long currentTimeMs)","target_class":"","rationale":""},{"method_name":"autoCommitEnabled","method_signature":"public autoCommitEnabled()","target_class":"","rationale":""},{"method_name":"resetAutoCommitTimer","method_signature":"public resetAutoCommitTimer()","target_class":"","rationale":""},{"method_name":"resetAutoCommitTimer","method_signature":"public resetAutoCommitTimer(long retryBackoffMs)","target_class":"","rationale":""},{"method_name":"drainPendingOffsetCommitRequests","method_signature":"public drainPendingOffsetCommitRequests()","target_class":"","rationale":""},{"method_name":"toUnsentRequest","method_signature":"public toUnsentRequest()","target_class":"","rationale":""},{"method_name":"maybeExpire","method_signature":" maybeExpire()","target_class":"","rationale":""},{"method_name":"buildRequestWithResponseHandling","method_signature":" buildRequestWithResponseHandling(final AbstractRequest.Builder\u003c?\u003e builder)","target_class":"","rationale":""},{"method_name":"handleClientResponse","method_signature":"private handleClientResponse(final ClientResponse response,\n                                          final Throwable error,\n                                          final long requestCompletionTimeMs)","target_class":"","rationale":""},{"method_name":"sameRequest","method_signature":"public sameRequest(final OffsetFetchRequestState request)","target_class":"","rationale":""},{"method_name":"toUnsentRequest","method_signature":"public toUnsentRequest()","target_class":"","rationale":""},{"method_name":"onFailure","method_signature":"private onFailure(final long currentTimeMs,\n                               final Errors responseError)","target_class":"","rationale":""},{"method_name":"onSuccess","method_signature":"private onSuccess(final long currentTimeMs,\n                               final OffsetFetchResponse response)","target_class":"","rationale":""},{"method_name":"chainFuture","method_signature":"private chainFuture(\n            final CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e otherFuture)","target_class":"","rationale":""},{"method_name":"hasUnsentRequests","method_signature":" hasUnsentRequests()","target_class":"","rationale":""},{"method_name":"addOffsetCommitRequest","method_signature":" addOffsetCommitRequest(OffsetCommitRequestState request)","target_class":"","rationale":""},{"method_name":"addOffsetFetchRequest","method_signature":"private addOffsetFetchRequest(final OffsetFetchRequestState request)","target_class":"","rationale":""},{"method_name":"drain","method_signature":" drain(final long currentTimeMs)","target_class":"","rationale":""},{"method_name":"failAndRemoveExpiredCommitRequests","method_signature":"private failAndRemoveExpiredCommitRequests()","target_class":"","rationale":""},{"method_name":"failAndRemoveExpiredFetchRequests","method_signature":"private failAndRemoveExpiredFetchRequests()","target_class":"","rationale":""},{"method_name":"clearAll","method_signature":"private clearAll()","target_class":"","rationale":""},{"method_name":"drainPendingCommits","method_signature":"private drainPendingCommits()","target_class":"","rationale":""},{"method_name":"shouldAutoCommit","method_signature":"public shouldAutoCommit()","target_class":"","rationale":""},{"method_name":"resetTimer","method_signature":"public resetTimer()","target_class":"","rationale":""},{"method_name":"resetTimer","method_signature":"public resetTimer(long retryBackoffMs)","target_class":"","rationale":""},{"method_name":"remainingMs","method_signature":"public remainingMs(final long currentTimeMs)","target_class":"","rationale":""},{"method_name":"updateTimer","method_signature":"public updateTimer(final long currentTimeMs)","target_class":"","rationale":""}],"llm_response_time":0},{"iteration_num":-2,"suggested_move_methods":[{"method_name":"maybeWrapAsTimeoutException","method_signature":"private maybeWrapAsTimeoutException(Throwable t)","target_class":"","rationale":""},{"method_name":"chainFuture","method_signature":"private chainFuture(\n            final CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e otherFuture)","target_class":"","rationale":""},{"method_name":"updateTimer","method_signature":"public updateTimer(final long currentTimeMs)","target_class":"","rationale":""},{"method_name":"remainingMs","method_signature":"public remainingMs(final long currentTimeMs)","target_class":"","rationale":""},{"method_name":"commitAsyncExceptionForError","method_signature":"private commitAsyncExceptionForError(Throwable error)","target_class":"","rationale":""},{"method_name":"sameRequest","method_signature":"public sameRequest(final OffsetFetchRequestState request)","target_class":"","rationale":""},{"method_name":"resetTimer","method_signature":"public resetTimer(long retryBackoffMs)","target_class":"","rationale":""},{"method_name":"maybeExpire","method_signature":" maybeExpire()","target_class":"","rationale":""},{"method_name":"findMinTime","method_signature":"private static findMinTime(final Collection\u003c? extends RequestState\u003e requests, final long currentTimeMs)","target_class":"","rationale":""},{"method_name":"isStaleEpochErrorAndValidEpochAvailable","method_signature":"private isStaleEpochErrorAndValidEpochAvailable(Throwable error)","target_class":"","rationale":""},{"method_name":"autoCommitEnabled","method_signature":"public autoCommitEnabled()","target_class":"","rationale":""},{"method_name":"buildRequestWithResponseHandling","method_signature":" buildRequestWithResponseHandling(final AbstractRequest.Builder\u003c?\u003e builder)","target_class":"","rationale":""},{"method_name":"handleClientResponse","method_signature":"private handleClientResponse(final ClientResponse response,\n                                          final Throwable error,\n                                          final long requestCompletionTimeMs)","target_class":"","rationale":""},{"method_name":"handleCoordinatorDisconnect","method_signature":"private handleCoordinatorDisconnect(Throwable exception, long currentTimeMs)","target_class":"","rationale":""},{"method_name":"updateAutoCommitTimer","method_signature":"public updateAutoCommitTimer(final long currentTimeMs)","target_class":"","rationale":""}],"llm_response_time":0}],"methodCompatibilityScores":{"private maybeWrapAsTimeoutException(Throwable t)":{"first":{"method_name":"maybeWrapAsTimeoutException","method_signature":"private maybeWrapAsTimeoutException(Throwable t)","target_class":"","rationale":""},"second":0.17654971082865653},"private chainFuture(\n            final CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e otherFuture)":{"first":{"method_name":"chainFuture","method_signature":"private chainFuture(\n            final CompletableFuture\u003cMap\u003cTopicPartition, OffsetAndMetadata\u003e\u003e otherFuture)","target_class":"","rationale":""},"second":0.22743968045577706},"public updateTimer(final long currentTimeMs)":{"first":{"method_name":"updateTimer","method_signature":"public updateTimer(final long currentTimeMs)","target_class":"","rationale":""},"second":0.24913868632121422},"public remainingMs(final long currentTimeMs)":{"first":{"method_name":"remainingMs","method_signature":"public remainingMs(final long currentTimeMs)","target_class":"","rationale":""},"second":0.2700209532189689},"private commitAsyncExceptionForError(Throwable error)":{"first":{"method_name":"commitAsyncExceptionForError","method_signature":"private commitAsyncExceptionForError(Throwable error)","target_class":"","rationale":""},"second":0.322750956999934},"public sameRequest(final OffsetFetchRequestState request)":{"first":{"method_name":"sameRequest","method_signature":"public sameRequest(final OffsetFetchRequestState request)","target_class":"","rationale":""},"second":0.32776042589335447},"public resetTimer(long retryBackoffMs)":{"first":{"method_name":"resetTimer","method_signature":"public resetTimer(long retryBackoffMs)","target_class":"","rationale":""},"second":0.34166550352570013}," maybeExpire()":{"first":{"method_name":"maybeExpire","method_signature":" maybeExpire()","target_class":"","rationale":""},"second":0.36119484562476595},"private static findMinTime(final Collection\u003c? extends RequestState\u003e requests, final long currentTimeMs)":{"first":{"method_name":"findMinTime","method_signature":"private static findMinTime(final Collection\u003c? extends RequestState\u003e requests, final long currentTimeMs)","target_class":"","rationale":""},"second":0.378705583658007},"private isStaleEpochErrorAndValidEpochAvailable(Throwable error)":{"first":{"method_name":"isStaleEpochErrorAndValidEpochAvailable","method_signature":"private isStaleEpochErrorAndValidEpochAvailable(Throwable error)","target_class":"","rationale":""},"second":0.3953990532959744},"public autoCommitEnabled()":{"first":{"method_name":"autoCommitEnabled","method_signature":"public autoCommitEnabled()","target_class":"","rationale":""},"second":0.4384146309556931}," buildRequestWithResponseHandling(final AbstractRequest.Builder\u003c?\u003e builder)":{"first":{"method_name":"buildRequestWithResponseHandling","method_signature":" buildRequestWithResponseHandling(final AbstractRequest.Builder\u003c?\u003e builder)","target_class":"","rationale":""},"second":0.439359478505803},"private handleClientResponse(final ClientResponse response,\n                                          final Throwable error,\n                                          final long requestCompletionTimeMs)":{"first":{"method_name":"handleClientResponse","method_signature":"private handleClientResponse(final ClientResponse response,\n                                          final Throwable error,\n                                          final long requestCompletionTimeMs)","target_class":"","rationale":""},"second":0.44786896225691497},"private handleCoordinatorDisconnect(Throwable exception, long currentTimeMs)":{"first":{"method_name":"handleCoordinatorDisconnect","method_signature":"private handleCoordinatorDisconnect(Throwable exception, long currentTimeMs)","target_class":"","rationale":""},"second":0.45210376891136533},"public updateAutoCommitTimer(final long currentTimeMs)":{"first":{"method_name":"updateAutoCommitTimer","method_signature":"public updateAutoCommitTimer(final long currentTimeMs)","target_class":"","rationale":""},"second":0.45590931711824845}},"llmMethodPriority":{"priority_method_names":["maybeWrapAsTimeoutException","chainFuture","remainingMs","commitAsyncExceptionForError","sameRequest","resetTimer","maybeExpire","findMinTime","isStaleEpochErrorAndValidEpochAvailable","autoCommitEnabled","buildRequestWithResponseHandling","handleClientResponse","handleCoordinatorDisconnect","updateAutoCommitTimer"],"llm_response_time":3903},"targetClassMap":{"maybeWrapAsTimeoutException":{"target_classes":[{"class_name":"Time","similarity_score":0.12689523713552092},{"class_name":"SubscriptionState","similarity_score":0.42954135980189057},{"class_name":"LogContext","similarity_score":0.3139845000807518},{"class_name":"CoordinatorRequestManager","similarity_score":0.2841977634627642},{"class_name":"OffsetCommitCallbackInvoker","similarity_score":0.22740621427592883},{"class_name":"OffsetCommitMetricsManager","similarity_score":0.13300169865599867},{"class_name":"PendingRequests","similarity_score":0.1892839136523794},{"class_name":"MemberInfo","similarity_score":0.23893907643579956}],"target_classes_sorted_by_llm":["CoordinatorRequestManager","PendingRequests","OffsetCommitCallbackInvoker","Time","LogContext","OffsetCommitMetricsManager","MemberInfo","SubscriptionState"],"llm_response_time":11800,"similarity_computation_time":14,"similarity_metric":"tfidf"},"chainFuture":{"target_classes":[],"target_classes_sorted_by_llm":[],"llm_response_time":452,"similarity_computation_time":0,"similarity_metric":"tfidf"},"remainingMs":{"target_classes":[{"class_name":"Timer","similarity_score":0.16163837228400127}],"target_classes_sorted_by_llm":["Timer"],"llm_response_time":3301,"similarity_computation_time":1,"similarity_metric":"tfidf"}}}
