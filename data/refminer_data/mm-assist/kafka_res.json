[
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "643db430a707479c9e87eec1ad67e1d4f43c9268",
        "url": "https://github.com/apache/kafka/commit/643db430a707479c9e87eec1ad67e1d4f43c9268",
        "move_method_refactoring": {
            "type": "Move Method",
            "description": "Move Method private buildAdminClient() : MockAdminClient from class org.apache.kafka.tools.FeatureCommandUnitTest to private buildAdminClient() : MockAdminClient from class org.apache.kafka.tools.FeatureCommandTest",
            "leftSideLocations": [
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/FeatureCommandTest.java",
                    "startLine": 198,
                    "endLine": 215,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "original method declaration",
                    "codeElement": "private buildAdminClient() : MockAdminClient"
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "tools/src/test/java/org/apache/kafka/tools/FeatureCommandTest.java",
                    "startLine": 192,
                    "endLine": 209,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "moved method declaration",
                    "codeElement": "private buildAdminClient() : MockAdminClient"
                }
            ],
            "isStatic": true
        },
        "telemetry": {
            "id": "06c786b7-4d78-4f0e-b6f5-aace5bfef1f8",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 118,
                "lineStart": 46,
                "lineEnd": 163,
                "bodyLineStart": 46,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/tools/src/test/java/org/apache/kafka/tools/FeatureCommandTest.java",
                "sourceCode": "@ExtendWith(value = ClusterTestExtensions.class)\n@ClusterTestDefaults(clusterType = Type.KRAFT)\n@Tag(\"integration\")\npublic class FeatureCommandTest {\n\n    private final ClusterInstance cluster;\n    public FeatureCommandTest(ClusterInstance cluster) {\n        this.cluster = cluster;\n    }\n\n    @ClusterTest(clusterType = Type.ZK, metadataVersion = MetadataVersion.IBP_3_3_IV1)\n    public void testDescribeWithZK() {\n        String commandOutput = ToolsTestUtils.captureStandardOut(() ->\n                assertEquals(0, FeatureCommand.mainNoExit(\"--bootstrap-server\", cluster.bootstrapServers(), \"describe\"))\n        );\n        assertEquals(\"\", commandOutput);\n    }\n\n    @ClusterTest(clusterType = Type.KRAFT, metadataVersion = MetadataVersion.IBP_3_3_IV1)\n    public void testDescribeWithKRaft() {\n        String commandOutput = ToolsTestUtils.captureStandardOut(() ->\n                assertEquals(0, FeatureCommand.mainNoExit(\"--bootstrap-server\", cluster.bootstrapServers(), \"describe\"))\n        );\n        // Change expected message to reflect latest MetadataVersion (SupportedMaxVersion increases when adding a new version)\n        assertEquals(\"Feature: metadata.version\\tSupportedMinVersion: 3.0-IV1\\t\" +\n                \"SupportedMaxVersion: 3.8-IV0\\tFinalizedVersionLevel: 3.3-IV1\\t\", outputWithoutEpoch(commandOutput));\n    }\n\n    @ClusterTest(clusterType = Type.KRAFT, metadataVersion = MetadataVersion.IBP_3_7_IV4)\n    public void testDescribeWithKRaftAndBootstrapControllers() {\n        String commandOutput = ToolsTestUtils.captureStandardOut(() ->\n                assertEquals(0, FeatureCommand.mainNoExit(\"--bootstrap-controller\", cluster.bootstrapControllers(), \"describe\"))\n        );\n        // Change expected message to reflect latest MetadataVersion (SupportedMaxVersion increases when adding a new version)\n        assertEquals(\"Feature: metadata.version\\tSupportedMinVersion: 3.0-IV1\\t\" +\n                \"SupportedMaxVersion: 3.8-IV0\\tFinalizedVersionLevel: 3.7-IV4\\t\", outputWithoutEpoch(commandOutput));\n    }\n\n    @ClusterTest(clusterType = Type.ZK, metadataVersion = MetadataVersion.IBP_3_3_IV1)\n    public void testUpgradeMetadataVersionWithZk() {\n        String commandOutput = ToolsTestUtils.captureStandardOut(() ->\n                assertEquals(1, FeatureCommand.mainNoExit(\"--bootstrap-server\", cluster.bootstrapServers(),\n                        \"upgrade\", \"--metadata\", \"3.3-IV2\"))\n        );\n        assertEquals(\"Could not upgrade metadata.version to 6. Could not apply finalized feature \" +\n                \"update because the provided feature is not supported.\", commandOutput);\n    }\n\n    @ClusterTest(clusterType = Type.KRAFT, metadataVersion = MetadataVersion.IBP_3_3_IV1)\n    public void testUpgradeMetadataVersionWithKraft() {\n        String commandOutput = ToolsTestUtils.captureStandardOut(() ->\n                assertEquals(0, FeatureCommand.mainNoExit(\"--bootstrap-server\", cluster.bootstrapServers(),\n                        \"upgrade\", \"--feature\", \"metadata.version=5\"))\n        );\n        assertEquals(\"metadata.version was upgraded to 5.\", commandOutput);\n\n        commandOutput = ToolsTestUtils.captureStandardOut(() ->\n                assertEquals(0, FeatureCommand.mainNoExit(\"--bootstrap-server\", cluster.bootstrapServers(),\n                        \"upgrade\", \"--metadata\", \"3.3-IV2\"))\n        );\n        assertEquals(\"metadata.version was upgraded to 6.\", commandOutput);\n    }\n\n    @ClusterTest(clusterType = Type.ZK, metadataVersion = MetadataVersion.IBP_3_3_IV1)\n    public void testDowngradeMetadataVersionWithZk() {\n        String commandOutput = ToolsTestUtils.captureStandardOut(() ->\n                assertEquals(1, FeatureCommand.mainNoExit(\"--bootstrap-server\", cluster.bootstrapServers(),\n                        \"disable\", \"--feature\", \"metadata.version\"))\n        );\n        assertEquals(\"Could not disable metadata.version. Can not delete non-existing finalized feature.\", commandOutput);\n\n        commandOutput = ToolsTestUtils.captureStandardOut(() ->\n                assertEquals(1, FeatureCommand.mainNoExit(\"--bootstrap-server\", cluster.bootstrapServers(),\n                        \"downgrade\", \"--metadata\", \"3.3-IV0\"))\n        );\n        assertEquals(\"Could not downgrade metadata.version to 4. Could not apply finalized feature \" +\n                        \"update because the provided feature is not supported.\", commandOutput);\n\n        commandOutput = ToolsTestUtils.captureStandardOut(() ->\n                assertEquals(1, FeatureCommand.mainNoExit(\"--bootstrap-server\", cluster.bootstrapServers(),\n                        \"downgrade\", \"--unsafe\", \"--metadata\", \"3.3-IV0\"))\n        );\n        assertEquals(\"Could not downgrade metadata.version to 4. Could not apply finalized feature \" +\n                \"update because the provided feature is not supported.\", commandOutput);\n    }\n\n    @ClusterTest(clusterType = Type.KRAFT, metadataVersion = MetadataVersion.IBP_3_3_IV1)\n    public void testDowngradeMetadataVersionWithKRaft() {\n        String commandOutput = ToolsTestUtils.captureStandardOut(() ->\n                assertEquals(1, FeatureCommand.mainNoExit(\"--bootstrap-server\", cluster.bootstrapServers(),\n                        \"disable\", \"--feature\", \"metadata.version\"))\n        );\n        // Change expected message to reflect possible MetadataVersion range 1-N (N increases when adding a new version)\n        assertEquals(\"Could not disable metadata.version. Invalid update version 0 for feature \" +\n                \"metadata.version. Local controller 3000 only supports versions 1-20\", commandOutput);\n\n        commandOutput = ToolsTestUtils.captureStandardOut(() ->\n                assertEquals(1, FeatureCommand.mainNoExit(\"--bootstrap-server\", cluster.bootstrapServers(),\n                        \"downgrade\", \"--metadata\", \"3.3-IV0\"))\n\n        );\n        assertEquals(\"Could not downgrade metadata.version to 4. Invalid metadata.version 4. \" +\n                \"Refusing to perform the requested downgrade because it might delete metadata information.\", commandOutput);\n\n        commandOutput = ToolsTestUtils.captureStandardOut(() ->\n                assertEquals(1, FeatureCommand.mainNoExit(\"--bootstrap-server\", cluster.bootstrapServers(),\n                        \"downgrade\", \"--unsafe\", \"--metadata\", \"3.3-IV0\"))\n\n        );\n        assertEquals(\"Could not downgrade metadata.version to 4. Invalid metadata.version 4. \" +\n                \"Unsafe metadata downgrade is not supported in this version.\", commandOutput);\n    }\n\n    private String outputWithoutEpoch(String output) {\n        int pos = output.indexOf(\"Epoch: \");\n        return (pos > 0) ? output.substring(0, pos) : output;\n    }\n}"
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [],
                    "llm_response_time": 4877
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [],
                    "llm_response_time": 2287
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [],
                    "llm_response_time": 3398
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [],
                    "llm_response_time": 0
                }
            ],
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "11ad5e8bca2444d8c51cd6b81ee84dcb41a047c2",
        "url": "https://github.com/apache/kafka/commit/11ad5e8bca2444d8c51cd6b81ee84dcb41a047c2",
        "move_method_refactoring": {
            "type": "Move Method",
            "description": "Move Method protected canParseSingleTokenLiteral(parser Parser, embedded boolean, tokenLiteral String) : boolean from class org.apache.kafka.connect.data.Values to private canParseSingleTokenLiteral(embedded boolean, tokenLiteral String) : boolean from class org.apache.kafka.connect.data.Values.ValueParser",
            "leftSideLocations": [
                {
                    "filePath": "connect/api/src/main/java/org/apache/kafka/connect/data/Values.java",
                    "startLine": 747,
                    "endLine": 764,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "original method declaration",
                    "codeElement": "protected canParseSingleTokenLiteral(parser Parser, embedded boolean, tokenLiteral String) : boolean"
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/api/src/main/java/org/apache/kafka/connect/data/Values.java",
                    "startLine": 826,
                    "endLine": 843,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "moved method declaration",
                    "codeElement": "private canParseSingleTokenLiteral(embedded boolean, tokenLiteral String) : boolean"
                }
            ],
            "isStatic": true
        },
        "telemetry": {
            "id": "8566c20f-128e-4dcc-a314-a6a304ed2459",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1230,
                "lineStart": 47,
                "lineEnd": 1276,
                "bodyLineStart": 47,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/connect/api/src/main/java/org/apache/kafka/connect/data/Values.java",
                "sourceCode": "/**\n * Utility for converting from one Connect value to a different form. This is useful when the caller expects a value of a particular type\n * but is uncertain whether the actual value is one that isn't directly that type but can be converted into that type.\n *\n * <p>For example, a caller might expect a particular {@link org.apache.kafka.connect.header.Header} to contain a {@link Type#INT64}\n * value, when in fact that header contains a string representation of a 32-bit integer. Here, the caller can use the methods in this\n * class to convert the value to the desired type:\n * <pre>\n *     Header header = ...\n *     long value = Values.convertToLong(header.schema(), header.value());\n * </pre>\n *\n * <p>This class is able to convert any value to a string representation as well as parse those string representations back into most of\n * the types. The only exception is {@link Struct} values that require a schema and thus cannot be parsed from a simple string.\n */\npublic class Values {\n\n    private static final Logger LOG = LoggerFactory.getLogger(Values.class);\n\n    private static final TimeZone UTC = TimeZone.getTimeZone(\"UTC\");\n    private static final SchemaAndValue NULL_SCHEMA_AND_VALUE = new SchemaAndValue(null, null);\n    private static final SchemaAndValue TRUE_SCHEMA_AND_VALUE = new SchemaAndValue(Schema.BOOLEAN_SCHEMA, Boolean.TRUE);\n    private static final SchemaAndValue FALSE_SCHEMA_AND_VALUE = new SchemaAndValue(Schema.BOOLEAN_SCHEMA, Boolean.FALSE);\n    private static final Schema ARRAY_SELECTOR_SCHEMA = SchemaBuilder.array(Schema.STRING_SCHEMA).build();\n    private static final Schema MAP_SELECTOR_SCHEMA = SchemaBuilder.map(Schema.STRING_SCHEMA, Schema.STRING_SCHEMA).build();\n    private static final Schema STRUCT_SELECTOR_SCHEMA = SchemaBuilder.struct().build();\n    private static final String TRUE_LITERAL = Boolean.TRUE.toString();\n    private static final String FALSE_LITERAL = Boolean.FALSE.toString();\n    private static final long MILLIS_PER_DAY = 24 * 60 * 60 * 1000;\n    private static final String NULL_VALUE = \"null\";\n    static final String ISO_8601_DATE_FORMAT_PATTERN = \"yyyy-MM-dd\";\n    static final String ISO_8601_TIME_FORMAT_PATTERN = \"HH:mm:ss.SSS'Z'\";\n    static final String ISO_8601_TIMESTAMP_FORMAT_PATTERN = ISO_8601_DATE_FORMAT_PATTERN + \"'T'\" + ISO_8601_TIME_FORMAT_PATTERN;\n    private static final Set<String> TEMPORAL_LOGICAL_TYPE_NAMES =\n            Collections.unmodifiableSet(\n                    new HashSet<>(\n                            Arrays.asList(Time.LOGICAL_NAME,\n                            Timestamp.LOGICAL_NAME,\n                            Date.LOGICAL_NAME\n                            )\n                    )\n            );\n\n    private static final String QUOTE_DELIMITER = \"\\\"\";\n    private static final String COMMA_DELIMITER = \",\";\n    private static final String ENTRY_DELIMITER = \":\";\n    private static final String ARRAY_BEGIN_DELIMITER = \"[\";\n    private static final String ARRAY_END_DELIMITER = \"]\";\n    private static final String MAP_BEGIN_DELIMITER = \"{\";\n    private static final String MAP_END_DELIMITER = \"}\";\n    private static final int ISO_8601_DATE_LENGTH = ISO_8601_DATE_FORMAT_PATTERN.length();\n    private static final int ISO_8601_TIME_LENGTH = ISO_8601_TIME_FORMAT_PATTERN.length() - 2; // subtract single quotes\n    private static final int ISO_8601_TIMESTAMP_LENGTH = ISO_8601_TIMESTAMP_FORMAT_PATTERN.length() - 4; // subtract single quotes\n\n    private static final Pattern TWO_BACKSLASHES = Pattern.compile(\"\\\\\\\\\");\n\n    private static final Pattern DOUBLE_QUOTE = Pattern.compile(\"\\\"\");\n\n    /**\n     * Convert the specified value to a {@link Type#BOOLEAN} value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a boolean.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a boolean, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a boolean\n     */\n    public static Boolean convertToBoolean(Schema schema, Object value) throws DataException {\n        return (Boolean) convertTo(Schema.OPTIONAL_BOOLEAN_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT8} byte value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a byte.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a byte, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a byte\n     */\n    public static Byte convertToByte(Schema schema, Object value) throws DataException {\n        return (Byte) convertTo(Schema.OPTIONAL_INT8_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT16} short value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a short.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a short, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a short\n     */\n    public static Short convertToShort(Schema schema, Object value) throws DataException {\n        return (Short) convertTo(Schema.OPTIONAL_INT16_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT32} int value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to an integer.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as an integer, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to an integer\n     */\n    public static Integer convertToInteger(Schema schema, Object value) throws DataException {\n        return (Integer) convertTo(Schema.OPTIONAL_INT32_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT64} long value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a long.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a long, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a long\n     */\n    public static Long convertToLong(Schema schema, Object value) throws DataException {\n        return (Long) convertTo(Schema.OPTIONAL_INT64_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#FLOAT32} float value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a floating point number.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a float, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a float\n     */\n    public static Float convertToFloat(Schema schema, Object value) throws DataException {\n        return (Float) convertTo(Schema.OPTIONAL_FLOAT32_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#FLOAT64} double value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a floating point number.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a double, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a double\n     */\n    public static Double convertToDouble(Schema schema, Object value) throws DataException {\n        return (Double) convertTo(Schema.OPTIONAL_FLOAT64_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#STRING} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a string, or null if the supplied value was null\n     */\n    public static String convertToString(Schema schema, Object value) {\n        return (String) convertTo(Schema.OPTIONAL_STRING_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#ARRAY} value. If the value is a string representation of an array, this method\n     * will parse the string and its elements to infer the schemas for those elements. Thus, this method supports\n     * arrays of other primitives and structured types. If the value is already an array (or list), this method simply casts and\n     * returns it.\n     *\n     * <p>This method currently does not use the schema, though it may be used in the future.</p>\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a list, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a list value\n     */\n    public static List<?> convertToList(Schema schema, Object value) {\n        return (List<?>) convertTo(ARRAY_SELECTOR_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#MAP} value. If the value is a string representation of a map, this method\n     * will parse the string and its entries to infer the schemas for those entries. Thus, this method supports\n     * maps with primitives and structured keys and values. If the value is already a map, this method simply casts and returns it.\n     *\n     * <p>This method currently does not use the schema, though it may be used in the future.</p>\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a map, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a map value\n     */\n    public static Map<?, ?> convertToMap(Schema schema, Object value) {\n        return (Map<?, ?>) convertTo(MAP_SELECTOR_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#STRUCT} value. Structs cannot be converted from other types, so this method returns\n     * a struct only if the supplied value is a struct. If not a struct, this method throws an exception.\n     *\n     * <p>This method currently does not use the schema, though it may be used in the future.</p>\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a struct, or null if the supplied value was null\n     * @throws DataException if the value is not a struct\n     */\n    public static Struct convertToStruct(Schema schema, Object value) {\n        return (Struct) convertTo(STRUCT_SELECTOR_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Time#SCHEMA time} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a time, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a time value\n     */\n    public static java.util.Date convertToTime(Schema schema, Object value) {\n        return (java.util.Date) convertTo(Time.SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Date#SCHEMA date} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a date, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a date value\n     */\n    public static java.util.Date convertToDate(Schema schema, Object value) {\n        return (java.util.Date) convertTo(Date.SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Timestamp#SCHEMA timestamp} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a timestamp, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a timestamp value\n     */\n    public static java.util.Date convertToTimestamp(Schema schema, Object value) {\n        return (java.util.Date) convertTo(Timestamp.SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Decimal decimal} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a decimal, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a decimal value\n     */\n    public static BigDecimal convertToDecimal(Schema schema, Object value, int scale) {\n        return (BigDecimal) convertTo(Decimal.schema(scale), schema, value);\n    }\n\n    /**\n     * If possible infer a schema for the given value.\n     *\n     * @param value the value whose schema is to be inferred; may be null\n     * @return the inferred schema, or null if the value is null or no schema could be inferred\n     */\n    public static Schema inferSchema(Object value) {\n        if (value instanceof String) {\n            return Schema.STRING_SCHEMA;\n        }\n        if (value instanceof Boolean) {\n            return Schema.BOOLEAN_SCHEMA;\n        }\n        if (value instanceof Byte) {\n            return Schema.INT8_SCHEMA;\n        }\n        if (value instanceof Short) {\n            return Schema.INT16_SCHEMA;\n        }\n        if (value instanceof Integer) {\n            return Schema.INT32_SCHEMA;\n        }\n        if (value instanceof Long) {\n            return Schema.INT64_SCHEMA;\n        }\n        if (value instanceof Float) {\n            return Schema.FLOAT32_SCHEMA;\n        }\n        if (value instanceof Double) {\n            return Schema.FLOAT64_SCHEMA;\n        }\n        if (value instanceof byte[] || value instanceof ByteBuffer) {\n            return Schema.BYTES_SCHEMA;\n        }\n        if (value instanceof List) {\n            List<?> list = (List<?>) value;\n            if (list.isEmpty()) {\n                return null;\n            }\n            SchemaDetector detector = new SchemaDetector();\n            for (Object element : list) {\n                if (!detector.canDetect(element)) {\n                    return null;\n                }\n            }\n            return SchemaBuilder.array(detector.schema()).build();\n        }\n        if (value instanceof Map) {\n            Map<?, ?> map = (Map<?, ?>) value;\n            if (map.isEmpty()) {\n                return null;\n            }\n            SchemaDetector keyDetector = new SchemaDetector();\n            SchemaDetector valueDetector = new SchemaDetector();\n            for (Map.Entry<?, ?> entry : map.entrySet()) {\n                if (!keyDetector.canDetect(entry.getKey()) || !valueDetector.canDetect(entry.getValue())) {\n                    return null;\n                }\n            }\n            return SchemaBuilder.map(keyDetector.schema(), valueDetector.schema()).build();\n        }\n        if (value instanceof Struct) {\n            return ((Struct) value).schema();\n        }\n        return null;\n    }\n\n\n    /**\n     * Parse the specified string representation of a value into its schema and value.\n     *\n     * @param value the string form of the value\n     * @return the schema and value; never null, but whose schema and value may be null\n     * @see #convertToString\n     */\n    public static SchemaAndValue parseString(String value) {\n        if (value == null) {\n            return NULL_SCHEMA_AND_VALUE;\n        }\n        if (value.isEmpty()) {\n            return new SchemaAndValue(Schema.STRING_SCHEMA, value);\n        }\n        Parser parser = new Parser(value);\n        return parse(parser, false);\n    }\n\n    /**\n     * Convert the value to the desired type.\n     *\n     * @param toSchema   the schema for the desired type; may not be null\n     * @param fromSchema the schema for the supplied value; may be null if not known\n     * @return the converted value; never null\n     * @throws DataException if the value could not be converted to the desired type\n     */\n    protected static Object convertTo(Schema toSchema, Schema fromSchema, Object value) throws DataException {\n        if (value == null) {\n            if (toSchema.isOptional()) {\n                return null;\n            }\n            throw new DataException(\"Unable to convert a null value to a schema that requires a value\");\n        }\n        switch (toSchema.type()) {\n            case BYTES:\n                if (Decimal.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof ByteBuffer) {\n                        value = Utils.toArray((ByteBuffer) value);\n                    }\n                    if (value instanceof byte[]) {\n                        return Decimal.toLogical(toSchema, (byte[]) value);\n                    }\n                    if (value instanceof BigDecimal) {\n                        return value;\n                    }\n                    if (value instanceof Number) {\n                        // Not already a decimal, so treat it as a double ...\n                        double converted = ((Number) value).doubleValue();\n                        return BigDecimal.valueOf(converted);\n                    }\n                    if (value instanceof String) {\n                        return new BigDecimal(value.toString());\n                    }\n                }\n                if (value instanceof ByteBuffer) {\n                    return Utils.toArray((ByteBuffer) value);\n                }\n                if (value instanceof byte[]) {\n                    return value;\n                }\n                if (value instanceof BigDecimal) {\n                    return Decimal.fromLogical(toSchema, (BigDecimal) value);\n                }\n                break;\n            case STRING:\n                StringBuilder sb = new StringBuilder();\n                append(sb, value, false);\n                return sb.toString();\n            case BOOLEAN:\n                if (value instanceof Boolean) {\n                    return value;\n                }\n                if (value instanceof String) {\n                    SchemaAndValue parsed = parseString(value.toString());\n                    if (parsed.value() instanceof Boolean) {\n                        return parsed.value();\n                    }\n                }\n                return asLong(value, fromSchema, null) == 0L ? Boolean.FALSE : Boolean.TRUE;\n            case INT8:\n                if (value instanceof Byte) {\n                    return value;\n                }\n                return (byte) asLong(value, fromSchema, null);\n            case INT16:\n                if (value instanceof Short) {\n                    return value;\n                }\n                return (short) asLong(value, fromSchema, null);\n            case INT32:\n                if (Date.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof String) {\n                        SchemaAndValue parsed = parseString(value.toString());\n                        value = parsed.value();\n                    }\n                    if (value instanceof java.util.Date) {\n                        if (fromSchema != null) {\n                            String fromSchemaName = fromSchema.name();\n                            if (Date.LOGICAL_NAME.equals(fromSchemaName)) {\n                                return value;\n                            }\n                            if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {\n                                // Just get the number of days from this timestamp\n                                long millis = ((java.util.Date) value).getTime();\n                                int days = (int) (millis / MILLIS_PER_DAY); // truncates\n                                return Date.toLogical(toSchema, days);\n                            }\n                        } else {\n                            // There is no fromSchema, so no conversion is needed\n                            return value;\n                        }\n                    }\n                    long numeric = asLong(value, fromSchema, null);\n                    return Date.toLogical(toSchema, (int) numeric);\n                }\n                if (Time.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof String) {\n                        SchemaAndValue parsed = parseString(value.toString());\n                        value = parsed.value();\n                    }\n                    if (value instanceof java.util.Date) {\n                        if (fromSchema != null) {\n                            String fromSchemaName = fromSchema.name();\n                            if (Time.LOGICAL_NAME.equals(fromSchemaName)) {\n                                return value;\n                            }\n                            if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {\n                                // Just get the time portion of this timestamp\n                                Calendar calendar = Calendar.getInstance(UTC);\n                                calendar.setTime((java.util.Date) value);\n                                calendar.set(Calendar.YEAR, 1970);\n                                calendar.set(Calendar.MONTH, 0); // Months are zero-based\n                                calendar.set(Calendar.DAY_OF_MONTH, 1);\n                                return Time.toLogical(toSchema, (int) calendar.getTimeInMillis());\n                            }\n                        } else {\n                            // There is no fromSchema, so no conversion is needed\n                            return value;\n                        }\n                    }\n                    long numeric = asLong(value, fromSchema, null);\n                    return Time.toLogical(toSchema, (int) numeric);\n                }\n                if (value instanceof Integer) {\n                    return value;\n                }\n                return (int) asLong(value, fromSchema, null);\n            case INT64:\n                if (Timestamp.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof String) {\n                        SchemaAndValue parsed = parseString(value.toString());\n                        value = parsed.value();\n                    }\n                    if (value instanceof java.util.Date) {\n                        java.util.Date date = (java.util.Date) value;\n                        if (fromSchema != null) {\n                            String fromSchemaName = fromSchema.name();\n                            if (Date.LOGICAL_NAME.equals(fromSchemaName)) {\n                                int days = Date.fromLogical(fromSchema, date);\n                                long millis = days * MILLIS_PER_DAY;\n                                return Timestamp.toLogical(toSchema, millis);\n                            }\n                            if (Time.LOGICAL_NAME.equals(fromSchemaName)) {\n                                long millis = Time.fromLogical(fromSchema, date);\n                                return Timestamp.toLogical(toSchema, millis);\n                            }\n                            if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {\n                                return value;\n                            }\n                        } else {\n                            // There is no fromSchema, so no conversion is needed\n                            return value;\n                        }\n                    }\n                    long numeric = asLong(value, fromSchema, null);\n                    return Timestamp.toLogical(toSchema, numeric);\n                }\n                if (value instanceof Long) {\n                    return value;\n                }\n                return asLong(value, fromSchema, null);\n            case FLOAT32:\n                if (value instanceof Float) {\n                    return value;\n                }\n                return (float) asDouble(value, fromSchema, null);\n            case FLOAT64:\n                if (value instanceof Double) {\n                    return value;\n                }\n                return asDouble(value, fromSchema, null);\n            case ARRAY:\n                if (value instanceof String) {\n                    SchemaAndValue schemaAndValue = parseString(value.toString());\n                    value = schemaAndValue.value();\n                }\n                if (value instanceof List) {\n                    return value;\n                }\n                break;\n            case MAP:\n                if (value instanceof String) {\n                    SchemaAndValue schemaAndValue = parseString(value.toString());\n                    value = schemaAndValue.value();\n                }\n                if (value instanceof Map) {\n                    return value;\n                }\n                break;\n            case STRUCT:\n                if (value instanceof Struct) {\n                    return value;\n                }\n        }\n        throw new DataException(\"Unable to convert \" + value + \" (\" + value.getClass() + \") to \" + toSchema);\n    }\n\n    /**\n     * Convert the specified value to the desired scalar value type.\n     *\n     * @param value      the value to be converted; may not be null\n     * @param fromSchema the schema for the current value type; may not be null\n     * @param error      any previous error that should be included in an exception message; may be null\n     * @return the long value after conversion; never null\n     * @throws DataException if the value could not be converted to a long\n     */\n    protected static long asLong(Object value, Schema fromSchema, Throwable error) {\n        try {\n            if (value instanceof Number) {\n                Number number = (Number) value;\n                return number.longValue();\n            }\n            if (value instanceof String) {\n                return new BigDecimal(value.toString()).longValue();\n            }\n        } catch (NumberFormatException e) {\n            error = e;\n            // fall through\n        }\n        if (fromSchema != null) {\n            String schemaName = fromSchema.name();\n            if (value instanceof java.util.Date) {\n                if (Date.LOGICAL_NAME.equals(schemaName)) {\n                    return Date.fromLogical(fromSchema, (java.util.Date) value);\n                }\n                if (Time.LOGICAL_NAME.equals(schemaName)) {\n                    return Time.fromLogical(fromSchema, (java.util.Date) value);\n                }\n                if (Timestamp.LOGICAL_NAME.equals(schemaName)) {\n                    return Timestamp.fromLogical(fromSchema, (java.util.Date) value);\n                }\n            }\n            throw new DataException(\"Unable to convert \" + value + \" (\" + value.getClass() + \") to \" + fromSchema, error);\n        }\n        throw new DataException(\"Unable to convert \" + value + \" (\" + value.getClass() + \") to a number\", error);\n    }\n\n    /**\n     * Convert the specified value with the desired floating point type.\n     *\n     * @param value  the value to be converted; may not be null\n     * @param schema the schema for the current value type; may not be null\n     * @param error  any previous error that should be included in an exception message; may be null\n     * @return the double value after conversion; never null\n     * @throws DataException if the value could not be converted to a double\n     */\n    protected static double asDouble(Object value, Schema schema, Throwable error) {\n        try {\n            if (value instanceof Number) {\n                Number number = (Number) value;\n                return number.doubleValue();\n            }\n            if (value instanceof String) {\n                return new BigDecimal(value.toString()).doubleValue();\n            }\n        } catch (NumberFormatException e) {\n            error = e;\n            // fall through\n        }\n        return asLong(value, schema, error);\n    }\n\n    protected static void append(StringBuilder sb, Object value, boolean embedded) {\n        if (value == null) {\n            sb.append(NULL_VALUE);\n        } else if (value instanceof Number) {\n            sb.append(value);\n        } else if (value instanceof Boolean) {\n            sb.append(value);\n        } else if (value instanceof String) {\n            if (embedded) {\n                String escaped = escape((String) value);\n                sb.append('\"').append(escaped).append('\"');\n            } else {\n                sb.append(value);\n            }\n        } else if (value instanceof byte[]) {\n            value = Base64.getEncoder().encodeToString((byte[]) value);\n            if (embedded) {\n                sb.append('\"').append(value).append('\"');\n            } else {\n                sb.append(value);\n            }\n        } else if (value instanceof ByteBuffer) {\n            byte[] bytes = Utils.readBytes((ByteBuffer) value);\n            append(sb, bytes, embedded);\n        } else if (value instanceof List) {\n            List<?> list = (List<?>) value;\n            sb.append('[');\n            appendIterable(sb, list.iterator());\n            sb.append(']');\n        } else if (value instanceof Map) {\n            Map<?, ?> map = (Map<?, ?>) value;\n            sb.append('{');\n            appendIterable(sb, map.entrySet().iterator());\n            sb.append('}');\n        } else if (value instanceof Struct) {\n            Struct struct = (Struct) value;\n            Schema schema = struct.schema();\n            boolean first = true;\n            sb.append('{');\n            for (Field field : schema.fields()) {\n                if (first) {\n                    first = false;\n                } else {\n                    sb.append(',');\n                }\n                append(sb, field.name(), true);\n                sb.append(':');\n                append(sb, struct.get(field), true);\n            }\n            sb.append('}');\n        } else if (value instanceof Map.Entry) {\n            Map.Entry<?, ?> entry = (Map.Entry<?, ?>) value;\n            append(sb, entry.getKey(), true);\n            sb.append(':');\n            append(sb, entry.getValue(), true);\n        } else if (value instanceof java.util.Date) {\n            java.util.Date dateValue = (java.util.Date) value;\n            String formatted = dateFormatFor(dateValue).format(dateValue);\n            sb.append(formatted);\n        } else {\n            throw new DataException(\"Failed to serialize unexpected value type \" + value.getClass().getName() + \": \" + value);\n        }\n    }\n\n    protected static void appendIterable(StringBuilder sb, Iterator<?> iter) {\n        if (iter.hasNext()) {\n            append(sb, iter.next(), true);\n            while (iter.hasNext()) {\n                sb.append(',');\n                append(sb, iter.next(), true);\n            }\n        }\n    }\n\n    protected static String escape(String value) {\n        String replace1 = TWO_BACKSLASHES.matcher(value).replaceAll(\"\\\\\\\\\\\\\\\\\");\n        return DOUBLE_QUOTE.matcher(replace1).replaceAll(\"\\\\\\\\\\\"\");\n    }\n\n    public static DateFormat dateFormatFor(java.util.Date value) {\n        if (value.getTime() < MILLIS_PER_DAY) {\n            return new SimpleDateFormat(ISO_8601_TIME_FORMAT_PATTERN);\n        }\n        if (value.getTime() % MILLIS_PER_DAY == 0) {\n            return new SimpleDateFormat(ISO_8601_DATE_FORMAT_PATTERN);\n        }\n        return new SimpleDateFormat(ISO_8601_TIMESTAMP_FORMAT_PATTERN);\n    }\n\n    protected static boolean canParseSingleTokenLiteral(Parser parser, boolean embedded, String tokenLiteral) {\n        int startPosition = parser.mark();\n        // If the next token is what we expect, then either...\n        if (parser.canConsume(tokenLiteral)) {\n            //   ...we're reading an embedded value, in which case the next token will be handled appropriately\n            //      by the caller if it's something like an end delimiter for a map or array, or a comma to\n            //      separate multiple embedded values...\n            //   ...or it's being parsed as part of a top-level string, in which case, any other tokens should\n            //      cause use to stop parsing this single-token literal as such and instead just treat it like\n            //      a string. For example, the top-level string \"true}\" will be tokenized as the tokens \"true\" and\n            //      \"}\", but should ultimately be parsed as just the string \"true}\" instead of the boolean true.\n            if (embedded || !parser.hasNext()) {\n                return true;\n            }\n        }\n        parser.rewindTo(startPosition);\n        return false;\n    }\n\n    protected static SchemaAndValue parse(Parser parser, boolean embedded) throws NoSuchElementException {\n        if (!parser.hasNext()) {\n            return null;\n        }\n        if (embedded) {\n            if (parser.canConsume(QUOTE_DELIMITER)) {\n                StringBuilder sb = new StringBuilder();\n                while (parser.hasNext()) {\n                    if (parser.canConsume(QUOTE_DELIMITER)) {\n                        break;\n                    }\n                    sb.append(parser.next());\n                }\n                String content = sb.toString();\n                // We can parse string literals as temporal logical types, but all others\n                // are treated as strings\n                SchemaAndValue parsed = parseString(content);\n                if (parsed != null && TEMPORAL_LOGICAL_TYPE_NAMES.contains(parsed.schema().name())) {\n                    return parsed;\n                }\n                return new SchemaAndValue(Schema.STRING_SCHEMA, content);\n            }\n        }\n\n        if (canParseSingleTokenLiteral(parser, embedded, NULL_VALUE)) {\n            return null;\n        }\n        if (canParseSingleTokenLiteral(parser, embedded, TRUE_LITERAL)) {\n            return TRUE_SCHEMA_AND_VALUE;\n        }\n        if (canParseSingleTokenLiteral(parser, embedded, FALSE_LITERAL)) {\n            return FALSE_SCHEMA_AND_VALUE;\n        }\n\n        int startPosition = parser.mark();\n\n        try {\n            if (parser.canConsume(ARRAY_BEGIN_DELIMITER)) {\n                List<Object> result = new ArrayList<>();\n                boolean compatible = true;\n                Schema elementSchema = null;\n                while (parser.hasNext()) {\n                    if (parser.canConsume(ARRAY_END_DELIMITER)) {\n                        Schema listSchema;\n                        if (elementSchema != null && compatible) {\n                            listSchema = SchemaBuilder.array(elementSchema).schema();\n                            result = alignListEntriesWithSchema(listSchema, result);\n                        } else {\n                            // Every value is null\n                            listSchema = SchemaBuilder.arrayOfNull().build();\n                        }\n                        return new SchemaAndValue(listSchema, result);\n                    }\n\n                    if (parser.canConsume(COMMA_DELIMITER)) {\n                        throw new DataException(\"Unable to parse an empty array element: \" + parser.original());\n                    }\n                    SchemaAndValue element = parse(parser, true);\n                    elementSchema = commonSchemaFor(elementSchema, element);\n                    if (elementSchema == null && element != null && element.schema() != null) {\n                        compatible = false;\n                    }\n                    result.add(element != null ? element.value() : null);\n\n                    int currentPosition = parser.mark();\n                    if (parser.canConsume(ARRAY_END_DELIMITER)) {\n                        parser.rewindTo(currentPosition);\n                    } else if (!parser.canConsume(COMMA_DELIMITER)) {\n                        throw new DataException(\"Array elements missing '\" + COMMA_DELIMITER + \"' delimiter\");\n                    }\n                }\n\n                // Missing either a comma or an end delimiter\n                if (COMMA_DELIMITER.equals(parser.previous())) {\n                    throw new DataException(\"Array is missing element after ',': \" + parser.original());\n                }\n                throw new DataException(\"Array is missing terminating ']': \" + parser.original());\n            }\n\n            if (parser.canConsume(MAP_BEGIN_DELIMITER)) {\n                Map<Object, Object> result = new LinkedHashMap<>();\n                boolean keyCompatible = true;\n                Schema keySchema = null;\n                boolean valueCompatible = true;\n                Schema valueSchema = null;\n                while (parser.hasNext()) {\n                    if (parser.canConsume(MAP_END_DELIMITER)) {\n                        Schema mapSchema;\n                        if (keySchema != null && valueSchema != null && keyCompatible && valueCompatible) {\n                            mapSchema = SchemaBuilder.map(keySchema, valueSchema).build();\n                            result = alignMapKeysAndValuesWithSchema(mapSchema, result);\n                        } else if (keySchema != null && keyCompatible) {\n                            mapSchema = SchemaBuilder.mapWithNullValues(keySchema);\n                            result = alignMapKeysWithSchema(mapSchema, result);\n                        } else {\n                            mapSchema = SchemaBuilder.mapOfNull().build();\n                        }\n                        return new SchemaAndValue(mapSchema, result);\n                    }\n\n                    if (parser.canConsume(COMMA_DELIMITER)) {\n                        throw new DataException(\"Unable to parse a map entry with no key or value: \" + parser.original());\n                    }\n                    SchemaAndValue key = parse(parser, true);\n                    if (key == null || key.value() == null) {\n                        throw new DataException(\"Map entry may not have a null key: \" + parser.original());\n                    }\n\n                    if (!parser.canConsume(ENTRY_DELIMITER)) {\n                        throw new DataException(\"Map entry is missing '\" + ENTRY_DELIMITER\n                                                + \"' at \" + parser.position()\n                                                + \" in \" + parser.original());\n                    }\n                    SchemaAndValue value = parse(parser, true);\n                    Object entryValue = value != null ? value.value() : null;\n                    result.put(key.value(), entryValue);\n\n                    parser.canConsume(COMMA_DELIMITER);\n                    keySchema = commonSchemaFor(keySchema, key);\n                    if (keySchema == null && key.schema() != null) {\n                        keyCompatible = false;\n                    }\n                    valueSchema = commonSchemaFor(valueSchema, value);\n                    if (valueSchema == null && value != null && value.schema() != null) {\n                        valueCompatible = false;\n                    }\n                }\n                // Missing either a comma or an end delimiter\n                if (COMMA_DELIMITER.equals(parser.previous())) {\n                    throw new DataException(\"Map is missing element after ',': \" + parser.original());\n                }\n                throw new DataException(\"Map is missing terminating '}': \" + parser.original());\n            }\n        } catch (DataException e) {\n            LOG.trace(\"Unable to parse the value as a map or an array; reverting to string\", e);\n            parser.rewindTo(startPosition);\n        }\n\n        String token = parser.next();\n        if (Utils.isBlank(token)) {\n            return new SchemaAndValue(Schema.STRING_SCHEMA, token);\n        }\n        token = token.trim();\n\n        char firstChar = token.charAt(0);\n        boolean firstCharIsDigit = Character.isDigit(firstChar);\n\n        // Temporal types are more restrictive, so try them first\n        if (firstCharIsDigit) {\n            // The time and timestamp literals may be split into 5 tokens since an unescaped colon\n            // is a delimiter. Check these first since the first of these tokens is a simple numeric\n            int position = parser.mark();\n            String remainder = parser.next(4);\n            if (remainder != null) {\n                String timeOrTimestampStr = token + remainder;\n                SchemaAndValue temporal = parseAsTemporal(timeOrTimestampStr);\n                if (temporal != null) {\n                    return temporal;\n                }\n            }\n            // No match was found using the 5 tokens, so rewind and see if the current token has a date, time, or timestamp\n            parser.rewindTo(position);\n            SchemaAndValue temporal = parseAsTemporal(token);\n            if (temporal != null) {\n                return temporal;\n            }\n        }\n        if (firstCharIsDigit || firstChar == '+' || firstChar == '-') {\n            try {\n                // Try to parse as a number ...\n                BigDecimal decimal = new BigDecimal(token);\n                try {\n                    return new SchemaAndValue(Schema.INT8_SCHEMA, decimal.byteValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                try {\n                    return new SchemaAndValue(Schema.INT16_SCHEMA, decimal.shortValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                try {\n                    return new SchemaAndValue(Schema.INT32_SCHEMA, decimal.intValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                try {\n                    return new SchemaAndValue(Schema.INT64_SCHEMA, decimal.longValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                float fValue = decimal.floatValue();\n                if (fValue != Float.NEGATIVE_INFINITY && fValue != Float.POSITIVE_INFINITY\n                    && decimal.scale() != 0) {\n                    return new SchemaAndValue(Schema.FLOAT32_SCHEMA, fValue);\n                }\n                double dValue = decimal.doubleValue();\n                if (dValue != Double.NEGATIVE_INFINITY && dValue != Double.POSITIVE_INFINITY\n                    && decimal.scale() != 0) {\n                    return new SchemaAndValue(Schema.FLOAT64_SCHEMA, dValue);\n                }\n                Schema schema = Decimal.schema(decimal.scale());\n                return new SchemaAndValue(schema, decimal);\n            } catch (NumberFormatException e) {\n                // can't parse as a number\n            }\n        }\n        if (embedded) {\n            throw new DataException(\"Failed to parse embedded value\");\n        }\n        // At this point, the only thing this non-embedded value can be is a string.\n        return new SchemaAndValue(Schema.STRING_SCHEMA, parser.original());\n    }\n\n    private static SchemaAndValue parseAsTemporal(String token) {\n        if (token == null) {\n            return null;\n        }\n        // If the colons were escaped, we'll see the escape chars and need to remove them\n        token = token.replace(\"\\\\:\", \":\");\n        int tokenLength = token.length();\n        if (tokenLength == ISO_8601_TIME_LENGTH) {\n            try {\n                return new SchemaAndValue(Time.SCHEMA, new SimpleDateFormat(ISO_8601_TIME_FORMAT_PATTERN).parse(token));\n            } catch (ParseException e) {\n              // not a valid date\n            }\n        } else if (tokenLength == ISO_8601_TIMESTAMP_LENGTH) {\n            try {\n                return new SchemaAndValue(Timestamp.SCHEMA, new SimpleDateFormat(ISO_8601_TIMESTAMP_FORMAT_PATTERN).parse(token));\n            } catch (ParseException e) {\n              // not a valid date\n            }\n        } else if (tokenLength == ISO_8601_DATE_LENGTH) {\n            try {\n                return new SchemaAndValue(Date.SCHEMA, new SimpleDateFormat(ISO_8601_DATE_FORMAT_PATTERN).parse(token));\n            } catch (ParseException e) {\n                // not a valid date\n            }\n        }\n        return null;\n    }\n\n    protected static Schema commonSchemaFor(Schema previous, SchemaAndValue latest) {\n        if (latest == null) {\n            return previous;\n        }\n        if (previous == null) {\n            return latest.schema();\n        }\n        Schema newSchema = latest.schema();\n        Type previousType = previous.type();\n        Type newType = newSchema.type();\n        if (previousType != newType) {\n            switch (previous.type()) {\n                case INT8:\n                    if (newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64 || newType == Type.FLOAT32 || newType ==\n                                                                                                                              Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case INT16:\n                    if (newType == Type.INT8) {\n                        return previous;\n                    }\n                    if (newType == Type.INT32 || newType == Type.INT64 || newType == Type.FLOAT32 || newType == Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case INT32:\n                    if (newType == Type.INT8 || newType == Type.INT16) {\n                        return previous;\n                    }\n                    if (newType == Type.INT64 || newType == Type.FLOAT32 || newType == Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case INT64:\n                    if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32) {\n                        return previous;\n                    }\n                    if (newType == Type.FLOAT32 || newType == Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case FLOAT32:\n                    if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64) {\n                        return previous;\n                    }\n                    if (newType == Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case FLOAT64:\n                    if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64 || newType ==\n                                                                                                                           Type.FLOAT32) {\n                        return previous;\n                    }\n                    break;\n            }\n            return null;\n        }\n        if (previous.isOptional() == newSchema.isOptional()) {\n            // Use the optional one\n            return previous.isOptional() ? previous : newSchema;\n        }\n        if (!previous.equals(newSchema)) {\n            return null;\n        }\n        return previous;\n    }\n\n    protected static List<Object> alignListEntriesWithSchema(Schema schema, List<Object> input) {\n        Schema valueSchema = schema.valueSchema();\n        List<Object> result = new ArrayList<>();\n        for (Object value : input) {\n            Object newValue = convertTo(valueSchema, null, value);\n            result.add(newValue);\n        }\n        return result;\n    }\n\n    protected static Map<Object, Object> alignMapKeysAndValuesWithSchema(Schema mapSchema, Map<Object, Object> input) {\n        Schema keySchema = mapSchema.keySchema();\n        Schema valueSchema = mapSchema.valueSchema();\n        Map<Object, Object> result = new LinkedHashMap<>();\n        for (Map.Entry<?, ?> entry : input.entrySet()) {\n            Object newKey = convertTo(keySchema, null, entry.getKey());\n            Object newValue = convertTo(valueSchema, null, entry.getValue());\n            result.put(newKey, newValue);\n        }\n        return result;\n    }\n\n    protected static Map<Object, Object> alignMapKeysWithSchema(Schema mapSchema, Map<Object, Object> input) {\n        Schema keySchema = mapSchema.keySchema();\n        Map<Object, Object> result = new LinkedHashMap<>();\n        for (Map.Entry<?, ?> entry : input.entrySet()) {\n            Object newKey = convertTo(keySchema, null, entry.getKey());\n            result.put(newKey, entry.getValue());\n        }\n        return result;\n    }\n\n    protected static class SchemaDetector {\n        private Type knownType = null;\n        private boolean optional = false;\n\n        public SchemaDetector() {\n        }\n\n        public boolean canDetect(Object value) {\n            if (value == null) {\n                optional = true;\n                return true;\n            }\n            Schema schema = inferSchema(value);\n            if (schema == null) {\n                return false;\n            }\n            if (knownType == null) {\n                knownType = schema.type();\n            } else if (knownType != schema.type()) {\n                return false;\n            }\n            return true;\n        }\n\n        public Schema schema() {\n            SchemaBuilder builder = SchemaBuilder.type(knownType);\n            if (optional) {\n                builder.optional();\n            }\n            return builder.schema();\n        }\n    }\n\n    protected static class Parser {\n        private final String original;\n        private final CharacterIterator iter;\n        private String nextToken = null;\n        private String previousToken = null;\n\n        public Parser(String original) {\n            this.original = original;\n            this.iter = new StringCharacterIterator(this.original);\n        }\n\n        public int position() {\n            return iter.getIndex();\n        }\n\n        public int mark() {\n            return iter.getIndex() - (nextToken != null ? nextToken.length() : 0);\n        }\n\n        public void rewindTo(int position) {\n            iter.setIndex(position);\n            nextToken = null;\n            previousToken = null;\n        }\n\n        public String original() {\n            return original;\n        }\n\n        public boolean hasNext() {\n            return nextToken != null || canConsumeNextToken();\n        }\n\n        protected boolean canConsumeNextToken() {\n            return iter.getEndIndex() > iter.getIndex();\n        }\n\n        public String next() {\n            if (nextToken != null) {\n                previousToken = nextToken;\n                nextToken = null;\n            } else {\n                previousToken = consumeNextToken();\n            }\n            return previousToken;\n        }\n\n        public String next(int n) {\n            int current = mark();\n            int start = mark();\n            for (int i = 0; i != n; ++i) {\n                if (!hasNext()) {\n                    rewindTo(start);\n                    return null;\n                }\n                next();\n            }\n            return original.substring(current, position());\n        }\n\n        private String consumeNextToken() throws NoSuchElementException {\n            boolean escaped = false;\n            int start = iter.getIndex();\n            char c = iter.current();\n            while (canConsumeNextToken()) {\n                switch (c) {\n                    case '\\\\':\n                        escaped = !escaped;\n                        break;\n                    case ':':\n                    case ',':\n                    case '{':\n                    case '}':\n                    case '[':\n                    case ']':\n                    case '\\\"':\n                        if (!escaped) {\n                            if (start < iter.getIndex()) {\n                                // Return the previous token\n                                return original.substring(start, iter.getIndex());\n                            }\n                            // Consume and return this delimiter as a token\n                            iter.next();\n                            return original.substring(start, start + 1);\n                        }\n                        // escaped, so continue\n                        escaped = false;\n                        break;\n                    default:\n                        // If escaped, then we don't care what was escaped\n                        escaped = false;\n                        break;\n                }\n                c = iter.next();\n            }\n            return original.substring(start, iter.getIndex());\n        }\n\n        public String previous() {\n            return previousToken;\n        }\n\n        public boolean canConsume(String expected) {\n            return canConsume(expected, true);\n        }\n\n        public boolean canConsume(String expected, boolean ignoreLeadingAndTrailingWhitespace) {\n            if (isNext(expected, ignoreLeadingAndTrailingWhitespace)) {\n                // consume this token ...\n                nextToken = null;\n                return true;\n            }\n            return false;\n        }\n\n        protected boolean isNext(String expected, boolean ignoreLeadingAndTrailingWhitespace) {\n            if (nextToken == null) {\n                if (!hasNext()) {\n                    return false;\n                }\n                // There's another token, so consume it\n                nextToken = consumeNextToken();\n            }\n            if (ignoreLeadingAndTrailingWhitespace) {\n                while (Utils.isBlank(nextToken) && canConsumeNextToken()) {\n                    nextToken = consumeNextToken();\n                }\n            }\n            return ignoreLeadingAndTrailingWhitespace\n                ? nextToken.trim().equals(expected)\n                : nextToken.equals(expected);\n        }\n    }\n}"
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [],
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "11ad5e8bca2444d8c51cd6b81ee84dcb41a047c2",
        "url": "https://github.com/apache/kafka/commit/11ad5e8bca2444d8c51cd6b81ee84dcb41a047c2",
        "move_method_refactoring": {
            "type": "Move Method",
            "description": "Move Method private parseAsTemporal(token String) : SchemaAndValue from class org.apache.kafka.connect.data.Values to private parseAsTemporal(token String) : SchemaAndValue from class org.apache.kafka.connect.data.Values.ValueParser",
            "leftSideLocations": [
                {
                    "filePath": "connect/api/src/main/java/org/apache/kafka/connect/data/Values.java",
                    "startLine": 980,
                    "endLine": 1007,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "original method declaration",
                    "codeElement": "private parseAsTemporal(token String) : SchemaAndValue"
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/api/src/main/java/org/apache/kafka/connect/data/Values.java",
                    "startLine": 1067,
                    "endLine": 1083,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "moved method declaration",
                    "codeElement": "private parseAsTemporal(token String) : SchemaAndValue"
                }
            ],
            "isStatic": true
        },
        "telemetry": {
            "id": "20235999-0b83-4f5f-9f21-545884d27024",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1230,
                "lineStart": 47,
                "lineEnd": 1276,
                "bodyLineStart": 47,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/connect/api/src/main/java/org/apache/kafka/connect/data/Values.java",
                "sourceCode": "/**\n * Utility for converting from one Connect value to a different form. This is useful when the caller expects a value of a particular type\n * but is uncertain whether the actual value is one that isn't directly that type but can be converted into that type.\n *\n * <p>For example, a caller might expect a particular {@link org.apache.kafka.connect.header.Header} to contain a {@link Type#INT64}\n * value, when in fact that header contains a string representation of a 32-bit integer. Here, the caller can use the methods in this\n * class to convert the value to the desired type:\n * <pre>\n *     Header header = ...\n *     long value = Values.convertToLong(header.schema(), header.value());\n * </pre>\n *\n * <p>This class is able to convert any value to a string representation as well as parse those string representations back into most of\n * the types. The only exception is {@link Struct} values that require a schema and thus cannot be parsed from a simple string.\n */\npublic class Values {\n\n    private static final Logger LOG = LoggerFactory.getLogger(Values.class);\n\n    private static final TimeZone UTC = TimeZone.getTimeZone(\"UTC\");\n    private static final SchemaAndValue NULL_SCHEMA_AND_VALUE = new SchemaAndValue(null, null);\n    private static final SchemaAndValue TRUE_SCHEMA_AND_VALUE = new SchemaAndValue(Schema.BOOLEAN_SCHEMA, Boolean.TRUE);\n    private static final SchemaAndValue FALSE_SCHEMA_AND_VALUE = new SchemaAndValue(Schema.BOOLEAN_SCHEMA, Boolean.FALSE);\n    private static final Schema ARRAY_SELECTOR_SCHEMA = SchemaBuilder.array(Schema.STRING_SCHEMA).build();\n    private static final Schema MAP_SELECTOR_SCHEMA = SchemaBuilder.map(Schema.STRING_SCHEMA, Schema.STRING_SCHEMA).build();\n    private static final Schema STRUCT_SELECTOR_SCHEMA = SchemaBuilder.struct().build();\n    private static final String TRUE_LITERAL = Boolean.TRUE.toString();\n    private static final String FALSE_LITERAL = Boolean.FALSE.toString();\n    private static final long MILLIS_PER_DAY = 24 * 60 * 60 * 1000;\n    private static final String NULL_VALUE = \"null\";\n    static final String ISO_8601_DATE_FORMAT_PATTERN = \"yyyy-MM-dd\";\n    static final String ISO_8601_TIME_FORMAT_PATTERN = \"HH:mm:ss.SSS'Z'\";\n    static final String ISO_8601_TIMESTAMP_FORMAT_PATTERN = ISO_8601_DATE_FORMAT_PATTERN + \"'T'\" + ISO_8601_TIME_FORMAT_PATTERN;\n    private static final Set<String> TEMPORAL_LOGICAL_TYPE_NAMES =\n            Collections.unmodifiableSet(\n                    new HashSet<>(\n                            Arrays.asList(Time.LOGICAL_NAME,\n                            Timestamp.LOGICAL_NAME,\n                            Date.LOGICAL_NAME\n                            )\n                    )\n            );\n\n    private static final String QUOTE_DELIMITER = \"\\\"\";\n    private static final String COMMA_DELIMITER = \",\";\n    private static final String ENTRY_DELIMITER = \":\";\n    private static final String ARRAY_BEGIN_DELIMITER = \"[\";\n    private static final String ARRAY_END_DELIMITER = \"]\";\n    private static final String MAP_BEGIN_DELIMITER = \"{\";\n    private static final String MAP_END_DELIMITER = \"}\";\n    private static final int ISO_8601_DATE_LENGTH = ISO_8601_DATE_FORMAT_PATTERN.length();\n    private static final int ISO_8601_TIME_LENGTH = ISO_8601_TIME_FORMAT_PATTERN.length() - 2; // subtract single quotes\n    private static final int ISO_8601_TIMESTAMP_LENGTH = ISO_8601_TIMESTAMP_FORMAT_PATTERN.length() - 4; // subtract single quotes\n\n    private static final Pattern TWO_BACKSLASHES = Pattern.compile(\"\\\\\\\\\");\n\n    private static final Pattern DOUBLE_QUOTE = Pattern.compile(\"\\\"\");\n\n    /**\n     * Convert the specified value to a {@link Type#BOOLEAN} value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a boolean.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a boolean, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a boolean\n     */\n    public static Boolean convertToBoolean(Schema schema, Object value) throws DataException {\n        return (Boolean) convertTo(Schema.OPTIONAL_BOOLEAN_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT8} byte value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a byte.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a byte, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a byte\n     */\n    public static Byte convertToByte(Schema schema, Object value) throws DataException {\n        return (Byte) convertTo(Schema.OPTIONAL_INT8_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT16} short value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a short.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a short, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a short\n     */\n    public static Short convertToShort(Schema schema, Object value) throws DataException {\n        return (Short) convertTo(Schema.OPTIONAL_INT16_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT32} int value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to an integer.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as an integer, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to an integer\n     */\n    public static Integer convertToInteger(Schema schema, Object value) throws DataException {\n        return (Integer) convertTo(Schema.OPTIONAL_INT32_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT64} long value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a long.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a long, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a long\n     */\n    public static Long convertToLong(Schema schema, Object value) throws DataException {\n        return (Long) convertTo(Schema.OPTIONAL_INT64_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#FLOAT32} float value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a floating point number.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a float, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a float\n     */\n    public static Float convertToFloat(Schema schema, Object value) throws DataException {\n        return (Float) convertTo(Schema.OPTIONAL_FLOAT32_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#FLOAT64} double value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a floating point number.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a double, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a double\n     */\n    public static Double convertToDouble(Schema schema, Object value) throws DataException {\n        return (Double) convertTo(Schema.OPTIONAL_FLOAT64_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#STRING} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a string, or null if the supplied value was null\n     */\n    public static String convertToString(Schema schema, Object value) {\n        return (String) convertTo(Schema.OPTIONAL_STRING_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#ARRAY} value. If the value is a string representation of an array, this method\n     * will parse the string and its elements to infer the schemas for those elements. Thus, this method supports\n     * arrays of other primitives and structured types. If the value is already an array (or list), this method simply casts and\n     * returns it.\n     *\n     * <p>This method currently does not use the schema, though it may be used in the future.</p>\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a list, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a list value\n     */\n    public static List<?> convertToList(Schema schema, Object value) {\n        return (List<?>) convertTo(ARRAY_SELECTOR_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#MAP} value. If the value is a string representation of a map, this method\n     * will parse the string and its entries to infer the schemas for those entries. Thus, this method supports\n     * maps with primitives and structured keys and values. If the value is already a map, this method simply casts and returns it.\n     *\n     * <p>This method currently does not use the schema, though it may be used in the future.</p>\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a map, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a map value\n     */\n    public static Map<?, ?> convertToMap(Schema schema, Object value) {\n        return (Map<?, ?>) convertTo(MAP_SELECTOR_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#STRUCT} value. Structs cannot be converted from other types, so this method returns\n     * a struct only if the supplied value is a struct. If not a struct, this method throws an exception.\n     *\n     * <p>This method currently does not use the schema, though it may be used in the future.</p>\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a struct, or null if the supplied value was null\n     * @throws DataException if the value is not a struct\n     */\n    public static Struct convertToStruct(Schema schema, Object value) {\n        return (Struct) convertTo(STRUCT_SELECTOR_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Time#SCHEMA time} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a time, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a time value\n     */\n    public static java.util.Date convertToTime(Schema schema, Object value) {\n        return (java.util.Date) convertTo(Time.SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Date#SCHEMA date} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a date, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a date value\n     */\n    public static java.util.Date convertToDate(Schema schema, Object value) {\n        return (java.util.Date) convertTo(Date.SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Timestamp#SCHEMA timestamp} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a timestamp, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a timestamp value\n     */\n    public static java.util.Date convertToTimestamp(Schema schema, Object value) {\n        return (java.util.Date) convertTo(Timestamp.SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Decimal decimal} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a decimal, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a decimal value\n     */\n    public static BigDecimal convertToDecimal(Schema schema, Object value, int scale) {\n        return (BigDecimal) convertTo(Decimal.schema(scale), schema, value);\n    }\n\n    /**\n     * If possible infer a schema for the given value.\n     *\n     * @param value the value whose schema is to be inferred; may be null\n     * @return the inferred schema, or null if the value is null or no schema could be inferred\n     */\n    public static Schema inferSchema(Object value) {\n        if (value instanceof String) {\n            return Schema.STRING_SCHEMA;\n        }\n        if (value instanceof Boolean) {\n            return Schema.BOOLEAN_SCHEMA;\n        }\n        if (value instanceof Byte) {\n            return Schema.INT8_SCHEMA;\n        }\n        if (value instanceof Short) {\n            return Schema.INT16_SCHEMA;\n        }\n        if (value instanceof Integer) {\n            return Schema.INT32_SCHEMA;\n        }\n        if (value instanceof Long) {\n            return Schema.INT64_SCHEMA;\n        }\n        if (value instanceof Float) {\n            return Schema.FLOAT32_SCHEMA;\n        }\n        if (value instanceof Double) {\n            return Schema.FLOAT64_SCHEMA;\n        }\n        if (value instanceof byte[] || value instanceof ByteBuffer) {\n            return Schema.BYTES_SCHEMA;\n        }\n        if (value instanceof List) {\n            List<?> list = (List<?>) value;\n            if (list.isEmpty()) {\n                return null;\n            }\n            SchemaDetector detector = new SchemaDetector();\n            for (Object element : list) {\n                if (!detector.canDetect(element)) {\n                    return null;\n                }\n            }\n            return SchemaBuilder.array(detector.schema()).build();\n        }\n        if (value instanceof Map) {\n            Map<?, ?> map = (Map<?, ?>) value;\n            if (map.isEmpty()) {\n                return null;\n            }\n            SchemaDetector keyDetector = new SchemaDetector();\n            SchemaDetector valueDetector = new SchemaDetector();\n            for (Map.Entry<?, ?> entry : map.entrySet()) {\n                if (!keyDetector.canDetect(entry.getKey()) || !valueDetector.canDetect(entry.getValue())) {\n                    return null;\n                }\n            }\n            return SchemaBuilder.map(keyDetector.schema(), valueDetector.schema()).build();\n        }\n        if (value instanceof Struct) {\n            return ((Struct) value).schema();\n        }\n        return null;\n    }\n\n\n    /**\n     * Parse the specified string representation of a value into its schema and value.\n     *\n     * @param value the string form of the value\n     * @return the schema and value; never null, but whose schema and value may be null\n     * @see #convertToString\n     */\n    public static SchemaAndValue parseString(String value) {\n        if (value == null) {\n            return NULL_SCHEMA_AND_VALUE;\n        }\n        if (value.isEmpty()) {\n            return new SchemaAndValue(Schema.STRING_SCHEMA, value);\n        }\n        Parser parser = new Parser(value);\n        return parse(parser, false);\n    }\n\n    /**\n     * Convert the value to the desired type.\n     *\n     * @param toSchema   the schema for the desired type; may not be null\n     * @param fromSchema the schema for the supplied value; may be null if not known\n     * @return the converted value; never null\n     * @throws DataException if the value could not be converted to the desired type\n     */\n    protected static Object convertTo(Schema toSchema, Schema fromSchema, Object value) throws DataException {\n        if (value == null) {\n            if (toSchema.isOptional()) {\n                return null;\n            }\n            throw new DataException(\"Unable to convert a null value to a schema that requires a value\");\n        }\n        switch (toSchema.type()) {\n            case BYTES:\n                if (Decimal.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof ByteBuffer) {\n                        value = Utils.toArray((ByteBuffer) value);\n                    }\n                    if (value instanceof byte[]) {\n                        return Decimal.toLogical(toSchema, (byte[]) value);\n                    }\n                    if (value instanceof BigDecimal) {\n                        return value;\n                    }\n                    if (value instanceof Number) {\n                        // Not already a decimal, so treat it as a double ...\n                        double converted = ((Number) value).doubleValue();\n                        return BigDecimal.valueOf(converted);\n                    }\n                    if (value instanceof String) {\n                        return new BigDecimal(value.toString());\n                    }\n                }\n                if (value instanceof ByteBuffer) {\n                    return Utils.toArray((ByteBuffer) value);\n                }\n                if (value instanceof byte[]) {\n                    return value;\n                }\n                if (value instanceof BigDecimal) {\n                    return Decimal.fromLogical(toSchema, (BigDecimal) value);\n                }\n                break;\n            case STRING:\n                StringBuilder sb = new StringBuilder();\n                append(sb, value, false);\n                return sb.toString();\n            case BOOLEAN:\n                if (value instanceof Boolean) {\n                    return value;\n                }\n                if (value instanceof String) {\n                    SchemaAndValue parsed = parseString(value.toString());\n                    if (parsed.value() instanceof Boolean) {\n                        return parsed.value();\n                    }\n                }\n                return asLong(value, fromSchema, null) == 0L ? Boolean.FALSE : Boolean.TRUE;\n            case INT8:\n                if (value instanceof Byte) {\n                    return value;\n                }\n                return (byte) asLong(value, fromSchema, null);\n            case INT16:\n                if (value instanceof Short) {\n                    return value;\n                }\n                return (short) asLong(value, fromSchema, null);\n            case INT32:\n                if (Date.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof String) {\n                        SchemaAndValue parsed = parseString(value.toString());\n                        value = parsed.value();\n                    }\n                    if (value instanceof java.util.Date) {\n                        if (fromSchema != null) {\n                            String fromSchemaName = fromSchema.name();\n                            if (Date.LOGICAL_NAME.equals(fromSchemaName)) {\n                                return value;\n                            }\n                            if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {\n                                // Just get the number of days from this timestamp\n                                long millis = ((java.util.Date) value).getTime();\n                                int days = (int) (millis / MILLIS_PER_DAY); // truncates\n                                return Date.toLogical(toSchema, days);\n                            }\n                        } else {\n                            // There is no fromSchema, so no conversion is needed\n                            return value;\n                        }\n                    }\n                    long numeric = asLong(value, fromSchema, null);\n                    return Date.toLogical(toSchema, (int) numeric);\n                }\n                if (Time.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof String) {\n                        SchemaAndValue parsed = parseString(value.toString());\n                        value = parsed.value();\n                    }\n                    if (value instanceof java.util.Date) {\n                        if (fromSchema != null) {\n                            String fromSchemaName = fromSchema.name();\n                            if (Time.LOGICAL_NAME.equals(fromSchemaName)) {\n                                return value;\n                            }\n                            if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {\n                                // Just get the time portion of this timestamp\n                                Calendar calendar = Calendar.getInstance(UTC);\n                                calendar.setTime((java.util.Date) value);\n                                calendar.set(Calendar.YEAR, 1970);\n                                calendar.set(Calendar.MONTH, 0); // Months are zero-based\n                                calendar.set(Calendar.DAY_OF_MONTH, 1);\n                                return Time.toLogical(toSchema, (int) calendar.getTimeInMillis());\n                            }\n                        } else {\n                            // There is no fromSchema, so no conversion is needed\n                            return value;\n                        }\n                    }\n                    long numeric = asLong(value, fromSchema, null);\n                    return Time.toLogical(toSchema, (int) numeric);\n                }\n                if (value instanceof Integer) {\n                    return value;\n                }\n                return (int) asLong(value, fromSchema, null);\n            case INT64:\n                if (Timestamp.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof String) {\n                        SchemaAndValue parsed = parseString(value.toString());\n                        value = parsed.value();\n                    }\n                    if (value instanceof java.util.Date) {\n                        java.util.Date date = (java.util.Date) value;\n                        if (fromSchema != null) {\n                            String fromSchemaName = fromSchema.name();\n                            if (Date.LOGICAL_NAME.equals(fromSchemaName)) {\n                                int days = Date.fromLogical(fromSchema, date);\n                                long millis = days * MILLIS_PER_DAY;\n                                return Timestamp.toLogical(toSchema, millis);\n                            }\n                            if (Time.LOGICAL_NAME.equals(fromSchemaName)) {\n                                long millis = Time.fromLogical(fromSchema, date);\n                                return Timestamp.toLogical(toSchema, millis);\n                            }\n                            if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {\n                                return value;\n                            }\n                        } else {\n                            // There is no fromSchema, so no conversion is needed\n                            return value;\n                        }\n                    }\n                    long numeric = asLong(value, fromSchema, null);\n                    return Timestamp.toLogical(toSchema, numeric);\n                }\n                if (value instanceof Long) {\n                    return value;\n                }\n                return asLong(value, fromSchema, null);\n            case FLOAT32:\n                if (value instanceof Float) {\n                    return value;\n                }\n                return (float) asDouble(value, fromSchema, null);\n            case FLOAT64:\n                if (value instanceof Double) {\n                    return value;\n                }\n                return asDouble(value, fromSchema, null);\n            case ARRAY:\n                if (value instanceof String) {\n                    SchemaAndValue schemaAndValue = parseString(value.toString());\n                    value = schemaAndValue.value();\n                }\n                if (value instanceof List) {\n                    return value;\n                }\n                break;\n            case MAP:\n                if (value instanceof String) {\n                    SchemaAndValue schemaAndValue = parseString(value.toString());\n                    value = schemaAndValue.value();\n                }\n                if (value instanceof Map) {\n                    return value;\n                }\n                break;\n            case STRUCT:\n                if (value instanceof Struct) {\n                    return value;\n                }\n        }\n        throw new DataException(\"Unable to convert \" + value + \" (\" + value.getClass() + \") to \" + toSchema);\n    }\n\n    /**\n     * Convert the specified value to the desired scalar value type.\n     *\n     * @param value      the value to be converted; may not be null\n     * @param fromSchema the schema for the current value type; may not be null\n     * @param error      any previous error that should be included in an exception message; may be null\n     * @return the long value after conversion; never null\n     * @throws DataException if the value could not be converted to a long\n     */\n    protected static long asLong(Object value, Schema fromSchema, Throwable error) {\n        try {\n            if (value instanceof Number) {\n                Number number = (Number) value;\n                return number.longValue();\n            }\n            if (value instanceof String) {\n                return new BigDecimal(value.toString()).longValue();\n            }\n        } catch (NumberFormatException e) {\n            error = e;\n            // fall through\n        }\n        if (fromSchema != null) {\n            String schemaName = fromSchema.name();\n            if (value instanceof java.util.Date) {\n                if (Date.LOGICAL_NAME.equals(schemaName)) {\n                    return Date.fromLogical(fromSchema, (java.util.Date) value);\n                }\n                if (Time.LOGICAL_NAME.equals(schemaName)) {\n                    return Time.fromLogical(fromSchema, (java.util.Date) value);\n                }\n                if (Timestamp.LOGICAL_NAME.equals(schemaName)) {\n                    return Timestamp.fromLogical(fromSchema, (java.util.Date) value);\n                }\n            }\n            throw new DataException(\"Unable to convert \" + value + \" (\" + value.getClass() + \") to \" + fromSchema, error);\n        }\n        throw new DataException(\"Unable to convert \" + value + \" (\" + value.getClass() + \") to a number\", error);\n    }\n\n    /**\n     * Convert the specified value with the desired floating point type.\n     *\n     * @param value  the value to be converted; may not be null\n     * @param schema the schema for the current value type; may not be null\n     * @param error  any previous error that should be included in an exception message; may be null\n     * @return the double value after conversion; never null\n     * @throws DataException if the value could not be converted to a double\n     */\n    protected static double asDouble(Object value, Schema schema, Throwable error) {\n        try {\n            if (value instanceof Number) {\n                Number number = (Number) value;\n                return number.doubleValue();\n            }\n            if (value instanceof String) {\n                return new BigDecimal(value.toString()).doubleValue();\n            }\n        } catch (NumberFormatException e) {\n            error = e;\n            // fall through\n        }\n        return asLong(value, schema, error);\n    }\n\n    protected static void append(StringBuilder sb, Object value, boolean embedded) {\n        if (value == null) {\n            sb.append(NULL_VALUE);\n        } else if (value instanceof Number) {\n            sb.append(value);\n        } else if (value instanceof Boolean) {\n            sb.append(value);\n        } else if (value instanceof String) {\n            if (embedded) {\n                String escaped = escape((String) value);\n                sb.append('\"').append(escaped).append('\"');\n            } else {\n                sb.append(value);\n            }\n        } else if (value instanceof byte[]) {\n            value = Base64.getEncoder().encodeToString((byte[]) value);\n            if (embedded) {\n                sb.append('\"').append(value).append('\"');\n            } else {\n                sb.append(value);\n            }\n        } else if (value instanceof ByteBuffer) {\n            byte[] bytes = Utils.readBytes((ByteBuffer) value);\n            append(sb, bytes, embedded);\n        } else if (value instanceof List) {\n            List<?> list = (List<?>) value;\n            sb.append('[');\n            appendIterable(sb, list.iterator());\n            sb.append(']');\n        } else if (value instanceof Map) {\n            Map<?, ?> map = (Map<?, ?>) value;\n            sb.append('{');\n            appendIterable(sb, map.entrySet().iterator());\n            sb.append('}');\n        } else if (value instanceof Struct) {\n            Struct struct = (Struct) value;\n            Schema schema = struct.schema();\n            boolean first = true;\n            sb.append('{');\n            for (Field field : schema.fields()) {\n                if (first) {\n                    first = false;\n                } else {\n                    sb.append(',');\n                }\n                append(sb, field.name(), true);\n                sb.append(':');\n                append(sb, struct.get(field), true);\n            }\n            sb.append('}');\n        } else if (value instanceof Map.Entry) {\n            Map.Entry<?, ?> entry = (Map.Entry<?, ?>) value;\n            append(sb, entry.getKey(), true);\n            sb.append(':');\n            append(sb, entry.getValue(), true);\n        } else if (value instanceof java.util.Date) {\n            java.util.Date dateValue = (java.util.Date) value;\n            String formatted = dateFormatFor(dateValue).format(dateValue);\n            sb.append(formatted);\n        } else {\n            throw new DataException(\"Failed to serialize unexpected value type \" + value.getClass().getName() + \": \" + value);\n        }\n    }\n\n    protected static void appendIterable(StringBuilder sb, Iterator<?> iter) {\n        if (iter.hasNext()) {\n            append(sb, iter.next(), true);\n            while (iter.hasNext()) {\n                sb.append(',');\n                append(sb, iter.next(), true);\n            }\n        }\n    }\n\n    protected static String escape(String value) {\n        String replace1 = TWO_BACKSLASHES.matcher(value).replaceAll(\"\\\\\\\\\\\\\\\\\");\n        return DOUBLE_QUOTE.matcher(replace1).replaceAll(\"\\\\\\\\\\\"\");\n    }\n\n    public static DateFormat dateFormatFor(java.util.Date value) {\n        if (value.getTime() < MILLIS_PER_DAY) {\n            return new SimpleDateFormat(ISO_8601_TIME_FORMAT_PATTERN);\n        }\n        if (value.getTime() % MILLIS_PER_DAY == 0) {\n            return new SimpleDateFormat(ISO_8601_DATE_FORMAT_PATTERN);\n        }\n        return new SimpleDateFormat(ISO_8601_TIMESTAMP_FORMAT_PATTERN);\n    }\n\n    protected static boolean canParseSingleTokenLiteral(Parser parser, boolean embedded, String tokenLiteral) {\n        int startPosition = parser.mark();\n        // If the next token is what we expect, then either...\n        if (parser.canConsume(tokenLiteral)) {\n            //   ...we're reading an embedded value, in which case the next token will be handled appropriately\n            //      by the caller if it's something like an end delimiter for a map or array, or a comma to\n            //      separate multiple embedded values...\n            //   ...or it's being parsed as part of a top-level string, in which case, any other tokens should\n            //      cause use to stop parsing this single-token literal as such and instead just treat it like\n            //      a string. For example, the top-level string \"true}\" will be tokenized as the tokens \"true\" and\n            //      \"}\", but should ultimately be parsed as just the string \"true}\" instead of the boolean true.\n            if (embedded || !parser.hasNext()) {\n                return true;\n            }\n        }\n        parser.rewindTo(startPosition);\n        return false;\n    }\n\n    protected static SchemaAndValue parse(Parser parser, boolean embedded) throws NoSuchElementException {\n        if (!parser.hasNext()) {\n            return null;\n        }\n        if (embedded) {\n            if (parser.canConsume(QUOTE_DELIMITER)) {\n                StringBuilder sb = new StringBuilder();\n                while (parser.hasNext()) {\n                    if (parser.canConsume(QUOTE_DELIMITER)) {\n                        break;\n                    }\n                    sb.append(parser.next());\n                }\n                String content = sb.toString();\n                // We can parse string literals as temporal logical types, but all others\n                // are treated as strings\n                SchemaAndValue parsed = parseString(content);\n                if (parsed != null && TEMPORAL_LOGICAL_TYPE_NAMES.contains(parsed.schema().name())) {\n                    return parsed;\n                }\n                return new SchemaAndValue(Schema.STRING_SCHEMA, content);\n            }\n        }\n\n        if (canParseSingleTokenLiteral(parser, embedded, NULL_VALUE)) {\n            return null;\n        }\n        if (canParseSingleTokenLiteral(parser, embedded, TRUE_LITERAL)) {\n            return TRUE_SCHEMA_AND_VALUE;\n        }\n        if (canParseSingleTokenLiteral(parser, embedded, FALSE_LITERAL)) {\n            return FALSE_SCHEMA_AND_VALUE;\n        }\n\n        int startPosition = parser.mark();\n\n        try {\n            if (parser.canConsume(ARRAY_BEGIN_DELIMITER)) {\n                List<Object> result = new ArrayList<>();\n                boolean compatible = true;\n                Schema elementSchema = null;\n                while (parser.hasNext()) {\n                    if (parser.canConsume(ARRAY_END_DELIMITER)) {\n                        Schema listSchema;\n                        if (elementSchema != null && compatible) {\n                            listSchema = SchemaBuilder.array(elementSchema).schema();\n                            result = alignListEntriesWithSchema(listSchema, result);\n                        } else {\n                            // Every value is null\n                            listSchema = SchemaBuilder.arrayOfNull().build();\n                        }\n                        return new SchemaAndValue(listSchema, result);\n                    }\n\n                    if (parser.canConsume(COMMA_DELIMITER)) {\n                        throw new DataException(\"Unable to parse an empty array element: \" + parser.original());\n                    }\n                    SchemaAndValue element = parse(parser, true);\n                    elementSchema = commonSchemaFor(elementSchema, element);\n                    if (elementSchema == null && element != null && element.schema() != null) {\n                        compatible = false;\n                    }\n                    result.add(element != null ? element.value() : null);\n\n                    int currentPosition = parser.mark();\n                    if (parser.canConsume(ARRAY_END_DELIMITER)) {\n                        parser.rewindTo(currentPosition);\n                    } else if (!parser.canConsume(COMMA_DELIMITER)) {\n                        throw new DataException(\"Array elements missing '\" + COMMA_DELIMITER + \"' delimiter\");\n                    }\n                }\n\n                // Missing either a comma or an end delimiter\n                if (COMMA_DELIMITER.equals(parser.previous())) {\n                    throw new DataException(\"Array is missing element after ',': \" + parser.original());\n                }\n                throw new DataException(\"Array is missing terminating ']': \" + parser.original());\n            }\n\n            if (parser.canConsume(MAP_BEGIN_DELIMITER)) {\n                Map<Object, Object> result = new LinkedHashMap<>();\n                boolean keyCompatible = true;\n                Schema keySchema = null;\n                boolean valueCompatible = true;\n                Schema valueSchema = null;\n                while (parser.hasNext()) {\n                    if (parser.canConsume(MAP_END_DELIMITER)) {\n                        Schema mapSchema;\n                        if (keySchema != null && valueSchema != null && keyCompatible && valueCompatible) {\n                            mapSchema = SchemaBuilder.map(keySchema, valueSchema).build();\n                            result = alignMapKeysAndValuesWithSchema(mapSchema, result);\n                        } else if (keySchema != null && keyCompatible) {\n                            mapSchema = SchemaBuilder.mapWithNullValues(keySchema);\n                            result = alignMapKeysWithSchema(mapSchema, result);\n                        } else {\n                            mapSchema = SchemaBuilder.mapOfNull().build();\n                        }\n                        return new SchemaAndValue(mapSchema, result);\n                    }\n\n                    if (parser.canConsume(COMMA_DELIMITER)) {\n                        throw new DataException(\"Unable to parse a map entry with no key or value: \" + parser.original());\n                    }\n                    SchemaAndValue key = parse(parser, true);\n                    if (key == null || key.value() == null) {\n                        throw new DataException(\"Map entry may not have a null key: \" + parser.original());\n                    }\n\n                    if (!parser.canConsume(ENTRY_DELIMITER)) {\n                        throw new DataException(\"Map entry is missing '\" + ENTRY_DELIMITER\n                                                + \"' at \" + parser.position()\n                                                + \" in \" + parser.original());\n                    }\n                    SchemaAndValue value = parse(parser, true);\n                    Object entryValue = value != null ? value.value() : null;\n                    result.put(key.value(), entryValue);\n\n                    parser.canConsume(COMMA_DELIMITER);\n                    keySchema = commonSchemaFor(keySchema, key);\n                    if (keySchema == null && key.schema() != null) {\n                        keyCompatible = false;\n                    }\n                    valueSchema = commonSchemaFor(valueSchema, value);\n                    if (valueSchema == null && value != null && value.schema() != null) {\n                        valueCompatible = false;\n                    }\n                }\n                // Missing either a comma or an end delimiter\n                if (COMMA_DELIMITER.equals(parser.previous())) {\n                    throw new DataException(\"Map is missing element after ',': \" + parser.original());\n                }\n                throw new DataException(\"Map is missing terminating '}': \" + parser.original());\n            }\n        } catch (DataException e) {\n            LOG.trace(\"Unable to parse the value as a map or an array; reverting to string\", e);\n            parser.rewindTo(startPosition);\n        }\n\n        String token = parser.next();\n        if (Utils.isBlank(token)) {\n            return new SchemaAndValue(Schema.STRING_SCHEMA, token);\n        }\n        token = token.trim();\n\n        char firstChar = token.charAt(0);\n        boolean firstCharIsDigit = Character.isDigit(firstChar);\n\n        // Temporal types are more restrictive, so try them first\n        if (firstCharIsDigit) {\n            // The time and timestamp literals may be split into 5 tokens since an unescaped colon\n            // is a delimiter. Check these first since the first of these tokens is a simple numeric\n            int position = parser.mark();\n            String remainder = parser.next(4);\n            if (remainder != null) {\n                String timeOrTimestampStr = token + remainder;\n                SchemaAndValue temporal = parseAsTemporal(timeOrTimestampStr);\n                if (temporal != null) {\n                    return temporal;\n                }\n            }\n            // No match was found using the 5 tokens, so rewind and see if the current token has a date, time, or timestamp\n            parser.rewindTo(position);\n            SchemaAndValue temporal = parseAsTemporal(token);\n            if (temporal != null) {\n                return temporal;\n            }\n        }\n        if (firstCharIsDigit || firstChar == '+' || firstChar == '-') {\n            try {\n                // Try to parse as a number ...\n                BigDecimal decimal = new BigDecimal(token);\n                try {\n                    return new SchemaAndValue(Schema.INT8_SCHEMA, decimal.byteValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                try {\n                    return new SchemaAndValue(Schema.INT16_SCHEMA, decimal.shortValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                try {\n                    return new SchemaAndValue(Schema.INT32_SCHEMA, decimal.intValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                try {\n                    return new SchemaAndValue(Schema.INT64_SCHEMA, decimal.longValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                float fValue = decimal.floatValue();\n                if (fValue != Float.NEGATIVE_INFINITY && fValue != Float.POSITIVE_INFINITY\n                    && decimal.scale() != 0) {\n                    return new SchemaAndValue(Schema.FLOAT32_SCHEMA, fValue);\n                }\n                double dValue = decimal.doubleValue();\n                if (dValue != Double.NEGATIVE_INFINITY && dValue != Double.POSITIVE_INFINITY\n                    && decimal.scale() != 0) {\n                    return new SchemaAndValue(Schema.FLOAT64_SCHEMA, dValue);\n                }\n                Schema schema = Decimal.schema(decimal.scale());\n                return new SchemaAndValue(schema, decimal);\n            } catch (NumberFormatException e) {\n                // can't parse as a number\n            }\n        }\n        if (embedded) {\n            throw new DataException(\"Failed to parse embedded value\");\n        }\n        // At this point, the only thing this non-embedded value can be is a string.\n        return new SchemaAndValue(Schema.STRING_SCHEMA, parser.original());\n    }\n\n    private static SchemaAndValue parseAsTemporal(String token) {\n        if (token == null) {\n            return null;\n        }\n        // If the colons were escaped, we'll see the escape chars and need to remove them\n        token = token.replace(\"\\\\:\", \":\");\n        int tokenLength = token.length();\n        if (tokenLength == ISO_8601_TIME_LENGTH) {\n            try {\n                return new SchemaAndValue(Time.SCHEMA, new SimpleDateFormat(ISO_8601_TIME_FORMAT_PATTERN).parse(token));\n            } catch (ParseException e) {\n              // not a valid date\n            }\n        } else if (tokenLength == ISO_8601_TIMESTAMP_LENGTH) {\n            try {\n                return new SchemaAndValue(Timestamp.SCHEMA, new SimpleDateFormat(ISO_8601_TIMESTAMP_FORMAT_PATTERN).parse(token));\n            } catch (ParseException e) {\n              // not a valid date\n            }\n        } else if (tokenLength == ISO_8601_DATE_LENGTH) {\n            try {\n                return new SchemaAndValue(Date.SCHEMA, new SimpleDateFormat(ISO_8601_DATE_FORMAT_PATTERN).parse(token));\n            } catch (ParseException e) {\n                // not a valid date\n            }\n        }\n        return null;\n    }\n\n    protected static Schema commonSchemaFor(Schema previous, SchemaAndValue latest) {\n        if (latest == null) {\n            return previous;\n        }\n        if (previous == null) {\n            return latest.schema();\n        }\n        Schema newSchema = latest.schema();\n        Type previousType = previous.type();\n        Type newType = newSchema.type();\n        if (previousType != newType) {\n            switch (previous.type()) {\n                case INT8:\n                    if (newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64 || newType == Type.FLOAT32 || newType ==\n                                                                                                                              Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case INT16:\n                    if (newType == Type.INT8) {\n                        return previous;\n                    }\n                    if (newType == Type.INT32 || newType == Type.INT64 || newType == Type.FLOAT32 || newType == Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case INT32:\n                    if (newType == Type.INT8 || newType == Type.INT16) {\n                        return previous;\n                    }\n                    if (newType == Type.INT64 || newType == Type.FLOAT32 || newType == Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case INT64:\n                    if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32) {\n                        return previous;\n                    }\n                    if (newType == Type.FLOAT32 || newType == Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case FLOAT32:\n                    if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64) {\n                        return previous;\n                    }\n                    if (newType == Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case FLOAT64:\n                    if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64 || newType ==\n                                                                                                                           Type.FLOAT32) {\n                        return previous;\n                    }\n                    break;\n            }\n            return null;\n        }\n        if (previous.isOptional() == newSchema.isOptional()) {\n            // Use the optional one\n            return previous.isOptional() ? previous : newSchema;\n        }\n        if (!previous.equals(newSchema)) {\n            return null;\n        }\n        return previous;\n    }\n\n    protected static List<Object> alignListEntriesWithSchema(Schema schema, List<Object> input) {\n        Schema valueSchema = schema.valueSchema();\n        List<Object> result = new ArrayList<>();\n        for (Object value : input) {\n            Object newValue = convertTo(valueSchema, null, value);\n            result.add(newValue);\n        }\n        return result;\n    }\n\n    protected static Map<Object, Object> alignMapKeysAndValuesWithSchema(Schema mapSchema, Map<Object, Object> input) {\n        Schema keySchema = mapSchema.keySchema();\n        Schema valueSchema = mapSchema.valueSchema();\n        Map<Object, Object> result = new LinkedHashMap<>();\n        for (Map.Entry<?, ?> entry : input.entrySet()) {\n            Object newKey = convertTo(keySchema, null, entry.getKey());\n            Object newValue = convertTo(valueSchema, null, entry.getValue());\n            result.put(newKey, newValue);\n        }\n        return result;\n    }\n\n    protected static Map<Object, Object> alignMapKeysWithSchema(Schema mapSchema, Map<Object, Object> input) {\n        Schema keySchema = mapSchema.keySchema();\n        Map<Object, Object> result = new LinkedHashMap<>();\n        for (Map.Entry<?, ?> entry : input.entrySet()) {\n            Object newKey = convertTo(keySchema, null, entry.getKey());\n            result.put(newKey, entry.getValue());\n        }\n        return result;\n    }\n\n    protected static class SchemaDetector {\n        private Type knownType = null;\n        private boolean optional = false;\n\n        public SchemaDetector() {\n        }\n\n        public boolean canDetect(Object value) {\n            if (value == null) {\n                optional = true;\n                return true;\n            }\n            Schema schema = inferSchema(value);\n            if (schema == null) {\n                return false;\n            }\n            if (knownType == null) {\n                knownType = schema.type();\n            } else if (knownType != schema.type()) {\n                return false;\n            }\n            return true;\n        }\n\n        public Schema schema() {\n            SchemaBuilder builder = SchemaBuilder.type(knownType);\n            if (optional) {\n                builder.optional();\n            }\n            return builder.schema();\n        }\n    }\n\n    protected static class Parser {\n        private final String original;\n        private final CharacterIterator iter;\n        private String nextToken = null;\n        private String previousToken = null;\n\n        public Parser(String original) {\n            this.original = original;\n            this.iter = new StringCharacterIterator(this.original);\n        }\n\n        public int position() {\n            return iter.getIndex();\n        }\n\n        public int mark() {\n            return iter.getIndex() - (nextToken != null ? nextToken.length() : 0);\n        }\n\n        public void rewindTo(int position) {\n            iter.setIndex(position);\n            nextToken = null;\n            previousToken = null;\n        }\n\n        public String original() {\n            return original;\n        }\n\n        public boolean hasNext() {\n            return nextToken != null || canConsumeNextToken();\n        }\n\n        protected boolean canConsumeNextToken() {\n            return iter.getEndIndex() > iter.getIndex();\n        }\n\n        public String next() {\n            if (nextToken != null) {\n                previousToken = nextToken;\n                nextToken = null;\n            } else {\n                previousToken = consumeNextToken();\n            }\n            return previousToken;\n        }\n\n        public String next(int n) {\n            int current = mark();\n            int start = mark();\n            for (int i = 0; i != n; ++i) {\n                if (!hasNext()) {\n                    rewindTo(start);\n                    return null;\n                }\n                next();\n            }\n            return original.substring(current, position());\n        }\n\n        private String consumeNextToken() throws NoSuchElementException {\n            boolean escaped = false;\n            int start = iter.getIndex();\n            char c = iter.current();\n            while (canConsumeNextToken()) {\n                switch (c) {\n                    case '\\\\':\n                        escaped = !escaped;\n                        break;\n                    case ':':\n                    case ',':\n                    case '{':\n                    case '}':\n                    case '[':\n                    case ']':\n                    case '\\\"':\n                        if (!escaped) {\n                            if (start < iter.getIndex()) {\n                                // Return the previous token\n                                return original.substring(start, iter.getIndex());\n                            }\n                            // Consume and return this delimiter as a token\n                            iter.next();\n                            return original.substring(start, start + 1);\n                        }\n                        // escaped, so continue\n                        escaped = false;\n                        break;\n                    default:\n                        // If escaped, then we don't care what was escaped\n                        escaped = false;\n                        break;\n                }\n                c = iter.next();\n            }\n            return original.substring(start, iter.getIndex());\n        }\n\n        public String previous() {\n            return previousToken;\n        }\n\n        public boolean canConsume(String expected) {\n            return canConsume(expected, true);\n        }\n\n        public boolean canConsume(String expected, boolean ignoreLeadingAndTrailingWhitespace) {\n            if (isNext(expected, ignoreLeadingAndTrailingWhitespace)) {\n                // consume this token ...\n                nextToken = null;\n                return true;\n            }\n            return false;\n        }\n\n        protected boolean isNext(String expected, boolean ignoreLeadingAndTrailingWhitespace) {\n            if (nextToken == null) {\n                if (!hasNext()) {\n                    return false;\n                }\n                // There's another token, so consume it\n                nextToken = consumeNextToken();\n            }\n            if (ignoreLeadingAndTrailingWhitespace) {\n                while (Utils.isBlank(nextToken) && canConsumeNextToken()) {\n                    nextToken = consumeNextToken();\n                }\n            }\n            return ignoreLeadingAndTrailingWhitespace\n                ? nextToken.trim().equals(expected)\n                : nextToken.equals(expected);\n        }\n    }\n}"
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [],
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "11ad5e8bca2444d8c51cd6b81ee84dcb41a047c2",
        "url": "https://github.com/apache/kafka/commit/11ad5e8bca2444d8c51cd6b81ee84dcb41a047c2",
        "move_method_refactoring": {
            "type": "Move Method",
            "description": "Move Method protected parse(parser Parser, embedded boolean) : SchemaAndValue from class org.apache.kafka.connect.data.Values to public parse(embedded boolean) : SchemaAndValue from class org.apache.kafka.connect.data.Values.ValueParser",
            "leftSideLocations": [
                {
                    "filePath": "connect/api/src/main/java/org/apache/kafka/connect/data/Values.java",
                    "startLine": 766,
                    "endLine": 978,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "original method declaration",
                    "codeElement": "protected parse(parser Parser, embedded boolean) : SchemaAndValue"
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/api/src/main/java/org/apache/kafka/connect/data/Values.java",
                    "startLine": 845,
                    "endLine": 877,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "moved method declaration",
                    "codeElement": "public parse(embedded boolean) : SchemaAndValue"
                }
            ],
            "isStatic": true
        },
        "telemetry": {
            "id": "570002a4-e46d-477e-a0d3-fd0a9061a450",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1230,
                "lineStart": 47,
                "lineEnd": 1276,
                "bodyLineStart": 47,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/connect/api/src/main/java/org/apache/kafka/connect/data/Values.java",
                "sourceCode": "/**\n * Utility for converting from one Connect value to a different form. This is useful when the caller expects a value of a particular type\n * but is uncertain whether the actual value is one that isn't directly that type but can be converted into that type.\n *\n * <p>For example, a caller might expect a particular {@link org.apache.kafka.connect.header.Header} to contain a {@link Type#INT64}\n * value, when in fact that header contains a string representation of a 32-bit integer. Here, the caller can use the methods in this\n * class to convert the value to the desired type:\n * <pre>\n *     Header header = ...\n *     long value = Values.convertToLong(header.schema(), header.value());\n * </pre>\n *\n * <p>This class is able to convert any value to a string representation as well as parse those string representations back into most of\n * the types. The only exception is {@link Struct} values that require a schema and thus cannot be parsed from a simple string.\n */\npublic class Values {\n\n    private static final Logger LOG = LoggerFactory.getLogger(Values.class);\n\n    private static final TimeZone UTC = TimeZone.getTimeZone(\"UTC\");\n    private static final SchemaAndValue NULL_SCHEMA_AND_VALUE = new SchemaAndValue(null, null);\n    private static final SchemaAndValue TRUE_SCHEMA_AND_VALUE = new SchemaAndValue(Schema.BOOLEAN_SCHEMA, Boolean.TRUE);\n    private static final SchemaAndValue FALSE_SCHEMA_AND_VALUE = new SchemaAndValue(Schema.BOOLEAN_SCHEMA, Boolean.FALSE);\n    private static final Schema ARRAY_SELECTOR_SCHEMA = SchemaBuilder.array(Schema.STRING_SCHEMA).build();\n    private static final Schema MAP_SELECTOR_SCHEMA = SchemaBuilder.map(Schema.STRING_SCHEMA, Schema.STRING_SCHEMA).build();\n    private static final Schema STRUCT_SELECTOR_SCHEMA = SchemaBuilder.struct().build();\n    private static final String TRUE_LITERAL = Boolean.TRUE.toString();\n    private static final String FALSE_LITERAL = Boolean.FALSE.toString();\n    private static final long MILLIS_PER_DAY = 24 * 60 * 60 * 1000;\n    private static final String NULL_VALUE = \"null\";\n    static final String ISO_8601_DATE_FORMAT_PATTERN = \"yyyy-MM-dd\";\n    static final String ISO_8601_TIME_FORMAT_PATTERN = \"HH:mm:ss.SSS'Z'\";\n    static final String ISO_8601_TIMESTAMP_FORMAT_PATTERN = ISO_8601_DATE_FORMAT_PATTERN + \"'T'\" + ISO_8601_TIME_FORMAT_PATTERN;\n    private static final Set<String> TEMPORAL_LOGICAL_TYPE_NAMES =\n            Collections.unmodifiableSet(\n                    new HashSet<>(\n                            Arrays.asList(Time.LOGICAL_NAME,\n                            Timestamp.LOGICAL_NAME,\n                            Date.LOGICAL_NAME\n                            )\n                    )\n            );\n\n    private static final String QUOTE_DELIMITER = \"\\\"\";\n    private static final String COMMA_DELIMITER = \",\";\n    private static final String ENTRY_DELIMITER = \":\";\n    private static final String ARRAY_BEGIN_DELIMITER = \"[\";\n    private static final String ARRAY_END_DELIMITER = \"]\";\n    private static final String MAP_BEGIN_DELIMITER = \"{\";\n    private static final String MAP_END_DELIMITER = \"}\";\n    private static final int ISO_8601_DATE_LENGTH = ISO_8601_DATE_FORMAT_PATTERN.length();\n    private static final int ISO_8601_TIME_LENGTH = ISO_8601_TIME_FORMAT_PATTERN.length() - 2; // subtract single quotes\n    private static final int ISO_8601_TIMESTAMP_LENGTH = ISO_8601_TIMESTAMP_FORMAT_PATTERN.length() - 4; // subtract single quotes\n\n    private static final Pattern TWO_BACKSLASHES = Pattern.compile(\"\\\\\\\\\");\n\n    private static final Pattern DOUBLE_QUOTE = Pattern.compile(\"\\\"\");\n\n    /**\n     * Convert the specified value to a {@link Type#BOOLEAN} value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a boolean.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a boolean, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a boolean\n     */\n    public static Boolean convertToBoolean(Schema schema, Object value) throws DataException {\n        return (Boolean) convertTo(Schema.OPTIONAL_BOOLEAN_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT8} byte value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a byte.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a byte, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a byte\n     */\n    public static Byte convertToByte(Schema schema, Object value) throws DataException {\n        return (Byte) convertTo(Schema.OPTIONAL_INT8_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT16} short value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a short.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a short, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a short\n     */\n    public static Short convertToShort(Schema schema, Object value) throws DataException {\n        return (Short) convertTo(Schema.OPTIONAL_INT16_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT32} int value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to an integer.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as an integer, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to an integer\n     */\n    public static Integer convertToInteger(Schema schema, Object value) throws DataException {\n        return (Integer) convertTo(Schema.OPTIONAL_INT32_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#INT64} long value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a long.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a long, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a long\n     */\n    public static Long convertToLong(Schema schema, Object value) throws DataException {\n        return (Long) convertTo(Schema.OPTIONAL_INT64_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#FLOAT32} float value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a floating point number.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a float, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a float\n     */\n    public static Float convertToFloat(Schema schema, Object value) throws DataException {\n        return (Float) convertTo(Schema.OPTIONAL_FLOAT32_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#FLOAT64} double value. The supplied schema is required if the value is a logical\n     * type when the schema contains critical information that might be necessary for converting to a floating point number.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a double, or null if the supplied value was null\n     * @throws DataException if the value could not be converted to a double\n     */\n    public static Double convertToDouble(Schema schema, Object value) throws DataException {\n        return (Double) convertTo(Schema.OPTIONAL_FLOAT64_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#STRING} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a string, or null if the supplied value was null\n     */\n    public static String convertToString(Schema schema, Object value) {\n        return (String) convertTo(Schema.OPTIONAL_STRING_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to an {@link Type#ARRAY} value. If the value is a string representation of an array, this method\n     * will parse the string and its elements to infer the schemas for those elements. Thus, this method supports\n     * arrays of other primitives and structured types. If the value is already an array (or list), this method simply casts and\n     * returns it.\n     *\n     * <p>This method currently does not use the schema, though it may be used in the future.</p>\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a list, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a list value\n     */\n    public static List<?> convertToList(Schema schema, Object value) {\n        return (List<?>) convertTo(ARRAY_SELECTOR_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#MAP} value. If the value is a string representation of a map, this method\n     * will parse the string and its entries to infer the schemas for those entries. Thus, this method supports\n     * maps with primitives and structured keys and values. If the value is already a map, this method simply casts and returns it.\n     *\n     * <p>This method currently does not use the schema, though it may be used in the future.</p>\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a map, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a map value\n     */\n    public static Map<?, ?> convertToMap(Schema schema, Object value) {\n        return (Map<?, ?>) convertTo(MAP_SELECTOR_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Type#STRUCT} value. Structs cannot be converted from other types, so this method returns\n     * a struct only if the supplied value is a struct. If not a struct, this method throws an exception.\n     *\n     * <p>This method currently does not use the schema, though it may be used in the future.</p>\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a struct, or null if the supplied value was null\n     * @throws DataException if the value is not a struct\n     */\n    public static Struct convertToStruct(Schema schema, Object value) {\n        return (Struct) convertTo(STRUCT_SELECTOR_SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Time#SCHEMA time} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a time, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a time value\n     */\n    public static java.util.Date convertToTime(Schema schema, Object value) {\n        return (java.util.Date) convertTo(Time.SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Date#SCHEMA date} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a date, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a date value\n     */\n    public static java.util.Date convertToDate(Schema schema, Object value) {\n        return (java.util.Date) convertTo(Date.SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Timestamp#SCHEMA timestamp} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a timestamp, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a timestamp value\n     */\n    public static java.util.Date convertToTimestamp(Schema schema, Object value) {\n        return (java.util.Date) convertTo(Timestamp.SCHEMA, schema, value);\n    }\n\n    /**\n     * Convert the specified value to a {@link Decimal decimal} value.\n     * Not supplying a schema may limit the ability to convert to the desired type.\n     *\n     * @param schema the schema for the value; may be null\n     * @param value  the value to be converted; may be null\n     * @return the representation as a decimal, or null if the supplied value was null\n     * @throws DataException if the value cannot be converted to a decimal value\n     */\n    public static BigDecimal convertToDecimal(Schema schema, Object value, int scale) {\n        return (BigDecimal) convertTo(Decimal.schema(scale), schema, value);\n    }\n\n    /**\n     * If possible infer a schema for the given value.\n     *\n     * @param value the value whose schema is to be inferred; may be null\n     * @return the inferred schema, or null if the value is null or no schema could be inferred\n     */\n    public static Schema inferSchema(Object value) {\n        if (value instanceof String) {\n            return Schema.STRING_SCHEMA;\n        }\n        if (value instanceof Boolean) {\n            return Schema.BOOLEAN_SCHEMA;\n        }\n        if (value instanceof Byte) {\n            return Schema.INT8_SCHEMA;\n        }\n        if (value instanceof Short) {\n            return Schema.INT16_SCHEMA;\n        }\n        if (value instanceof Integer) {\n            return Schema.INT32_SCHEMA;\n        }\n        if (value instanceof Long) {\n            return Schema.INT64_SCHEMA;\n        }\n        if (value instanceof Float) {\n            return Schema.FLOAT32_SCHEMA;\n        }\n        if (value instanceof Double) {\n            return Schema.FLOAT64_SCHEMA;\n        }\n        if (value instanceof byte[] || value instanceof ByteBuffer) {\n            return Schema.BYTES_SCHEMA;\n        }\n        if (value instanceof List) {\n            List<?> list = (List<?>) value;\n            if (list.isEmpty()) {\n                return null;\n            }\n            SchemaDetector detector = new SchemaDetector();\n            for (Object element : list) {\n                if (!detector.canDetect(element)) {\n                    return null;\n                }\n            }\n            return SchemaBuilder.array(detector.schema()).build();\n        }\n        if (value instanceof Map) {\n            Map<?, ?> map = (Map<?, ?>) value;\n            if (map.isEmpty()) {\n                return null;\n            }\n            SchemaDetector keyDetector = new SchemaDetector();\n            SchemaDetector valueDetector = new SchemaDetector();\n            for (Map.Entry<?, ?> entry : map.entrySet()) {\n                if (!keyDetector.canDetect(entry.getKey()) || !valueDetector.canDetect(entry.getValue())) {\n                    return null;\n                }\n            }\n            return SchemaBuilder.map(keyDetector.schema(), valueDetector.schema()).build();\n        }\n        if (value instanceof Struct) {\n            return ((Struct) value).schema();\n        }\n        return null;\n    }\n\n\n    /**\n     * Parse the specified string representation of a value into its schema and value.\n     *\n     * @param value the string form of the value\n     * @return the schema and value; never null, but whose schema and value may be null\n     * @see #convertToString\n     */\n    public static SchemaAndValue parseString(String value) {\n        if (value == null) {\n            return NULL_SCHEMA_AND_VALUE;\n        }\n        if (value.isEmpty()) {\n            return new SchemaAndValue(Schema.STRING_SCHEMA, value);\n        }\n        Parser parser = new Parser(value);\n        return parse(parser, false);\n    }\n\n    /**\n     * Convert the value to the desired type.\n     *\n     * @param toSchema   the schema for the desired type; may not be null\n     * @param fromSchema the schema for the supplied value; may be null if not known\n     * @return the converted value; never null\n     * @throws DataException if the value could not be converted to the desired type\n     */\n    protected static Object convertTo(Schema toSchema, Schema fromSchema, Object value) throws DataException {\n        if (value == null) {\n            if (toSchema.isOptional()) {\n                return null;\n            }\n            throw new DataException(\"Unable to convert a null value to a schema that requires a value\");\n        }\n        switch (toSchema.type()) {\n            case BYTES:\n                if (Decimal.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof ByteBuffer) {\n                        value = Utils.toArray((ByteBuffer) value);\n                    }\n                    if (value instanceof byte[]) {\n                        return Decimal.toLogical(toSchema, (byte[]) value);\n                    }\n                    if (value instanceof BigDecimal) {\n                        return value;\n                    }\n                    if (value instanceof Number) {\n                        // Not already a decimal, so treat it as a double ...\n                        double converted = ((Number) value).doubleValue();\n                        return BigDecimal.valueOf(converted);\n                    }\n                    if (value instanceof String) {\n                        return new BigDecimal(value.toString());\n                    }\n                }\n                if (value instanceof ByteBuffer) {\n                    return Utils.toArray((ByteBuffer) value);\n                }\n                if (value instanceof byte[]) {\n                    return value;\n                }\n                if (value instanceof BigDecimal) {\n                    return Decimal.fromLogical(toSchema, (BigDecimal) value);\n                }\n                break;\n            case STRING:\n                StringBuilder sb = new StringBuilder();\n                append(sb, value, false);\n                return sb.toString();\n            case BOOLEAN:\n                if (value instanceof Boolean) {\n                    return value;\n                }\n                if (value instanceof String) {\n                    SchemaAndValue parsed = parseString(value.toString());\n                    if (parsed.value() instanceof Boolean) {\n                        return parsed.value();\n                    }\n                }\n                return asLong(value, fromSchema, null) == 0L ? Boolean.FALSE : Boolean.TRUE;\n            case INT8:\n                if (value instanceof Byte) {\n                    return value;\n                }\n                return (byte) asLong(value, fromSchema, null);\n            case INT16:\n                if (value instanceof Short) {\n                    return value;\n                }\n                return (short) asLong(value, fromSchema, null);\n            case INT32:\n                if (Date.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof String) {\n                        SchemaAndValue parsed = parseString(value.toString());\n                        value = parsed.value();\n                    }\n                    if (value instanceof java.util.Date) {\n                        if (fromSchema != null) {\n                            String fromSchemaName = fromSchema.name();\n                            if (Date.LOGICAL_NAME.equals(fromSchemaName)) {\n                                return value;\n                            }\n                            if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {\n                                // Just get the number of days from this timestamp\n                                long millis = ((java.util.Date) value).getTime();\n                                int days = (int) (millis / MILLIS_PER_DAY); // truncates\n                                return Date.toLogical(toSchema, days);\n                            }\n                        } else {\n                            // There is no fromSchema, so no conversion is needed\n                            return value;\n                        }\n                    }\n                    long numeric = asLong(value, fromSchema, null);\n                    return Date.toLogical(toSchema, (int) numeric);\n                }\n                if (Time.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof String) {\n                        SchemaAndValue parsed = parseString(value.toString());\n                        value = parsed.value();\n                    }\n                    if (value instanceof java.util.Date) {\n                        if (fromSchema != null) {\n                            String fromSchemaName = fromSchema.name();\n                            if (Time.LOGICAL_NAME.equals(fromSchemaName)) {\n                                return value;\n                            }\n                            if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {\n                                // Just get the time portion of this timestamp\n                                Calendar calendar = Calendar.getInstance(UTC);\n                                calendar.setTime((java.util.Date) value);\n                                calendar.set(Calendar.YEAR, 1970);\n                                calendar.set(Calendar.MONTH, 0); // Months are zero-based\n                                calendar.set(Calendar.DAY_OF_MONTH, 1);\n                                return Time.toLogical(toSchema, (int) calendar.getTimeInMillis());\n                            }\n                        } else {\n                            // There is no fromSchema, so no conversion is needed\n                            return value;\n                        }\n                    }\n                    long numeric = asLong(value, fromSchema, null);\n                    return Time.toLogical(toSchema, (int) numeric);\n                }\n                if (value instanceof Integer) {\n                    return value;\n                }\n                return (int) asLong(value, fromSchema, null);\n            case INT64:\n                if (Timestamp.LOGICAL_NAME.equals(toSchema.name())) {\n                    if (value instanceof String) {\n                        SchemaAndValue parsed = parseString(value.toString());\n                        value = parsed.value();\n                    }\n                    if (value instanceof java.util.Date) {\n                        java.util.Date date = (java.util.Date) value;\n                        if (fromSchema != null) {\n                            String fromSchemaName = fromSchema.name();\n                            if (Date.LOGICAL_NAME.equals(fromSchemaName)) {\n                                int days = Date.fromLogical(fromSchema, date);\n                                long millis = days * MILLIS_PER_DAY;\n                                return Timestamp.toLogical(toSchema, millis);\n                            }\n                            if (Time.LOGICAL_NAME.equals(fromSchemaName)) {\n                                long millis = Time.fromLogical(fromSchema, date);\n                                return Timestamp.toLogical(toSchema, millis);\n                            }\n                            if (Timestamp.LOGICAL_NAME.equals(fromSchemaName)) {\n                                return value;\n                            }\n                        } else {\n                            // There is no fromSchema, so no conversion is needed\n                            return value;\n                        }\n                    }\n                    long numeric = asLong(value, fromSchema, null);\n                    return Timestamp.toLogical(toSchema, numeric);\n                }\n                if (value instanceof Long) {\n                    return value;\n                }\n                return asLong(value, fromSchema, null);\n            case FLOAT32:\n                if (value instanceof Float) {\n                    return value;\n                }\n                return (float) asDouble(value, fromSchema, null);\n            case FLOAT64:\n                if (value instanceof Double) {\n                    return value;\n                }\n                return asDouble(value, fromSchema, null);\n            case ARRAY:\n                if (value instanceof String) {\n                    SchemaAndValue schemaAndValue = parseString(value.toString());\n                    value = schemaAndValue.value();\n                }\n                if (value instanceof List) {\n                    return value;\n                }\n                break;\n            case MAP:\n                if (value instanceof String) {\n                    SchemaAndValue schemaAndValue = parseString(value.toString());\n                    value = schemaAndValue.value();\n                }\n                if (value instanceof Map) {\n                    return value;\n                }\n                break;\n            case STRUCT:\n                if (value instanceof Struct) {\n                    return value;\n                }\n        }\n        throw new DataException(\"Unable to convert \" + value + \" (\" + value.getClass() + \") to \" + toSchema);\n    }\n\n    /**\n     * Convert the specified value to the desired scalar value type.\n     *\n     * @param value      the value to be converted; may not be null\n     * @param fromSchema the schema for the current value type; may not be null\n     * @param error      any previous error that should be included in an exception message; may be null\n     * @return the long value after conversion; never null\n     * @throws DataException if the value could not be converted to a long\n     */\n    protected static long asLong(Object value, Schema fromSchema, Throwable error) {\n        try {\n            if (value instanceof Number) {\n                Number number = (Number) value;\n                return number.longValue();\n            }\n            if (value instanceof String) {\n                return new BigDecimal(value.toString()).longValue();\n            }\n        } catch (NumberFormatException e) {\n            error = e;\n            // fall through\n        }\n        if (fromSchema != null) {\n            String schemaName = fromSchema.name();\n            if (value instanceof java.util.Date) {\n                if (Date.LOGICAL_NAME.equals(schemaName)) {\n                    return Date.fromLogical(fromSchema, (java.util.Date) value);\n                }\n                if (Time.LOGICAL_NAME.equals(schemaName)) {\n                    return Time.fromLogical(fromSchema, (java.util.Date) value);\n                }\n                if (Timestamp.LOGICAL_NAME.equals(schemaName)) {\n                    return Timestamp.fromLogical(fromSchema, (java.util.Date) value);\n                }\n            }\n            throw new DataException(\"Unable to convert \" + value + \" (\" + value.getClass() + \") to \" + fromSchema, error);\n        }\n        throw new DataException(\"Unable to convert \" + value + \" (\" + value.getClass() + \") to a number\", error);\n    }\n\n    /**\n     * Convert the specified value with the desired floating point type.\n     *\n     * @param value  the value to be converted; may not be null\n     * @param schema the schema for the current value type; may not be null\n     * @param error  any previous error that should be included in an exception message; may be null\n     * @return the double value after conversion; never null\n     * @throws DataException if the value could not be converted to a double\n     */\n    protected static double asDouble(Object value, Schema schema, Throwable error) {\n        try {\n            if (value instanceof Number) {\n                Number number = (Number) value;\n                return number.doubleValue();\n            }\n            if (value instanceof String) {\n                return new BigDecimal(value.toString()).doubleValue();\n            }\n        } catch (NumberFormatException e) {\n            error = e;\n            // fall through\n        }\n        return asLong(value, schema, error);\n    }\n\n    protected static void append(StringBuilder sb, Object value, boolean embedded) {\n        if (value == null) {\n            sb.append(NULL_VALUE);\n        } else if (value instanceof Number) {\n            sb.append(value);\n        } else if (value instanceof Boolean) {\n            sb.append(value);\n        } else if (value instanceof String) {\n            if (embedded) {\n                String escaped = escape((String) value);\n                sb.append('\"').append(escaped).append('\"');\n            } else {\n                sb.append(value);\n            }\n        } else if (value instanceof byte[]) {\n            value = Base64.getEncoder().encodeToString((byte[]) value);\n            if (embedded) {\n                sb.append('\"').append(value).append('\"');\n            } else {\n                sb.append(value);\n            }\n        } else if (value instanceof ByteBuffer) {\n            byte[] bytes = Utils.readBytes((ByteBuffer) value);\n            append(sb, bytes, embedded);\n        } else if (value instanceof List) {\n            List<?> list = (List<?>) value;\n            sb.append('[');\n            appendIterable(sb, list.iterator());\n            sb.append(']');\n        } else if (value instanceof Map) {\n            Map<?, ?> map = (Map<?, ?>) value;\n            sb.append('{');\n            appendIterable(sb, map.entrySet().iterator());\n            sb.append('}');\n        } else if (value instanceof Struct) {\n            Struct struct = (Struct) value;\n            Schema schema = struct.schema();\n            boolean first = true;\n            sb.append('{');\n            for (Field field : schema.fields()) {\n                if (first) {\n                    first = false;\n                } else {\n                    sb.append(',');\n                }\n                append(sb, field.name(), true);\n                sb.append(':');\n                append(sb, struct.get(field), true);\n            }\n            sb.append('}');\n        } else if (value instanceof Map.Entry) {\n            Map.Entry<?, ?> entry = (Map.Entry<?, ?>) value;\n            append(sb, entry.getKey(), true);\n            sb.append(':');\n            append(sb, entry.getValue(), true);\n        } else if (value instanceof java.util.Date) {\n            java.util.Date dateValue = (java.util.Date) value;\n            String formatted = dateFormatFor(dateValue).format(dateValue);\n            sb.append(formatted);\n        } else {\n            throw new DataException(\"Failed to serialize unexpected value type \" + value.getClass().getName() + \": \" + value);\n        }\n    }\n\n    protected static void appendIterable(StringBuilder sb, Iterator<?> iter) {\n        if (iter.hasNext()) {\n            append(sb, iter.next(), true);\n            while (iter.hasNext()) {\n                sb.append(',');\n                append(sb, iter.next(), true);\n            }\n        }\n    }\n\n    protected static String escape(String value) {\n        String replace1 = TWO_BACKSLASHES.matcher(value).replaceAll(\"\\\\\\\\\\\\\\\\\");\n        return DOUBLE_QUOTE.matcher(replace1).replaceAll(\"\\\\\\\\\\\"\");\n    }\n\n    public static DateFormat dateFormatFor(java.util.Date value) {\n        if (value.getTime() < MILLIS_PER_DAY) {\n            return new SimpleDateFormat(ISO_8601_TIME_FORMAT_PATTERN);\n        }\n        if (value.getTime() % MILLIS_PER_DAY == 0) {\n            return new SimpleDateFormat(ISO_8601_DATE_FORMAT_PATTERN);\n        }\n        return new SimpleDateFormat(ISO_8601_TIMESTAMP_FORMAT_PATTERN);\n    }\n\n    protected static boolean canParseSingleTokenLiteral(Parser parser, boolean embedded, String tokenLiteral) {\n        int startPosition = parser.mark();\n        // If the next token is what we expect, then either...\n        if (parser.canConsume(tokenLiteral)) {\n            //   ...we're reading an embedded value, in which case the next token will be handled appropriately\n            //      by the caller if it's something like an end delimiter for a map or array, or a comma to\n            //      separate multiple embedded values...\n            //   ...or it's being parsed as part of a top-level string, in which case, any other tokens should\n            //      cause use to stop parsing this single-token literal as such and instead just treat it like\n            //      a string. For example, the top-level string \"true}\" will be tokenized as the tokens \"true\" and\n            //      \"}\", but should ultimately be parsed as just the string \"true}\" instead of the boolean true.\n            if (embedded || !parser.hasNext()) {\n                return true;\n            }\n        }\n        parser.rewindTo(startPosition);\n        return false;\n    }\n\n    protected static SchemaAndValue parse(Parser parser, boolean embedded) throws NoSuchElementException {\n        if (!parser.hasNext()) {\n            return null;\n        }\n        if (embedded) {\n            if (parser.canConsume(QUOTE_DELIMITER)) {\n                StringBuilder sb = new StringBuilder();\n                while (parser.hasNext()) {\n                    if (parser.canConsume(QUOTE_DELIMITER)) {\n                        break;\n                    }\n                    sb.append(parser.next());\n                }\n                String content = sb.toString();\n                // We can parse string literals as temporal logical types, but all others\n                // are treated as strings\n                SchemaAndValue parsed = parseString(content);\n                if (parsed != null && TEMPORAL_LOGICAL_TYPE_NAMES.contains(parsed.schema().name())) {\n                    return parsed;\n                }\n                return new SchemaAndValue(Schema.STRING_SCHEMA, content);\n            }\n        }\n\n        if (canParseSingleTokenLiteral(parser, embedded, NULL_VALUE)) {\n            return null;\n        }\n        if (canParseSingleTokenLiteral(parser, embedded, TRUE_LITERAL)) {\n            return TRUE_SCHEMA_AND_VALUE;\n        }\n        if (canParseSingleTokenLiteral(parser, embedded, FALSE_LITERAL)) {\n            return FALSE_SCHEMA_AND_VALUE;\n        }\n\n        int startPosition = parser.mark();\n\n        try {\n            if (parser.canConsume(ARRAY_BEGIN_DELIMITER)) {\n                List<Object> result = new ArrayList<>();\n                boolean compatible = true;\n                Schema elementSchema = null;\n                while (parser.hasNext()) {\n                    if (parser.canConsume(ARRAY_END_DELIMITER)) {\n                        Schema listSchema;\n                        if (elementSchema != null && compatible) {\n                            listSchema = SchemaBuilder.array(elementSchema).schema();\n                            result = alignListEntriesWithSchema(listSchema, result);\n                        } else {\n                            // Every value is null\n                            listSchema = SchemaBuilder.arrayOfNull().build();\n                        }\n                        return new SchemaAndValue(listSchema, result);\n                    }\n\n                    if (parser.canConsume(COMMA_DELIMITER)) {\n                        throw new DataException(\"Unable to parse an empty array element: \" + parser.original());\n                    }\n                    SchemaAndValue element = parse(parser, true);\n                    elementSchema = commonSchemaFor(elementSchema, element);\n                    if (elementSchema == null && element != null && element.schema() != null) {\n                        compatible = false;\n                    }\n                    result.add(element != null ? element.value() : null);\n\n                    int currentPosition = parser.mark();\n                    if (parser.canConsume(ARRAY_END_DELIMITER)) {\n                        parser.rewindTo(currentPosition);\n                    } else if (!parser.canConsume(COMMA_DELIMITER)) {\n                        throw new DataException(\"Array elements missing '\" + COMMA_DELIMITER + \"' delimiter\");\n                    }\n                }\n\n                // Missing either a comma or an end delimiter\n                if (COMMA_DELIMITER.equals(parser.previous())) {\n                    throw new DataException(\"Array is missing element after ',': \" + parser.original());\n                }\n                throw new DataException(\"Array is missing terminating ']': \" + parser.original());\n            }\n\n            if (parser.canConsume(MAP_BEGIN_DELIMITER)) {\n                Map<Object, Object> result = new LinkedHashMap<>();\n                boolean keyCompatible = true;\n                Schema keySchema = null;\n                boolean valueCompatible = true;\n                Schema valueSchema = null;\n                while (parser.hasNext()) {\n                    if (parser.canConsume(MAP_END_DELIMITER)) {\n                        Schema mapSchema;\n                        if (keySchema != null && valueSchema != null && keyCompatible && valueCompatible) {\n                            mapSchema = SchemaBuilder.map(keySchema, valueSchema).build();\n                            result = alignMapKeysAndValuesWithSchema(mapSchema, result);\n                        } else if (keySchema != null && keyCompatible) {\n                            mapSchema = SchemaBuilder.mapWithNullValues(keySchema);\n                            result = alignMapKeysWithSchema(mapSchema, result);\n                        } else {\n                            mapSchema = SchemaBuilder.mapOfNull().build();\n                        }\n                        return new SchemaAndValue(mapSchema, result);\n                    }\n\n                    if (parser.canConsume(COMMA_DELIMITER)) {\n                        throw new DataException(\"Unable to parse a map entry with no key or value: \" + parser.original());\n                    }\n                    SchemaAndValue key = parse(parser, true);\n                    if (key == null || key.value() == null) {\n                        throw new DataException(\"Map entry may not have a null key: \" + parser.original());\n                    }\n\n                    if (!parser.canConsume(ENTRY_DELIMITER)) {\n                        throw new DataException(\"Map entry is missing '\" + ENTRY_DELIMITER\n                                                + \"' at \" + parser.position()\n                                                + \" in \" + parser.original());\n                    }\n                    SchemaAndValue value = parse(parser, true);\n                    Object entryValue = value != null ? value.value() : null;\n                    result.put(key.value(), entryValue);\n\n                    parser.canConsume(COMMA_DELIMITER);\n                    keySchema = commonSchemaFor(keySchema, key);\n                    if (keySchema == null && key.schema() != null) {\n                        keyCompatible = false;\n                    }\n                    valueSchema = commonSchemaFor(valueSchema, value);\n                    if (valueSchema == null && value != null && value.schema() != null) {\n                        valueCompatible = false;\n                    }\n                }\n                // Missing either a comma or an end delimiter\n                if (COMMA_DELIMITER.equals(parser.previous())) {\n                    throw new DataException(\"Map is missing element after ',': \" + parser.original());\n                }\n                throw new DataException(\"Map is missing terminating '}': \" + parser.original());\n            }\n        } catch (DataException e) {\n            LOG.trace(\"Unable to parse the value as a map or an array; reverting to string\", e);\n            parser.rewindTo(startPosition);\n        }\n\n        String token = parser.next();\n        if (Utils.isBlank(token)) {\n            return new SchemaAndValue(Schema.STRING_SCHEMA, token);\n        }\n        token = token.trim();\n\n        char firstChar = token.charAt(0);\n        boolean firstCharIsDigit = Character.isDigit(firstChar);\n\n        // Temporal types are more restrictive, so try them first\n        if (firstCharIsDigit) {\n            // The time and timestamp literals may be split into 5 tokens since an unescaped colon\n            // is a delimiter. Check these first since the first of these tokens is a simple numeric\n            int position = parser.mark();\n            String remainder = parser.next(4);\n            if (remainder != null) {\n                String timeOrTimestampStr = token + remainder;\n                SchemaAndValue temporal = parseAsTemporal(timeOrTimestampStr);\n                if (temporal != null) {\n                    return temporal;\n                }\n            }\n            // No match was found using the 5 tokens, so rewind and see if the current token has a date, time, or timestamp\n            parser.rewindTo(position);\n            SchemaAndValue temporal = parseAsTemporal(token);\n            if (temporal != null) {\n                return temporal;\n            }\n        }\n        if (firstCharIsDigit || firstChar == '+' || firstChar == '-') {\n            try {\n                // Try to parse as a number ...\n                BigDecimal decimal = new BigDecimal(token);\n                try {\n                    return new SchemaAndValue(Schema.INT8_SCHEMA, decimal.byteValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                try {\n                    return new SchemaAndValue(Schema.INT16_SCHEMA, decimal.shortValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                try {\n                    return new SchemaAndValue(Schema.INT32_SCHEMA, decimal.intValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                try {\n                    return new SchemaAndValue(Schema.INT64_SCHEMA, decimal.longValueExact());\n                } catch (ArithmeticException e) {\n                    // continue\n                }\n                float fValue = decimal.floatValue();\n                if (fValue != Float.NEGATIVE_INFINITY && fValue != Float.POSITIVE_INFINITY\n                    && decimal.scale() != 0) {\n                    return new SchemaAndValue(Schema.FLOAT32_SCHEMA, fValue);\n                }\n                double dValue = decimal.doubleValue();\n                if (dValue != Double.NEGATIVE_INFINITY && dValue != Double.POSITIVE_INFINITY\n                    && decimal.scale() != 0) {\n                    return new SchemaAndValue(Schema.FLOAT64_SCHEMA, dValue);\n                }\n                Schema schema = Decimal.schema(decimal.scale());\n                return new SchemaAndValue(schema, decimal);\n            } catch (NumberFormatException e) {\n                // can't parse as a number\n            }\n        }\n        if (embedded) {\n            throw new DataException(\"Failed to parse embedded value\");\n        }\n        // At this point, the only thing this non-embedded value can be is a string.\n        return new SchemaAndValue(Schema.STRING_SCHEMA, parser.original());\n    }\n\n    private static SchemaAndValue parseAsTemporal(String token) {\n        if (token == null) {\n            return null;\n        }\n        // If the colons were escaped, we'll see the escape chars and need to remove them\n        token = token.replace(\"\\\\:\", \":\");\n        int tokenLength = token.length();\n        if (tokenLength == ISO_8601_TIME_LENGTH) {\n            try {\n                return new SchemaAndValue(Time.SCHEMA, new SimpleDateFormat(ISO_8601_TIME_FORMAT_PATTERN).parse(token));\n            } catch (ParseException e) {\n              // not a valid date\n            }\n        } else if (tokenLength == ISO_8601_TIMESTAMP_LENGTH) {\n            try {\n                return new SchemaAndValue(Timestamp.SCHEMA, new SimpleDateFormat(ISO_8601_TIMESTAMP_FORMAT_PATTERN).parse(token));\n            } catch (ParseException e) {\n              // not a valid date\n            }\n        } else if (tokenLength == ISO_8601_DATE_LENGTH) {\n            try {\n                return new SchemaAndValue(Date.SCHEMA, new SimpleDateFormat(ISO_8601_DATE_FORMAT_PATTERN).parse(token));\n            } catch (ParseException e) {\n                // not a valid date\n            }\n        }\n        return null;\n    }\n\n    protected static Schema commonSchemaFor(Schema previous, SchemaAndValue latest) {\n        if (latest == null) {\n            return previous;\n        }\n        if (previous == null) {\n            return latest.schema();\n        }\n        Schema newSchema = latest.schema();\n        Type previousType = previous.type();\n        Type newType = newSchema.type();\n        if (previousType != newType) {\n            switch (previous.type()) {\n                case INT8:\n                    if (newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64 || newType == Type.FLOAT32 || newType ==\n                                                                                                                              Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case INT16:\n                    if (newType == Type.INT8) {\n                        return previous;\n                    }\n                    if (newType == Type.INT32 || newType == Type.INT64 || newType == Type.FLOAT32 || newType == Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case INT32:\n                    if (newType == Type.INT8 || newType == Type.INT16) {\n                        return previous;\n                    }\n                    if (newType == Type.INT64 || newType == Type.FLOAT32 || newType == Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case INT64:\n                    if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32) {\n                        return previous;\n                    }\n                    if (newType == Type.FLOAT32 || newType == Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case FLOAT32:\n                    if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64) {\n                        return previous;\n                    }\n                    if (newType == Type.FLOAT64) {\n                        return newSchema;\n                    }\n                    break;\n                case FLOAT64:\n                    if (newType == Type.INT8 || newType == Type.INT16 || newType == Type.INT32 || newType == Type.INT64 || newType ==\n                                                                                                                           Type.FLOAT32) {\n                        return previous;\n                    }\n                    break;\n            }\n            return null;\n        }\n        if (previous.isOptional() == newSchema.isOptional()) {\n            // Use the optional one\n            return previous.isOptional() ? previous : newSchema;\n        }\n        if (!previous.equals(newSchema)) {\n            return null;\n        }\n        return previous;\n    }\n\n    protected static List<Object> alignListEntriesWithSchema(Schema schema, List<Object> input) {\n        Schema valueSchema = schema.valueSchema();\n        List<Object> result = new ArrayList<>();\n        for (Object value : input) {\n            Object newValue = convertTo(valueSchema, null, value);\n            result.add(newValue);\n        }\n        return result;\n    }\n\n    protected static Map<Object, Object> alignMapKeysAndValuesWithSchema(Schema mapSchema, Map<Object, Object> input) {\n        Schema keySchema = mapSchema.keySchema();\n        Schema valueSchema = mapSchema.valueSchema();\n        Map<Object, Object> result = new LinkedHashMap<>();\n        for (Map.Entry<?, ?> entry : input.entrySet()) {\n            Object newKey = convertTo(keySchema, null, entry.getKey());\n            Object newValue = convertTo(valueSchema, null, entry.getValue());\n            result.put(newKey, newValue);\n        }\n        return result;\n    }\n\n    protected static Map<Object, Object> alignMapKeysWithSchema(Schema mapSchema, Map<Object, Object> input) {\n        Schema keySchema = mapSchema.keySchema();\n        Map<Object, Object> result = new LinkedHashMap<>();\n        for (Map.Entry<?, ?> entry : input.entrySet()) {\n            Object newKey = convertTo(keySchema, null, entry.getKey());\n            result.put(newKey, entry.getValue());\n        }\n        return result;\n    }\n\n    protected static class SchemaDetector {\n        private Type knownType = null;\n        private boolean optional = false;\n\n        public SchemaDetector() {\n        }\n\n        public boolean canDetect(Object value) {\n            if (value == null) {\n                optional = true;\n                return true;\n            }\n            Schema schema = inferSchema(value);\n            if (schema == null) {\n                return false;\n            }\n            if (knownType == null) {\n                knownType = schema.type();\n            } else if (knownType != schema.type()) {\n                return false;\n            }\n            return true;\n        }\n\n        public Schema schema() {\n            SchemaBuilder builder = SchemaBuilder.type(knownType);\n            if (optional) {\n                builder.optional();\n            }\n            return builder.schema();\n        }\n    }\n\n    protected static class Parser {\n        private final String original;\n        private final CharacterIterator iter;\n        private String nextToken = null;\n        private String previousToken = null;\n\n        public Parser(String original) {\n            this.original = original;\n            this.iter = new StringCharacterIterator(this.original);\n        }\n\n        public int position() {\n            return iter.getIndex();\n        }\n\n        public int mark() {\n            return iter.getIndex() - (nextToken != null ? nextToken.length() : 0);\n        }\n\n        public void rewindTo(int position) {\n            iter.setIndex(position);\n            nextToken = null;\n            previousToken = null;\n        }\n\n        public String original() {\n            return original;\n        }\n\n        public boolean hasNext() {\n            return nextToken != null || canConsumeNextToken();\n        }\n\n        protected boolean canConsumeNextToken() {\n            return iter.getEndIndex() > iter.getIndex();\n        }\n\n        public String next() {\n            if (nextToken != null) {\n                previousToken = nextToken;\n                nextToken = null;\n            } else {\n                previousToken = consumeNextToken();\n            }\n            return previousToken;\n        }\n\n        public String next(int n) {\n            int current = mark();\n            int start = mark();\n            for (int i = 0; i != n; ++i) {\n                if (!hasNext()) {\n                    rewindTo(start);\n                    return null;\n                }\n                next();\n            }\n            return original.substring(current, position());\n        }\n\n        private String consumeNextToken() throws NoSuchElementException {\n            boolean escaped = false;\n            int start = iter.getIndex();\n            char c = iter.current();\n            while (canConsumeNextToken()) {\n                switch (c) {\n                    case '\\\\':\n                        escaped = !escaped;\n                        break;\n                    case ':':\n                    case ',':\n                    case '{':\n                    case '}':\n                    case '[':\n                    case ']':\n                    case '\\\"':\n                        if (!escaped) {\n                            if (start < iter.getIndex()) {\n                                // Return the previous token\n                                return original.substring(start, iter.getIndex());\n                            }\n                            // Consume and return this delimiter as a token\n                            iter.next();\n                            return original.substring(start, start + 1);\n                        }\n                        // escaped, so continue\n                        escaped = false;\n                        break;\n                    default:\n                        // If escaped, then we don't care what was escaped\n                        escaped = false;\n                        break;\n                }\n                c = iter.next();\n            }\n            return original.substring(start, iter.getIndex());\n        }\n\n        public String previous() {\n            return previousToken;\n        }\n\n        public boolean canConsume(String expected) {\n            return canConsume(expected, true);\n        }\n\n        public boolean canConsume(String expected, boolean ignoreLeadingAndTrailingWhitespace) {\n            if (isNext(expected, ignoreLeadingAndTrailingWhitespace)) {\n                // consume this token ...\n                nextToken = null;\n                return true;\n            }\n            return false;\n        }\n\n        protected boolean isNext(String expected, boolean ignoreLeadingAndTrailingWhitespace) {\n            if (nextToken == null) {\n                if (!hasNext()) {\n                    return false;\n                }\n                // There's another token, so consume it\n                nextToken = consumeNextToken();\n            }\n            if (ignoreLeadingAndTrailingWhitespace) {\n                while (Utils.isBlank(nextToken) && canConsumeNextToken()) {\n                    nextToken = consumeNextToken();\n                }\n            }\n            return ignoreLeadingAndTrailingWhitespace\n                ? nextToken.trim().equals(expected)\n                : nextToken.equals(expected);\n        }\n    }\n}"
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [],
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "d5592d8fe62ec88e202ace61414233a730b48367",
        "url": "https://github.com/apache/kafka/commit/d5592d8fe62ec88e202ace61414233a730b48367",
        "move_method_refactoring": {
            "type": "Move Method",
            "description": "Move Method private buildThrowingMockRequest(t Throwable) : Request from class org.apache.kafka.connect.runtime.rest.RestClientTest.RequestFailureParameterizedTest to private buildThrowingMockRequest(t Throwable) : Request from class org.apache.kafka.connect.runtime.rest.RestClientTest",
            "leftSideLocations": [
                {
                    "filePath": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/RestClientTest.java",
                    "startLine": 130,
                    "endLine": 135,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "original method declaration",
                    "codeElement": "private buildThrowingMockRequest(t Throwable) : Request"
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/RestClientTest.java",
                    "startLine": 116,
                    "endLine": 121,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "moved method declaration",
                    "codeElement": "private buildThrowingMockRequest(t Throwable) : Request"
                }
            ],
            "isStatic": true
        },
        "telemetry": {
            "id": "3063400f-280d-453b-9b18-1a7dc8dd24e9",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 292,
                "lineStart": 64,
                "lineEnd": 355,
                "bodyLineStart": 64,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/RestClientTest.java",
                "sourceCode": "@RunWith(Enclosed.class)\npublic class RestClientTest {\n\n    private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();\n    private static final String MOCK_URL = \"http://localhost:1234/api/endpoint\";\n    private static final String TEST_METHOD = \"GET\";\n    private static final TestDTO TEST_DTO = new TestDTO(\"requestBodyData\");\n    private static final TypeReference<TestDTO> TEST_TYPE = new TypeReference<TestDTO>() {\n    };\n    private static final SecretKey MOCK_SECRET_KEY = getMockSecretKey();\n    private static final String TEST_SIGNATURE_ALGORITHM = \"HmacSHA1\";\n\n    private static void assertIsInternalServerError(ConnectRestException e) {\n        assertEquals(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), e.statusCode());\n        assertEquals(Response.Status.INTERNAL_SERVER_ERROR.getStatusCode(), e.errorCode());\n    }\n\n    private static SecretKey getMockSecretKey() {\n        SecretKey mockKey = mock(SecretKey.class);\n        when(mockKey.getFormat()).thenReturn(\"RAW\"); // supported format by\n        when(mockKey.getEncoded()).thenReturn(\"SomeKey\".getBytes(StandardCharsets.UTF_8));\n        return mockKey;\n    }\n\n    private static <T> RestClient.HttpResponse<T> httpRequest(\n            HttpClient httpClient,\n            String url,\n            String method,\n            TypeReference<T> responseFormat,\n            String requestSignatureAlgorithm\n    ) {\n        RestClient client = spy(new RestClient(null));\n        doReturn(httpClient).when(client).httpClient(any());\n        return client.httpRequest(\n                url,\n                method,\n                null,\n                TEST_DTO,\n                responseFormat,\n                MOCK_SECRET_KEY,\n                requestSignatureAlgorithm\n        );\n    }\n\n\n    @RunWith(Parameterized.class)\n    public static class RequestFailureParameterizedTest {\n\n        @Rule\n        public MockitoRule initRule = MockitoJUnit.rule().strictness(Strictness.STRICT_STUBS);\n\n        @Mock\n        private HttpClient httpClient;\n\n        @Parameterized.Parameter\n        public Throwable requestException;\n        \n        @Parameterized.Parameters\n        public static Collection<Object[]> requestExceptions() {\n            return Arrays.asList(new Object[][]{\n                    {new InterruptedException()},\n                    {new ExecutionException(null)},\n                    {new TimeoutException()}\n            });\n        }\n\n        private static Request buildThrowingMockRequest(Throwable t) throws ExecutionException, InterruptedException, TimeoutException {\n            Request req = mock(Request.class);\n            when(req.header(anyString(), anyString())).thenReturn(req);\n            when(req.send()).thenThrow(t);\n            return req;\n        }\n\n        @Test\n        public void testFailureDuringRequestCausesInternalServerError() throws Exception {\n            Request request = buildThrowingMockRequest(requestException);\n            when(httpClient.newRequest(anyString())).thenReturn(request);\n            ConnectRestException e = assertThrows(ConnectRestException.class, () -> httpRequest(\n                    httpClient, MOCK_URL, TEST_METHOD, TEST_TYPE, TEST_SIGNATURE_ALGORITHM\n            ));\n            assertIsInternalServerError(e);\n            assertEquals(requestException, e.getCause());\n        }\n    }\n\n\n    @RunWith(MockitoJUnitRunner.StrictStubs.class)\n    public static class Tests {\n        @Mock\n        private HttpClient httpClient;\n\n        private static String toJsonString(Object obj) {\n            try {\n                return OBJECT_MAPPER.writeValueAsString(obj);\n            } catch (JsonProcessingException e) {\n                throw new RuntimeException(e);\n            }\n        }\n\n        private void setupHttpClient(int responseCode, String responseJsonString) throws Exception {\n            Request req = mock(Request.class);\n            ContentResponse resp = mock(ContentResponse.class);\n            when(resp.getStatus()).thenReturn(responseCode);\n            when(resp.getContentAsString()).thenReturn(responseJsonString);\n            when(req.send()).thenReturn(resp);\n            when(req.header(anyString(), anyString())).thenReturn(req);\n            when(httpClient.newRequest(anyString())).thenReturn(req);\n        }\n\n        @Test\n        public void testNullUrl() throws Exception {\n            int statusCode = Response.Status.OK.getStatusCode();\n            setupHttpClient(statusCode, toJsonString(TEST_DTO));\n\n            assertThrows(NullPointerException.class, () -> httpRequest(\n                    httpClient, null, TEST_METHOD, TEST_TYPE, TEST_SIGNATURE_ALGORITHM\n            ));\n        }\n\n        @Test\n        public void testNullMethod() throws Exception {\n            int statusCode = Response.Status.OK.getStatusCode();\n            setupHttpClient(statusCode, toJsonString(TEST_DTO));\n\n            assertThrows(NullPointerException.class, () -> httpRequest(\n                    httpClient, MOCK_URL, null, TEST_TYPE, TEST_SIGNATURE_ALGORITHM\n            ));\n        }\n\n        @Test\n        public void testNullResponseType() throws Exception {\n            int statusCode = Response.Status.OK.getStatusCode();\n            setupHttpClient(statusCode, toJsonString(TEST_DTO));\n\n            assertThrows(NullPointerException.class, () -> httpRequest(\n                    httpClient, MOCK_URL, TEST_METHOD, null, TEST_SIGNATURE_ALGORITHM\n            ));\n        }\n\n        @Test\n        public void testSuccess() throws Exception {\n            int statusCode = Response.Status.OK.getStatusCode();\n            setupHttpClient(statusCode, toJsonString(TEST_DTO));\n\n            RestClient.HttpResponse<TestDTO> httpResp = httpRequest(\n                    httpClient, MOCK_URL, TEST_METHOD, TEST_TYPE, TEST_SIGNATURE_ALGORITHM\n            );\n            assertEquals(statusCode, httpResp.status());\n            assertEquals(TEST_DTO, httpResp.body());\n        }\n\n        @Test\n        public void testNoContent() throws Exception {\n            int statusCode = Response.Status.NO_CONTENT.getStatusCode();\n            setupHttpClient(statusCode, null);\n\n            RestClient.HttpResponse<TestDTO> httpResp = httpRequest(\n                    httpClient, MOCK_URL, TEST_METHOD, TEST_TYPE, TEST_SIGNATURE_ALGORITHM\n            );\n            assertEquals(statusCode, httpResp.status());\n            assertNull(httpResp.body());\n        }\n\n        @Test\n        public void testStatusCodeAndErrorMessagePreserved() throws Exception {\n            int statusCode = Response.Status.CONFLICT.getStatusCode();\n            ErrorMessage errorMsg = new ErrorMessage(Response.Status.GONE.getStatusCode(), \"Some Error Message\");\n            setupHttpClient(statusCode, toJsonString(errorMsg));\n\n            ConnectRestException e = assertThrows(ConnectRestException.class, () -> httpRequest(\n                    httpClient, MOCK_URL, TEST_METHOD, TEST_TYPE, TEST_SIGNATURE_ALGORITHM\n            ));\n            assertEquals(statusCode, e.statusCode());\n            assertEquals(errorMsg.errorCode(), e.errorCode());\n            assertEquals(errorMsg.message(), e.getMessage());\n        }\n\n        @Test\n        public void testNonEmptyResponseWithVoidResponseType() throws Exception {\n            int statusCode = Response.Status.OK.getStatusCode();\n            setupHttpClient(statusCode, toJsonString(TEST_DTO));\n\n            TypeReference<Void> voidResponse = new TypeReference<Void>() { };\n            RestClient.HttpResponse<Void> httpResp = httpRequest(\n                    httpClient, MOCK_URL, TEST_METHOD, voidResponse, TEST_SIGNATURE_ALGORITHM\n            );\n            assertEquals(statusCode, httpResp.status());\n            assertNull(httpResp.body());\n        }\n\n        @Test\n        public void testUnexpectedHttpResponseCausesInternalServerError() throws Exception {\n            int statusCode = Response.Status.NOT_MODIFIED.getStatusCode(); // never thrown explicitly -\n            // should be treated as an unexpected error and translated into 500 INTERNAL_SERVER_ERROR\n\n            setupHttpClient(statusCode, null);\n            ConnectRestException e = assertThrows(ConnectRestException.class, () -> httpRequest(\n                    httpClient, MOCK_URL, TEST_METHOD, TEST_TYPE, TEST_SIGNATURE_ALGORITHM\n            ));\n            assertIsInternalServerError(e);\n        }\n\n        @Test\n        public void testRuntimeExceptionCausesInternalServerError() {\n            when(httpClient.newRequest(anyString())).thenThrow(new RuntimeException());\n\n            ConnectRestException e = assertThrows(ConnectRestException.class, () -> httpRequest(\n                    httpClient, MOCK_URL, TEST_METHOD, TEST_TYPE, TEST_SIGNATURE_ALGORITHM\n            ));\n            assertIsInternalServerError(e);\n        }\n\n        @Test\n        public void testRequestSignatureFailureCausesInternalServerError() throws Exception {\n            setupHttpClient(0, null);\n\n            String invalidRequestSignatureAlgorithm = \"Foo\";\n            ConnectRestException e = assertThrows(ConnectRestException.class, () -> httpRequest(\n                    httpClient, MOCK_URL, TEST_METHOD, TEST_TYPE, invalidRequestSignatureAlgorithm\n            ));\n            assertIsInternalServerError(e);\n        }\n\n        @Test\n        public void testIOExceptionCausesInternalServerError() throws Exception {\n            String invalidJsonString = \"Invalid\";\n            setupHttpClient(201, invalidJsonString);\n\n            ConnectRestException e = assertThrows(ConnectRestException.class, () -> httpRequest(\n                    httpClient, MOCK_URL, TEST_METHOD, TEST_TYPE, TEST_SIGNATURE_ALGORITHM\n            ));\n            assertIsInternalServerError(e);\n        }\n\n        @Test\n        public void testUseSslConfigsOnlyWhenNecessary() throws Exception {\n            // See KAFKA-14816; we want to make sure that even if the worker is configured with invalid SSL properties,\n            // REST requests only fail if we try to contact a URL using HTTPS (but not HTTP)\n            int statusCode = Response.Status.OK.getStatusCode();\n            setupHttpClient(statusCode, toJsonString(TEST_DTO));\n\n            assertDoesNotThrow(() -> httpRequest(\n                    httpClient, MOCK_URL, TEST_METHOD, TEST_TYPE, TEST_SIGNATURE_ALGORITHM\n            ));\n            String httpsUrl = \"https://localhost:1234/api/endpoint\";\n            assertThrows(RuntimeException.class, () -> httpRequest(\n                    httpClient, httpsUrl, TEST_METHOD, TEST_TYPE, TEST_SIGNATURE_ALGORITHM\n            ));\n        }\n\n        @Test\n        public void testHttpRequestInterrupted() throws ExecutionException, InterruptedException, TimeoutException {\n            Request req = mock(Request.class);\n            doThrow(new InterruptedException()).when(req).send();\n            doReturn(req).when(req).header(anyString(), anyString());\n            doReturn(req).when(httpClient).newRequest(anyString());\n            ConnectRestException e = assertThrows(ConnectRestException.class, () -> httpRequest(\n                    httpClient, MOCK_URL, TEST_METHOD, TEST_TYPE, TEST_SIGNATURE_ALGORITHM\n            ));\n            assertIsInternalServerError(e);\n            assertInstanceOf(InterruptedException.class, e.getCause());\n            assertTrue(Thread.interrupted());\n        }\n    }\n\n\n    private static class TestDTO {\n        private final String content;\n\n        @JsonCreator\n        private TestDTO(@JsonProperty(value = \"content\") String content) {\n            this.content = content;\n        }\n\n        public String getContent() {\n            return content;\n        }\n\n        @Override\n        public boolean equals(Object o) {\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n            TestDTO testDTO = (TestDTO) o;\n            return content.equals(testDTO.content);\n        }\n\n        @Override\n        public int hashCode() {\n            return Objects.hash(content);\n        }\n    }\n}"
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "toJsonString",
                            "method_signature": "private static toJsonString(Object obj): String",
                            "target_class": "JsonUtils",
                            "rationale": "This method is for converting any object to its JSON string representation, a functionality generally usable and not specific to RestClientTest class. JsonUtils is a hypothetical class focusing on JSON-related utilities."
                        },
                        {
                            "method_name": "getMockSecretKey",
                            "method_signature": "private static getMockSecretKey(): SecretKey",
                            "target_class": "SecurityUtils",
                            "rationale": "The method is not directly related to the testing of the RestClient, but it is a utility method for creating mock secret keys. A hypothetical SecurityUtils class focusing on security-related utilities would be a more suitable location."
                        },
                        {
                            "method_name": "assertIsInternalServerError",
                            "method_signature": "private static assertIsInternalServerError(ConnectRestException e): void",
                            "target_class": "TestUtils",
                            "rationale": "This method is asserting whether a given exception corresponds to an internal server error. As such assertions could be generally useful in other test cases, it should be moved to a TestUtils class."
                        },
                        {
                            "method_name": "httpRequest",
                            "method_signature": "private static httpRequest(HttpClient httpClient, String url, String method, TypeReference<T> responseFormat, String requestSignatureAlgorithm): RestClient.HttpResponse<T>",
                            "target_class": "RestClient",
                            "rationale": "The method deals with making HTTP requests, which is part of the responsibilities of the RestClient class, not the RestClientTest class. Therefore, it should be moved to the RestClient class."
                        }
                    ],
                    "llm_response_time": 14494
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "setupHttpClient",
                            "method_signature": "private setupHttpClient(int responseCode, String responseJsonString): void",
                            "target_class": "HttpClientMockSetup",
                            "rationale": "The 'setupHttpClient' method is creating a mock HttpClient instance, its signature, behaviour and its usage throughout the test indicates it's more fitting to be in a separate class, which can be used across multiple tests."
                        },
                        {
                            "method_name": "toJsonString",
                            "method_signature": "private static toJsonString(Object obj): String",
                            "target_class": "JsonHelper",
                            "rationale": "This method converts an object to its JSON format string. It can be used by multiple classes which perform conversion of object to JSON, so it should be moved to a general utilities or helper class."
                        },
                        {
                            "method_name": "getMockSecretKey",
                            "method_signature": "private static getMockSecretKey(): SecretKey",
                            "target_class": "SecretKeyMockSetup",
                            "rationale": "The 'getMockSecretKey' method is creating a mock SecretKey instance. This method can be used in several places where mock SecretKey setup is required, hence it should be moved to a separate class responsible for mock setup."
                        }
                    ],
                    "llm_response_time": 8995
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "assertIsInternalServerError",
                            "method_signature": "private assertIsInternalServerError(ConnectRestException e): void",
                            "target_class": "ConnectRestException",
                            "rationale": "The method 'assertIsInternalServerError' is testing a specific behavior of the ConnectRestException, hence it should be on ConnectRestException test class for better organization and understanding."
                        },
                        {
                            "method_name": "getMockSecretKey",
                            "method_signature": "private getMockSecretKey(): SecretKey",
                            "target_class": "SecretKey",
                            "rationale": "'getMockSecretKey' is creating a mocked instance of SecretKey. This type of methods would be more relevant on a 'MockHelper' class if such exists, as it serves a general purpose and is not directly bound to RestClient behavior."
                        },
                        {
                            "method_name": "toJsonString",
                            "method_signature": "private static toJsonString(Object obj): String",
                            "target_class": "JsonUtils",
                            "rationale": "The toJsonString method seems like a utility method for converting any object to a JSON string. This is a general purpose method not specific to RestClientTest and should be in a separate utility class like 'JsonUtils' that handles JSON related operations."
                        }
                    ],
                    "llm_response_time": 8804
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "toJsonString",
                            "method_signature": "private static toJsonString(Object obj): String",
                            "target_class": "JsonUtils",
                            "rationale": "This method is for converting any object to its JSON string representation, a functionality generally usable and not specific to RestClientTest class. JsonUtils is a hypothetical class focusing on JSON-related utilities."
                        },
                        {
                            "method_name": "getMockSecretKey",
                            "method_signature": "private static getMockSecretKey(): SecretKey",
                            "target_class": "SecurityUtils",
                            "rationale": "The method is not directly related to the testing of the RestClient, but it is a utility method for creating mock secret keys. A hypothetical SecurityUtils class focusing on security-related utilities would be a more suitable location."
                        },
                        {
                            "method_name": "assertIsInternalServerError",
                            "method_signature": "private static assertIsInternalServerError(ConnectRestException e): void",
                            "target_class": "TestUtils",
                            "rationale": "This method is asserting whether a given exception corresponds to an internal server error. As such assertions could be generally useful in other test cases, it should be moved to a TestUtils class."
                        },
                        {
                            "method_name": "httpRequest",
                            "method_signature": "private static httpRequest(HttpClient httpClient, String url, String method, TypeReference<T> responseFormat, String requestSignatureAlgorithm): RestClient.HttpResponse<T>",
                            "target_class": "RestClient",
                            "rationale": "The method deals with making HTTP requests, which is part of the responsibilities of the RestClient class, not the RestClientTest class. Therefore, it should be moved to the RestClient class."
                        },
                        {
                            "method_name": "setupHttpClient",
                            "method_signature": "private setupHttpClient(int responseCode, String responseJsonString): void",
                            "target_class": "HttpClientMockSetup",
                            "rationale": "The 'setupHttpClient' method is creating a mock HttpClient instance, its signature, behaviour and its usage throughout the test indicates it's more fitting to be in a separate class, which can be used across multiple tests."
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "llmMethodPriority": {
                "explanation": "[\"httpRequest\", \"setupHttpClient\", \"getMockSecretKey\", \"toJsonString\"]\nThe method \"assertIsInternalServerError\" does not appear as an appropriate candidate for the move method refactoring, since it is a simple assertion utility exclusive to this testing class.",
                "llm_response_time": 2989
            },
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "url": "https://github.com/apache/kafka/commit/adee6f0cc11c37edaf7b47c1f77457d66e80ebe4",
        "move_method_refactoring": {
            "type": "Move Method",
            "description": "Move Method private snapshotFetchResponse(topicPartition TopicPartition, topicId Uuid, epoch int, leaderId int, snapshotId OffsetAndEpoch, highWatermark long) : FetchResponseData from class org.apache.kafka.raft.KafkaRaftClientSnapshotTest to package snapshotFetchResponse(epoch int, leaderId int, snapshotId OffsetAndEpoch, highWatermark long) : FetchResponseData from class org.apache.kafka.raft.RaftClientTestContext",
            "leftSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientSnapshotTest.java",
                    "startLine": 1919,
                    "endLine": 1938,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "original method declaration",
                    "codeElement": "private snapshotFetchResponse(topicPartition TopicPartition, topicId Uuid, epoch int, leaderId int, snapshotId OffsetAndEpoch, highWatermark long) : FetchResponseData"
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java",
                    "startLine": 1354,
                    "endLine": 1380,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "moved method declaration",
                    "codeElement": "package snapshotFetchResponse(epoch int, leaderId int, snapshotId OffsetAndEpoch, highWatermark long) : FetchResponseData"
                }
            ],
            "isStatic": true
        },
        "telemetry": {
            "id": "645b4db3-ae48-4da4-8a8e-6746cf776054",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1976,
                "lineStart": 59,
                "lineEnd": 2034,
                "bodyLineStart": 59,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientSnapshotTest.java",
                "sourceCode": "public final class KafkaRaftClientSnapshotTest {\n    @Test\n    public void testLatestSnapshotId() throws Exception {\n        int localId = 0;\n        int leaderId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(3, 1);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .withEmptySnapshot(snapshotId)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        assertEquals(Optional.of(snapshotId), context.client.latestSnapshotId());\n    }\n\n    @Test\n    public void testLatestSnapshotIdMissing() throws Exception {\n        int localId = 0;\n        int leaderId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(3, 1);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        assertEquals(Optional.empty(), context.client.latestSnapshotId());\n    }\n\n    @ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public void testLeaderListenerNotified(boolean entireLog) throws Exception {\n        int localId = 0;\n        int otherNodeId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, otherNodeId);\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(3, 1);\n\n        RaftClientTestContext.Builder contextBuilder = new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .withEmptySnapshot(snapshotId);\n\n        if (!entireLog) {\n            contextBuilder.deleteBeforeSnapshot(snapshotId);\n        }\n\n        RaftClientTestContext context = contextBuilder.build();\n\n        context.becomeLeader();\n        int epoch = context.currentEpoch();\n\n        // Advance the highWatermark\n        long localLogEndOffset = context.log.endOffset().offset;\n        context.deliverRequest(context.fetchRequest(epoch, otherNodeId, localLogEndOffset, epoch, 0));\n        context.pollUntilResponse();\n        context.assertSentFetchPartitionResponse(Errors.NONE, epoch, OptionalInt.of(localId));\n        assertEquals(localLogEndOffset, context.client.highWatermark().getAsLong());\n\n        // Check that listener was notified of the new snapshot\n        try (SnapshotReader<String> snapshot = context.listener.drainHandledSnapshot().get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            SnapshotWriterReaderTest.assertSnapshot(Collections.emptyList(), snapshot);\n        }\n    }\n\n    @ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public void testFollowerListenerNotified(boolean entireLog) throws Exception {\n        int localId = 0;\n        int leaderId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(3, 1);\n\n        RaftClientTestContext.Builder contextBuilder = new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .withEmptySnapshot(snapshotId)\n            .withElectedLeader(epoch, leaderId);\n\n        if (!entireLog) {\n            contextBuilder.deleteBeforeSnapshot(snapshotId);\n        }\n\n        RaftClientTestContext context = contextBuilder.build();\n\n        // Advance the highWatermark\n        long localLogEndOffset = context.log.endOffset().offset;\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, localLogEndOffset, snapshotId.epoch());\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            context.fetchResponse(epoch, leaderId, MemoryRecords.EMPTY, localLogEndOffset, Errors.NONE)\n        );\n\n        context.pollUntilRequest();\n        context.assertSentFetchRequest(epoch, localLogEndOffset, snapshotId.epoch());\n\n        // Check that listener was notified of the new snapshot\n        try (SnapshotReader<String> snapshot = context.listener.drainHandledSnapshot().get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            SnapshotWriterReaderTest.assertSnapshot(Collections.emptyList(), snapshot);\n        }\n    }\n\n    @ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public void testSecondListenerNotified(boolean entireLog) throws Exception {\n        int localId = 0;\n        int leaderId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(3, 1);\n\n        RaftClientTestContext.Builder contextBuilder = new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .withEmptySnapshot(snapshotId)\n            .withElectedLeader(epoch, leaderId);\n\n        if (!entireLog) {\n            contextBuilder.deleteBeforeSnapshot(snapshotId);\n        }\n\n        RaftClientTestContext context = contextBuilder.build();\n\n        // Advance the highWatermark\n        long localLogEndOffset = context.log.endOffset().offset;\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, localLogEndOffset, snapshotId.epoch());\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            context.fetchResponse(epoch, leaderId, MemoryRecords.EMPTY, localLogEndOffset, Errors.NONE)\n        );\n\n        context.pollUntilRequest();\n        context.assertSentFetchRequest(epoch, localLogEndOffset, snapshotId.epoch());\n\n        RaftClientTestContext.MockListener secondListener = new RaftClientTestContext.MockListener(OptionalInt.of(localId));\n        context.client.register(secondListener);\n        context.client.poll();\n\n        // Check that the second listener was notified of the new snapshot\n        try (SnapshotReader<String> snapshot = secondListener.drainHandledSnapshot().get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            SnapshotWriterReaderTest.assertSnapshot(Collections.emptyList(), snapshot);\n        }\n    }\n\n    @Test\n    public void testListenerRenotified() throws Exception {\n        int localId = 0;\n        int otherNodeId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, otherNodeId);\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(3, 1);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"g\", \"h\", \"i\"))\n            .withEmptySnapshot(snapshotId)\n            .deleteBeforeSnapshot(snapshotId)\n            .build();\n\n        context.becomeLeader();\n        int epoch = context.currentEpoch();\n\n        // Stop the listener from reading commit batches\n        context.listener.updateReadCommit(false);\n\n        // Advance the highWatermark\n        long localLogEndOffset = context.log.endOffset().offset;\n        context.deliverRequest(context.fetchRequest(epoch, otherNodeId, localLogEndOffset, epoch, 0));\n        context.pollUntilResponse();\n        context.assertSentFetchPartitionResponse(Errors.NONE, epoch, OptionalInt.of(localId));\n        assertEquals(localLogEndOffset, context.client.highWatermark().getAsLong());\n\n        // Check that listener was notified of the new snapshot\n        try (SnapshotReader<String> snapshot = context.listener.drainHandledSnapshot().get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            SnapshotWriterReaderTest.assertSnapshot(Collections.emptyList(), snapshot);\n        }\n\n        // Generate a new snapshot\n        OffsetAndEpoch secondSnapshotId = new OffsetAndEpoch(localLogEndOffset, epoch);\n        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(secondSnapshotId, 0).get()) {\n            assertEquals(secondSnapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n        context.log.deleteBeforeSnapshot(secondSnapshotId);\n        context.client.poll();\n\n        // Resume the listener from reading commit batches\n        context.listener.updateReadCommit(true);\n\n        context.client.poll();\n        // Check that listener was notified of the second snapshot\n        try (SnapshotReader<String> snapshot = context.listener.drainHandledSnapshot().get()) {\n            assertEquals(secondSnapshotId, snapshot.snapshotId());\n            SnapshotWriterReaderTest.assertSnapshot(Collections.emptyList(), snapshot);\n        }\n    }\n\n    @Test\n    public void testLeaderImmediatelySendsSnapshotId() throws Exception {\n        int localId = 0;\n        int otherNodeId = 1;\n        Set<Integer> voters = Utils.mkSet(localId, otherNodeId);\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(3, 4);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withUnknownLeader(snapshotId.epoch())\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .appendToLog(snapshotId.epoch(), Arrays.asList(\"g\", \"h\", \"i\"))\n            .withEmptySnapshot(snapshotId)\n            .deleteBeforeSnapshot(snapshotId)\n            .build();\n\n        context.becomeLeader();\n        int epoch = context.currentEpoch();\n\n        // Send a fetch request for an end offset and epoch which has been snapshotted\n        context.deliverRequest(context.fetchRequest(epoch, otherNodeId, 6, 2, 500));\n        context.client.poll();\n\n        // Expect that the leader replies immediately with a snapshot id\n        FetchResponseData.PartitionData partitionResponse = context.assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(localId, partitionResponse.currentLeader().leaderId());\n        assertEquals(snapshotId.epoch(), partitionResponse.snapshotId().epoch());\n        assertEquals(snapshotId.offset(), partitionResponse.snapshotId().endOffset());\n    }\n\n    @Test\n    public void testFetchRequestOffsetLessThanLogStart() throws Exception {\n        int localId = 0;\n        int otherNodeId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, otherNodeId);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withAppendLingerMs(1)\n            .build();\n\n        context.becomeLeader();\n        int epoch = context.currentEpoch();\n\n        List<String> appendRecords = Arrays.asList(\"a\", \"b\", \"c\");\n        context.client.scheduleAppend(epoch, appendRecords);\n        context.time.sleep(context.appendLingerMs());\n        context.client.poll();\n\n        long localLogEndOffset = context.log.endOffset().offset;\n        assertTrue(\n            appendRecords.size() <= localLogEndOffset,\n            String.format(\"Record length = %s, log end offset = %s\", appendRecords.size(), localLogEndOffset)\n        );\n\n        // Advance the highWatermark\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(localLogEndOffset, epoch);\n        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(snapshotId, 0).get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n        context.log.deleteBeforeSnapshot(snapshotId);\n        context.client.poll();\n\n        // Send Fetch request less than start offset\n        context.deliverRequest(context.fetchRequest(epoch, otherNodeId, snapshotId.offset() - 2, snapshotId.epoch(), 0));\n        context.pollUntilResponse();\n        FetchResponseData.PartitionData partitionResponse = context.assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(localId, partitionResponse.currentLeader().leaderId());\n        assertEquals(snapshotId.epoch(), partitionResponse.snapshotId().epoch());\n        assertEquals(snapshotId.offset(), partitionResponse.snapshotId().endOffset());\n    }\n\n    @Test\n    public void testFetchRequestOffsetAtZero() throws Exception {\n        // When the follower sends a FETCH request at offset 0, reply with snapshot id if it exists\n        int localId = 0;\n        int otherNodeId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, otherNodeId);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withAppendLingerMs(1)\n            .build();\n\n        context.becomeLeader();\n        int epoch = context.currentEpoch();\n\n        List<String> appendRecords = Arrays.asList(\"a\", \"b\", \"c\");\n        context.client.scheduleAppend(epoch, appendRecords);\n        context.time.sleep(context.appendLingerMs());\n        context.client.poll();\n\n        long localLogEndOffset = context.log.endOffset().offset;\n        assertTrue(\n            appendRecords.size() <= localLogEndOffset,\n            String.format(\"Record length = %s, log end offset = %s\", appendRecords.size(), localLogEndOffset)\n        );\n\n        // Advance the highWatermark\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        // Generate a snapshot at the LEO\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(localLogEndOffset, epoch);\n        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(snapshotId, 0).get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n\n        // Send Fetch request for offset 0\n        context.deliverRequest(context.fetchRequest(epoch, otherNodeId, 0, 0, 0));\n        context.pollUntilResponse();\n        FetchResponseData.PartitionData partitionResponse = context.assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(localId, partitionResponse.currentLeader().leaderId());\n        assertEquals(snapshotId.epoch(), partitionResponse.snapshotId().epoch());\n        assertEquals(snapshotId.offset(), partitionResponse.snapshotId().endOffset());\n    }\n\n    @Test\n    public void testFetchRequestWithLargerLastFetchedEpoch() throws Exception {\n        int localId = 0;\n        int otherNodeId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, otherNodeId);\n\n        OffsetAndEpoch oldestSnapshotId = new OffsetAndEpoch(3, 2);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .withAppendLingerMs(1)\n            .build();\n\n        context.becomeLeader();\n        int epoch = context.currentEpoch();\n        assertEquals(oldestSnapshotId.epoch() + 1, epoch);\n\n        // Advance the highWatermark\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        // Create a snapshot at the high watermark\n        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(oldestSnapshotId, 0).get()) {\n            assertEquals(oldestSnapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n        context.client.poll();\n\n        context.client.scheduleAppend(epoch, Arrays.asList(\"g\", \"h\", \"i\"));\n        context.time.sleep(context.appendLingerMs());\n        context.client.poll();\n\n        // It is an invalid request to send an last fetched epoch greater than the current epoch\n        context.deliverRequest(context.fetchRequest(epoch, otherNodeId, oldestSnapshotId.offset() + 1, epoch + 1, 0));\n        context.pollUntilResponse();\n        context.assertSentFetchPartitionResponse(Errors.INVALID_REQUEST, epoch, OptionalInt.of(localId));\n    }\n\n    @Test\n    public void testFetchRequestTruncateToLogStart() throws Exception {\n        int localId = 0;\n        int otherNodeId = localId + 1;\n        int syncNodeId = otherNodeId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, otherNodeId, syncNodeId);\n\n        OffsetAndEpoch oldestSnapshotId = new OffsetAndEpoch(3, 2);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(oldestSnapshotId.epoch() + 2, Arrays.asList(\"d\", \"e\", \"f\"))\n            .withAppendLingerMs(1)\n            .build();\n\n        context.becomeLeader();\n        int epoch = context.currentEpoch();\n        assertEquals(oldestSnapshotId.epoch() + 2 + 1, epoch);\n\n        // Advance the highWatermark\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        // Create a snapshot at the high watermark\n        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(oldestSnapshotId, 0).get()) {\n            assertEquals(oldestSnapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n        context.client.poll();\n\n        // This should truncate to the old snapshot\n        context.deliverRequest(\n            context.fetchRequest(epoch, otherNodeId, oldestSnapshotId.offset() + 1, oldestSnapshotId.epoch() + 1, 0)\n        );\n        context.pollUntilResponse();\n        FetchResponseData.PartitionData partitionResponse = context.assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(localId, partitionResponse.currentLeader().leaderId());\n        assertEquals(oldestSnapshotId.epoch(), partitionResponse.divergingEpoch().epoch());\n        assertEquals(oldestSnapshotId.offset(), partitionResponse.divergingEpoch().endOffset());\n    }\n\n    @Test\n    public void testFetchRequestAtLogStartOffsetWithValidEpoch() throws Exception {\n        int localId = 0;\n        int otherNodeId = localId + 1;\n        int syncNodeId = otherNodeId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, otherNodeId, syncNodeId);\n\n        OffsetAndEpoch oldestSnapshotId = new OffsetAndEpoch(3, 2);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .appendToLog(oldestSnapshotId.epoch() + 2, Arrays.asList(\"g\", \"h\", \"i\"))\n            .withAppendLingerMs(1)\n            .build();\n\n        context.becomeLeader();\n        int epoch = context.currentEpoch();\n        assertEquals(oldestSnapshotId.epoch() + 2 + 1, epoch);\n\n        // Advance the highWatermark\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        // Create a snapshot at the high watermark\n        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(oldestSnapshotId, 0).get()) {\n            assertEquals(oldestSnapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n        context.client.poll();\n\n        // Send fetch request at log start offset with valid last fetched epoch\n        context.deliverRequest(\n            context.fetchRequest(epoch, otherNodeId, oldestSnapshotId.offset(), oldestSnapshotId.epoch(), 0)\n        );\n        context.pollUntilResponse();\n        context.assertSentFetchPartitionResponse(Errors.NONE, epoch, OptionalInt.of(localId));\n    }\n\n    @Test\n    public void testFetchRequestAtLogStartOffsetWithInvalidEpoch() throws Exception {\n        int localId = 0;\n        int otherNodeId = localId + 1;\n        int syncNodeId = otherNodeId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, otherNodeId, syncNodeId);\n\n        OffsetAndEpoch oldestSnapshotId = new OffsetAndEpoch(3, 2);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .appendToLog(oldestSnapshotId.epoch() + 2, Arrays.asList(\"g\", \"h\", \"i\"))\n            .withAppendLingerMs(1)\n            .build();\n\n        context.becomeLeader();\n        int epoch = context.currentEpoch();\n        assertEquals(oldestSnapshotId.epoch() + 2 + 1, epoch);\n\n        // Advance the highWatermark\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        // Create a snapshot at the high watermark\n        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(oldestSnapshotId, 0).get()) {\n            assertEquals(oldestSnapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n        context.log.deleteBeforeSnapshot(oldestSnapshotId);\n        context.client.poll();\n\n        // Send fetch with log start offset and invalid last fetched epoch\n        context.deliverRequest(\n            context.fetchRequest(epoch, otherNodeId, oldestSnapshotId.offset(), oldestSnapshotId.epoch() + 1, 0)\n        );\n        context.pollUntilResponse();\n        FetchResponseData.PartitionData partitionResponse = context.assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(localId, partitionResponse.currentLeader().leaderId());\n        assertEquals(oldestSnapshotId.epoch(), partitionResponse.snapshotId().epoch());\n        assertEquals(oldestSnapshotId.offset(), partitionResponse.snapshotId().endOffset());\n    }\n\n    @Test\n    public void testFetchRequestWithLastFetchedEpochLessThanOldestSnapshot() throws Exception {\n        int localId = 0;\n        int otherNodeId = localId + 1;\n        int syncNodeId = otherNodeId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, otherNodeId, syncNodeId);\n\n        OffsetAndEpoch oldestSnapshotId = new OffsetAndEpoch(3, 2);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"a\", \"b\", \"c\"))\n            .appendToLog(oldestSnapshotId.epoch(), Arrays.asList(\"d\", \"e\", \"f\"))\n            .appendToLog(oldestSnapshotId.epoch() + 2, Arrays.asList(\"g\", \"h\", \"i\"))\n            .withAppendLingerMs(1)\n            .build();\n\n        context.becomeLeader();\n        int epoch = context.currentEpoch();\n        assertEquals(oldestSnapshotId.epoch() + 2 + 1, epoch);\n\n        // Advance the highWatermark\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        // Create a snapshot at the high watermark\n        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(oldestSnapshotId, 0).get()) {\n            assertEquals(oldestSnapshotId, snapshot.snapshotId());\n            snapshot.freeze();\n        }\n        context.client.poll();\n\n        // Send a epoch less than the oldest snapshot\n        context.deliverRequest(\n            context.fetchRequest(\n                epoch,\n                otherNodeId,\n                context.log.endOffset().offset,\n                oldestSnapshotId.epoch() - 1,\n                0\n            )\n        );\n        context.pollUntilResponse();\n        FetchResponseData.PartitionData partitionResponse = context.assertSentFetchPartitionResponse();\n        assertEquals(Errors.NONE, Errors.forCode(partitionResponse.errorCode()));\n        assertEquals(epoch, partitionResponse.currentLeader().leaderEpoch());\n        assertEquals(localId, partitionResponse.currentLeader().leaderId());\n        assertEquals(oldestSnapshotId.epoch(), partitionResponse.snapshotId().epoch());\n        assertEquals(oldestSnapshotId.offset(), partitionResponse.snapshotId().endOffset());\n    }\n\n    @Test\n    public void testFetchSnapshotRequestMissingSnapshot() throws Exception {\n        int localId = 0;\n        int epoch = 2;\n        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n\n        RaftClientTestContext context = RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch,\n                new OffsetAndEpoch(0, 0),\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response = context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n        assertEquals(Errors.SNAPSHOT_NOT_FOUND, Errors.forCode(response.errorCode()));\n    }\n\n    @Test\n    public void testFetchSnapshotRequestUnknownPartition() throws Exception {\n        int localId = 0;\n        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n        int epoch = 2;\n        TopicPartition topicPartition = new TopicPartition(\"unknown\", 0);\n\n        RaftClientTestContext context = RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                topicPartition,\n                epoch,\n                new OffsetAndEpoch(0, 0),\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response = context.assertSentFetchSnapshotResponse(topicPartition).get();\n        assertEquals(Errors.UNKNOWN_TOPIC_OR_PARTITION, Errors.forCode(response.errorCode()));\n    }\n\n    @Test\n    public void testFetchSnapshotRequestAsLeader() throws Exception {\n        int localId = 0;\n        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(1, 1);\n        List<String> records = Arrays.asList(\"foo\", \"bar\");\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Collections.singletonList(\"a\"))\n            .build();\n\n        context.becomeLeader();\n        int epoch = context.currentEpoch();\n\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(snapshotId, 0).get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            snapshot.append(records);\n            snapshot.freeze();\n        }\n\n        RawSnapshotReader snapshot = context.log.readSnapshot(snapshotId).get();\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response = context\n            .assertSentFetchSnapshotResponse(context.metadataPartition)\n            .get();\n\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n        assertEquals(snapshot.sizeInBytes(), response.size());\n        assertEquals(0, response.position());\n        assertEquals(snapshot.sizeInBytes(), response.unalignedRecords().sizeInBytes());\n\n        UnalignedMemoryRecords memoryRecords = (UnalignedMemoryRecords) snapshot.slice(0, Math.toIntExact(snapshot.sizeInBytes()));\n\n        assertEquals(memoryRecords.buffer(), ((UnalignedMemoryRecords) response.unalignedRecords()).buffer());\n    }\n\n    @Test\n    public void testLeaderShouldResignLeadershipIfNotGetFetchSnapshotRequestFromMajorityVoters() throws Exception {\n        int localId = 0;\n        int voter1 = 1;\n        int voter2 = 2;\n        int observerId3 = 3;\n        Set<Integer> voters = Utils.mkSet(localId, voter1, voter2);\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(1, 1);\n        List<String> records = Arrays.asList(\"foo\", \"bar\");\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n                .appendToLog(snapshotId.epoch(), Collections.singletonList(\"a\"))\n                .build();\n\n        int resignLeadershipTimeout = context.checkQuorumTimeoutMs;\n        context.becomeLeader();\n        int epoch = context.currentEpoch();\n\n        FetchSnapshotRequestData voter1FetchSnapshotRequest = fetchSnapshotRequest(\n                context.clusterId.toString(),\n                voter1,\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                0\n        );\n\n        FetchSnapshotRequestData voter2FetchSnapshotRequest = fetchSnapshotRequest(\n                context.clusterId.toString(),\n                voter2,\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                0\n        );\n\n        FetchSnapshotRequestData observerFetchSnapshotRequest = fetchSnapshotRequest(\n                context.clusterId.toString(),\n                observerId3,\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                0\n        );\n\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(snapshotId, 0).get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            snapshot.append(records);\n            snapshot.freeze();\n        }\n\n        // fetch timeout is not expired, the leader should not get resigned\n        context.time.sleep(resignLeadershipTimeout / 2);\n        context.client.poll();\n        assertFalse(context.client.quorum().isResigned());\n\n        // voter1 sends fetchSnapshotRequest, the fetch timer should be reset\n        context.deliverRequest(voter1FetchSnapshotRequest);\n        context.client.poll();\n        context.assertSentFetchSnapshotResponse(context.metadataPartition);\n\n        // Since the fetch timer is reset, the leader should not get resigned\n        context.time.sleep(resignLeadershipTimeout / 2);\n        context.client.poll();\n        assertFalse(context.client.quorum().isResigned());\n\n        // voter2 sends fetchSnapshotRequest, the fetch timer should be reset\n        context.deliverRequest(voter2FetchSnapshotRequest);\n        context.client.poll();\n        context.assertSentFetchSnapshotResponse(context.metadataPartition);\n\n        // Since the fetch timer is reset, the leader should not get resigned\n        context.time.sleep(resignLeadershipTimeout / 2);\n        context.client.poll();\n        assertFalse(context.client.quorum().isResigned());\n\n        // An observer sends fetchSnapshotRequest, but the fetch timer should not be reset.\n        context.deliverRequest(observerFetchSnapshotRequest);\n        context.client.poll();\n        context.assertSentFetchSnapshotResponse(context.metadataPartition);\n\n        // After this sleep, the fetch timeout should expire since we don't receive fetch request from the majority voters within fetchTimeoutMs\n        context.time.sleep(resignLeadershipTimeout / 2);\n        context.client.poll();\n        assertTrue(context.client.quorum().isResigned());\n    }\n\n    @Test\n    public void testPartialFetchSnapshotRequestAsLeader() throws Exception {\n        int localId = 0;\n        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(2, 1);\n        List<String> records = Arrays.asList(\"foo\", \"bar\");\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), records)\n            .build();\n\n        context.becomeLeader();\n        int epoch = context.currentEpoch();\n\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(snapshotId, 0).get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            snapshot.append(records);\n            snapshot.freeze();\n        }\n\n        RawSnapshotReader snapshot = context.log.readSnapshot(snapshotId).get();\n        // Fetch half of the snapshot\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Math.toIntExact(snapshot.sizeInBytes() / 2),\n                0\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response = context\n            .assertSentFetchSnapshotResponse(context.metadataPartition)\n            .get();\n\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n        assertEquals(snapshot.sizeInBytes(), response.size());\n        assertEquals(0, response.position());\n        assertEquals(snapshot.sizeInBytes() / 2, response.unalignedRecords().sizeInBytes());\n\n        UnalignedMemoryRecords memoryRecords = (UnalignedMemoryRecords) snapshot.slice(0, Math.toIntExact(snapshot.sizeInBytes()));\n        ByteBuffer snapshotBuffer = memoryRecords.buffer();\n\n        ByteBuffer responseBuffer = ByteBuffer.allocate(Math.toIntExact(snapshot.sizeInBytes()));\n        responseBuffer.put(((UnalignedMemoryRecords) response.unalignedRecords()).buffer());\n\n        ByteBuffer expectedBytes = snapshotBuffer.duplicate();\n        expectedBytes.limit(Math.toIntExact(snapshot.sizeInBytes() / 2));\n\n        assertEquals(expectedBytes, responseBuffer.duplicate().flip());\n\n        // Fetch the remainder of the snapshot\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                responseBuffer.position()\n            )\n        );\n\n        context.client.poll();\n\n        response = context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n        assertEquals(Errors.NONE, Errors.forCode(response.errorCode()));\n        assertEquals(snapshot.sizeInBytes(), response.size());\n        assertEquals(responseBuffer.position(), response.position());\n        assertEquals(snapshot.sizeInBytes() - (snapshot.sizeInBytes() / 2), response.unalignedRecords().sizeInBytes());\n\n        responseBuffer.put(((UnalignedMemoryRecords) response.unalignedRecords()).buffer());\n        assertEquals(snapshotBuffer, responseBuffer.flip());\n    }\n\n    @Test\n    public void testFetchSnapshotRequestAsFollower() throws IOException {\n        int localId = 0;\n        int leaderId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(0, 0);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response = context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n        assertEquals(Errors.NOT_LEADER_OR_FOLLOWER, Errors.forCode(response.errorCode()));\n        assertEquals(epoch, response.currentLeader().leaderEpoch());\n        assertEquals(leaderId, response.currentLeader().leaderId());\n    }\n\n    @Test\n    public void testFetchSnapshotRequestWithInvalidPosition() throws Exception {\n        int localId = 0;\n        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(1, 1);\n        List<String> records = Arrays.asList(\"foo\", \"bar\");\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .appendToLog(snapshotId.epoch(), Collections.singletonList(\"a\"))\n            .build();\n\n        context.becomeLeader();\n        int epoch = context.currentEpoch();\n\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n\n        try (SnapshotWriter<String> snapshot = context.client.createSnapshot(snapshotId, 0).get()) {\n            assertEquals(snapshotId, snapshot.snapshotId());\n            snapshot.append(records);\n            snapshot.freeze();\n        }\n\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                -1\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response = context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n        assertEquals(Errors.POSITION_OUT_OF_RANGE, Errors.forCode(response.errorCode()));\n        assertEquals(epoch, response.currentLeader().leaderEpoch());\n        assertEquals(localId, response.currentLeader().leaderId());\n\n        RawSnapshotReader snapshot = context.log.readSnapshot(snapshotId).get();\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch,\n                snapshotId,\n                Integer.MAX_VALUE,\n                snapshot.sizeInBytes()\n            )\n        );\n\n        context.client.poll();\n\n        response = context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n        assertEquals(Errors.POSITION_OUT_OF_RANGE, Errors.forCode(response.errorCode()));\n        assertEquals(epoch, response.currentLeader().leaderEpoch());\n        assertEquals(localId, response.currentLeader().leaderId());\n    }\n\n    @Test\n    public void testFetchSnapshotRequestWithOlderEpoch() throws Exception {\n        int localId = 0;\n        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(0, 0);\n\n        RaftClientTestContext context = RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch - 1,\n                snapshotId,\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response = context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n        assertEquals(Errors.FENCED_LEADER_EPOCH, Errors.forCode(response.errorCode()));\n        assertEquals(epoch, response.currentLeader().leaderEpoch());\n        assertEquals(localId, response.currentLeader().leaderId());\n    }\n\n    @Test\n    public void testFetchSnapshotRequestWithNewerEpoch() throws Exception {\n        int localId = 0;\n        Set<Integer> voters = Utils.mkSet(localId, localId + 1);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(0, 0);\n\n        RaftClientTestContext context = RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.metadataPartition,\n                epoch + 1,\n                snapshotId,\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n\n        context.client.poll();\n\n        FetchSnapshotResponseData.PartitionSnapshot response = context.assertSentFetchSnapshotResponse(context.metadataPartition).get();\n        assertEquals(Errors.UNKNOWN_LEADER_EPOCH, Errors.forCode(response.errorCode()));\n        assertEquals(epoch, response.currentLeader().leaderEpoch());\n        assertEquals(localId, response.currentLeader().leaderId());\n    }\n\n    @Test\n    public void testFetchResponseWithInvalidSnapshotId() throws Exception {\n        int localId = 0;\n        int leaderId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n        int epoch = 2;\n        OffsetAndEpoch invalidEpoch = new OffsetAndEpoch(100L, -1);\n        OffsetAndEpoch invalidEndOffset = new OffsetAndEpoch(-1L, 1);\n        int slept = 0;\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, invalidEpoch, 200L)\n        );\n\n        // Handle the invalid response\n        context.client.poll();\n\n        // Expect another fetch request after backoff has expired\n        context.time.sleep(context.retryBackoffMs);\n        slept += context.retryBackoffMs;\n\n        context.pollUntilRequest();\n        fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, invalidEndOffset, 200L)\n        );\n\n        // Handle the invalid response\n        context.client.poll();\n\n        // Expect another fetch request after backoff has expired\n        context.time.sleep(context.retryBackoffMs);\n        slept += context.retryBackoffMs;\n\n        context.pollUntilRequest();\n        fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        // Fetch timer is not reset; sleeping for remainder should transition to candidate\n        context.time.sleep(context.fetchTimeoutMs - slept);\n\n        context.pollUntilRequest();\n\n        context.assertSentVoteRequest(epoch + 1, 0, 0L, 1);\n        context.assertVotedCandidate(epoch + 1, localId);\n    }\n\n    @Test\n    public void testFetchResponseWithSnapshotId() throws Exception {\n        int localId = 0;\n        int leaderId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound snapshotRequest = context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request = assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        List<String> records = Arrays.asList(\"foo\", \"bar\");\n        MemorySnapshotWriter memorySnapshot = new MemorySnapshotWriter(snapshotId);\n        try (SnapshotWriter<String> snapshotWriter = snapshotWriter(context, memorySnapshot)) {\n            snapshotWriter.append(records);\n            snapshotWriter.freeze();\n        }\n\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            fetchSnapshotResponse(\n                context.metadataPartition,\n                epoch,\n                leaderId,\n                snapshotId,\n                memorySnapshot.buffer().remaining(),\n                0L,\n                memorySnapshot.buffer().slice()\n            )\n        );\n\n        context.pollUntilRequest();\n        fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, snapshotId.offset(), snapshotId.epoch());\n\n        // Check that the snapshot was written to the log\n        RawSnapshotReader snapshot = context.log.readSnapshot(snapshotId).get();\n        assertEquals(memorySnapshot.buffer().remaining(), snapshot.sizeInBytes());\n        SnapshotWriterReaderTest.assertSnapshot(Collections.singletonList(records), snapshot);\n\n        // Check that listener was notified of the new snapshot\n        try (SnapshotReader<String> reader = context.listener.drainHandledSnapshot().get()) {\n            assertEquals(snapshotId, reader.snapshotId());\n            SnapshotWriterReaderTest.assertSnapshot(Collections.singletonList(records), reader);\n        }\n    }\n\n    @Test\n    public void testFetchSnapshotResponsePartialData() throws Exception {\n        int localId = 0;\n        int leaderId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound snapshotRequest = context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request = assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        List<String> records = Arrays.asList(\"foo\", \"bar\");\n        MemorySnapshotWriter memorySnapshot = new MemorySnapshotWriter(snapshotId);\n        try (SnapshotWriter<String> snapshotWriter = snapshotWriter(context, memorySnapshot)) {\n            snapshotWriter.append(records);\n            snapshotWriter.freeze();\n        }\n\n        ByteBuffer sendingBuffer = memorySnapshot.buffer().slice();\n        sendingBuffer.limit(sendingBuffer.limit() / 2);\n\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            fetchSnapshotResponse(\n                context.metadataPartition,\n                epoch,\n                leaderId,\n                snapshotId,\n                memorySnapshot.buffer().remaining(),\n                0L,\n                sendingBuffer\n            )\n        );\n\n        context.pollUntilRequest();\n        snapshotRequest = context.assertSentFetchSnapshotRequest();\n        request = assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(sendingBuffer.limit(), request.position());\n\n        sendingBuffer = memorySnapshot.buffer().slice();\n        sendingBuffer.position(Math.toIntExact(request.position()));\n\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            fetchSnapshotResponse(\n                context.metadataPartition,\n                epoch,\n                leaderId,\n                snapshotId,\n                memorySnapshot.buffer().remaining(),\n                request.position(),\n                sendingBuffer\n            )\n        );\n\n        context.pollUntilRequest();\n        fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, snapshotId.offset(), snapshotId.epoch());\n\n        // Check that the snapshot was written to the log\n        RawSnapshotReader snapshot = context.log.readSnapshot(snapshotId).get();\n        assertEquals(memorySnapshot.buffer().remaining(), snapshot.sizeInBytes());\n        SnapshotWriterReaderTest.assertSnapshot(Collections.singletonList(records), snapshot);\n\n        // Check that listener was notified of the new snapshot\n        try (SnapshotReader<String> reader = context.listener.drainHandledSnapshot().get()) {\n            assertEquals(snapshotId, reader.snapshotId());\n            SnapshotWriterReaderTest.assertSnapshot(Collections.singletonList(records), reader);\n        }\n    }\n\n    @Test\n    public void testFetchSnapshotResponseMissingSnapshot() throws Exception {\n        int localId = 0;\n        int leaderId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound snapshotRequest = context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request = assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        // Reply with a snapshot not found error\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            FetchSnapshotResponse.singleton(\n                context.metadataPartition,\n                responsePartitionSnapshot -> {\n                    responsePartitionSnapshot\n                        .currentLeader()\n                        .setLeaderEpoch(epoch)\n                        .setLeaderId(leaderId);\n\n                    return responsePartitionSnapshot\n                        .setErrorCode(Errors.SNAPSHOT_NOT_FOUND.code());\n                }\n            )\n        );\n\n        context.pollUntilRequest();\n        fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n    }\n\n    @Test\n    public void testFetchSnapshotResponseFromNewerEpochNotLeader() throws Exception {\n        int localId = 0;\n        int firstLeaderId = localId + 1;\n        int secondLeaderId = firstLeaderId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, firstLeaderId, secondLeaderId);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, firstLeaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, firstLeaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound snapshotRequest = context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request = assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        // Reply with new leader response\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            FetchSnapshotResponse.singleton(\n                context.metadataPartition,\n                responsePartitionSnapshot -> {\n                    responsePartitionSnapshot\n                        .currentLeader()\n                        .setLeaderEpoch(epoch + 1)\n                        .setLeaderId(secondLeaderId);\n\n                    return responsePartitionSnapshot\n                        .setErrorCode(Errors.FENCED_LEADER_EPOCH.code());\n                }\n            )\n        );\n\n        context.pollUntilRequest();\n        fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch + 1, 0L, 0);\n    }\n\n    @Test\n    public void testFetchSnapshotResponseFromNewerEpochLeader() throws Exception {\n        int localId = 0;\n        int leaderId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound snapshotRequest = context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request = assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        // Reply with new leader epoch\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            FetchSnapshotResponse.singleton(\n                context.metadataPartition,\n                responsePartitionSnapshot -> {\n                    responsePartitionSnapshot\n                        .currentLeader()\n                        .setLeaderEpoch(epoch + 1)\n                        .setLeaderId(leaderId);\n\n                    return responsePartitionSnapshot\n                        .setErrorCode(Errors.FENCED_LEADER_EPOCH.code());\n                }\n            )\n        );\n\n        context.pollUntilRequest();\n        fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch + 1, 0L, 0);\n    }\n\n    @Test\n    public void testFetchSnapshotResponseFromOlderEpoch() throws Exception {\n        int localId = 0;\n        int leaderId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound snapshotRequest = context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request = assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        // Reply with unknown leader epoch\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            FetchSnapshotResponse.singleton(\n                context.metadataPartition,\n                responsePartitionSnapshot -> {\n                    responsePartitionSnapshot\n                        .currentLeader()\n                        .setLeaderEpoch(epoch - 1)\n                        .setLeaderId(leaderId + 1);\n\n                    return responsePartitionSnapshot\n                        .setErrorCode(Errors.UNKNOWN_LEADER_EPOCH.code());\n                }\n            )\n        );\n\n        context.pollUntilRequest();\n\n        // Follower should resend the fetch snapshot request\n        snapshotRequest = context.assertSentFetchSnapshotRequest();\n        request = assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n    }\n\n    @Test\n    public void testFetchSnapshotResponseWithInvalidId() throws Exception {\n        int localId = 0;\n        int leaderId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound snapshotRequest = context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request = assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        // Reply with an invalid snapshot id endOffset\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            FetchSnapshotResponse.singleton(\n                context.metadataPartition,\n                responsePartitionSnapshot -> {\n                    responsePartitionSnapshot\n                        .currentLeader()\n                        .setLeaderEpoch(epoch)\n                        .setLeaderId(leaderId);\n\n                    responsePartitionSnapshot\n                        .snapshotId()\n                        .setEndOffset(-1)\n                        .setEpoch(snapshotId.epoch());\n\n                    return responsePartitionSnapshot;\n                }\n            )\n        );\n\n        context.pollUntilRequest();\n\n        // Follower should send a fetch request\n        fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n\n        snapshotRequest = context.assertSentFetchSnapshotRequest();\n        request = assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        // Reply with an invalid snapshot id epoch\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            FetchSnapshotResponse.singleton(\n                context.metadataPartition,\n                responsePartitionSnapshot -> {\n                    responsePartitionSnapshot\n                        .currentLeader()\n                        .setLeaderEpoch(epoch)\n                        .setLeaderId(leaderId);\n\n                    responsePartitionSnapshot\n                        .snapshotId()\n                        .setEndOffset(snapshotId.offset())\n                        .setEpoch(-1);\n\n                    return responsePartitionSnapshot;\n                }\n            )\n        );\n\n        context.pollUntilRequest();\n\n        // Follower should send a fetch request\n        fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n    }\n\n    @Test\n    public void testFetchSnapshotResponseToNotFollower() throws Exception {\n        int localId = 0;\n        int leaderId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, leaderId);\n        int epoch = 2;\n        OffsetAndEpoch snapshotId = new OffsetAndEpoch(100L, 1);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n            .withElectedLeader(epoch, leaderId)\n            .build();\n\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = context.assertSentFetchRequest();\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            snapshotFetchResponse(context.metadataPartition, context.metadataTopicId, epoch, leaderId, snapshotId, 200L)\n        );\n\n        context.pollUntilRequest();\n\n        RaftRequest.Outbound snapshotRequest = context.assertSentFetchSnapshotRequest();\n        FetchSnapshotRequestData.PartitionSnapshot request = assertFetchSnapshotRequest(\n                snapshotRequest,\n                context.metadataPartition,\n                localId,\n                Integer.MAX_VALUE\n        ).get();\n        assertEquals(snapshotId.offset(), request.snapshotId().endOffset());\n        assertEquals(snapshotId.epoch(), request.snapshotId().epoch());\n        assertEquals(0, request.position());\n\n        // Sleeping for fetch timeout should transition to candidate\n        context.time.sleep(context.fetchTimeoutMs);\n\n        context.pollUntilRequest();\n\n        context.assertSentVoteRequest(epoch + 1, 0, 0L, 1);\n        context.assertVotedCandidate(epoch + 1, localId);\n\n        // Send the response late\n        context.deliverResponse(\n            snapshotRequest.correlationId(),\n            snapshotRequest.destination(),\n            FetchSnapshotResponse.singleton(\n                context.metadataPartition,\n                responsePartitionSnapshot -> {\n                    responsePartitionSnapshot\n                        .currentLeader()\n                        .setLeaderEpoch(epoch)\n                        .setLeaderId(leaderId);\n\n                    responsePartitionSnapshot\n                        .snapshotId()\n                        .setEndOffset(snapshotId.offset())\n                        .setEpoch(snapshotId.epoch());\n\n                    return responsePartitionSnapshot;\n                }\n            )\n        );\n\n        // Assert that the response is ignored and the replicas stays as a candidate\n        context.client.poll();\n        context.assertVotedCandidate(epoch + 1, localId);\n    }\n\n    @Test\n    public void testFetchSnapshotRequestClusterIdValidation() throws Exception {\n        int localId = 0;\n        int otherNodeId = 1;\n        int epoch = 5;\n        Set<Integer> voters = Utils.mkSet(localId, otherNodeId);\n\n        RaftClientTestContext context = RaftClientTestContext.initializeAsLeader(localId, voters, epoch);\n\n        // null cluster id is accepted\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                context.clusterId.toString(),\n                otherNodeId,\n                context.metadataPartition,\n                epoch,\n                new OffsetAndEpoch(0, 0),\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n        context.pollUntilResponse();\n        context.assertSentFetchSnapshotResponse(context.metadataPartition);\n\n        // null cluster id is accepted\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                null,\n                otherNodeId,\n                context.metadataPartition,\n                epoch,\n                new OffsetAndEpoch(0, 0),\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n        context.pollUntilResponse();\n        context.assertSentFetchSnapshotResponse(context.metadataPartition);\n\n        // empty cluster id is rejected\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                \"\",\n                otherNodeId,\n                context.metadataPartition,\n                epoch,\n                new OffsetAndEpoch(0, 0),\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n        context.pollUntilResponse();\n        context.assertSentFetchSnapshotResponse(Errors.INCONSISTENT_CLUSTER_ID);\n\n        // invalid cluster id is rejected\n        context.deliverRequest(\n            fetchSnapshotRequest(\n                \"invalid-uuid\",\n                otherNodeId,\n                context.metadataPartition,\n                epoch,\n                new OffsetAndEpoch(0, 0),\n                Integer.MAX_VALUE,\n                0\n            )\n        );\n        context.pollUntilResponse();\n        context.assertSentFetchSnapshotResponse(Errors.INCONSISTENT_CLUSTER_ID);\n    }\n\n    @Test\n    public void testCreateSnapshotAsLeaderWithInvalidSnapshotId() throws Exception {\n        int localId = 0;\n        int otherNodeId = localId + 1;\n        Set<Integer> voters = Utils.mkSet(localId, otherNodeId);\n        int epoch = 2;\n\n        List<String> appendRecords = Arrays.asList(\"a\", \"b\", \"c\");\n        OffsetAndEpoch invalidSnapshotId1 = new OffsetAndEpoch(4, epoch);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n                .appendToLog(epoch, appendRecords)\n                .withAppendLingerMs(1)\n                .build();\n\n        context.becomeLeader();\n        int currentEpoch = context.currentEpoch();\n\n        // When leader creating snapshot:\n        // 1.1 high watermark cannot be empty\n        assertEquals(OptionalLong.empty(), context.client.highWatermark());\n        assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId1, 0));\n\n        // 1.2 high watermark must larger than or equal to the snapshotId's endOffset\n        context.advanceLocalLeaderHighWatermarkToLogEndOffset();\n        // append some more records to make the LEO > high watermark\n        List<String> newRecords = Arrays.asList(\"d\", \"e\", \"f\");\n        context.client.scheduleAppend(currentEpoch, newRecords);\n        context.time.sleep(context.appendLingerMs());\n        context.client.poll();\n        assertEquals(context.log.endOffset().offset, context.client.highWatermark().getAsLong() + newRecords.size());\n\n        OffsetAndEpoch invalidSnapshotId2 = new OffsetAndEpoch(context.client.highWatermark().getAsLong() + 2, currentEpoch);\n        assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId2, 0));\n\n        // 2 the quorum epoch must larger than or equal to the snapshotId's epoch\n        OffsetAndEpoch invalidSnapshotId3 = new OffsetAndEpoch(context.client.highWatermark().getAsLong(), currentEpoch + 1);\n        assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId3, 0));\n\n        // 3 the snapshotId should be validated against endOffsetForEpoch\n        OffsetAndEpoch endOffsetForEpoch = context.log.endOffsetForEpoch(epoch);\n        assertEquals(epoch, endOffsetForEpoch.epoch());\n        OffsetAndEpoch invalidSnapshotId4 = new OffsetAndEpoch(endOffsetForEpoch.offset() + 2, epoch);\n        assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId4, 0));\n    }\n\n    @Test\n    public void testCreateSnapshotAsFollowerWithInvalidSnapshotId() throws Exception {\n        int localId = 0;\n        int leaderId = 1;\n        int otherFollowerId = 2;\n        int epoch = 5;\n        Set<Integer> voters = Utils.mkSet(localId, leaderId, otherFollowerId);\n\n        RaftClientTestContext context = new RaftClientTestContext.Builder(localId, voters)\n                .withElectedLeader(epoch, leaderId)\n                .build();\n        context.assertElectedLeader(epoch, leaderId);\n\n        // When follower creating snapshot:\n        // 1) The high watermark cannot be empty\n        assertEquals(OptionalLong.empty(), context.client.highWatermark());\n        OffsetAndEpoch invalidSnapshotId1 = new OffsetAndEpoch(1, 0);\n        assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId1, 0));\n\n        // Poll for our first fetch request\n        context.pollUntilRequest();\n        RaftRequest.Outbound fetchRequest = context.assertSentFetchRequest();\n        assertTrue(voters.contains(fetchRequest.destination().id()));\n        context.assertFetchRequestData(fetchRequest, epoch, 0L, 0);\n\n        // The response does not advance the high watermark\n        List<String> records1 = Arrays.asList(\"a\", \"b\", \"c\");\n        MemoryRecords batch1 = context.buildBatch(0L, 3, records1);\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            context.fetchResponse(epoch, leaderId, batch1, 0L, Errors.NONE)\n        );\n        context.client.poll();\n\n        // 2) The high watermark must be larger than or equal to the snapshotId's endOffset\n        int currentEpoch = context.currentEpoch();\n        OffsetAndEpoch invalidSnapshotId2 = new OffsetAndEpoch(context.client.highWatermark().getAsLong() + 1, currentEpoch);\n        assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId2, 0));\n\n        // 3) The quorum epoch must be larger than or equal to the snapshotId's epoch\n        OffsetAndEpoch invalidSnapshotId3 = new OffsetAndEpoch(context.client.highWatermark().getAsLong() + 1, currentEpoch + 1);\n        assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId3, 0));\n\n        // The high watermark advances to be larger than log.endOffsetForEpoch(3), to test the case 3\n        context.pollUntilRequest();\n        fetchRequest = context.assertSentFetchRequest();\n        assertTrue(voters.contains(fetchRequest.destination().id()));\n        context.assertFetchRequestData(fetchRequest, epoch, 3L, 3);\n\n        List<String> records2 = Arrays.asList(\"d\", \"e\", \"f\");\n        MemoryRecords batch2 = context.buildBatch(3L, 4, records2);\n        context.deliverResponse(\n            fetchRequest.correlationId(),\n            fetchRequest.destination(),\n            context.fetchResponse(epoch, leaderId, batch2, 6L, Errors.NONE)\n        );\n        context.client.poll();\n        assertEquals(6L, context.client.highWatermark().getAsLong());\n\n        // 4) The snapshotId should be validated against endOffsetForEpoch\n        OffsetAndEpoch endOffsetForEpoch = context.log.endOffsetForEpoch(3);\n        assertEquals(3, endOffsetForEpoch.epoch());\n        OffsetAndEpoch invalidSnapshotId4 = new OffsetAndEpoch(endOffsetForEpoch.offset() + 1, epoch);\n        assertThrows(IllegalArgumentException.class, () -> context.client.createSnapshot(invalidSnapshotId4, 0));\n    }\n\n    private static FetchSnapshotRequestData fetchSnapshotRequest(\n            TopicPartition topicPartition,\n            int epoch,\n            OffsetAndEpoch offsetAndEpoch,\n            int maxBytes,\n            long position\n    ) {\n        return fetchSnapshotRequest(null, -1, topicPartition, epoch, offsetAndEpoch, maxBytes, position);\n    }\n\n    private static FetchSnapshotRequestData fetchSnapshotRequest(\n        String clusterId,\n        int replicaId,\n        TopicPartition topicPartition,\n        int epoch,\n        OffsetAndEpoch offsetAndEpoch,\n        int maxBytes,\n        long position\n    ) {\n        FetchSnapshotRequestData.SnapshotId snapshotId = new FetchSnapshotRequestData.SnapshotId()\n            .setEndOffset(offsetAndEpoch.offset())\n            .setEpoch(offsetAndEpoch.epoch());\n\n        FetchSnapshotRequestData request = FetchSnapshotRequest.singleton(\n            clusterId,\n            replicaId,\n            topicPartition,\n            snapshotPartition -> snapshotPartition\n                .setCurrentLeaderEpoch(epoch)\n                .setSnapshotId(snapshotId)\n                .setPosition(position)\n        );\n\n        return request.setMaxBytes(maxBytes);\n    }\n\n    private static FetchSnapshotResponseData fetchSnapshotResponse(\n        TopicPartition topicPartition,\n        int leaderEpoch,\n        int leaderId,\n        OffsetAndEpoch snapshotId,\n        long size,\n        long position,\n        ByteBuffer buffer\n    ) {\n        return FetchSnapshotResponse.singleton(\n            topicPartition,\n            partitionSnapshot -> {\n                partitionSnapshot.currentLeader()\n                    .setLeaderEpoch(leaderEpoch)\n                    .setLeaderId(leaderId);\n\n                partitionSnapshot.snapshotId()\n                    .setEndOffset(snapshotId.offset())\n                    .setEpoch(snapshotId.epoch());\n\n                return partitionSnapshot\n                    .setSize(size)\n                    .setPosition(position)\n                    .setUnalignedRecords(MemoryRecords.readableRecords(buffer.slice()));\n            }\n        );\n    }\n\n    private static FetchResponseData snapshotFetchResponse(\n        TopicPartition topicPartition,\n        Uuid topicId,\n        int epoch,\n        int leaderId,\n        OffsetAndEpoch snapshotId,\n        long highWatermark\n    ) {\n        return RaftUtil.singletonFetchResponse(topicPartition, topicId, Errors.NONE, partitionData -> {\n            partitionData.setHighWatermark(highWatermark);\n\n            partitionData.currentLeader()\n                .setLeaderEpoch(epoch)\n                .setLeaderId(leaderId);\n\n            partitionData.snapshotId()\n                .setEpoch(snapshotId.epoch())\n                .setEndOffset(snapshotId.offset());\n        });\n    }\n\n    private static Optional<FetchSnapshotRequestData.PartitionSnapshot> assertFetchSnapshotRequest(\n        RaftRequest.Outbound request,\n        TopicPartition topicPartition,\n        int replicaId,\n        int maxBytes\n    ) {\n        assertInstanceOf(FetchSnapshotRequestData.class, request.data());\n\n        FetchSnapshotRequestData data = (FetchSnapshotRequestData) request.data();\n\n        assertEquals(replicaId, data.replicaId());\n        assertEquals(maxBytes, data.maxBytes());\n\n        return FetchSnapshotRequest.forTopicPartition(data, topicPartition);\n    }\n\n    private static SnapshotWriter<String> snapshotWriter(RaftClientTestContext context, RawSnapshotWriter snapshot) {\n        return new RecordsSnapshotWriter.Builder()\n            .setTime(context.time)\n            .setRawSnapshotWriter(snapshot)\n            .build(new StringSerde());\n    }\n\n    private static final class MemorySnapshotWriter implements RawSnapshotWriter {\n        private final OffsetAndEpoch snapshotId;\n        private final AtomicLong frozenPosition;\n        private ByteBuffer data;\n\n        public MemorySnapshotWriter(OffsetAndEpoch snapshotId) {\n            this.snapshotId = snapshotId;\n            this.data = ByteBuffer.allocate(0);\n            this.frozenPosition = new AtomicLong(-1L);\n        }\n\n        @Override\n        public OffsetAndEpoch snapshotId() {\n            return snapshotId;\n        }\n\n        @Override\n        public long sizeInBytes() {\n            long position = frozenPosition.get();\n            return (position < 0) ? data.position() : position;\n        }\n\n        @Override\n        public void append(UnalignedMemoryRecords records) {\n            if (isFrozen()) {\n                throw new RuntimeException(\"Snapshot is already frozen \" + snapshotId);\n            }\n            append(records.buffer());\n        }\n\n        @Override\n        public void append(MemoryRecords records) {\n            if (isFrozen()) {\n                throw new RuntimeException(\"Snapshot is already frozen \" + snapshotId);\n            }\n            append(records.buffer());\n        }\n\n        private void append(ByteBuffer buffer) {\n            if (!(data.remaining() >= buffer.remaining())) {\n                ByteBuffer old = data;\n                old.flip();\n\n                int newSize = Math.max(data.capacity() * 2, data.capacity() + buffer.remaining());\n                data = ByteBuffer.allocate(newSize);\n\n                data.put(old);\n            }\n            data.put(buffer);\n        }\n\n        @Override\n        public boolean isFrozen() {\n            return frozenPosition.get() >= 0;\n        }\n\n        @Override\n        public void freeze() {\n            if (!frozenPosition.compareAndSet(-1L, data.position())) {\n                throw new RuntimeException(\"Snapshot is already frozen \" + snapshotId);\n            }\n            data.flip();\n        }\n\n        @Override\n        public void close() {}\n\n        public ByteBuffer buffer() {\n            return data;\n        }\n    }\n}"
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [],
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "60114a46a775e597410131dd67ac236d62f57b0c",
        "url": "https://github.com/apache/kafka/commit/60114a46a775e597410131dd67ac236d62f57b0c",
        "move_method_refactoring": {
            "type": "Move Method",
            "description": "Move Method private decValue(key String, value Integer) : Integer from class org.apache.kafka.coordinator.group.consumer.ConsumerGroup to public decValue(key String, value Integer) : Integer from class org.apache.kafka.coordinator.group.Utils",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/consumer/ConsumerGroup.java",
                    "startLine": 1256,
                    "endLine": 1263,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "original method declaration",
                    "codeElement": "private decValue(key String, value Integer) : Integer"
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/Utils.java",
                    "startLine": 86,
                    "endLine": 93,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "moved method declaration",
                    "codeElement": "public decValue(key String, value Integer) : Integer"
                }
            ],
            "isStatic": true
        },
        "telemetry": {
            "id": "d3825b5f-6255-4eb7-87ca-97981e6bebb8",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1374,
                "lineStart": 67,
                "lineEnd": 1440,
                "bodyLineStart": 67,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/consumer/ConsumerGroup.java",
                "sourceCode": "/**\n * A Consumer Group. All the metadata in this class are backed by\n * records in the __consumer_offsets partitions.\n */\npublic class ConsumerGroup implements Group {\n\n    public enum ConsumerGroupState {\n        EMPTY(\"Empty\"),\n        ASSIGNING(\"Assigning\"),\n        RECONCILING(\"Reconciling\"),\n        STABLE(\"Stable\"),\n        DEAD(\"Dead\");\n\n        private final String name;\n\n        private final String lowerCaseName;\n\n        ConsumerGroupState(String name) {\n            this.name = name;\n            this.lowerCaseName = name.toLowerCase(Locale.ROOT);\n        }\n\n        @Override\n        public String toString() {\n            return name;\n        }\n\n        public String toLowerCaseString() {\n            return lowerCaseName;\n        }\n    }\n\n    public static class DeadlineAndEpoch {\n        static final DeadlineAndEpoch EMPTY = new DeadlineAndEpoch(0L, 0);\n\n        public final long deadlineMs;\n        public final int epoch;\n\n        DeadlineAndEpoch(long deadlineMs, int epoch) {\n            this.deadlineMs = deadlineMs;\n            this.epoch = epoch;\n        }\n    }\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The group id.\n     */\n    private final String groupId;\n\n    /**\n     * The group state.\n     */\n    private final TimelineObject<ConsumerGroupState> state;\n\n    /**\n     * The group epoch. The epoch is incremented whenever the subscriptions\n     * are updated and it will trigger the computation of a new assignment\n     * for the group.\n     */\n    private final TimelineInteger groupEpoch;\n\n    /**\n     * The group members.\n     */\n    private final TimelineHashMap<String, ConsumerGroupMember> members;\n\n    /**\n     * The static group members.\n     */\n    private final TimelineHashMap<String, String> staticMembers;\n\n    /**\n     * The number of members supporting each server assignor name.\n     */\n    private final TimelineHashMap<String, Integer> serverAssignors;\n\n    /**\n     * The number of subscribers per topic.\n     */\n    private final TimelineHashMap<String, Integer> subscribedTopicNames;\n\n    /**\n     * The metadata associated with each subscribed topic name.\n     */\n    private final TimelineHashMap<String, TopicMetadata> subscribedTopicMetadata;\n\n    /**\n     * The consumer group's subscription type.\n     * This value is set to Homogeneous by default.\n     */\n    private final TimelineObject<SubscriptionType> subscriptionType;\n\n    /**\n     * The target assignment epoch. An assignment epoch smaller than the group epoch\n     * means that a new assignment is required. The assignment epoch is updated when\n     * a new assignment is installed.\n     */\n    private final TimelineInteger targetAssignmentEpoch;\n\n    /**\n     * The target assignment per member id.\n     */\n    private final TimelineHashMap<String, Assignment> targetAssignment;\n\n    /**\n     * Reverse lookup map representing topic partitions with\n     * their current member assignments.\n     */\n    private final TimelineHashMap<Uuid, TimelineHashMap<Integer, String>> invertedTargetAssignment;\n\n    /**\n     * The current partition epoch maps each topic-partitions to their current epoch where\n     * the epoch is the epoch of their owners. When a member revokes a partition, it removes\n     * its epochs from this map. When a member gets a partition, it adds its epochs to this map.\n     */\n    private final TimelineHashMap<Uuid, TimelineHashMap<Integer, Integer>> currentPartitionEpoch;\n\n    /**\n     * The coordinator metrics.\n     */\n    private final GroupCoordinatorMetricsShard metrics;\n\n    /**\n     * The metadata refresh deadline. It consists of a timestamp in milliseconds together with\n     * the group epoch at the time of setting it. The metadata refresh time is considered as a\n     * soft state (read that it is not stored in a timeline data structure). It is like this\n     * because it is not persisted to the log. The group epoch is here to ensure that the\n     * metadata refresh deadline is invalidated if the group epoch does not correspond to\n     * the current group epoch. This can happen if the metadata refresh deadline is updated\n     * after having refreshed the metadata but the write operation failed. In this case, the\n     * time is not automatically rolled back.\n     */\n    private DeadlineAndEpoch metadataRefreshDeadline = DeadlineAndEpoch.EMPTY;\n\n    /**\n     * The number of members that use the classic protocol.\n     */\n    private final TimelineInteger numClassicProtocolMembers;\n\n    /**\n     * Map of protocol names to the number of members that use classic protocol and support them.\n     */\n    private final TimelineHashMap<String, Integer> classicProtocolMembersSupportedProtocols;\n\n    public ConsumerGroup(\n        SnapshotRegistry snapshotRegistry,\n        String groupId,\n        GroupCoordinatorMetricsShard metrics\n    ) {\n        this.snapshotRegistry = Objects.requireNonNull(snapshotRegistry);\n        this.groupId = Objects.requireNonNull(groupId);\n        this.state = new TimelineObject<>(snapshotRegistry, EMPTY);\n        this.groupEpoch = new TimelineInteger(snapshotRegistry);\n        this.members = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.staticMembers = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.serverAssignors = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.subscribedTopicNames = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.subscribedTopicMetadata = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.subscriptionType = new TimelineObject<>(snapshotRegistry, HOMOGENEOUS);\n        this.targetAssignmentEpoch = new TimelineInteger(snapshotRegistry);\n        this.targetAssignment = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.invertedTargetAssignment = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.currentPartitionEpoch = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.metrics = Objects.requireNonNull(metrics);\n        this.numClassicProtocolMembers = new TimelineInteger(snapshotRegistry);\n        this.classicProtocolMembersSupportedProtocols = new TimelineHashMap<>(snapshotRegistry, 0);\n    }\n\n    /**\n     * @return The group type (Consumer).\n     */\n    @Override\n    public GroupType type() {\n        return GroupType.CONSUMER;\n    }\n\n    /**\n     * @return The current state as a String.\n     */\n    @Override\n    public String stateAsString() {\n        return state.get().toString();\n    }\n\n    /**\n     * @return The current state as a String with given committedOffset.\n     */\n    public String stateAsString(long committedOffset) {\n        return state.get(committedOffset).toString();\n    }\n\n    /**\n     * @return the group formatted as a list group response based on the committed offset.\n     */\n    public ListGroupsResponseData.ListedGroup asListedGroup(long committedOffset) {\n        return new ListGroupsResponseData.ListedGroup()\n            .setGroupId(groupId)\n            .setProtocolType(ConsumerProtocol.PROTOCOL_TYPE)\n            .setGroupState(state.get(committedOffset).toString())\n            .setGroupType(type().toString());\n    }\n\n    /**\n     * @return The group id.\n     */\n    @Override\n    public String groupId() {\n        return groupId;\n    }\n\n    /**\n     * @return The current state.\n     */\n    public ConsumerGroupState state() {\n        return state.get();\n    }\n\n    /**\n     * @return The current state based on committed offset.\n     */\n    public ConsumerGroupState state(long committedOffset) {\n        return state.get(committedOffset);\n    }\n\n    /**\n     * @return The group epoch.\n     */\n    public int groupEpoch() {\n        return groupEpoch.get();\n    }\n\n    /**\n     * Sets the group epoch.\n     *\n     * @param groupEpoch The new group epoch.\n     */\n    public void setGroupEpoch(int groupEpoch) {\n        this.groupEpoch.set(groupEpoch);\n        maybeUpdateGroupState();\n    }\n\n    /**\n     * @return The target assignment epoch.\n     */\n    public int assignmentEpoch() {\n        return targetAssignmentEpoch.get();\n    }\n\n    /**\n     * Sets the assignment epoch.\n     *\n     * @param targetAssignmentEpoch The new assignment epoch.\n     */\n    public void setTargetAssignmentEpoch(int targetAssignmentEpoch) {\n        this.targetAssignmentEpoch.set(targetAssignmentEpoch);\n        maybeUpdateGroupState();\n    }\n\n    /**\n     * Sets the number of members using the classic protocol.\n     *\n     * @param numClassicProtocolMembers The new NumClassicProtocolMembers.\n     */\n    public void setNumClassicProtocolMembers(int numClassicProtocolMembers) {\n        this.numClassicProtocolMembers.set(numClassicProtocolMembers);\n    }\n\n    /**\n     * Get member id of a static member that matches the given group\n     * instance id.\n     *\n     * @param groupInstanceId The group instance id.\n     *\n     * @return The member id corresponding to the given instance id or null if it does not exist\n     */\n    public String staticMemberId(String groupInstanceId) {\n        return staticMembers.get(groupInstanceId);\n    }\n\n    /**\n     * Gets or creates a new member but without adding it to the group. Adding a member\n     * is done via the {@link ConsumerGroup#updateMember(ConsumerGroupMember)} method.\n     *\n     * @param memberId          The member id.\n     * @param createIfNotExists Booleans indicating whether the member must be\n     *                          created if it does not exist.\n     *\n     * @return A ConsumerGroupMember.\n     */\n    public ConsumerGroupMember getOrMaybeCreateMember(\n        String memberId,\n        boolean createIfNotExists\n    ) {\n        ConsumerGroupMember member = members.get(memberId);\n        if (member != null) return member;\n\n        if (!createIfNotExists) {\n            throw new UnknownMemberIdException(\n                String.format(\"Member %s is not a member of group %s.\", memberId, groupId)\n            );\n        }\n\n        return new ConsumerGroupMember.Builder(memberId).build();\n    }\n\n    /**\n     * Gets a static member.\n     *\n     * @param instanceId The group instance id.\n     *\n     * @return The member corresponding to the given instance id or null if it does not exist\n     */\n    public ConsumerGroupMember staticMember(String instanceId) {\n        String existingMemberId = staticMemberId(instanceId);\n        return existingMemberId == null ? null : getOrMaybeCreateMember(existingMemberId, false);\n    }\n\n    /**\n     * Adds or updates the member.\n     *\n     * @param newMember The new member state.\n     */\n    public void updateMember(ConsumerGroupMember newMember) {\n        if (newMember == null) {\n            throw new IllegalArgumentException(\"newMember cannot be null.\");\n        }\n        ConsumerGroupMember oldMember = members.put(newMember.memberId(), newMember);\n        maybeUpdateSubscribedTopicNamesAndGroupSubscriptionType(oldMember, newMember);\n        maybeUpdateServerAssignors(oldMember, newMember);\n        maybeUpdatePartitionEpoch(oldMember, newMember);\n        updateStaticMember(newMember);\n        maybeUpdateGroupState();\n        maybeUpdateNumClassicProtocolMembers(oldMember, newMember);\n        maybeUpdateClassicProtocolMembersSupportedProtocols(oldMember, newMember);\n    }\n\n    /**\n     * Updates the member id stored against the instance id if the member is a static member.\n     *\n     * @param newMember The new member state.\n     */\n    private void updateStaticMember(ConsumerGroupMember newMember) {\n        if (newMember.instanceId() != null) {\n            staticMembers.put(newMember.instanceId(), newMember.memberId());\n        }\n    }\n\n    /**\n     * Remove the member from the group.\n     *\n     * @param memberId The member id to remove.\n     */\n    public void removeMember(String memberId) {\n        ConsumerGroupMember oldMember = members.remove(memberId);\n        maybeUpdateSubscribedTopicNamesAndGroupSubscriptionType(oldMember, null);\n        maybeUpdateServerAssignors(oldMember, null);\n        maybeRemovePartitionEpoch(oldMember);\n        removeStaticMember(oldMember);\n        maybeUpdateGroupState();\n        maybeUpdateNumClassicProtocolMembers(oldMember, null);\n        maybeUpdateClassicProtocolMembersSupportedProtocols(oldMember, null);\n    }\n\n    /**\n     * Remove the static member mapping if the removed member is static.\n     *\n     * @param oldMember The member to remove.\n     */\n    private void removeStaticMember(ConsumerGroupMember oldMember) {\n        if (oldMember.instanceId() != null) {\n            staticMembers.remove(oldMember.instanceId());\n        }\n    }\n\n    /**\n     * Returns true if the member exists.\n     *\n     * @param memberId The member id.\n     *\n     * @return A boolean indicating whether the member exists or not.\n     */\n    public boolean hasMember(String memberId) {\n        return members.containsKey(memberId);\n    }\n\n    /**\n     * @return The number of members.\n     */\n    public int numMembers() {\n        return members.size();\n    }\n\n    /**\n     * @return The number of members that use the classic protocol.\n     */\n    public int numClassicProtocolMembers() {\n        return numClassicProtocolMembers.get();\n    }\n\n    /**\n     * @return The map of the protocol name and the number of members using the classic protocol that support it.\n     */\n    public Map<String, Integer> classicMembersSupportedProtocols() {\n        return Collections.unmodifiableMap(classicProtocolMembersSupportedProtocols);\n    }\n\n    /**\n     * @return An immutable Map containing all the members keyed by their id.\n     */\n    public Map<String, ConsumerGroupMember> members() {\n        return Collections.unmodifiableMap(members);\n    }\n\n    /**\n     * @return An immutable Map containing all the static members keyed by instance id.\n     */\n    public Map<String, String> staticMembers() {\n        return Collections.unmodifiableMap(staticMembers);\n    }\n\n    /**\n     * @return An immutable map containing all the subscribed topic names\n     *         with the subscribers counts per topic.\n     */\n    public Map<String, Integer> subscribedTopicNames() {\n        return Collections.unmodifiableMap(subscribedTopicNames);\n    }\n\n    /**\n     * Returns true if the consumer group is actively subscribed to the topic.\n     *\n     * @param topic  The topic name.\n     *\n     * @return Whether the group is subscribed to the topic.\n     */\n    @Override\n    public boolean isSubscribedToTopic(String topic) {\n        return subscribedTopicNames.containsKey(topic);\n    }\n\n    /**\n     * @return The group's subscription type.\n     */\n    public SubscriptionType subscriptionType() {\n        return subscriptionType.get();\n    }\n\n    /**\n     * Returns the target assignment of the member.\n     *\n     * @return The ConsumerGroupMemberAssignment or an EMPTY one if it does not\n     *         exist.\n     */\n    public Assignment targetAssignment(String memberId) {\n        return targetAssignment.getOrDefault(memberId, Assignment.EMPTY);\n    }\n\n    /**\n     * @return An immutable map containing all the topic partitions\n     *         with their current member assignments.\n     */\n    public Map<Uuid, Map<Integer, String>> invertedTargetAssignment() {\n        return Collections.unmodifiableMap(invertedTargetAssignment);\n    }\n\n    /**\n     * Updates the target assignment of a member.\n     *\n     * @param memberId              The member id.\n     * @param newTargetAssignment   The new target assignment.\n     */\n    public void updateTargetAssignment(String memberId, Assignment newTargetAssignment) {\n        updateInvertedTargetAssignment(\n            memberId,\n            targetAssignment.getOrDefault(memberId, new Assignment(Collections.emptyMap())),\n            newTargetAssignment\n        );\n        targetAssignment.put(memberId, newTargetAssignment);\n    }\n\n    /**\n     * Updates the reverse lookup map of the target assignment.\n     *\n     * @param memberId              The member Id.\n     * @param oldTargetAssignment   The old target assignment.\n     * @param newTargetAssignment   The new target assignment.\n     */\n    private void updateInvertedTargetAssignment(\n        String memberId,\n        Assignment oldTargetAssignment,\n        Assignment newTargetAssignment\n    ) {\n        // Combine keys from both old and new assignments.\n        Set<Uuid> allTopicIds = new HashSet<>();\n        allTopicIds.addAll(oldTargetAssignment.partitions().keySet());\n        allTopicIds.addAll(newTargetAssignment.partitions().keySet());\n\n        for (Uuid topicId : allTopicIds) {\n            Set<Integer> oldPartitions = oldTargetAssignment.partitions().getOrDefault(topicId, Collections.emptySet());\n            Set<Integer> newPartitions = newTargetAssignment.partitions().getOrDefault(topicId, Collections.emptySet());\n\n            TimelineHashMap<Integer, String> topicPartitionAssignment = invertedTargetAssignment.computeIfAbsent(\n                topicId, k -> new TimelineHashMap<>(snapshotRegistry, Math.max(oldPartitions.size(), newPartitions.size()))\n            );\n\n            // Remove partitions that aren't present in the new assignment only if the partition is currently\n            // still assigned to the member in question.\n            // If p0 was moved from A to B, and the target assignment map was updated for B first, we don't want to\n            // remove the key p0 from the inverted map and undo the action when A eventually tries to update its assignment.\n            for (Integer partition : oldPartitions) {\n                if (!newPartitions.contains(partition) && memberId.equals(topicPartitionAssignment.get(partition))) {\n                    topicPartitionAssignment.remove(partition);\n                }\n            }\n\n            // Add partitions that are in the new assignment but not in the old assignment.\n            for (Integer partition : newPartitions) {\n                if (!oldPartitions.contains(partition)) {\n                    topicPartitionAssignment.put(partition, memberId);\n                }\n            }\n\n            if (topicPartitionAssignment.isEmpty()) {\n                invertedTargetAssignment.remove(topicId);\n            } else {\n                invertedTargetAssignment.put(topicId, topicPartitionAssignment);\n            }\n        }\n    }\n\n    /**\n     * Removes the target assignment of a member.\n     *\n     * @param memberId The member id.\n     */\n    public void removeTargetAssignment(String memberId) {\n        updateInvertedTargetAssignment(\n            memberId,\n            targetAssignment.getOrDefault(memberId, Assignment.EMPTY),\n            Assignment.EMPTY\n        );\n        targetAssignment.remove(memberId);\n    }\n\n    /**\n     * @return An immutable Map containing all the target assignment keyed by member id.\n     */\n    public Map<String, Assignment> targetAssignment() {\n        return Collections.unmodifiableMap(targetAssignment);\n    }\n\n    /**\n     * Returns the current epoch of a partition or -1 if the partition\n     * does not have one.\n     *\n     * @param topicId       The topic id.\n     * @param partitionId   The partition id.\n     *\n     * @return The epoch or -1.\n     */\n    public int currentPartitionEpoch(\n        Uuid topicId, int partitionId\n    ) {\n        Map<Integer, Integer> partitions = currentPartitionEpoch.get(topicId);\n        if (partitions == null) {\n            return -1;\n        } else {\n            return partitions.getOrDefault(partitionId, -1);\n        }\n    }\n\n    /**\n     * Compute the preferred (server side) assignor for the group while\n     * taking into account the updated member. The computation relies\n     * on {{@link ConsumerGroup#serverAssignors}} persisted structure\n     * but it does not update it.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     *\n     * @return An Optional containing the preferred assignor.\n     */\n    public Optional<String> computePreferredServerAssignor(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        // Copy the current count and update it.\n        Map<String, Integer> counts = new HashMap<>(this.serverAssignors);\n        maybeUpdateServerAssignors(counts, oldMember, newMember);\n\n        return counts.entrySet().stream()\n            .max(Map.Entry.comparingByValue())\n            .map(Map.Entry::getKey);\n    }\n\n    /**\n     * @return The preferred assignor for the group.\n     */\n    public Optional<String> preferredServerAssignor() {\n        return preferredServerAssignor(Long.MAX_VALUE);\n    }\n\n    /**\n     * @return The preferred assignor for the group with given offset.\n     */\n    public Optional<String> preferredServerAssignor(long committedOffset) {\n        return serverAssignors.entrySet(committedOffset).stream()\n            .max(Map.Entry.comparingByValue())\n            .map(Map.Entry::getKey);\n    }\n\n    /**\n     * @return An immutable Map of subscription metadata for\n     *         each topic that the consumer group is subscribed to.\n     */\n    public Map<String, TopicMetadata> subscriptionMetadata() {\n        return Collections.unmodifiableMap(subscribedTopicMetadata);\n    }\n\n    /**\n     * Updates the subscription metadata. This replaces the previous one.\n     *\n     * @param subscriptionMetadata The new subscription metadata.\n     */\n    public void setSubscriptionMetadata(\n        Map<String, TopicMetadata> subscriptionMetadata\n    ) {\n        this.subscribedTopicMetadata.clear();\n        this.subscribedTopicMetadata.putAll(subscriptionMetadata);\n    }\n\n    /**\n     * Computes the subscription metadata based on the current subscription info.\n     *\n     * @param subscribedTopicNames      Map of topic names to the number of subscribers.\n     * @param topicsImage               The current metadata for all available topics.\n     * @param clusterImage              The current metadata for the Kafka cluster.\n     *\n     * @return An immutable map of subscription metadata for each topic that the consumer group is subscribed to.\n     */\n    public Map<String, TopicMetadata> computeSubscriptionMetadata(\n        Map<String, Integer> subscribedTopicNames,\n        TopicsImage topicsImage,\n        ClusterImage clusterImage\n    ) {\n        // Create the topic metadata for each subscribed topic.\n        Map<String, TopicMetadata> newSubscriptionMetadata = new HashMap<>(subscribedTopicNames.size());\n\n        subscribedTopicNames.forEach((topicName, count) -> {\n            TopicImage topicImage = topicsImage.getTopic(topicName);\n            if (topicImage != null) {\n                Map<Integer, Set<String>> partitionRacks = new HashMap<>();\n                topicImage.partitions().forEach((partition, partitionRegistration) -> {\n                    Set<String> racks = new HashSet<>();\n                    for (int replica : partitionRegistration.replicas) {\n                        Optional<String> rackOptional = clusterImage.broker(replica).rack();\n                        // Only add the rack if it is available for the broker/replica.\n                        rackOptional.ifPresent(racks::add);\n                    }\n                    // If rack information is unavailable for all replicas of this partition,\n                    // no corresponding entry will be stored for it in the map.\n                    if (!racks.isEmpty())\n                        partitionRacks.put(partition, racks);\n                });\n\n                newSubscriptionMetadata.put(topicName, new TopicMetadata(\n                    topicImage.id(),\n                    topicImage.name(),\n                    topicImage.partitions().size(),\n                    partitionRacks)\n                );\n            }\n        });\n\n        return Collections.unmodifiableMap(newSubscriptionMetadata);\n    }\n\n    /**\n     * Updates the metadata refresh deadline.\n     *\n     * @param deadlineMs The deadline in milliseconds.\n     * @param groupEpoch The associated group epoch.\n     */\n    public void setMetadataRefreshDeadline(\n        long deadlineMs,\n        int groupEpoch\n    ) {\n        this.metadataRefreshDeadline = new DeadlineAndEpoch(deadlineMs, groupEpoch);\n    }\n\n    /**\n     * Requests a metadata refresh.\n     */\n    public void requestMetadataRefresh() {\n        this.metadataRefreshDeadline = DeadlineAndEpoch.EMPTY;\n    }\n\n    /**\n     * Checks if a metadata refresh is required. A refresh is required in two cases:\n     * 1) The deadline is smaller or equal to the current time;\n     * 2) The group epoch associated with the deadline is larger than\n     *    the current group epoch. This means that the operations which updated\n     *    the deadline failed.\n     *\n     * @param currentTimeMs The current time in milliseconds.\n     * @return A boolean indicating whether a refresh is required or not.\n     */\n    public boolean hasMetadataExpired(long currentTimeMs) {\n        return currentTimeMs >= metadataRefreshDeadline.deadlineMs || groupEpoch() < metadataRefreshDeadline.epoch;\n    }\n\n    /**\n     * @return The metadata refresh deadline.\n     */\n    public DeadlineAndEpoch metadataRefreshDeadline() {\n        return metadataRefreshDeadline;\n    }\n\n    /**\n     * Validates the OffsetCommit request.\n     *\n     * @param memberId          The member id.\n     * @param groupInstanceId   The group instance id.\n     * @param memberEpoch       The member epoch.\n     * @param isTransactional   Whether the offset commit is transactional or not. It has no\n     *                          impact when a consumer group is used.\n     * @param apiVersion        The api version.\n     * @throws UnknownMemberIdException     If the member is not found.\n     * @throws StaleMemberEpochException    If the member uses the consumer protocol and the provided\n     *                                      member epoch doesn't match the actual member epoch.\n     * @throws IllegalGenerationException   If the member uses the classic protocol and the provided\n     *                                      generation id is not equal to the member epoch.\n     */\n    @Override\n    public void validateOffsetCommit(\n        String memberId,\n        String groupInstanceId,\n        int memberEpoch,\n        boolean isTransactional,\n        short apiVersion\n    ) throws UnknownMemberIdException, StaleMemberEpochException, IllegalGenerationException {\n        // When the member epoch is -1, the request comes from either the admin client\n        // or a consumer which does not use the group management facility. In this case,\n        // the request can commit offsets if the group is empty.\n        if (memberEpoch < 0 && members().isEmpty()) return;\n\n        final ConsumerGroupMember member = getOrMaybeCreateMember(memberId, false);\n\n        // If the commit is not transactional and the member uses the new consumer protocol (KIP-848),\n        // the member should be using the OffsetCommit API version >= 9.\n        if (!isTransactional && !member.useClassicProtocol() && apiVersion < 9) {\n            throw new UnsupportedVersionException(\"OffsetCommit version 9 or above must be used \" +\n                \"by members using the consumer group protocol\");\n        }\n\n        validateMemberEpoch(memberEpoch, member.memberEpoch(), member.useClassicProtocol());\n    }\n\n    /**\n     * Validates the OffsetFetch request.\n     *\n     * @param memberId              The member id for consumer groups.\n     * @param memberEpoch           The member epoch for consumer groups.\n     * @param lastCommittedOffset   The last committed offsets in the timeline.\n     * @throws UnknownMemberIdException     If the member is not found.\n     * @throws StaleMemberEpochException    If the member uses the consumer protocol and the provided\n     *                                      member epoch doesn't match the actual member epoch.\n     * @throws IllegalGenerationException   If the member uses the classic protocol and the provided\n     *                                      generation id is not equal to the member epoch.\n     */\n    @Override\n    public void validateOffsetFetch(\n        String memberId,\n        int memberEpoch,\n        long lastCommittedOffset\n    ) throws UnknownMemberIdException, StaleMemberEpochException, IllegalGenerationException {\n        // When the member id is null and the member epoch is -1, the request either comes\n        // from the admin client or from a client which does not provide them. In this case,\n        // the fetch request is accepted.\n        if (memberId == null && memberEpoch < 0) return;\n\n        final ConsumerGroupMember member = members.get(memberId, lastCommittedOffset);\n        if (member == null) {\n            throw new UnknownMemberIdException(String.format(\"Member %s is not a member of group %s.\",\n                memberId, groupId));\n        }\n        validateMemberEpoch(memberEpoch, member.memberEpoch(), member.useClassicProtocol());\n    }\n\n    /**\n     * Validates the OffsetDelete request.\n     */\n    @Override\n    public void validateOffsetDelete() {}\n\n    /**\n     * Validates the DeleteGroups request.\n     */\n    @Override\n    public void validateDeleteGroup() throws ApiException {\n        if (state() != ConsumerGroupState.EMPTY) {\n            throw Errors.NON_EMPTY_GROUP.exception();\n        }\n    }\n\n    /**\n     * Populates the list of records with tombstone(s) for deleting the group.\n     *\n     * @param records The list of records.\n     */\n    @Override\n    public void createGroupTombstoneRecords(List<CoordinatorRecord> records) {\n        members().forEach((memberId, member) ->\n            records.add(CoordinatorRecordHelpers.newCurrentAssignmentTombstoneRecord(groupId(), memberId))\n        );\n\n        members().forEach((memberId, member) ->\n            records.add(CoordinatorRecordHelpers.newTargetAssignmentTombstoneRecord(groupId(), memberId))\n        );\n        records.add(CoordinatorRecordHelpers.newTargetAssignmentEpochTombstoneRecord(groupId()));\n\n        members().forEach((memberId, member) ->\n            records.add(CoordinatorRecordHelpers.newMemberSubscriptionTombstoneRecord(groupId(), memberId))\n        );\n\n        records.add(CoordinatorRecordHelpers.newGroupSubscriptionMetadataTombstoneRecord(groupId()));\n        records.add(CoordinatorRecordHelpers.newGroupEpochTombstoneRecord(groupId()));\n    }\n\n    @Override\n    public boolean isEmpty() {\n        return state() == ConsumerGroupState.EMPTY;\n    }\n\n    /**\n     * See {@link org.apache.kafka.coordinator.group.OffsetExpirationCondition}\n     *\n     * @return The offset expiration condition for the group or Empty if no such condition exists.\n     */\n    @Override\n    public Optional<OffsetExpirationCondition> offsetExpirationCondition() {\n        return Optional.of(new OffsetExpirationConditionImpl(offsetAndMetadata -> offsetAndMetadata.commitTimestampMs));\n    }\n\n    @Override\n    public boolean isInStates(Set<String> statesFilter, long committedOffset) {\n        return statesFilter.contains(state.get(committedOffset).toLowerCaseString());\n    }\n\n    /**\n     * Throws an exception if the received member epoch does not match the expected member epoch.\n     *\n     * @param receivedMemberEpoch   The received member epoch or generation id.\n     * @param expectedMemberEpoch   The expected member epoch.\n     * @param useClassicProtocol    The boolean indicating whether the checked member uses the classic protocol.\n     * @throws StaleMemberEpochException    if the member with unmatched member epoch uses the consumer protocol.\n     * @throws IllegalGenerationException   if the member with unmatched generation id uses the classic protocol.\n     */\n    private void validateMemberEpoch(\n        int receivedMemberEpoch,\n        int expectedMemberEpoch,\n        boolean useClassicProtocol\n    ) throws StaleMemberEpochException, IllegalGenerationException {\n        if (receivedMemberEpoch != expectedMemberEpoch) {\n            if (useClassicProtocol) {\n                throw new IllegalGenerationException(String.format(\"The received generation id %d does not match \" +\n                    \"the expected member epoch %d.\", receivedMemberEpoch, expectedMemberEpoch));\n            } else {\n                throw new StaleMemberEpochException(String.format(\"The received member epoch %d does not match \"\n                    + \"the expected member epoch %d.\", receivedMemberEpoch, expectedMemberEpoch));\n            }\n        }\n    }\n\n    /**\n     * Updates the current state of the group.\n     */\n    private void maybeUpdateGroupState() {\n        ConsumerGroupState previousState = state.get();\n        ConsumerGroupState newState = STABLE;\n        if (members.isEmpty()) {\n            newState = EMPTY;\n        } else if (groupEpoch.get() > targetAssignmentEpoch.get()) {\n            newState = ASSIGNING;\n        } else {\n            for (ConsumerGroupMember member : members.values()) {\n                if (!member.isReconciledTo(targetAssignmentEpoch.get())) {\n                    newState = RECONCILING;\n                    break;\n                }\n            }\n        }\n\n        state.set(newState);\n        metrics.onConsumerGroupStateTransition(previousState, newState);\n    }\n\n    /**\n     * Updates the server assignors count.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdateServerAssignors(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        maybeUpdateServerAssignors(serverAssignors, oldMember, newMember);\n    }\n\n    /**\n     * Updates the server assignors count.\n     *\n     * @param serverAssignorCount   The count to update.\n     * @param oldMember             The old member.\n     * @param newMember             The new member.\n     */\n    private static void maybeUpdateServerAssignors(\n        Map<String, Integer> serverAssignorCount,\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        if (oldMember != null) {\n            oldMember.serverAssignorName().ifPresent(name ->\n                serverAssignorCount.compute(name, ConsumerGroup::decValue)\n            );\n        }\n        if (newMember != null) {\n            newMember.serverAssignorName().ifPresent(name ->\n                serverAssignorCount.compute(name, ConsumerGroup::incValue)\n            );\n        }\n    }\n\n    /**\n     * Updates the number of the members that use the classic protocol.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdateNumClassicProtocolMembers(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        int delta = 0;\n        if (oldMember != null && oldMember.useClassicProtocol()) {\n            delta--;\n        }\n        if (newMember != null && newMember.useClassicProtocol()) {\n            delta++;\n        }\n        setNumClassicProtocolMembers(numClassicProtocolMembers() + delta);\n    }\n\n    /**\n     * Updates the supported protocol count of the members that use the classic protocol.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdateClassicProtocolMembersSupportedProtocols(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        if (oldMember != null) {\n            oldMember.supportedClassicProtocols().ifPresent(protocols ->\n                protocols.forEach(protocol ->\n                    classicProtocolMembersSupportedProtocols.compute(protocol.name(), ConsumerGroup::decValue)\n                )\n            );\n        }\n        if (newMember != null) {\n            newMember.supportedClassicProtocols().ifPresent(protocols ->\n                protocols.forEach(protocol ->\n                    classicProtocolMembersSupportedProtocols.compute(protocol.name(), ConsumerGroup::incValue)\n                )\n            );\n        }\n    }\n\n    /**\n     * Updates the subscribed topic names count.\n     * The subscription type is updated as a consequence.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdateSubscribedTopicNamesAndGroupSubscriptionType(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        maybeUpdateSubscribedTopicNames(subscribedTopicNames, oldMember, newMember);\n        subscriptionType.set(subscriptionType(subscribedTopicNames, members.size()));\n    }\n\n    /**\n     * Updates the subscription count.\n     *\n     * @param subscribedTopicCount  The map to update.\n     * @param oldMember             The old member.\n     * @param newMember             The new member.\n     */\n    private static void maybeUpdateSubscribedTopicNames(\n        Map<String, Integer> subscribedTopicCount,\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        if (oldMember != null) {\n            oldMember.subscribedTopicNames().forEach(topicName ->\n                subscribedTopicCount.compute(topicName, ConsumerGroup::decValue)\n            );\n        }\n\n        if (newMember != null) {\n            newMember.subscribedTopicNames().forEach(topicName ->\n                subscribedTopicCount.compute(topicName, ConsumerGroup::incValue)\n            );\n        }\n    }\n\n    /**\n     * Updates the subscription count.\n     *\n     * @param oldMember             The old member.\n     * @param newMember             The new member.\n     *\n     * @return Copy of the map of topics to the count of number of subscribers.\n     */\n    public Map<String, Integer> computeSubscribedTopicNames(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        Map<String, Integer> subscribedTopicNames = new HashMap<>(this.subscribedTopicNames);\n        maybeUpdateSubscribedTopicNames(\n            subscribedTopicNames,\n            oldMember,\n            newMember\n        );\n        return subscribedTopicNames;\n    }\n\n    /**\n     * Updates the subscription count with a set of members removed.\n     *\n     * @param removedMembers        The set of removed members.\n     *\n     * @return Copy of the map of topics to the count of number of subscribers.\n     */\n    public Map<String, Integer> computeSubscribedTopicNames(\n        Set<ConsumerGroupMember> removedMembers\n    ) {\n        Map<String, Integer> subscribedTopicNames = new HashMap<>(this.subscribedTopicNames);\n        if (removedMembers != null) {\n            removedMembers.forEach(removedMember ->\n                maybeUpdateSubscribedTopicNames(\n                    subscribedTopicNames,\n                    removedMember,\n                    null\n                )\n            );\n        }\n        return subscribedTopicNames;\n    }\n\n    /**\n     * Compute the subscription type of the consumer group.\n     *\n     * @param subscribedTopicNames      A map of topic names to the count of members subscribed to each topic.\n     *\n     * @return {@link SubscriptionType#HOMOGENEOUS} if all members are subscribed to exactly the same topics;\n     *         otherwise, {@link SubscriptionType#HETEROGENEOUS}.\n     */\n    public static SubscriptionType subscriptionType(\n        Map<String, Integer> subscribedTopicNames,\n        int numberOfMembers\n    ) {\n        if (subscribedTopicNames.isEmpty()) {\n            return HOMOGENEOUS;\n        }\n\n        for (int subscriberCount : subscribedTopicNames.values()) {\n            if (subscriberCount != numberOfMembers) {\n                return HETEROGENEOUS;\n            }\n        }\n        return HOMOGENEOUS;\n    }\n\n    /**\n     * Updates the partition epochs based on the old and the new member.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdatePartitionEpoch(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        maybeRemovePartitionEpoch(oldMember);\n        addPartitionEpochs(newMember.assignedPartitions(), newMember.memberEpoch());\n        addPartitionEpochs(newMember.partitionsPendingRevocation(), newMember.memberEpoch());\n    }\n\n    /**\n     * Removes the partition epochs for the provided member.\n     *\n     * @param oldMember The old member.\n     */\n    private void maybeRemovePartitionEpoch(\n        ConsumerGroupMember oldMember\n    ) {\n        if (oldMember != null) {\n            removePartitionEpochs(oldMember.assignedPartitions(), oldMember.memberEpoch());\n            removePartitionEpochs(oldMember.partitionsPendingRevocation(), oldMember.memberEpoch());\n        }\n    }\n\n    /**\n     * Removes the partition epochs based on the provided assignment.\n     *\n     * @param assignment    The assignment.\n     * @param expectedEpoch The expected epoch.\n     * @throws IllegalStateException if the epoch does not match the expected one.\n     * package-private for testing.\n     */\n    void removePartitionEpochs(\n        Map<Uuid, Set<Integer>> assignment,\n        int expectedEpoch\n    ) {\n        assignment.forEach((topicId, assignedPartitions) -> {\n            currentPartitionEpoch.compute(topicId, (__, partitionsOrNull) -> {\n                if (partitionsOrNull != null) {\n                    assignedPartitions.forEach(partitionId -> {\n                        Integer prevValue = partitionsOrNull.remove(partitionId);\n                        if (prevValue != expectedEpoch) {\n                            throw new IllegalStateException(\n                                String.format(\"Cannot remove the epoch %d from %s-%s because the partition is \" +\n                                    \"still owned at a different epoch %d\", expectedEpoch, topicId, partitionId, prevValue));\n                        }\n                    });\n                    if (partitionsOrNull.isEmpty()) {\n                        return null;\n                    } else {\n                        return partitionsOrNull;\n                    }\n                } else {\n                    throw new IllegalStateException(\n                        String.format(\"Cannot remove the epoch %d from %s because it does not have any epoch\",\n                            expectedEpoch, topicId));\n                }\n            });\n        });\n    }\n\n    /**\n     * Adds the partitions epoch based on the provided assignment.\n     *\n     * @param assignment    The assignment.\n     * @param epoch         The new epoch.\n     * @throws IllegalStateException if the partition already has an epoch assigned.\n     * package-private for testing.\n     */\n    void addPartitionEpochs(\n        Map<Uuid, Set<Integer>> assignment,\n        int epoch\n    ) {\n        assignment.forEach((topicId, assignedPartitions) -> {\n            currentPartitionEpoch.compute(topicId, (__, partitionsOrNull) -> {\n                if (partitionsOrNull == null) {\n                    partitionsOrNull = new TimelineHashMap<>(snapshotRegistry, assignedPartitions.size());\n                }\n                for (Integer partitionId : assignedPartitions) {\n                    Integer prevValue = partitionsOrNull.put(partitionId, epoch);\n                    if (prevValue != null) {\n                        throw new IllegalStateException(\n                            String.format(\"Cannot set the epoch of %s-%s to %d because the partition is \" +\n                                \"still owned at epoch %d\", topicId, partitionId, epoch, prevValue));\n                    }\n                }\n                return partitionsOrNull;\n            });\n        });\n    }\n\n    /**\n     * Decrements value by 1; returns null when reaching zero. This helper is\n     * meant to be used with Map#compute.\n     */\n    private static Integer decValue(String key, Integer value) {\n        if (value == null) return null;\n        return value == 1 ? null : value - 1;\n    }\n\n    /**\n     * Increments value by 1; This helper is meant to be used with Map#compute.\n     */\n    private static Integer incValue(String key, Integer value) {\n        return value == null ? 1 : value + 1;\n    }\n\n    public ConsumerGroupDescribeResponseData.DescribedGroup asDescribedGroup(\n        long committedOffset,\n        String defaultAssignor,\n        TopicsImage topicsImage\n    ) {\n        ConsumerGroupDescribeResponseData.DescribedGroup describedGroup = new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(groupId)\n            .setAssignorName(preferredServerAssignor(committedOffset).orElse(defaultAssignor))\n            .setGroupEpoch(groupEpoch.get(committedOffset))\n            .setGroupState(state.get(committedOffset).toString())\n            .setAssignmentEpoch(targetAssignmentEpoch.get(committedOffset));\n        members.entrySet(committedOffset).forEach(\n            entry -> describedGroup.members().add(\n                entry.getValue().asConsumerGroupDescribeMember(\n                    targetAssignment.get(entry.getValue().memberId(), committedOffset),\n                    topicsImage\n                )\n            )\n        );\n        return describedGroup;\n    }\n\n    /**\n     * Create a new consumer group according to the given classic group.\n     *\n     * @param snapshotRegistry  The SnapshotRegistry.\n     * @param metrics           The GroupCoordinatorMetricsShard.\n     * @param classicGroup      The converted classic group.\n     * @param topicsImage       The TopicsImage for topic id and topic name conversion.\n     * @return  The created ConsumerGruop.\n     */\n    public static ConsumerGroup fromClassicGroup(\n        SnapshotRegistry snapshotRegistry,\n        GroupCoordinatorMetricsShard metrics,\n        ClassicGroup classicGroup,\n        TopicsImage topicsImage\n    ) {\n        String groupId = classicGroup.groupId();\n        ConsumerGroup consumerGroup = new ConsumerGroup(snapshotRegistry, groupId, metrics);\n        consumerGroup.setGroupEpoch(classicGroup.generationId());\n        consumerGroup.setTargetAssignmentEpoch(classicGroup.generationId());\n\n        classicGroup.allMembers().forEach(classicGroupMember -> {\n            Map<Uuid, Set<Integer>> assignedPartitions = toTopicPartitionMap(\n                ConsumerProtocol.deserializeConsumerProtocolAssignment(\n                    ByteBuffer.wrap(classicGroupMember.assignment())\n                ),\n                topicsImage\n            );\n            ConsumerProtocolSubscription subscription = ConsumerProtocol.deserializeConsumerProtocolSubscription(\n                ByteBuffer.wrap(classicGroupMember.metadata(classicGroup.protocolName().get()))\n            );\n\n            // The target assignment and the assigned partitions of each member are set based on the last\n            // assignment of the classic group. All the members are put in the Stable state. If the classic\n            // group was in Preparing Rebalance or Completing Rebalance states, the classic members are\n            // asked to rejoin the group to re-trigger a rebalance or collect their assignments.\n            ConsumerGroupMember newMember = new ConsumerGroupMember.Builder(classicGroupMember.memberId())\n                .setMemberEpoch(classicGroup.generationId())\n                .setState(MemberState.STABLE)\n                .setPreviousMemberEpoch(classicGroup.generationId())\n                .setInstanceId(classicGroupMember.groupInstanceId().orElse(null))\n                .setRackId(toOptional(subscription.rackId()).orElse(null))\n                .setRebalanceTimeoutMs(classicGroupMember.rebalanceTimeoutMs())\n                .setClientId(classicGroupMember.clientId())\n                .setClientHost(classicGroupMember.clientHost())\n                .setSubscribedTopicNames(subscription.topics())\n                .setAssignedPartitions(assignedPartitions)\n                .setClassicMemberMetadata(\n                    new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                        .setSessionTimeoutMs(classicGroupMember.sessionTimeoutMs())\n                        .setSupportedProtocols(ConsumerGroupMember.classicProtocolListFromJoinRequestProtocolCollection(\n                            classicGroupMember.supportedProtocols()\n                        ))\n                )\n                .build();\n            consumerGroup.updateTargetAssignment(newMember.memberId(), new Assignment(assignedPartitions));\n            consumerGroup.updateMember(newMember);\n        });\n\n        return consumerGroup;\n    }\n\n    /**\n     * Populate the record list with the records needed to create the given consumer group.\n     *\n     * @param records The list to which the new records are added.\n     */\n    public void createConsumerGroupRecords(\n        List<CoordinatorRecord> records\n    ) {\n        members().forEach((__, consumerGroupMember) ->\n            records.add(CoordinatorRecordHelpers.newMemberSubscriptionRecord(groupId(), consumerGroupMember))\n        );\n\n        records.add(CoordinatorRecordHelpers.newGroupEpochRecord(groupId(), groupEpoch()));\n\n        members().forEach((consumerGroupMemberId, consumerGroupMember) ->\n            records.add(CoordinatorRecordHelpers.newTargetAssignmentRecord(\n                groupId(),\n                consumerGroupMemberId,\n                targetAssignment(consumerGroupMemberId).partitions()\n            ))\n        );\n\n        records.add(CoordinatorRecordHelpers.newTargetAssignmentEpochRecord(groupId(), groupEpoch()));\n\n        members().forEach((__, consumerGroupMember) ->\n            records.add(CoordinatorRecordHelpers.newCurrentAssignmentRecord(groupId(), consumerGroupMember))\n        );\n    }\n\n    /**\n     * Checks whether at least one of the given protocols can be supported. A\n     * protocol can be supported if it is supported by all members that use the\n     * classic protocol.\n     *\n     * @param memberProtocolType  The member protocol type.\n     * @param memberProtocols     The set of protocol names.\n     *\n     * @return A boolean based on the condition mentioned above.\n     */\n    public boolean supportsClassicProtocols(String memberProtocolType, Set<String> memberProtocols) {\n        if (ConsumerProtocol.PROTOCOL_TYPE.equals(memberProtocolType)) {\n            if (isEmpty()) {\n                return !memberProtocols.isEmpty();\n            } else {\n                return memberProtocols.stream().anyMatch(\n                    name -> classicProtocolMembersSupportedProtocols.getOrDefault(name, 0) == numClassicProtocolMembers()\n                );\n            }\n        }\n        return false;\n    }\n\n    /**\n     * Checks whether all the members use the classic protocol except the given member.\n     *\n     * @param memberId The member to remove.\n     * @return A boolean indicating whether all the members use the classic protocol.\n     */\n    public boolean allMembersUseClassicProtocolExcept(String memberId) {\n        return numClassicProtocolMembers() == members().size() - 1 &&\n            !getOrMaybeCreateMember(memberId, false).useClassicProtocol();\n    }\n\n    /**\n     * Checks whether the member has any unreleased partition.\n     *\n     * @param member The member to check.\n     * @return A boolean indicating whether the member has partitions in the target\n     *         assignment that hasn't been revoked by other members.\n     */\n    public boolean waitingOnUnreleasedPartition(ConsumerGroupMember member) {\n        if (member.state() == MemberState.UNRELEASED_PARTITIONS) {\n            for (Map.Entry<Uuid, Set<Integer>> entry : targetAssignment().get(member.memberId()).partitions().entrySet()) {\n                Uuid topicId = entry.getKey();\n                Set<Integer> assignedPartitions = member.assignedPartitions().getOrDefault(topicId, Collections.emptySet());\n\n                for (int partition : entry.getValue()) {\n                    if (!assignedPartitions.contains(partition) && currentPartitionEpoch(topicId, partition) != -1) {\n                        return true;\n                    }\n                }\n            }\n        }\n        return false;\n    }\n}"
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [],
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "60114a46a775e597410131dd67ac236d62f57b0c",
        "url": "https://github.com/apache/kafka/commit/60114a46a775e597410131dd67ac236d62f57b0c",
        "move_method_refactoring": {
            "type": "Move Method",
            "description": "Move Method private incValue(key String, value Integer) : Integer from class org.apache.kafka.coordinator.group.consumer.ConsumerGroup to public incValue(key String, value Integer) : Integer from class org.apache.kafka.coordinator.group.Utils",
            "leftSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/consumer/ConsumerGroup.java",
                    "startLine": 1265,
                    "endLine": 1270,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "original method declaration",
                    "codeElement": "private incValue(key String, value Integer) : Integer"
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/Utils.java",
                    "startLine": 95,
                    "endLine": 100,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "moved method declaration",
                    "codeElement": "public incValue(key String, value Integer) : Integer"
                }
            ],
            "isStatic": true
        },
        "telemetry": {
            "id": "39a54171-b319-4d60-8c4d-11cd47d2cd9a",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1374,
                "lineStart": 67,
                "lineEnd": 1440,
                "bodyLineStart": 67,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/consumer/ConsumerGroup.java",
                "sourceCode": "/**\n * A Consumer Group. All the metadata in this class are backed by\n * records in the __consumer_offsets partitions.\n */\npublic class ConsumerGroup implements Group {\n\n    public enum ConsumerGroupState {\n        EMPTY(\"Empty\"),\n        ASSIGNING(\"Assigning\"),\n        RECONCILING(\"Reconciling\"),\n        STABLE(\"Stable\"),\n        DEAD(\"Dead\");\n\n        private final String name;\n\n        private final String lowerCaseName;\n\n        ConsumerGroupState(String name) {\n            this.name = name;\n            this.lowerCaseName = name.toLowerCase(Locale.ROOT);\n        }\n\n        @Override\n        public String toString() {\n            return name;\n        }\n\n        public String toLowerCaseString() {\n            return lowerCaseName;\n        }\n    }\n\n    public static class DeadlineAndEpoch {\n        static final DeadlineAndEpoch EMPTY = new DeadlineAndEpoch(0L, 0);\n\n        public final long deadlineMs;\n        public final int epoch;\n\n        DeadlineAndEpoch(long deadlineMs, int epoch) {\n            this.deadlineMs = deadlineMs;\n            this.epoch = epoch;\n        }\n    }\n\n    /**\n     * The snapshot registry.\n     */\n    private final SnapshotRegistry snapshotRegistry;\n\n    /**\n     * The group id.\n     */\n    private final String groupId;\n\n    /**\n     * The group state.\n     */\n    private final TimelineObject<ConsumerGroupState> state;\n\n    /**\n     * The group epoch. The epoch is incremented whenever the subscriptions\n     * are updated and it will trigger the computation of a new assignment\n     * for the group.\n     */\n    private final TimelineInteger groupEpoch;\n\n    /**\n     * The group members.\n     */\n    private final TimelineHashMap<String, ConsumerGroupMember> members;\n\n    /**\n     * The static group members.\n     */\n    private final TimelineHashMap<String, String> staticMembers;\n\n    /**\n     * The number of members supporting each server assignor name.\n     */\n    private final TimelineHashMap<String, Integer> serverAssignors;\n\n    /**\n     * The number of subscribers per topic.\n     */\n    private final TimelineHashMap<String, Integer> subscribedTopicNames;\n\n    /**\n     * The metadata associated with each subscribed topic name.\n     */\n    private final TimelineHashMap<String, TopicMetadata> subscribedTopicMetadata;\n\n    /**\n     * The consumer group's subscription type.\n     * This value is set to Homogeneous by default.\n     */\n    private final TimelineObject<SubscriptionType> subscriptionType;\n\n    /**\n     * The target assignment epoch. An assignment epoch smaller than the group epoch\n     * means that a new assignment is required. The assignment epoch is updated when\n     * a new assignment is installed.\n     */\n    private final TimelineInteger targetAssignmentEpoch;\n\n    /**\n     * The target assignment per member id.\n     */\n    private final TimelineHashMap<String, Assignment> targetAssignment;\n\n    /**\n     * Reverse lookup map representing topic partitions with\n     * their current member assignments.\n     */\n    private final TimelineHashMap<Uuid, TimelineHashMap<Integer, String>> invertedTargetAssignment;\n\n    /**\n     * The current partition epoch maps each topic-partitions to their current epoch where\n     * the epoch is the epoch of their owners. When a member revokes a partition, it removes\n     * its epochs from this map. When a member gets a partition, it adds its epochs to this map.\n     */\n    private final TimelineHashMap<Uuid, TimelineHashMap<Integer, Integer>> currentPartitionEpoch;\n\n    /**\n     * The coordinator metrics.\n     */\n    private final GroupCoordinatorMetricsShard metrics;\n\n    /**\n     * The metadata refresh deadline. It consists of a timestamp in milliseconds together with\n     * the group epoch at the time of setting it. The metadata refresh time is considered as a\n     * soft state (read that it is not stored in a timeline data structure). It is like this\n     * because it is not persisted to the log. The group epoch is here to ensure that the\n     * metadata refresh deadline is invalidated if the group epoch does not correspond to\n     * the current group epoch. This can happen if the metadata refresh deadline is updated\n     * after having refreshed the metadata but the write operation failed. In this case, the\n     * time is not automatically rolled back.\n     */\n    private DeadlineAndEpoch metadataRefreshDeadline = DeadlineAndEpoch.EMPTY;\n\n    /**\n     * The number of members that use the classic protocol.\n     */\n    private final TimelineInteger numClassicProtocolMembers;\n\n    /**\n     * Map of protocol names to the number of members that use classic protocol and support them.\n     */\n    private final TimelineHashMap<String, Integer> classicProtocolMembersSupportedProtocols;\n\n    public ConsumerGroup(\n        SnapshotRegistry snapshotRegistry,\n        String groupId,\n        GroupCoordinatorMetricsShard metrics\n    ) {\n        this.snapshotRegistry = Objects.requireNonNull(snapshotRegistry);\n        this.groupId = Objects.requireNonNull(groupId);\n        this.state = new TimelineObject<>(snapshotRegistry, EMPTY);\n        this.groupEpoch = new TimelineInteger(snapshotRegistry);\n        this.members = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.staticMembers = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.serverAssignors = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.subscribedTopicNames = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.subscribedTopicMetadata = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.subscriptionType = new TimelineObject<>(snapshotRegistry, HOMOGENEOUS);\n        this.targetAssignmentEpoch = new TimelineInteger(snapshotRegistry);\n        this.targetAssignment = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.invertedTargetAssignment = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.currentPartitionEpoch = new TimelineHashMap<>(snapshotRegistry, 0);\n        this.metrics = Objects.requireNonNull(metrics);\n        this.numClassicProtocolMembers = new TimelineInteger(snapshotRegistry);\n        this.classicProtocolMembersSupportedProtocols = new TimelineHashMap<>(snapshotRegistry, 0);\n    }\n\n    /**\n     * @return The group type (Consumer).\n     */\n    @Override\n    public GroupType type() {\n        return GroupType.CONSUMER;\n    }\n\n    /**\n     * @return The current state as a String.\n     */\n    @Override\n    public String stateAsString() {\n        return state.get().toString();\n    }\n\n    /**\n     * @return The current state as a String with given committedOffset.\n     */\n    public String stateAsString(long committedOffset) {\n        return state.get(committedOffset).toString();\n    }\n\n    /**\n     * @return the group formatted as a list group response based on the committed offset.\n     */\n    public ListGroupsResponseData.ListedGroup asListedGroup(long committedOffset) {\n        return new ListGroupsResponseData.ListedGroup()\n            .setGroupId(groupId)\n            .setProtocolType(ConsumerProtocol.PROTOCOL_TYPE)\n            .setGroupState(state.get(committedOffset).toString())\n            .setGroupType(type().toString());\n    }\n\n    /**\n     * @return The group id.\n     */\n    @Override\n    public String groupId() {\n        return groupId;\n    }\n\n    /**\n     * @return The current state.\n     */\n    public ConsumerGroupState state() {\n        return state.get();\n    }\n\n    /**\n     * @return The current state based on committed offset.\n     */\n    public ConsumerGroupState state(long committedOffset) {\n        return state.get(committedOffset);\n    }\n\n    /**\n     * @return The group epoch.\n     */\n    public int groupEpoch() {\n        return groupEpoch.get();\n    }\n\n    /**\n     * Sets the group epoch.\n     *\n     * @param groupEpoch The new group epoch.\n     */\n    public void setGroupEpoch(int groupEpoch) {\n        this.groupEpoch.set(groupEpoch);\n        maybeUpdateGroupState();\n    }\n\n    /**\n     * @return The target assignment epoch.\n     */\n    public int assignmentEpoch() {\n        return targetAssignmentEpoch.get();\n    }\n\n    /**\n     * Sets the assignment epoch.\n     *\n     * @param targetAssignmentEpoch The new assignment epoch.\n     */\n    public void setTargetAssignmentEpoch(int targetAssignmentEpoch) {\n        this.targetAssignmentEpoch.set(targetAssignmentEpoch);\n        maybeUpdateGroupState();\n    }\n\n    /**\n     * Sets the number of members using the classic protocol.\n     *\n     * @param numClassicProtocolMembers The new NumClassicProtocolMembers.\n     */\n    public void setNumClassicProtocolMembers(int numClassicProtocolMembers) {\n        this.numClassicProtocolMembers.set(numClassicProtocolMembers);\n    }\n\n    /**\n     * Get member id of a static member that matches the given group\n     * instance id.\n     *\n     * @param groupInstanceId The group instance id.\n     *\n     * @return The member id corresponding to the given instance id or null if it does not exist\n     */\n    public String staticMemberId(String groupInstanceId) {\n        return staticMembers.get(groupInstanceId);\n    }\n\n    /**\n     * Gets or creates a new member but without adding it to the group. Adding a member\n     * is done via the {@link ConsumerGroup#updateMember(ConsumerGroupMember)} method.\n     *\n     * @param memberId          The member id.\n     * @param createIfNotExists Booleans indicating whether the member must be\n     *                          created if it does not exist.\n     *\n     * @return A ConsumerGroupMember.\n     */\n    public ConsumerGroupMember getOrMaybeCreateMember(\n        String memberId,\n        boolean createIfNotExists\n    ) {\n        ConsumerGroupMember member = members.get(memberId);\n        if (member != null) return member;\n\n        if (!createIfNotExists) {\n            throw new UnknownMemberIdException(\n                String.format(\"Member %s is not a member of group %s.\", memberId, groupId)\n            );\n        }\n\n        return new ConsumerGroupMember.Builder(memberId).build();\n    }\n\n    /**\n     * Gets a static member.\n     *\n     * @param instanceId The group instance id.\n     *\n     * @return The member corresponding to the given instance id or null if it does not exist\n     */\n    public ConsumerGroupMember staticMember(String instanceId) {\n        String existingMemberId = staticMemberId(instanceId);\n        return existingMemberId == null ? null : getOrMaybeCreateMember(existingMemberId, false);\n    }\n\n    /**\n     * Adds or updates the member.\n     *\n     * @param newMember The new member state.\n     */\n    public void updateMember(ConsumerGroupMember newMember) {\n        if (newMember == null) {\n            throw new IllegalArgumentException(\"newMember cannot be null.\");\n        }\n        ConsumerGroupMember oldMember = members.put(newMember.memberId(), newMember);\n        maybeUpdateSubscribedTopicNamesAndGroupSubscriptionType(oldMember, newMember);\n        maybeUpdateServerAssignors(oldMember, newMember);\n        maybeUpdatePartitionEpoch(oldMember, newMember);\n        updateStaticMember(newMember);\n        maybeUpdateGroupState();\n        maybeUpdateNumClassicProtocolMembers(oldMember, newMember);\n        maybeUpdateClassicProtocolMembersSupportedProtocols(oldMember, newMember);\n    }\n\n    /**\n     * Updates the member id stored against the instance id if the member is a static member.\n     *\n     * @param newMember The new member state.\n     */\n    private void updateStaticMember(ConsumerGroupMember newMember) {\n        if (newMember.instanceId() != null) {\n            staticMembers.put(newMember.instanceId(), newMember.memberId());\n        }\n    }\n\n    /**\n     * Remove the member from the group.\n     *\n     * @param memberId The member id to remove.\n     */\n    public void removeMember(String memberId) {\n        ConsumerGroupMember oldMember = members.remove(memberId);\n        maybeUpdateSubscribedTopicNamesAndGroupSubscriptionType(oldMember, null);\n        maybeUpdateServerAssignors(oldMember, null);\n        maybeRemovePartitionEpoch(oldMember);\n        removeStaticMember(oldMember);\n        maybeUpdateGroupState();\n        maybeUpdateNumClassicProtocolMembers(oldMember, null);\n        maybeUpdateClassicProtocolMembersSupportedProtocols(oldMember, null);\n    }\n\n    /**\n     * Remove the static member mapping if the removed member is static.\n     *\n     * @param oldMember The member to remove.\n     */\n    private void removeStaticMember(ConsumerGroupMember oldMember) {\n        if (oldMember.instanceId() != null) {\n            staticMembers.remove(oldMember.instanceId());\n        }\n    }\n\n    /**\n     * Returns true if the member exists.\n     *\n     * @param memberId The member id.\n     *\n     * @return A boolean indicating whether the member exists or not.\n     */\n    public boolean hasMember(String memberId) {\n        return members.containsKey(memberId);\n    }\n\n    /**\n     * @return The number of members.\n     */\n    public int numMembers() {\n        return members.size();\n    }\n\n    /**\n     * @return The number of members that use the classic protocol.\n     */\n    public int numClassicProtocolMembers() {\n        return numClassicProtocolMembers.get();\n    }\n\n    /**\n     * @return The map of the protocol name and the number of members using the classic protocol that support it.\n     */\n    public Map<String, Integer> classicMembersSupportedProtocols() {\n        return Collections.unmodifiableMap(classicProtocolMembersSupportedProtocols);\n    }\n\n    /**\n     * @return An immutable Map containing all the members keyed by their id.\n     */\n    public Map<String, ConsumerGroupMember> members() {\n        return Collections.unmodifiableMap(members);\n    }\n\n    /**\n     * @return An immutable Map containing all the static members keyed by instance id.\n     */\n    public Map<String, String> staticMembers() {\n        return Collections.unmodifiableMap(staticMembers);\n    }\n\n    /**\n     * @return An immutable map containing all the subscribed topic names\n     *         with the subscribers counts per topic.\n     */\n    public Map<String, Integer> subscribedTopicNames() {\n        return Collections.unmodifiableMap(subscribedTopicNames);\n    }\n\n    /**\n     * Returns true if the consumer group is actively subscribed to the topic.\n     *\n     * @param topic  The topic name.\n     *\n     * @return Whether the group is subscribed to the topic.\n     */\n    @Override\n    public boolean isSubscribedToTopic(String topic) {\n        return subscribedTopicNames.containsKey(topic);\n    }\n\n    /**\n     * @return The group's subscription type.\n     */\n    public SubscriptionType subscriptionType() {\n        return subscriptionType.get();\n    }\n\n    /**\n     * Returns the target assignment of the member.\n     *\n     * @return The ConsumerGroupMemberAssignment or an EMPTY one if it does not\n     *         exist.\n     */\n    public Assignment targetAssignment(String memberId) {\n        return targetAssignment.getOrDefault(memberId, Assignment.EMPTY);\n    }\n\n    /**\n     * @return An immutable map containing all the topic partitions\n     *         with their current member assignments.\n     */\n    public Map<Uuid, Map<Integer, String>> invertedTargetAssignment() {\n        return Collections.unmodifiableMap(invertedTargetAssignment);\n    }\n\n    /**\n     * Updates the target assignment of a member.\n     *\n     * @param memberId              The member id.\n     * @param newTargetAssignment   The new target assignment.\n     */\n    public void updateTargetAssignment(String memberId, Assignment newTargetAssignment) {\n        updateInvertedTargetAssignment(\n            memberId,\n            targetAssignment.getOrDefault(memberId, new Assignment(Collections.emptyMap())),\n            newTargetAssignment\n        );\n        targetAssignment.put(memberId, newTargetAssignment);\n    }\n\n    /**\n     * Updates the reverse lookup map of the target assignment.\n     *\n     * @param memberId              The member Id.\n     * @param oldTargetAssignment   The old target assignment.\n     * @param newTargetAssignment   The new target assignment.\n     */\n    private void updateInvertedTargetAssignment(\n        String memberId,\n        Assignment oldTargetAssignment,\n        Assignment newTargetAssignment\n    ) {\n        // Combine keys from both old and new assignments.\n        Set<Uuid> allTopicIds = new HashSet<>();\n        allTopicIds.addAll(oldTargetAssignment.partitions().keySet());\n        allTopicIds.addAll(newTargetAssignment.partitions().keySet());\n\n        for (Uuid topicId : allTopicIds) {\n            Set<Integer> oldPartitions = oldTargetAssignment.partitions().getOrDefault(topicId, Collections.emptySet());\n            Set<Integer> newPartitions = newTargetAssignment.partitions().getOrDefault(topicId, Collections.emptySet());\n\n            TimelineHashMap<Integer, String> topicPartitionAssignment = invertedTargetAssignment.computeIfAbsent(\n                topicId, k -> new TimelineHashMap<>(snapshotRegistry, Math.max(oldPartitions.size(), newPartitions.size()))\n            );\n\n            // Remove partitions that aren't present in the new assignment only if the partition is currently\n            // still assigned to the member in question.\n            // If p0 was moved from A to B, and the target assignment map was updated for B first, we don't want to\n            // remove the key p0 from the inverted map and undo the action when A eventually tries to update its assignment.\n            for (Integer partition : oldPartitions) {\n                if (!newPartitions.contains(partition) && memberId.equals(topicPartitionAssignment.get(partition))) {\n                    topicPartitionAssignment.remove(partition);\n                }\n            }\n\n            // Add partitions that are in the new assignment but not in the old assignment.\n            for (Integer partition : newPartitions) {\n                if (!oldPartitions.contains(partition)) {\n                    topicPartitionAssignment.put(partition, memberId);\n                }\n            }\n\n            if (topicPartitionAssignment.isEmpty()) {\n                invertedTargetAssignment.remove(topicId);\n            } else {\n                invertedTargetAssignment.put(topicId, topicPartitionAssignment);\n            }\n        }\n    }\n\n    /**\n     * Removes the target assignment of a member.\n     *\n     * @param memberId The member id.\n     */\n    public void removeTargetAssignment(String memberId) {\n        updateInvertedTargetAssignment(\n            memberId,\n            targetAssignment.getOrDefault(memberId, Assignment.EMPTY),\n            Assignment.EMPTY\n        );\n        targetAssignment.remove(memberId);\n    }\n\n    /**\n     * @return An immutable Map containing all the target assignment keyed by member id.\n     */\n    public Map<String, Assignment> targetAssignment() {\n        return Collections.unmodifiableMap(targetAssignment);\n    }\n\n    /**\n     * Returns the current epoch of a partition or -1 if the partition\n     * does not have one.\n     *\n     * @param topicId       The topic id.\n     * @param partitionId   The partition id.\n     *\n     * @return The epoch or -1.\n     */\n    public int currentPartitionEpoch(\n        Uuid topicId, int partitionId\n    ) {\n        Map<Integer, Integer> partitions = currentPartitionEpoch.get(topicId);\n        if (partitions == null) {\n            return -1;\n        } else {\n            return partitions.getOrDefault(partitionId, -1);\n        }\n    }\n\n    /**\n     * Compute the preferred (server side) assignor for the group while\n     * taking into account the updated member. The computation relies\n     * on {{@link ConsumerGroup#serverAssignors}} persisted structure\n     * but it does not update it.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     *\n     * @return An Optional containing the preferred assignor.\n     */\n    public Optional<String> computePreferredServerAssignor(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        // Copy the current count and update it.\n        Map<String, Integer> counts = new HashMap<>(this.serverAssignors);\n        maybeUpdateServerAssignors(counts, oldMember, newMember);\n\n        return counts.entrySet().stream()\n            .max(Map.Entry.comparingByValue())\n            .map(Map.Entry::getKey);\n    }\n\n    /**\n     * @return The preferred assignor for the group.\n     */\n    public Optional<String> preferredServerAssignor() {\n        return preferredServerAssignor(Long.MAX_VALUE);\n    }\n\n    /**\n     * @return The preferred assignor for the group with given offset.\n     */\n    public Optional<String> preferredServerAssignor(long committedOffset) {\n        return serverAssignors.entrySet(committedOffset).stream()\n            .max(Map.Entry.comparingByValue())\n            .map(Map.Entry::getKey);\n    }\n\n    /**\n     * @return An immutable Map of subscription metadata for\n     *         each topic that the consumer group is subscribed to.\n     */\n    public Map<String, TopicMetadata> subscriptionMetadata() {\n        return Collections.unmodifiableMap(subscribedTopicMetadata);\n    }\n\n    /**\n     * Updates the subscription metadata. This replaces the previous one.\n     *\n     * @param subscriptionMetadata The new subscription metadata.\n     */\n    public void setSubscriptionMetadata(\n        Map<String, TopicMetadata> subscriptionMetadata\n    ) {\n        this.subscribedTopicMetadata.clear();\n        this.subscribedTopicMetadata.putAll(subscriptionMetadata);\n    }\n\n    /**\n     * Computes the subscription metadata based on the current subscription info.\n     *\n     * @param subscribedTopicNames      Map of topic names to the number of subscribers.\n     * @param topicsImage               The current metadata for all available topics.\n     * @param clusterImage              The current metadata for the Kafka cluster.\n     *\n     * @return An immutable map of subscription metadata for each topic that the consumer group is subscribed to.\n     */\n    public Map<String, TopicMetadata> computeSubscriptionMetadata(\n        Map<String, Integer> subscribedTopicNames,\n        TopicsImage topicsImage,\n        ClusterImage clusterImage\n    ) {\n        // Create the topic metadata for each subscribed topic.\n        Map<String, TopicMetadata> newSubscriptionMetadata = new HashMap<>(subscribedTopicNames.size());\n\n        subscribedTopicNames.forEach((topicName, count) -> {\n            TopicImage topicImage = topicsImage.getTopic(topicName);\n            if (topicImage != null) {\n                Map<Integer, Set<String>> partitionRacks = new HashMap<>();\n                topicImage.partitions().forEach((partition, partitionRegistration) -> {\n                    Set<String> racks = new HashSet<>();\n                    for (int replica : partitionRegistration.replicas) {\n                        Optional<String> rackOptional = clusterImage.broker(replica).rack();\n                        // Only add the rack if it is available for the broker/replica.\n                        rackOptional.ifPresent(racks::add);\n                    }\n                    // If rack information is unavailable for all replicas of this partition,\n                    // no corresponding entry will be stored for it in the map.\n                    if (!racks.isEmpty())\n                        partitionRacks.put(partition, racks);\n                });\n\n                newSubscriptionMetadata.put(topicName, new TopicMetadata(\n                    topicImage.id(),\n                    topicImage.name(),\n                    topicImage.partitions().size(),\n                    partitionRacks)\n                );\n            }\n        });\n\n        return Collections.unmodifiableMap(newSubscriptionMetadata);\n    }\n\n    /**\n     * Updates the metadata refresh deadline.\n     *\n     * @param deadlineMs The deadline in milliseconds.\n     * @param groupEpoch The associated group epoch.\n     */\n    public void setMetadataRefreshDeadline(\n        long deadlineMs,\n        int groupEpoch\n    ) {\n        this.metadataRefreshDeadline = new DeadlineAndEpoch(deadlineMs, groupEpoch);\n    }\n\n    /**\n     * Requests a metadata refresh.\n     */\n    public void requestMetadataRefresh() {\n        this.metadataRefreshDeadline = DeadlineAndEpoch.EMPTY;\n    }\n\n    /**\n     * Checks if a metadata refresh is required. A refresh is required in two cases:\n     * 1) The deadline is smaller or equal to the current time;\n     * 2) The group epoch associated with the deadline is larger than\n     *    the current group epoch. This means that the operations which updated\n     *    the deadline failed.\n     *\n     * @param currentTimeMs The current time in milliseconds.\n     * @return A boolean indicating whether a refresh is required or not.\n     */\n    public boolean hasMetadataExpired(long currentTimeMs) {\n        return currentTimeMs >= metadataRefreshDeadline.deadlineMs || groupEpoch() < metadataRefreshDeadline.epoch;\n    }\n\n    /**\n     * @return The metadata refresh deadline.\n     */\n    public DeadlineAndEpoch metadataRefreshDeadline() {\n        return metadataRefreshDeadline;\n    }\n\n    /**\n     * Validates the OffsetCommit request.\n     *\n     * @param memberId          The member id.\n     * @param groupInstanceId   The group instance id.\n     * @param memberEpoch       The member epoch.\n     * @param isTransactional   Whether the offset commit is transactional or not. It has no\n     *                          impact when a consumer group is used.\n     * @param apiVersion        The api version.\n     * @throws UnknownMemberIdException     If the member is not found.\n     * @throws StaleMemberEpochException    If the member uses the consumer protocol and the provided\n     *                                      member epoch doesn't match the actual member epoch.\n     * @throws IllegalGenerationException   If the member uses the classic protocol and the provided\n     *                                      generation id is not equal to the member epoch.\n     */\n    @Override\n    public void validateOffsetCommit(\n        String memberId,\n        String groupInstanceId,\n        int memberEpoch,\n        boolean isTransactional,\n        short apiVersion\n    ) throws UnknownMemberIdException, StaleMemberEpochException, IllegalGenerationException {\n        // When the member epoch is -1, the request comes from either the admin client\n        // or a consumer which does not use the group management facility. In this case,\n        // the request can commit offsets if the group is empty.\n        if (memberEpoch < 0 && members().isEmpty()) return;\n\n        final ConsumerGroupMember member = getOrMaybeCreateMember(memberId, false);\n\n        // If the commit is not transactional and the member uses the new consumer protocol (KIP-848),\n        // the member should be using the OffsetCommit API version >= 9.\n        if (!isTransactional && !member.useClassicProtocol() && apiVersion < 9) {\n            throw new UnsupportedVersionException(\"OffsetCommit version 9 or above must be used \" +\n                \"by members using the consumer group protocol\");\n        }\n\n        validateMemberEpoch(memberEpoch, member.memberEpoch(), member.useClassicProtocol());\n    }\n\n    /**\n     * Validates the OffsetFetch request.\n     *\n     * @param memberId              The member id for consumer groups.\n     * @param memberEpoch           The member epoch for consumer groups.\n     * @param lastCommittedOffset   The last committed offsets in the timeline.\n     * @throws UnknownMemberIdException     If the member is not found.\n     * @throws StaleMemberEpochException    If the member uses the consumer protocol and the provided\n     *                                      member epoch doesn't match the actual member epoch.\n     * @throws IllegalGenerationException   If the member uses the classic protocol and the provided\n     *                                      generation id is not equal to the member epoch.\n     */\n    @Override\n    public void validateOffsetFetch(\n        String memberId,\n        int memberEpoch,\n        long lastCommittedOffset\n    ) throws UnknownMemberIdException, StaleMemberEpochException, IllegalGenerationException {\n        // When the member id is null and the member epoch is -1, the request either comes\n        // from the admin client or from a client which does not provide them. In this case,\n        // the fetch request is accepted.\n        if (memberId == null && memberEpoch < 0) return;\n\n        final ConsumerGroupMember member = members.get(memberId, lastCommittedOffset);\n        if (member == null) {\n            throw new UnknownMemberIdException(String.format(\"Member %s is not a member of group %s.\",\n                memberId, groupId));\n        }\n        validateMemberEpoch(memberEpoch, member.memberEpoch(), member.useClassicProtocol());\n    }\n\n    /**\n     * Validates the OffsetDelete request.\n     */\n    @Override\n    public void validateOffsetDelete() {}\n\n    /**\n     * Validates the DeleteGroups request.\n     */\n    @Override\n    public void validateDeleteGroup() throws ApiException {\n        if (state() != ConsumerGroupState.EMPTY) {\n            throw Errors.NON_EMPTY_GROUP.exception();\n        }\n    }\n\n    /**\n     * Populates the list of records with tombstone(s) for deleting the group.\n     *\n     * @param records The list of records.\n     */\n    @Override\n    public void createGroupTombstoneRecords(List<CoordinatorRecord> records) {\n        members().forEach((memberId, member) ->\n            records.add(CoordinatorRecordHelpers.newCurrentAssignmentTombstoneRecord(groupId(), memberId))\n        );\n\n        members().forEach((memberId, member) ->\n            records.add(CoordinatorRecordHelpers.newTargetAssignmentTombstoneRecord(groupId(), memberId))\n        );\n        records.add(CoordinatorRecordHelpers.newTargetAssignmentEpochTombstoneRecord(groupId()));\n\n        members().forEach((memberId, member) ->\n            records.add(CoordinatorRecordHelpers.newMemberSubscriptionTombstoneRecord(groupId(), memberId))\n        );\n\n        records.add(CoordinatorRecordHelpers.newGroupSubscriptionMetadataTombstoneRecord(groupId()));\n        records.add(CoordinatorRecordHelpers.newGroupEpochTombstoneRecord(groupId()));\n    }\n\n    @Override\n    public boolean isEmpty() {\n        return state() == ConsumerGroupState.EMPTY;\n    }\n\n    /**\n     * See {@link org.apache.kafka.coordinator.group.OffsetExpirationCondition}\n     *\n     * @return The offset expiration condition for the group or Empty if no such condition exists.\n     */\n    @Override\n    public Optional<OffsetExpirationCondition> offsetExpirationCondition() {\n        return Optional.of(new OffsetExpirationConditionImpl(offsetAndMetadata -> offsetAndMetadata.commitTimestampMs));\n    }\n\n    @Override\n    public boolean isInStates(Set<String> statesFilter, long committedOffset) {\n        return statesFilter.contains(state.get(committedOffset).toLowerCaseString());\n    }\n\n    /**\n     * Throws an exception if the received member epoch does not match the expected member epoch.\n     *\n     * @param receivedMemberEpoch   The received member epoch or generation id.\n     * @param expectedMemberEpoch   The expected member epoch.\n     * @param useClassicProtocol    The boolean indicating whether the checked member uses the classic protocol.\n     * @throws StaleMemberEpochException    if the member with unmatched member epoch uses the consumer protocol.\n     * @throws IllegalGenerationException   if the member with unmatched generation id uses the classic protocol.\n     */\n    private void validateMemberEpoch(\n        int receivedMemberEpoch,\n        int expectedMemberEpoch,\n        boolean useClassicProtocol\n    ) throws StaleMemberEpochException, IllegalGenerationException {\n        if (receivedMemberEpoch != expectedMemberEpoch) {\n            if (useClassicProtocol) {\n                throw new IllegalGenerationException(String.format(\"The received generation id %d does not match \" +\n                    \"the expected member epoch %d.\", receivedMemberEpoch, expectedMemberEpoch));\n            } else {\n                throw new StaleMemberEpochException(String.format(\"The received member epoch %d does not match \"\n                    + \"the expected member epoch %d.\", receivedMemberEpoch, expectedMemberEpoch));\n            }\n        }\n    }\n\n    /**\n     * Updates the current state of the group.\n     */\n    private void maybeUpdateGroupState() {\n        ConsumerGroupState previousState = state.get();\n        ConsumerGroupState newState = STABLE;\n        if (members.isEmpty()) {\n            newState = EMPTY;\n        } else if (groupEpoch.get() > targetAssignmentEpoch.get()) {\n            newState = ASSIGNING;\n        } else {\n            for (ConsumerGroupMember member : members.values()) {\n                if (!member.isReconciledTo(targetAssignmentEpoch.get())) {\n                    newState = RECONCILING;\n                    break;\n                }\n            }\n        }\n\n        state.set(newState);\n        metrics.onConsumerGroupStateTransition(previousState, newState);\n    }\n\n    /**\n     * Updates the server assignors count.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdateServerAssignors(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        maybeUpdateServerAssignors(serverAssignors, oldMember, newMember);\n    }\n\n    /**\n     * Updates the server assignors count.\n     *\n     * @param serverAssignorCount   The count to update.\n     * @param oldMember             The old member.\n     * @param newMember             The new member.\n     */\n    private static void maybeUpdateServerAssignors(\n        Map<String, Integer> serverAssignorCount,\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        if (oldMember != null) {\n            oldMember.serverAssignorName().ifPresent(name ->\n                serverAssignorCount.compute(name, ConsumerGroup::decValue)\n            );\n        }\n        if (newMember != null) {\n            newMember.serverAssignorName().ifPresent(name ->\n                serverAssignorCount.compute(name, ConsumerGroup::incValue)\n            );\n        }\n    }\n\n    /**\n     * Updates the number of the members that use the classic protocol.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdateNumClassicProtocolMembers(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        int delta = 0;\n        if (oldMember != null && oldMember.useClassicProtocol()) {\n            delta--;\n        }\n        if (newMember != null && newMember.useClassicProtocol()) {\n            delta++;\n        }\n        setNumClassicProtocolMembers(numClassicProtocolMembers() + delta);\n    }\n\n    /**\n     * Updates the supported protocol count of the members that use the classic protocol.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdateClassicProtocolMembersSupportedProtocols(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        if (oldMember != null) {\n            oldMember.supportedClassicProtocols().ifPresent(protocols ->\n                protocols.forEach(protocol ->\n                    classicProtocolMembersSupportedProtocols.compute(protocol.name(), ConsumerGroup::decValue)\n                )\n            );\n        }\n        if (newMember != null) {\n            newMember.supportedClassicProtocols().ifPresent(protocols ->\n                protocols.forEach(protocol ->\n                    classicProtocolMembersSupportedProtocols.compute(protocol.name(), ConsumerGroup::incValue)\n                )\n            );\n        }\n    }\n\n    /**\n     * Updates the subscribed topic names count.\n     * The subscription type is updated as a consequence.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdateSubscribedTopicNamesAndGroupSubscriptionType(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        maybeUpdateSubscribedTopicNames(subscribedTopicNames, oldMember, newMember);\n        subscriptionType.set(subscriptionType(subscribedTopicNames, members.size()));\n    }\n\n    /**\n     * Updates the subscription count.\n     *\n     * @param subscribedTopicCount  The map to update.\n     * @param oldMember             The old member.\n     * @param newMember             The new member.\n     */\n    private static void maybeUpdateSubscribedTopicNames(\n        Map<String, Integer> subscribedTopicCount,\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        if (oldMember != null) {\n            oldMember.subscribedTopicNames().forEach(topicName ->\n                subscribedTopicCount.compute(topicName, ConsumerGroup::decValue)\n            );\n        }\n\n        if (newMember != null) {\n            newMember.subscribedTopicNames().forEach(topicName ->\n                subscribedTopicCount.compute(topicName, ConsumerGroup::incValue)\n            );\n        }\n    }\n\n    /**\n     * Updates the subscription count.\n     *\n     * @param oldMember             The old member.\n     * @param newMember             The new member.\n     *\n     * @return Copy of the map of topics to the count of number of subscribers.\n     */\n    public Map<String, Integer> computeSubscribedTopicNames(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        Map<String, Integer> subscribedTopicNames = new HashMap<>(this.subscribedTopicNames);\n        maybeUpdateSubscribedTopicNames(\n            subscribedTopicNames,\n            oldMember,\n            newMember\n        );\n        return subscribedTopicNames;\n    }\n\n    /**\n     * Updates the subscription count with a set of members removed.\n     *\n     * @param removedMembers        The set of removed members.\n     *\n     * @return Copy of the map of topics to the count of number of subscribers.\n     */\n    public Map<String, Integer> computeSubscribedTopicNames(\n        Set<ConsumerGroupMember> removedMembers\n    ) {\n        Map<String, Integer> subscribedTopicNames = new HashMap<>(this.subscribedTopicNames);\n        if (removedMembers != null) {\n            removedMembers.forEach(removedMember ->\n                maybeUpdateSubscribedTopicNames(\n                    subscribedTopicNames,\n                    removedMember,\n                    null\n                )\n            );\n        }\n        return subscribedTopicNames;\n    }\n\n    /**\n     * Compute the subscription type of the consumer group.\n     *\n     * @param subscribedTopicNames      A map of topic names to the count of members subscribed to each topic.\n     *\n     * @return {@link SubscriptionType#HOMOGENEOUS} if all members are subscribed to exactly the same topics;\n     *         otherwise, {@link SubscriptionType#HETEROGENEOUS}.\n     */\n    public static SubscriptionType subscriptionType(\n        Map<String, Integer> subscribedTopicNames,\n        int numberOfMembers\n    ) {\n        if (subscribedTopicNames.isEmpty()) {\n            return HOMOGENEOUS;\n        }\n\n        for (int subscriberCount : subscribedTopicNames.values()) {\n            if (subscriberCount != numberOfMembers) {\n                return HETEROGENEOUS;\n            }\n        }\n        return HOMOGENEOUS;\n    }\n\n    /**\n     * Updates the partition epochs based on the old and the new member.\n     *\n     * @param oldMember The old member.\n     * @param newMember The new member.\n     */\n    private void maybeUpdatePartitionEpoch(\n        ConsumerGroupMember oldMember,\n        ConsumerGroupMember newMember\n    ) {\n        maybeRemovePartitionEpoch(oldMember);\n        addPartitionEpochs(newMember.assignedPartitions(), newMember.memberEpoch());\n        addPartitionEpochs(newMember.partitionsPendingRevocation(), newMember.memberEpoch());\n    }\n\n    /**\n     * Removes the partition epochs for the provided member.\n     *\n     * @param oldMember The old member.\n     */\n    private void maybeRemovePartitionEpoch(\n        ConsumerGroupMember oldMember\n    ) {\n        if (oldMember != null) {\n            removePartitionEpochs(oldMember.assignedPartitions(), oldMember.memberEpoch());\n            removePartitionEpochs(oldMember.partitionsPendingRevocation(), oldMember.memberEpoch());\n        }\n    }\n\n    /**\n     * Removes the partition epochs based on the provided assignment.\n     *\n     * @param assignment    The assignment.\n     * @param expectedEpoch The expected epoch.\n     * @throws IllegalStateException if the epoch does not match the expected one.\n     * package-private for testing.\n     */\n    void removePartitionEpochs(\n        Map<Uuid, Set<Integer>> assignment,\n        int expectedEpoch\n    ) {\n        assignment.forEach((topicId, assignedPartitions) -> {\n            currentPartitionEpoch.compute(topicId, (__, partitionsOrNull) -> {\n                if (partitionsOrNull != null) {\n                    assignedPartitions.forEach(partitionId -> {\n                        Integer prevValue = partitionsOrNull.remove(partitionId);\n                        if (prevValue != expectedEpoch) {\n                            throw new IllegalStateException(\n                                String.format(\"Cannot remove the epoch %d from %s-%s because the partition is \" +\n                                    \"still owned at a different epoch %d\", expectedEpoch, topicId, partitionId, prevValue));\n                        }\n                    });\n                    if (partitionsOrNull.isEmpty()) {\n                        return null;\n                    } else {\n                        return partitionsOrNull;\n                    }\n                } else {\n                    throw new IllegalStateException(\n                        String.format(\"Cannot remove the epoch %d from %s because it does not have any epoch\",\n                            expectedEpoch, topicId));\n                }\n            });\n        });\n    }\n\n    /**\n     * Adds the partitions epoch based on the provided assignment.\n     *\n     * @param assignment    The assignment.\n     * @param epoch         The new epoch.\n     * @throws IllegalStateException if the partition already has an epoch assigned.\n     * package-private for testing.\n     */\n    void addPartitionEpochs(\n        Map<Uuid, Set<Integer>> assignment,\n        int epoch\n    ) {\n        assignment.forEach((topicId, assignedPartitions) -> {\n            currentPartitionEpoch.compute(topicId, (__, partitionsOrNull) -> {\n                if (partitionsOrNull == null) {\n                    partitionsOrNull = new TimelineHashMap<>(snapshotRegistry, assignedPartitions.size());\n                }\n                for (Integer partitionId : assignedPartitions) {\n                    Integer prevValue = partitionsOrNull.put(partitionId, epoch);\n                    if (prevValue != null) {\n                        throw new IllegalStateException(\n                            String.format(\"Cannot set the epoch of %s-%s to %d because the partition is \" +\n                                \"still owned at epoch %d\", topicId, partitionId, epoch, prevValue));\n                    }\n                }\n                return partitionsOrNull;\n            });\n        });\n    }\n\n    /**\n     * Decrements value by 1; returns null when reaching zero. This helper is\n     * meant to be used with Map#compute.\n     */\n    private static Integer decValue(String key, Integer value) {\n        if (value == null) return null;\n        return value == 1 ? null : value - 1;\n    }\n\n    /**\n     * Increments value by 1; This helper is meant to be used with Map#compute.\n     */\n    private static Integer incValue(String key, Integer value) {\n        return value == null ? 1 : value + 1;\n    }\n\n    public ConsumerGroupDescribeResponseData.DescribedGroup asDescribedGroup(\n        long committedOffset,\n        String defaultAssignor,\n        TopicsImage topicsImage\n    ) {\n        ConsumerGroupDescribeResponseData.DescribedGroup describedGroup = new ConsumerGroupDescribeResponseData.DescribedGroup()\n            .setGroupId(groupId)\n            .setAssignorName(preferredServerAssignor(committedOffset).orElse(defaultAssignor))\n            .setGroupEpoch(groupEpoch.get(committedOffset))\n            .setGroupState(state.get(committedOffset).toString())\n            .setAssignmentEpoch(targetAssignmentEpoch.get(committedOffset));\n        members.entrySet(committedOffset).forEach(\n            entry -> describedGroup.members().add(\n                entry.getValue().asConsumerGroupDescribeMember(\n                    targetAssignment.get(entry.getValue().memberId(), committedOffset),\n                    topicsImage\n                )\n            )\n        );\n        return describedGroup;\n    }\n\n    /**\n     * Create a new consumer group according to the given classic group.\n     *\n     * @param snapshotRegistry  The SnapshotRegistry.\n     * @param metrics           The GroupCoordinatorMetricsShard.\n     * @param classicGroup      The converted classic group.\n     * @param topicsImage       The TopicsImage for topic id and topic name conversion.\n     * @return  The created ConsumerGruop.\n     */\n    public static ConsumerGroup fromClassicGroup(\n        SnapshotRegistry snapshotRegistry,\n        GroupCoordinatorMetricsShard metrics,\n        ClassicGroup classicGroup,\n        TopicsImage topicsImage\n    ) {\n        String groupId = classicGroup.groupId();\n        ConsumerGroup consumerGroup = new ConsumerGroup(snapshotRegistry, groupId, metrics);\n        consumerGroup.setGroupEpoch(classicGroup.generationId());\n        consumerGroup.setTargetAssignmentEpoch(classicGroup.generationId());\n\n        classicGroup.allMembers().forEach(classicGroupMember -> {\n            Map<Uuid, Set<Integer>> assignedPartitions = toTopicPartitionMap(\n                ConsumerProtocol.deserializeConsumerProtocolAssignment(\n                    ByteBuffer.wrap(classicGroupMember.assignment())\n                ),\n                topicsImage\n            );\n            ConsumerProtocolSubscription subscription = ConsumerProtocol.deserializeConsumerProtocolSubscription(\n                ByteBuffer.wrap(classicGroupMember.metadata(classicGroup.protocolName().get()))\n            );\n\n            // The target assignment and the assigned partitions of each member are set based on the last\n            // assignment of the classic group. All the members are put in the Stable state. If the classic\n            // group was in Preparing Rebalance or Completing Rebalance states, the classic members are\n            // asked to rejoin the group to re-trigger a rebalance or collect their assignments.\n            ConsumerGroupMember newMember = new ConsumerGroupMember.Builder(classicGroupMember.memberId())\n                .setMemberEpoch(classicGroup.generationId())\n                .setState(MemberState.STABLE)\n                .setPreviousMemberEpoch(classicGroup.generationId())\n                .setInstanceId(classicGroupMember.groupInstanceId().orElse(null))\n                .setRackId(toOptional(subscription.rackId()).orElse(null))\n                .setRebalanceTimeoutMs(classicGroupMember.rebalanceTimeoutMs())\n                .setClientId(classicGroupMember.clientId())\n                .setClientHost(classicGroupMember.clientHost())\n                .setSubscribedTopicNames(subscription.topics())\n                .setAssignedPartitions(assignedPartitions)\n                .setClassicMemberMetadata(\n                    new ConsumerGroupMemberMetadataValue.ClassicMemberMetadata()\n                        .setSessionTimeoutMs(classicGroupMember.sessionTimeoutMs())\n                        .setSupportedProtocols(ConsumerGroupMember.classicProtocolListFromJoinRequestProtocolCollection(\n                            classicGroupMember.supportedProtocols()\n                        ))\n                )\n                .build();\n            consumerGroup.updateTargetAssignment(newMember.memberId(), new Assignment(assignedPartitions));\n            consumerGroup.updateMember(newMember);\n        });\n\n        return consumerGroup;\n    }\n\n    /**\n     * Populate the record list with the records needed to create the given consumer group.\n     *\n     * @param records The list to which the new records are added.\n     */\n    public void createConsumerGroupRecords(\n        List<CoordinatorRecord> records\n    ) {\n        members().forEach((__, consumerGroupMember) ->\n            records.add(CoordinatorRecordHelpers.newMemberSubscriptionRecord(groupId(), consumerGroupMember))\n        );\n\n        records.add(CoordinatorRecordHelpers.newGroupEpochRecord(groupId(), groupEpoch()));\n\n        members().forEach((consumerGroupMemberId, consumerGroupMember) ->\n            records.add(CoordinatorRecordHelpers.newTargetAssignmentRecord(\n                groupId(),\n                consumerGroupMemberId,\n                targetAssignment(consumerGroupMemberId).partitions()\n            ))\n        );\n\n        records.add(CoordinatorRecordHelpers.newTargetAssignmentEpochRecord(groupId(), groupEpoch()));\n\n        members().forEach((__, consumerGroupMember) ->\n            records.add(CoordinatorRecordHelpers.newCurrentAssignmentRecord(groupId(), consumerGroupMember))\n        );\n    }\n\n    /**\n     * Checks whether at least one of the given protocols can be supported. A\n     * protocol can be supported if it is supported by all members that use the\n     * classic protocol.\n     *\n     * @param memberProtocolType  The member protocol type.\n     * @param memberProtocols     The set of protocol names.\n     *\n     * @return A boolean based on the condition mentioned above.\n     */\n    public boolean supportsClassicProtocols(String memberProtocolType, Set<String> memberProtocols) {\n        if (ConsumerProtocol.PROTOCOL_TYPE.equals(memberProtocolType)) {\n            if (isEmpty()) {\n                return !memberProtocols.isEmpty();\n            } else {\n                return memberProtocols.stream().anyMatch(\n                    name -> classicProtocolMembersSupportedProtocols.getOrDefault(name, 0) == numClassicProtocolMembers()\n                );\n            }\n        }\n        return false;\n    }\n\n    /**\n     * Checks whether all the members use the classic protocol except the given member.\n     *\n     * @param memberId The member to remove.\n     * @return A boolean indicating whether all the members use the classic protocol.\n     */\n    public boolean allMembersUseClassicProtocolExcept(String memberId) {\n        return numClassicProtocolMembers() == members().size() - 1 &&\n            !getOrMaybeCreateMember(memberId, false).useClassicProtocol();\n    }\n\n    /**\n     * Checks whether the member has any unreleased partition.\n     *\n     * @param member The member to check.\n     * @return A boolean indicating whether the member has partitions in the target\n     *         assignment that hasn't been revoked by other members.\n     */\n    public boolean waitingOnUnreleasedPartition(ConsumerGroupMember member) {\n        if (member.state() == MemberState.UNRELEASED_PARTITIONS) {\n            for (Map.Entry<Uuid, Set<Integer>> entry : targetAssignment().get(member.memberId()).partitions().entrySet()) {\n                Uuid topicId = entry.getKey();\n                Set<Integer> assignedPartitions = member.assignedPartitions().getOrDefault(topicId, Collections.emptySet());\n\n                for (int partition : entry.getValue()) {\n                    if (!assignedPartitions.contains(partition) && currentPartitionEpoch(topicId, partition) != -1) {\n                        return true;\n                    }\n                }\n            }\n        }\n        return false;\n    }\n}"
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [],
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "e1b2adea07cebaac6d1138ec18778c575aef57f4",
        "url": "https://github.com/apache/kafka/commit/e1b2adea07cebaac6d1138ec18778c575aef57f4",
        "move_method_refactoring": {
            "type": "Move Method",
            "description": "Move Method package findMetric(name String) : Metric from class org.apache.kafka.server.AssignmentsManagerTest to package findMetric(name MetricName) : Metric from class org.apache.kafka.server.AssignmentsManagerTest.TestEnv",
            "leftSideLocations": [
                {
                    "filePath": "server/src/test/java/org/apache/kafka/server/AssignmentsManagerTest.java",
                    "startLine": 384,
                    "endLine": 392,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "original method declaration",
                    "codeElement": "package findMetric(name String) : Metric"
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "server/src/test/java/org/apache/kafka/server/AssignmentsManagerTest.java",
                    "startLine": 232,
                    "endLine": 239,
                    "startColumn": 9,
                    "endColumn": 10,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "moved method declaration",
                    "codeElement": "package findMetric(name MetricName) : Metric"
                }
            ],
            "isStatic": true
        },
        "telemetry": {
            "id": "dc73f430-0208-40ed-9a93-6e23e5fe58b8",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 395,
                "lineStart": 70,
                "lineEnd": 464,
                "bodyLineStart": 70,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/server/src/test/java/org/apache/kafka/server/AssignmentsManagerTest.java",
                "sourceCode": "public class AssignmentsManagerTest {\n\n    private static final Uuid TOPIC_1 = Uuid.fromString(\"88rnFIqYSZykX4ZSKv81bg\");\n    private static final Uuid TOPIC_2 = Uuid.fromString(\"VKCnzHdhR5uDQc1shqBYrQ\");\n    private static final Uuid DIR_1 = Uuid.fromString(\"cbgD8WdLQCyzLrFIMBhv3w\");\n    private static final Uuid DIR_2 = Uuid.fromString(\"zO0bDc0vSuam7Db9iH7rYQ\");\n    private static final Uuid DIR_3 = Uuid.fromString(\"CGBWbrFkRkeJQy6Aryzq2Q\");\n\n    private MockTime time;\n    private NodeToControllerChannelManager channelManager;\n    private AssignmentsManager manager;\n\n    @BeforeEach\n    public void setup() {\n        time = new MockTime();\n        channelManager = mock(NodeToControllerChannelManager.class);\n        Map<Uuid, String> topicNames = new HashMap<>();\n        topicNames.put(TOPIC_1, \"TOPIC_1\");\n        topicNames.put(TOPIC_2, \"TOPIC_2\");\n        Map<Uuid, String> dirPaths = new HashMap<>();\n        dirPaths.put(DIR_1, \"DIR_1\");\n        dirPaths.put(DIR_2, \"DIR_2\");\n        dirPaths.put(DIR_3, \"DIR_3\");\n        manager = new AssignmentsManager(time, channelManager, 8, () -> 100L,\n                id -> Optional.ofNullable(dirPaths.get(id)), id -> Optional.ofNullable(topicNames.get(id)));\n    }\n\n    @AfterEach\n    void tearDown() throws InterruptedException {\n        manager.close();\n    }\n\n    AssignReplicasToDirsRequestData normalize(AssignReplicasToDirsRequestData request) {\n        request = request.duplicate();\n        request.directories().sort(Comparator.comparing(\n            AssignReplicasToDirsRequestData.DirectoryData::id));\n        for (AssignReplicasToDirsRequestData.DirectoryData directory : request.directories()) {\n            directory.topics().sort(Comparator.comparing(\n                AssignReplicasToDirsRequestData.TopicData::topicId));\n            for (AssignReplicasToDirsRequestData.TopicData topic : directory.topics()) {\n                topic.partitions().sort(Comparator.comparing(\n                    AssignReplicasToDirsRequestData.PartitionData::partitionIndex));\n            }\n        }\n        return request;\n    }\n\n\n    void assertRequestEquals(\n        AssignReplicasToDirsRequestData expected,\n        AssignReplicasToDirsRequestData actual\n    ) {\n        assertEquals(normalize(expected), normalize(actual));\n    }\n\n    @Test\n    void testBuildRequestData() {\n        Map<TopicIdPartition, Uuid> assignment = new HashMap<TopicIdPartition, Uuid>() {{\n                put(new TopicIdPartition(TOPIC_1, 1), DIR_1);\n                put(new TopicIdPartition(TOPIC_1, 2), DIR_2);\n                put(new TopicIdPartition(TOPIC_1, 3), DIR_3);\n                put(new TopicIdPartition(TOPIC_1, 4), DIR_1);\n                put(new TopicIdPartition(TOPIC_2, 5), DIR_2);\n            }};\n        AssignReplicasToDirsRequestData built = AssignmentsManager.buildRequestData(8, 100L, assignment);\n        AssignReplicasToDirsRequestData expected = new AssignReplicasToDirsRequestData()\n            .setBrokerId(8)\n            .setBrokerEpoch(100L)\n            .setDirectories(Arrays.asList(\n                new AssignReplicasToDirsRequestData.DirectoryData()\n                    .setId(DIR_2)\n                    .setTopics(Arrays.asList(\n                        new AssignReplicasToDirsRequestData.TopicData()\n                            .setTopicId(TOPIC_1)\n                            .setPartitions(Collections.singletonList(\n                                    new AssignReplicasToDirsRequestData.PartitionData()\n                                            .setPartitionIndex(2))),\n                new AssignReplicasToDirsRequestData.TopicData()\n                    .setTopicId(TOPIC_2)\n                    .setPartitions(Collections.singletonList(\n                            new AssignReplicasToDirsRequestData.PartitionData()\n                                    .setPartitionIndex(5))))),\n            new AssignReplicasToDirsRequestData.DirectoryData()\n                .setId(DIR_3)\n                .setTopics(Collections.singletonList(\n                    new AssignReplicasToDirsRequestData.TopicData()\n                        .setTopicId(TOPIC_1)\n                        .setPartitions(Collections.singletonList(\n                            new AssignReplicasToDirsRequestData.PartitionData()\n                                    .setPartitionIndex(3))))),\n            new AssignReplicasToDirsRequestData.DirectoryData()\n                .setId(DIR_1)\n                .setTopics(Collections.singletonList(\n                    new AssignReplicasToDirsRequestData.TopicData()\n                        .setTopicId(TOPIC_1)\n                        .setPartitions(Arrays.asList(\n                            new AssignReplicasToDirsRequestData.PartitionData()\n                                .setPartitionIndex(4),\n                            new AssignReplicasToDirsRequestData.PartitionData()\n                                .setPartitionIndex(1)))))));\n        assertRequestEquals(expected, built);\n    }\n\n    @Test\n    public void testAssignmentAggregation() throws InterruptedException {\n        CountDownLatch readyToAssert = new CountDownLatch(1);\n        doAnswer(invocation -> {\n            readyToAssert.countDown();\n            return null;\n        }).when(channelManager).sendRequest(any(AssignReplicasToDirsRequest.Builder.class),\n            any(ControllerRequestCompletionHandler.class));\n\n        manager.onAssignment(new TopicIdPartition(TOPIC_1, 1), DIR_1, \"testAssignmentAggregation\", () -> { });\n        manager.onAssignment(new TopicIdPartition(TOPIC_1, 2), DIR_2, \"testAssignmentAggregation\", () -> { });\n        manager.onAssignment(new TopicIdPartition(TOPIC_1, 3), DIR_3, \"testAssignmentAggregation\", () -> { });\n        manager.onAssignment(new TopicIdPartition(TOPIC_1, 4), DIR_1, \"testAssignmentAggregation\", () -> { });\n        manager.onAssignment(new TopicIdPartition(TOPIC_2, 5), DIR_2, \"testAssignmentAggregation\", () -> { });\n        TestUtils.waitForCondition(() -> {\n            time.sleep(100);\n            manager.wakeup();\n            return readyToAssert.await(1, TimeUnit.MILLISECONDS);\n        }, \"Timed out waiting for AssignReplicasToDirsRequest to be sent.\");\n\n        ArgumentCaptor<AssignReplicasToDirsRequest.Builder> captor =\n            ArgumentCaptor.forClass(AssignReplicasToDirsRequest.Builder.class);\n        verify(channelManager, times(1)).start();\n        verify(channelManager).sendRequest(captor.capture(), any(ControllerRequestCompletionHandler.class));\n        verify(channelManager, atMostOnce()).shutdown();\n        verifyNoMoreInteractions(channelManager);\n        assertEquals(1, captor.getAllValues().size());\n        AssignReplicasToDirsRequestData actual = captor.getValue().build().data();\n        AssignReplicasToDirsRequestData expected = buildRequestData(\n            8, 100L, new HashMap<TopicIdPartition, Uuid>() {{\n                    put(new TopicIdPartition(TOPIC_1, 1), DIR_1);\n                    put(new TopicIdPartition(TOPIC_1, 2), DIR_2);\n                    put(new TopicIdPartition(TOPIC_1, 3), DIR_3);\n                    put(new TopicIdPartition(TOPIC_1, 4), DIR_1);\n                    put(new TopicIdPartition(TOPIC_2, 5), DIR_2);\n                }}\n        );\n        assertRequestEquals(expected, actual);\n    }\n\n    @Test\n    void testRequeuesFailedAssignmentPropagations() throws InterruptedException {\n        CountDownLatch readyToAssert = new CountDownLatch(5);\n        doAnswer(invocation -> {\n            readyToAssert.countDown();\n            if (readyToAssert.getCount() == 4) {\n                invocation.getArgument(1, ControllerRequestCompletionHandler.class).onTimeout();\n                manager.onAssignment(new TopicIdPartition(TOPIC_1, 2), DIR_3, \"testRequeuesFailedAssignmentPropagations\", () -> { });\n            }\n            if (readyToAssert.getCount() == 3) {\n                invocation.getArgument(1, ControllerRequestCompletionHandler.class).onComplete(\n                    new ClientResponse(null, null, null, 0L, 0L, false, false,\n                        new UnsupportedVersionException(\"test unsupported version exception\"), null, null));\n\n                // duplicate should be ignored\n                manager.onAssignment(new TopicIdPartition(TOPIC_1, 2), DIR_3, \"testRequeuesFailedAssignmentPropagations\", () -> { });\n\n                manager.onAssignment(new TopicIdPartition(TOPIC_1, 3),\n                     Uuid.fromString(\"xHLCnG54R9W3lZxTPnpk1Q\"), \"testRequeuesFailedAssignmentPropagations\", () -> { });\n            }\n            if (readyToAssert.getCount() == 2) {\n                invocation.getArgument(1, ControllerRequestCompletionHandler.class).onComplete(\n                        new ClientResponse(null, null, null, 0L, 0L, false, false, null,\n                                new AuthenticationException(\"test authentication exception\"), null)\n                );\n\n                // duplicate should be ignored\n                manager.onAssignment(new TopicIdPartition(TOPIC_1, 3),\n                     Uuid.fromString(\"xHLCnG54R9W3lZxTPnpk1Q\"), \"testRequeuesFailedAssignmentPropagations\", () -> { });\n\n                manager.onAssignment(new TopicIdPartition(TOPIC_1, 4),\n                     Uuid.fromString(\"RCYu1A0CTa6eEIpuKDOfxw\"), \"testRequeuesFailedAssignmentPropagations\", () -> { });\n            }\n            if (readyToAssert.getCount() == 1) {\n                invocation.getArgument(1, ControllerRequestCompletionHandler.class).onComplete(\n                    new ClientResponse(null, null, null, 0L, 0L, false, false, null, null,\n                        new AssignReplicasToDirsResponse(new AssignReplicasToDirsResponseData()\n                            .setErrorCode(Errors.NOT_CONTROLLER.code())\n                            .setThrottleTimeMs(0))));\n            }\n            return null;\n        }).when(channelManager).sendRequest(any(AssignReplicasToDirsRequest.Builder.class),\n            any(ControllerRequestCompletionHandler.class));\n\n        manager.onAssignment(new TopicIdPartition(TOPIC_1, 1), DIR_1, \"testRequeuesFailedAssignmentPropagations\", () -> { });\n        TestUtils.waitForCondition(() -> {\n            time.sleep(TimeUnit.SECONDS.toMillis(1));\n            manager.wakeup();\n            return readyToAssert.await(1, TimeUnit.MILLISECONDS);\n        }, \"Timed out waiting for AssignReplicasToDirsRequest to be sent.\");\n\n        ArgumentCaptor<AssignReplicasToDirsRequest.Builder> captor =\n            ArgumentCaptor.forClass(AssignReplicasToDirsRequest.Builder.class);\n        verify(channelManager, times(1)).start();\n        verify(channelManager, times(5)).sendRequest(captor.capture(),\n            any(ControllerRequestCompletionHandler.class));\n        verify(channelManager, atMostOnce()).shutdown();\n        verifyNoMoreInteractions(channelManager);\n        assertEquals(5, captor.getAllValues().size());\n        assertRequestEquals(buildRequestData(\n            8, 100L, new HashMap<TopicIdPartition, Uuid>() {{\n                    put(new TopicIdPartition(TOPIC_1, 1), DIR_1);\n                }}\n        ), captor.getAllValues().get(0).build().data());\n        assertRequestEquals(buildRequestData(\n            8, 100L, new HashMap<TopicIdPartition, Uuid>() {{\n                    put(new TopicIdPartition(TOPIC_1, 1), DIR_1);\n                    put(new TopicIdPartition(TOPIC_1, 2), DIR_3);\n                }}\n        ), captor.getAllValues().get(1).build().data());\n        assertRequestEquals(buildRequestData(\n            8, 100L, new HashMap<TopicIdPartition, Uuid>() {{\n                    put(new TopicIdPartition(TOPIC_1, 1), DIR_1);\n                    put(new TopicIdPartition(TOPIC_1, 2), DIR_3);\n                    put(new TopicIdPartition(TOPIC_1, 3), Uuid.fromString(\"xHLCnG54R9W3lZxTPnpk1Q\"));\n                    put(new TopicIdPartition(TOPIC_1, 4), Uuid.fromString(\"RCYu1A0CTa6eEIpuKDOfxw\"));\n                }}\n        ), captor.getAllValues().get(4).build().data());\n    }\n\n    @Timeout(30)\n    @Test\n    void testOnCompletion() throws Exception {\n        CountDownLatch readyToAssert = new CountDownLatch(300);\n        doAnswer(invocation -> {\n            AssignReplicasToDirsRequestData request = invocation.getArgument(0, AssignReplicasToDirsRequest.Builder.class).build().data();\n            ControllerRequestCompletionHandler completionHandler = invocation.getArgument(1, ControllerRequestCompletionHandler.class);\n            completionHandler.onComplete(buildSuccessfulResponse(request));\n\n            return null;\n        }).when(channelManager).sendRequest(any(AssignReplicasToDirsRequest.Builder.class),\n                any(ControllerRequestCompletionHandler.class));\n\n        for (int i = 0; i < 300; i++) {\n            manager.onAssignment(new TopicIdPartition(TOPIC_1, i % 5), DIR_1, \"testOnCompletion\", readyToAssert::countDown);\n        }\n\n        TestUtils.waitForCondition(() -> {\n            time.sleep(TimeUnit.SECONDS.toMillis(1));\n            manager.wakeup();\n            return readyToAssert.await(1, TimeUnit.MILLISECONDS);\n        }, \"Timed out waiting for AssignReplicasToDirsRequest to be sent.\");\n    }\n\n    private static ClientResponse buildSuccessfulResponse(AssignReplicasToDirsRequestData request) {\n        return buildResponse(request, topicIdPartition -> Errors.NONE);\n    }\n\n    private static ClientResponse buildResponse(AssignReplicasToDirsRequestData request,\n                                                Function<TopicIdPartition, Errors> perPartitionError) {\n        Map<Uuid, Map<TopicIdPartition, Errors>> errors = new HashMap<>();\n        for (AssignReplicasToDirsRequestData.DirectoryData directory : request.directories()) {\n            for (AssignReplicasToDirsRequestData.TopicData topic : directory.topics()) {\n                for (AssignReplicasToDirsRequestData.PartitionData partition : topic.partitions()) {\n                    TopicIdPartition topicIdPartition = new TopicIdPartition(topic.topicId(), partition.partitionIndex());\n                    Errors error = perPartitionError.apply(topicIdPartition);\n                    if (error == null) {\n                        error = Errors.NONE;\n                    }\n                    errors.computeIfAbsent(directory.id(), d -> new HashMap<>()).put(topicIdPartition, error);\n                }\n            }\n        }\n        AssignReplicasToDirsResponseData responseData = AssignmentsHelper.buildResponseData(Errors.NONE.code(), 0, errors);\n        return new ClientResponse(null, null, null,\n                0L, 0L, false, false, null, null,\n                new AssignReplicasToDirsResponse(responseData));\n    }\n\n    @Test\n    public void testAssignmentCompaction() throws Exception {\n        // Delay the first controller response to force assignment compaction logic\n        CompletableFuture<Runnable> completionFuture = new CompletableFuture<>();\n        doAnswer(invocation -> {\n            AssignReplicasToDirsRequestData request = invocation.getArgument(0, AssignReplicasToDirsRequest.Builder.class).build().data();\n            ControllerRequestCompletionHandler completionHandler = invocation.getArgument(1, ControllerRequestCompletionHandler.class);\n            ClientResponse response = buildSuccessfulResponse(request);\n            Runnable completion = () -> completionHandler.onComplete(response);\n            if (completionFuture.isDone()) completion.run();\n            else completionFuture.complete(completion);\n            return null;\n        }).when(channelManager).sendRequest(any(AssignReplicasToDirsRequest.Builder.class),\n                any(ControllerRequestCompletionHandler.class));\n\n        CountDownLatch remainingInvocations = new CountDownLatch(20);\n        Runnable onComplete = () -> {\n            assertTrue(completionFuture.isDone(), \"Premature invocation\");\n            assertTrue(remainingInvocations.getCount() > 0, \"Extra invocation\");\n            remainingInvocations.countDown();\n        };\n        Uuid[] dirs = {DIR_1, DIR_2, DIR_3};\n        for (int i = 0; i < remainingInvocations.getCount(); i++) {\n            time.sleep(100);\n            manager.onAssignment(new TopicIdPartition(TOPIC_1, 0), dirs[i % 3], \"testAssignmentCompaction\", onComplete);\n        }\n        activeWait(completionFuture::isDone);\n        completionFuture.get().run();\n        activeWait(() -> remainingInvocations.getCount() == 0);\n    }\n\n    void activeWait(Supplier<Boolean> predicate) throws InterruptedException {\n        TestUtils.waitForCondition(() -> {\n            boolean conditionSatisfied = predicate.get();\n            if (!conditionSatisfied) {\n                time.sleep(100);\n                manager.wakeup();\n            }\n            return conditionSatisfied;\n        }, TestUtils.DEFAULT_MAX_WAIT_MS, 50, null);\n    }\n\n    static Metric findMetric(String name) {\n        for (Map.Entry<MetricName, Metric> entry : KafkaYammerMetrics.defaultRegistry().allMetrics().entrySet()) {\n            MetricName metricName = entry.getKey();\n            if (AssignmentsManager.class.getSimpleName().equals(metricName.getType()) && metricName.getName().equals(name)) {\n                return entry.getValue();\n            }\n        }\n        throw new IllegalArgumentException(\"metric named \" + name + \" not found\");\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    @Test\n    void testQueuedReplicaToDirAssignmentsMetric() throws Exception {\n        CountDownLatch readyToAssert = new CountDownLatch(1);\n        doAnswer(invocation -> {\n            readyToAssert.countDown();\n            return null;\n        }).when(channelManager).sendRequest(any(AssignReplicasToDirsRequest.Builder.class), any(ControllerRequestCompletionHandler.class));\n\n        Gauge<Integer> queuedReplicaToDirAssignments = (Gauge<Integer>) findMetric(AssignmentsManager.QUEUE_REPLICA_TO_DIR_ASSIGNMENTS_METRIC_NAME);\n        assertEquals(0, queuedReplicaToDirAssignments.value());\n\n        for (int i = 0; i < 4; i++) {\n            manager.onAssignment(new TopicIdPartition(TOPIC_1, i), DIR_1, \"testQueuedReplicaToDirAssignmentsMetric\", () -> { });\n        }\n        TestUtils.waitForCondition(() -> {\n            time.sleep(100);\n            return readyToAssert.await(1, TimeUnit.MILLISECONDS);\n        }, \"Timed out waiting for AssignReplicasToDirsRequest to be sent.\");\n        assertEquals(4, queuedReplicaToDirAssignments.value());\n\n        for (int i = 4; i < 8; i++) {\n            manager.onAssignment(new TopicIdPartition(TOPIC_1, i), DIR_1, \"testQueuedReplicaToDirAssignmentsMetric\", () -> { });\n        }\n        TestUtils.retryOnExceptionWithTimeout(5_000, () -> assertEquals(8, queuedReplicaToDirAssignments.value()));\n    }\n\n    // AssignmentsManager retries to propagate assignments (via AssignReplicasToDirsRequest) after failures.\n    // When an assignment fails to propagate with NOT_LEADER_OR_FOLLOWER, AssignmentsManager should conclude\n    // that the broker has been removed as a replica for the partition, and stop trying to propagate it.\n    @Test\n    void testDropsOldAssignments() throws InterruptedException {\n        TopicIdPartition tp1 = new TopicIdPartition(TOPIC_1, 1), tp2 = new TopicIdPartition(TOPIC_1, 2);\n        List<AssignReplicasToDirsRequestData> requests = new ArrayList<>();\n        CountDownLatch readyToAssert = new CountDownLatch(2);\n        doAnswer(invocation -> {\n            AssignReplicasToDirsRequestData request = invocation.getArgument(0, AssignReplicasToDirsRequest.Builder.class).build().data();\n            ControllerRequestCompletionHandler completionHandler = invocation.getArgument(1, ControllerRequestCompletionHandler.class);\n            if (readyToAssert.getCount() == 2) {\n                // First request, reply with a partition-level NOT_LEADER_OR_FOLLOWER error and queue a different assignment\n                completionHandler.onComplete(buildResponse(request, topicIdPartition -> Errors.NOT_LEADER_OR_FOLLOWER));\n                manager.onAssignment(tp2, DIR_1, \"testDropsOldAssignments-second\");\n            }\n            if (readyToAssert.getCount() == 1) {\n                // Second request, reply with success\n                completionHandler.onComplete(buildSuccessfulResponse(request));\n            }\n            requests.add(request);\n            readyToAssert.countDown();\n            return null;\n        }).when(channelManager).sendRequest(any(), any());\n\n        manager.onAssignment(tp1, DIR_1, \"testDropsOldAssignments-first\");\n        TestUtils.waitForCondition(() -> {\n            time.sleep(TimeUnit.SECONDS.toMillis(1));\n            manager.wakeup();\n            return readyToAssert.await(1, TimeUnit.MILLISECONDS);\n        }, \"Timed out waiting for AssignReplicasToDirsRequest to be sent.\");\n\n        assertEquals(Arrays.asList(\n                buildRequestData(8, 100, new HashMap<TopicIdPartition, Uuid>() {{\n                        put(tp1, DIR_1);\n                    }}),\n                // Even though the controller replied with NOT_LEADER_OR_FOLLOWER, the second request does not include\n                // partition 1, meaning AssignmentManager dropped (no longer retries) the assignment.\n                buildRequestData(8, 100, new HashMap<TopicIdPartition, Uuid>() {{\n                        put(tp2, DIR_1);\n                    }})\n        ), requests);\n    }\n}"
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 4,
                "candidates": [
                    {
                        "lineStart": 372,
                        "lineEnd": 381,
                        "refactoringType": "MyMoveMethodRefactoring",
                        "refactoringInfo": "Move method activeWait to class NodeToControllerChannelManager",
                        "description": "Move method to org.apache.kafka.server.NodeToControllerChannelManager\nRationale: The 'activeWait' method seems to be interacting significantly with a 'manager' object and it specifically calls 'manager.wakeup()'. If the 'manager' is an instance or related functionality of the 'NodeToControllerChannelManager' class, this method would be better located there.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "undone": false
                    },
                    {
                        "lineStart": 383,
                        "lineEnd": 391,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method findMetric to class AssignmentsManager",
                        "description": "move method to org.apache.kafka.server.AssignmentsManager\nRationale: The method `findMetric` directly uses the `AssignmentsManager.class.getSimpleName()` reflecting a significant relationship with that class. It is likely that it deals operations related to AssignmentsManager metrics.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "undone": false
                    },
                    {
                        "lineStart": 320,
                        "lineEnd": 339,
                        "refactoringType": "MyMoveStaticMethodRefactoring",
                        "refactoringInfo": "Move Static method buildResponse to class AssignmentsManager",
                        "description": "move method to org.apache.kafka.server.AssignmentsManager\nRationale: The buildResponse() method prepares a ClientResponse by handling different types of errors in the response, which seems to be related to managing assignments since it also involves utilizing AssignmentsHelper. Thus the AssignmentsManager class would probably be the best place.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "undone": false
                    },
                    {
                        "lineStart": 117,
                        "lineEnd": 122,
                        "refactoringType": "MyMoveMethodRefactoring",
                        "refactoringInfo": "Move method assertRequestEquals to class AssignmentsManager",
                        "description": "Move method to org.apache.kafka.server.AssignmentsManager\nRationale: The method assertRequestEquals() is operating with AssignReplicasToDirsRequestData objects which seems related to some kind of resource or task assignment, hence the AssignmentsManager could be the logical place.",
                        "couldCreateRefObject": true,
                        "applied": false,
                        "undone": false
                    }
                ]
            },
            "processingTime": {
                "llmResponseTime": -1,
                "pluginProcessingTime": -1,
                "totalTime": 47990
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "normalize",
                            "method_signature": "public normalize(AssignReplicasToDirsRequestData request): AssignReplicasToDirsRequestData",
                            "target_class": "AssignReplicasToDirsRequestData",
                            "rationale": "This method is used to normalize the AssignReplicasToDirsRequestData object. Thus, it would be more oriented towards a method in the AssignReplicasToDirsRequestData class."
                        },
                        {
                            "method_name": "assertRequestEquals",
                            "method_signature": "public assertRequestEquals(AssignReplicasToDirsRequestData expected, AssignReplicasToDirsRequestData actual): void",
                            "target_class": "AssignReplicasToDirsRequestDataTest",
                            "rationale": "This method is used for testing to assert if two AssignReplicasToDirsRequestData requests are same. Therefore, it is more appropriate to move this method to a test class for AssignReplicasToDirsRequestData."
                        },
                        {
                            "method_name": "buildSuccessfulResponse",
                            "method_signature": "public static buildSuccessfulResponse(AssignReplicasToDirsRequestData request): ClientResponse",
                            "target_class": "AssignReplicasToDirsRequestDataHelper",
                            "rationale": "This method is used to build successful responses of type AssignReplicasToDirsRequestData. It would be suitable to move the method to a helper class for AssignReplicasToDirsRequestData."
                        },
                        {
                            "method_name": "buildResponse",
                            "method_signature": "public static buildResponse(AssignReplicasToDirsRequestData request, Function<TopicIdPartition, Errors> perPartitionError): ClientResponse",
                            "target_class": "AssignReplicasToDirsRequestDataHelper",
                            "rationale": "This method is used to build the responses of type AssignReplicasToDirsRequestData with a function to map TopicIdPartition to Errors. It would be more logical to move the method to a helper class for AssignReplicasToDirsRequestData."
                        },
                        {
                            "method_name": "findMetric",
                            "method_signature": "static findMetric(String name): Metric",
                            "target_class": "MetricsManager",
                            "rationale": "This method is plainly related with metric management, hence it would be more logical to move it to a class that handles metrics related tasks."
                        }
                    ],
                    "llm_response_time": 15336
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "buildSuccessfulResponse",
                            "method_signature": "private static buildSuccessfulResponse(AssignReplicasToDirsRequestData request):ClientResponse",
                            "target_class": "AssignmentsManager",
                            "rationale": "The method buildSuccessfulResponse operates on an AssignReplicasToDirsRequestData type and returns a ClientResponse, which are both more associated with the AssignmentsManager class."
                        },
                        {
                            "method_name": "buildResponse",
                            "method_signature": "private static buildResponse(AssignReplicasToDirsRequestData request, Function<TopicIdPartition, Errors> perPartitionError):ClientResponse",
                            "target_class": "AssignmentsManager",
                            "rationale": "The method buildResponse operates on an AssignReplicasToDirsRequestData type and returns a ClientResponse, which are both more associated with the AssignmentsManager class."
                        },
                        {
                            "method_name": "findMetric",
                            "method_signature": "static findMetric(String name):Ref",
                            "target_class": "KafkaYammerMetrics",
                            "rationale": "findMetric method is related to metrics, thus it could be better placed in a class related to metrics handling, such as KafkaYammerMetrics."
                        }
                    ],
                    "llm_response_time": 15420
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "normalize",
                            "method_signature": "AssignReplicasToDirsRequestData normalize(AssignReplicasToDirsRequestData request)",
                            "target_class": "AssignReplicasToDirsRequestData",
                            "rationale": "normalize method operation is more related to the data of AssignReplicasToDirsRequestData class itself, and hence should reside there."
                        },
                        {
                            "method_name": "assertRequestEquals",
                            "method_signature": "void assertRequestEquals(AssignReplicasToDirsRequestData expected, AssignReplicasToDirsRequestData actual)",
                            "target_class": "AssignReplicasToDirsRequestDataTest",
                            "rationale": "assertRequestEquals is a method used for testing equivalence between AssignReplicasToDirsRequestData objects and hence should reside in a testing class for that object."
                        },
                        {
                            "method_name": "buildSuccessfulResponse",
                            "method_signature": "private ClientResponse buildSuccessfulResponse(AssignReplicasToDirsRequestData request)",
                            "target_class": "AssignReplicasToDirsRequestDataTest",
                            "rationale": "buildSuccessfulResponse is a method for creating a mock response, this operation is common in testing, so it can reside in corresponding testing class."
                        },
                        {
                            "method_name": "buildResponse",
                            "method_signature": "private ClientResponse buildSuccessfulResponse(AssignReplicasToDirsRequestData request, Function<TopicIdPartition, Errors> perPartitionError)",
                            "target_class": "AssignReplicasToDirsRequestDataTest",
                            "rationale": "buildResponse is a method for creating a mock response considering different potential errors, this operation is common in testing, so it can reside in corresponding testing class."
                        },
                        {
                            "method_name": "findMetric",
                            "method_signature": "static Metric findMetric(String name)",
                            "target_class": "MetricsManager",
                            "rationale": "findMetric method is a utility function related to fetching metrics, and should therefore reside in a class managing metrics."
                        },
                        {
                            "method_name": "activeWait",
                            "method_signature": "private void activeWait(Supplier<Boolean> predicate) throws InterruptedException",
                            "target_class": "TestUtils",
                            "rationale": "activeWait is meant for reducing the time that a thread is likely to spend before becoming enabled to continue execution because of the invocation of testify method. This type of method are usually included in utility classes for testing."
                        }
                    ],
                    "llm_response_time": 14048
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [
                        {
                            "method_name": "normalize",
                            "method_signature": "public normalize(AssignReplicasToDirsRequestData request): AssignReplicasToDirsRequestData",
                            "target_class": "AssignReplicasToDirsRequestData",
                            "rationale": "This method is used to normalize the AssignReplicasToDirsRequestData object. Thus, it would be more oriented towards a method in the AssignReplicasToDirsRequestData class."
                        },
                        {
                            "method_name": "assertRequestEquals",
                            "method_signature": "public assertRequestEquals(AssignReplicasToDirsRequestData expected, AssignReplicasToDirsRequestData actual): void",
                            "target_class": "AssignReplicasToDirsRequestDataTest",
                            "rationale": "This method is used for testing to assert if two AssignReplicasToDirsRequestData requests are same. Therefore, it is more appropriate to move this method to a test class for AssignReplicasToDirsRequestData."
                        },
                        {
                            "method_name": "buildSuccessfulResponse",
                            "method_signature": "public static buildSuccessfulResponse(AssignReplicasToDirsRequestData request): ClientResponse",
                            "target_class": "AssignReplicasToDirsRequestDataHelper",
                            "rationale": "This method is used to build successful responses of type AssignReplicasToDirsRequestData. It would be suitable to move the method to a helper class for AssignReplicasToDirsRequestData."
                        },
                        {
                            "method_name": "buildResponse",
                            "method_signature": "public static buildResponse(AssignReplicasToDirsRequestData request, Function<TopicIdPartition, Errors> perPartitionError): ClientResponse",
                            "target_class": "AssignReplicasToDirsRequestDataHelper",
                            "rationale": "This method is used to build the responses of type AssignReplicasToDirsRequestData with a function to map TopicIdPartition to Errors. It would be more logical to move the method to a helper class for AssignReplicasToDirsRequestData."
                        },
                        {
                            "method_name": "findMetric",
                            "method_signature": "static findMetric(String name): Metric",
                            "target_class": "MetricsManager",
                            "rationale": "This method is plainly related with metric management, hence it would be more logical to move it to a class that handles metrics related tasks."
                        },
                        {
                            "method_name": "activeWait",
                            "method_signature": "private void activeWait(Supplier<Boolean> predicate) throws InterruptedException",
                            "target_class": "TestUtils",
                            "rationale": "activeWait is meant for reducing the time that a thread is likely to spend before becoming enabled to continue execution because of the invocation of testify method. This type of method are usually included in utility classes for testing."
                        }
                    ],
                    "llm_response_time": 0
                }
            ],
            "llmMethodPriority": {
                "priority_method_names": [
                    "activeWait",
                    "findMetric",
                    "buildResponse",
                    "buildSuccessfulResponse",
                    "assertRequestEquals",
                    "normalize"
                ],
                "llm_response_time": 3133
            },
            "targetClassMap": {
                "activeWait": {
                    "target_classes": [
                        {
                            "class_name": "NodeToControllerChannelManager",
                            "similarity_score": 0.269069117598525
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "NodeToControllerChannelManager"
                    ],
                    "llm_response_time": 3481,
                    "similarity_computation_time": 3,
                    "similarity_metric": "cosine"
                },
                "findMetric": {
                    "target_classes": [
                        {
                            "class_name": "ClientMetricsManagerTest",
                            "similarity_score": 0.3880712950703007
                        },
                        {
                            "class_name": "ProcessRole",
                            "similarity_score": 0.38924947208076155
                        },
                        {
                            "class_name": "AssignmentsManager",
                            "similarity_score": 0.6070190927408787
                        },
                        {
                            "class_name": "ClientMetricsManager",
                            "similarity_score": 0.4947235126060283
                        },
                        {
                            "class_name": "NodeToControllerChannelManager",
                            "similarity_score": 0.15866576560438353
                        },
                        {
                            "class_name": "ControllerRequestCompletionHandler",
                            "similarity_score": 0.09601536368738331
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "AssignmentsManager",
                        "ClientMetricsManager",
                        "NodeToControllerChannelManager",
                        "ProcessRole",
                        "ClientMetricsManagerTest",
                        "ControllerRequestCompletionHandler"
                    ],
                    "llm_response_time": 6383,
                    "similarity_computation_time": 16,
                    "similarity_metric": "cosine"
                },
                "buildResponse": {
                    "target_classes": [
                        {
                            "class_name": "ClientMetricsManagerTest",
                            "similarity_score": 0.5280944805340596
                        },
                        {
                            "class_name": "ProcessRole",
                            "similarity_score": 0.38146357071457415
                        },
                        {
                            "class_name": "AssignmentsManager",
                            "similarity_score": 0.6686175691510048
                        },
                        {
                            "class_name": "ClientMetricsManager",
                            "similarity_score": 0.546116095097109
                        },
                        {
                            "class_name": "NodeToControllerChannelManager",
                            "similarity_score": 0.1540912490371446
                        },
                        {
                            "class_name": "ControllerRequestCompletionHandler",
                            "similarity_score": 0.0847701184424181
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "AssignmentsManager",
                        "NodeToControllerChannelManager",
                        "ControllerRequestCompletionHandler",
                        "ClientMetricsManager",
                        "ClientMetricsManagerTest",
                        "ProcessRole"
                    ],
                    "llm_response_time": 11192,
                    "similarity_computation_time": 9,
                    "similarity_metric": "cosine"
                },
                "buildSuccessfulResponse": {
                    "target_classes": [],
                    "target_classes_sorted_by_llm": [],
                    "llm_response_time": 10595,
                    "similarity_computation_time": 0,
                    "similarity_metric": "cosine"
                },
                "assertRequestEquals": {
                    "target_classes": [
                        {
                            "class_name": "NodeToControllerChannelManager",
                            "similarity_score": 0.2719641466102106
                        },
                        {
                            "class_name": "AssignmentsManager",
                            "similarity_score": 0.3036178402259392
                        }
                    ],
                    "target_classes_sorted_by_llm": [
                        "AssignmentsManager",
                        "NodeToControllerChannelManager"
                    ],
                    "llm_response_time": 3700,
                    "similarity_computation_time": 2,
                    "similarity_metric": "cosine"
                }
            }
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "21dd5cd421139d6134d738d6e7555f0db6e1f622",
        "url": "https://github.com/apache/kafka/commit/21dd5cd421139d6134d738d6e7555f0db6e1f622",
        "move_method_refactoring": {
            "type": "Move Method",
            "description": "Move Method private applicationEvents() : Stream<Arguments> from class org.apache.kafka.clients.consumer.internals.ConsumerNetworkThreadTest to private applicationEvents() : Stream<Arguments> from class org.apache.kafka.clients.consumer.internals.events.ApplicationEventProcessorTest",
            "leftSideLocations": [
                {
                    "filePath": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkThreadTest.java",
                    "startLine": 258,
                    "endLine": 270,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "original method declaration",
                    "codeElement": "private applicationEvents() : Stream<Arguments>"
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/events/ApplicationEventProcessorTest.java",
                    "startLine": 116,
                    "endLine": 128,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "moved method declaration",
                    "codeElement": "private applicationEvents() : Stream<Arguments>"
                }
            ],
            "isStatic": true
        },
        "telemetry": {
            "id": "f465b308-6fba-49a9-8ccc-eda004063cf7",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 198,
                "lineStart": 74,
                "lineEnd": 271,
                "bodyLineStart": 74,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkThreadTest.java",
                "sourceCode": "public class ConsumerNetworkThreadTest {\n    private final Time time;\n    private final BlockingQueue<ApplicationEvent> applicationEventsQueue;\n    private final ApplicationEventProcessor applicationEventProcessor;\n    private final OffsetsRequestManager offsetsRequestManager;\n    private final HeartbeatRequestManager heartbeatRequestManager;\n    private final CoordinatorRequestManager coordinatorRequestManager;\n    private final ConsumerNetworkThread consumerNetworkThread;\n    private final NetworkClientDelegate networkClientDelegate;\n    private final RequestManagers requestManagers;\n    private final CompletableEventReaper applicationEventReaper;\n\n    ConsumerNetworkThreadTest() {\n        this.networkClientDelegate = mock(NetworkClientDelegate.class);\n        this.requestManagers = mock(RequestManagers.class);\n        this.offsetsRequestManager = mock(OffsetsRequestManager.class);\n        this.heartbeatRequestManager = mock(HeartbeatRequestManager.class);\n        this.coordinatorRequestManager = mock(CoordinatorRequestManager.class);\n        this.applicationEventProcessor = mock(ApplicationEventProcessor.class);\n        this.applicationEventReaper = mock(CompletableEventReaper.class);\n        this.time = new MockTime();\n        this.applicationEventsQueue = new LinkedBlockingQueue<>();\n        LogContext logContext = new LogContext();\n\n        this.consumerNetworkThread = new ConsumerNetworkThread(\n                logContext,\n                time,\n                applicationEventsQueue,\n                applicationEventReaper,\n                () -> applicationEventProcessor,\n                () -> networkClientDelegate,\n                () -> requestManagers\n        );\n    }\n\n    @BeforeEach\n    public void setup() {\n        consumerNetworkThread.initializeResources();\n    }\n\n    @AfterEach\n    public void tearDown() {\n        if (consumerNetworkThread != null)\n            consumerNetworkThread.close();\n    }\n\n    @Test\n    public void testEnsureCloseStopsRunningThread() {\n        assertTrue(consumerNetworkThread.isRunning(),\n            \"ConsumerNetworkThread should start running when created\");\n\n        consumerNetworkThread.close();\n        assertFalse(consumerNetworkThread.isRunning(),\n            \"close() should make consumerNetworkThread.running false by calling closeInternal(Duration timeout)\");\n    }\n\n    @ParameterizedTest\n    @ValueSource(longs = {ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS - 1, ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS, ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS + 1})\n    public void testConsumerNetworkThreadPollTimeComputations(long exampleTime) {\n        List<Optional<? extends RequestManager>> list = new ArrayList<>();\n        list.add(Optional.of(coordinatorRequestManager));\n        list.add(Optional.of(heartbeatRequestManager));\n\n        when(requestManagers.entries()).thenReturn(list);\n\n        NetworkClientDelegate.PollResult pollResult = new NetworkClientDelegate.PollResult(exampleTime);\n        NetworkClientDelegate.PollResult pollResult1 = new NetworkClientDelegate.PollResult(exampleTime + 100);\n\n        long t = time.milliseconds();\n        when(coordinatorRequestManager.poll(t)).thenReturn(pollResult);\n        when(coordinatorRequestManager.maximumTimeToWait(t)).thenReturn(exampleTime);\n        when(heartbeatRequestManager.poll(t)).thenReturn(pollResult1);\n        when(heartbeatRequestManager.maximumTimeToWait(t)).thenReturn(exampleTime + 100);\n        when(networkClientDelegate.addAll(pollResult)).thenReturn(pollResult.timeUntilNextPollMs);\n        when(networkClientDelegate.addAll(pollResult1)).thenReturn(pollResult1.timeUntilNextPollMs);\n        consumerNetworkThread.runOnce();\n\n        verify(networkClientDelegate).poll(Math.min(exampleTime, ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS), time.milliseconds());\n        assertEquals(consumerNetworkThread.maximumTimeToWait(), exampleTime);\n    }\n\n    @Test\n    public void testStartupAndTearDown() throws InterruptedException {\n        consumerNetworkThread.start();\n        TestCondition isStarted = consumerNetworkThread::isRunning;\n        TestCondition isClosed = () -> !(consumerNetworkThread.isRunning() || consumerNetworkThread.isAlive());\n\n        // There's a nonzero amount of time between starting the thread and having it\n        // begin to execute our code. Wait for a bit before checking...\n        TestUtils.waitForCondition(isStarted,\n                \"The consumer network thread did not start within \" + DEFAULT_MAX_WAIT_MS + \" ms\");\n\n        consumerNetworkThread.close(Duration.ofMillis(DEFAULT_MAX_WAIT_MS));\n\n        TestUtils.waitForCondition(isClosed,\n                \"The consumer network thread did not stop within \" + DEFAULT_MAX_WAIT_MS + \" ms\");\n    }\n\n    @Test\n    public void testRequestsTransferFromManagersToClientOnThreadRun() {\n        List<Optional<? extends RequestManager>> list = new ArrayList<>();\n        list.add(Optional.of(coordinatorRequestManager));\n        list.add(Optional.of(heartbeatRequestManager));\n        list.add(Optional.of(offsetsRequestManager));\n\n        when(requestManagers.entries()).thenReturn(list);\n        when(coordinatorRequestManager.poll(anyLong())).thenReturn(mock(NetworkClientDelegate.PollResult.class));\n        consumerNetworkThread.runOnce();\n        requestManagers.entries().forEach(rmo -> rmo.ifPresent(rm -> verify(rm).poll(anyLong())));\n        requestManagers.entries().forEach(rmo -> rmo.ifPresent(rm -> verify(rm).maximumTimeToWait(anyLong())));\n        verify(networkClientDelegate).addAll(any(NetworkClientDelegate.PollResult.class));\n        verify(networkClientDelegate).poll(anyLong(), anyLong());\n    }\n\n    @ParameterizedTest\n    @MethodSource(\"applicationEvents\")\n    public void testApplicationEventIsProcessed(ApplicationEvent e) {\n        applicationEventsQueue.add(e);\n        consumerNetworkThread.runOnce();\n\n        if (e instanceof CompletableEvent)\n            verify(applicationEventReaper).add((CompletableEvent<?>) e);\n\n        verify(applicationEventProcessor).process(any(e.getClass()));\n        assertTrue(applicationEventsQueue.isEmpty());\n    }\n\n    @ParameterizedTest\n    @ValueSource(booleans = {true, false})\n    public void testListOffsetsEventIsProcessed(boolean requireTimestamp) {\n        Map<TopicPartition, Long> timestamps = Collections.singletonMap(new TopicPartition(\"topic1\", 1), 5L);\n        ApplicationEvent e = new ListOffsetsEvent(timestamps, calculateDeadlineMs(time, 100), requireTimestamp);\n        applicationEventsQueue.add(e);\n        consumerNetworkThread.runOnce();\n        verify(applicationEventProcessor).process(any(ListOffsetsEvent.class));\n        assertTrue(applicationEventsQueue.isEmpty());\n    }\n\n    @Test\n    public void testResetPositionsProcessFailureIsIgnored() {\n        doThrow(new NullPointerException()).when(offsetsRequestManager).resetPositionsIfNeeded();\n\n        ResetPositionsEvent event = new ResetPositionsEvent(calculateDeadlineMs(time, 100));\n        applicationEventsQueue.add(event);\n        assertDoesNotThrow(() -> consumerNetworkThread.runOnce());\n\n        verify(applicationEventProcessor).process(any(ResetPositionsEvent.class));\n    }\n\n    @Test\n    public void testMaximumTimeToWait() {\n        final int defaultHeartbeatIntervalMs = 1000;\n        // Initial value before runOnce has been called\n        assertEquals(ConsumerNetworkThread.MAX_POLL_TIMEOUT_MS, consumerNetworkThread.maximumTimeToWait());\n\n        when(requestManagers.entries()).thenReturn(Collections.singletonList(Optional.of(heartbeatRequestManager)));\n        when(heartbeatRequestManager.maximumTimeToWait(time.milliseconds())).thenReturn((long) defaultHeartbeatIntervalMs);\n\n        consumerNetworkThread.runOnce();\n        // After runOnce has been called, it takes the default heartbeat interval from the heartbeat request manager\n        assertEquals(defaultHeartbeatIntervalMs, consumerNetworkThread.maximumTimeToWait());\n    }\n\n    @Test\n    public void testCleanupInvokesReaper() {\n        LinkedList<NetworkClientDelegate.UnsentRequest> queue = new LinkedList<>();\n        when(networkClientDelegate.unsentRequests()).thenReturn(queue);\n        consumerNetworkThread.cleanup();\n        verify(applicationEventReaper).reap(applicationEventsQueue);\n    }\n\n    @Test\n    public void testRunOnceInvokesReaper() {\n        consumerNetworkThread.runOnce();\n        verify(applicationEventReaper).reap(any(Long.class));\n    }\n\n    @Test\n    public void testSendUnsentRequests() {\n        when(networkClientDelegate.hasAnyPendingRequests()).thenReturn(true).thenReturn(true).thenReturn(false);\n        consumerNetworkThread.cleanup();\n        verify(networkClientDelegate, times(2)).poll(anyLong(), anyLong());\n    }\n\n    private static Stream<Arguments> applicationEvents() {\n        Map<TopicPartition, OffsetAndMetadata> offset = new HashMap<>();\n        final long currentTimeMs = 12345;\n        return Stream.of(\n                Arguments.of(new PollEvent(100)),\n                Arguments.of(new NewTopicsMetadataUpdateRequestEvent()),\n                Arguments.of(new AsyncCommitEvent(new HashMap<>())),\n                Arguments.of(new SyncCommitEvent(new HashMap<>(), 500)),\n                Arguments.of(new ResetPositionsEvent(500)),\n                Arguments.of(new ValidatePositionsEvent(500)),\n                Arguments.of(new TopicMetadataEvent(\"topic\", Long.MAX_VALUE)),\n                Arguments.of(new AssignmentChangeEvent(offset, currentTimeMs)));\n    }\n}"
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [
                {
                    "iteration_num": 1,
                    "suggested_move_methods": [
                        {
                            "method_name": "failed to unparse",
                            "method_signature": "failed to unparse",
                            "target_class": "failed to unparse",
                            "rationale": "Since the class provided is a test class intended to unit test the ConsumerNetworkThread class and all methods are related to testing, there's no specific method in the ConsumerNetworkThreadTest class that needs to be moved to another class. All methods seem to be in the right place, each one performing different kind of tests for the ConsumerNetworkThread class. Therefore, no recommendations for Move Method refactorings can be offered at this moment."
                        }
                    ],
                    "llm_response_time": 4868
                },
                {
                    "iteration_num": 2,
                    "suggested_move_methods": [
                        {
                            "method_name": "failed to unparse",
                            "method_signature": "failed to unparse",
                            "target_class": "failed to unparse",
                            "rationale": "The given code is a unit test class for ConsumerNetworkThread. All the methods in this class are intended for testing purposes and belong to this class itself. They don't need to be moved to other classes as they are responsible for setting up the test environment, executing the tests, and verifying the test results. Hence, no refactoring is suggested in this case in terms of moving methods."
                        }
                    ],
                    "llm_response_time": 3642
                },
                {
                    "iteration_num": 3,
                    "suggested_move_methods": [
                        {
                            "method_name": "failed to unparse",
                            "method_signature": "failed to unparse",
                            "target_class": "failed to unparse",
                            "rationale": "Looking at this Java Class, it appears to be a test class. All the methods in it are test-related methods and they directly test the functionality of the class `ConsumerNetworkThread`. Therefore, no refactoring operation with regards to the task, \"Move Method\", is necessary or recommended. All methods belong to this class, as they are all for testing specific functionalities in the `ConsumerNetworkThread` class."
                        }
                    ],
                    "llm_response_time": 3751
                },
                {
                    "iteration_num": -1,
                    "suggested_move_methods": [],
                    "llm_response_time": 0
                }
            ],
            "targetClassMap": {}
        }
    },
    {
        "repository": "https://github.com/apache/kafka.git",
        "sha1": "6a2789cf70665bc564bfa582fec17f42b93f3616",
        "url": "https://github.com/apache/kafka/commit/6a2789cf70665bc564bfa582fec17f42b93f3616",
        "move_method_refactoring": {
            "type": "Move Method",
            "description": "Move Method private handleCoordinatorDisconnect(exception Throwable, currentTimeMs long) : void from class org.apache.kafka.clients.consumer.internals.CommitRequestManager to public handleCoordinatorDisconnect(exception Throwable, currentTimeMs long) : void from class org.apache.kafka.clients.consumer.internals.CoordinatorRequestManager",
            "leftSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/CommitRequestManager.java",
                    "startLine": 568,
                    "endLine": 572,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "original method declaration",
                    "codeElement": "private handleCoordinatorDisconnect(exception Throwable, currentTimeMs long) : void"
                }
            ],
            "rightSideLocations": [
                {
                    "filePath": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/CoordinatorRequestManager.java",
                    "startLine": 126,
                    "endLine": 139,
                    "startColumn": 5,
                    "endColumn": 6,
                    "codeElementType": "METHOD_DECLARATION",
                    "description": "moved method declaration",
                    "codeElement": "public handleCoordinatorDisconnect(exception Throwable, currentTimeMs long) : void"
                }
            ],
            "isStatic": false
        },
        "telemetry": {
            "id": "c6722188-1495-4fc7-8205-59a143efec24",
            "hostFunctionTelemetryData": {
                "hostFunctionSize": 1244,
                "lineStart": 72,
                "lineEnd": 1315,
                "bodyLineStart": 72,
                "language": "java",
                "filePath": "/Users/abhiram/Documents/TBE/evaluation_projects/kafka/clients/src/main/java/org/apache/kafka/clients/consumer/internals/CommitRequestManager.java",
                "sourceCode": "public class CommitRequestManager implements RequestManager, MemberStateListener {\n    private final Time time;\n    private final SubscriptionState subscriptions;\n    private final LogContext logContext;\n    private final Logger log;\n    private final Optional<AutoCommitState> autoCommitState;\n    private final CoordinatorRequestManager coordinatorRequestManager;\n    private final OffsetCommitCallbackInvoker offsetCommitCallbackInvoker;\n    private final OffsetCommitMetricsManager metricsManager;\n    private final long retryBackoffMs;\n    private final String groupId;\n    private final Optional<String> groupInstanceId;\n    private final long retryBackoffMaxMs;\n    // For testing only\n    private final OptionalDouble jitter;\n    private final boolean throwOnFetchStableOffsetUnsupported;\n    final PendingRequests pendingRequests;\n    private boolean closing = false;\n\n    /**\n     * Last member epoch sent in a commit request. Empty if no epoch was included in the last\n     * request. Used for logging.\n     */\n    private Optional<Integer> lastEpochSentOnCommit;\n\n    /**\n     *  Latest member ID and epoch received via the {@link #onMemberEpochUpdated(Optional, Optional)},\n     *  to be included in the OffsetFetch and OffsetCommit requests if present. This will have\n     *  the latest values received from the broker, or empty of the member is not part of the\n     *  group anymore.\n     */\n    private final MemberInfo memberInfo;\n\n    public CommitRequestManager(\n            final Time time,\n            final LogContext logContext,\n            final SubscriptionState subscriptions,\n            final ConsumerConfig config,\n            final CoordinatorRequestManager coordinatorRequestManager,\n            final OffsetCommitCallbackInvoker offsetCommitCallbackInvoker,\n            final String groupId,\n            final Optional<String> groupInstanceId,\n            final Metrics metrics) {\n        this(time,\n            logContext,\n            subscriptions,\n            config,\n            coordinatorRequestManager,\n            offsetCommitCallbackInvoker,\n            groupId,\n            groupInstanceId,\n            config.getLong(ConsumerConfig.RETRY_BACKOFF_MS_CONFIG),\n            config.getLong(ConsumerConfig.RETRY_BACKOFF_MAX_MS_CONFIG),\n            OptionalDouble.empty(),\n            metrics);\n    }\n\n    // Visible for testing\n    CommitRequestManager(\n        final Time time,\n        final LogContext logContext,\n        final SubscriptionState subscriptions,\n        final ConsumerConfig config,\n        final CoordinatorRequestManager coordinatorRequestManager,\n        final OffsetCommitCallbackInvoker offsetCommitCallbackInvoker,\n        final String groupId,\n        final Optional<String> groupInstanceId,\n        final long retryBackoffMs,\n        final long retryBackoffMaxMs,\n        final OptionalDouble jitter,\n        final Metrics metrics) {\n        Objects.requireNonNull(coordinatorRequestManager, \"Coordinator is needed upon committing offsets\");\n        this.time = time;\n        this.logContext = logContext;\n        this.log = logContext.logger(getClass());\n        this.pendingRequests = new PendingRequests();\n        if (config.getBoolean(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG)) {\n            final long autoCommitInterval =\n                Integer.toUnsignedLong(config.getInt(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG));\n            this.autoCommitState = Optional.of(new AutoCommitState(time, autoCommitInterval, logContext));\n        } else {\n            this.autoCommitState = Optional.empty();\n        }\n        this.coordinatorRequestManager = coordinatorRequestManager;\n        this.groupId = groupId;\n        this.groupInstanceId = groupInstanceId;\n        this.subscriptions = subscriptions;\n        this.retryBackoffMs = retryBackoffMs;\n        this.retryBackoffMaxMs = retryBackoffMaxMs;\n        this.jitter = jitter;\n        this.throwOnFetchStableOffsetUnsupported = config.getBoolean(THROW_ON_FETCH_STABLE_OFFSET_UNSUPPORTED);\n        this.memberInfo = new MemberInfo();\n        this.metricsManager = new OffsetCommitMetricsManager(metrics);\n        this.offsetCommitCallbackInvoker = offsetCommitCallbackInvoker;\n        this.lastEpochSentOnCommit = Optional.empty();\n    }\n\n    /**\n     * Poll for the {@link OffsetFetchRequest} and {@link OffsetCommitRequest} request if there's any. The function will\n     * also try to autocommit the offsets, if feature is enabled.\n     */\n    @Override\n    public NetworkClientDelegate.PollResult poll(final long currentTimeMs) {\n        // poll only when the coordinator node is known.\n        if (!coordinatorRequestManager.coordinator().isPresent())\n            return EMPTY;\n\n        if (closing) {\n            return drainPendingOffsetCommitRequests();\n        }\n\n        maybeAutoCommitAsync();\n        if (!pendingRequests.hasUnsentRequests())\n            return EMPTY;\n\n        List<NetworkClientDelegate.UnsentRequest> requests = pendingRequests.drain(currentTimeMs);\n        // min of the remainingBackoffMs of all the request that are still backing off\n        final long timeUntilNextPoll = Math.min(\n            findMinTime(unsentOffsetCommitRequests(), currentTimeMs),\n            findMinTime(unsentOffsetFetchRequests(), currentTimeMs));\n        return new NetworkClientDelegate.PollResult(timeUntilNextPoll, requests);\n    }\n\n    @Override\n    public void signalClose() {\n        closing = true;\n    }\n\n    /**\n     * Returns the delay for which the application thread can safely wait before it should be responsive\n     * to results from the request managers. For example, the subscription state can change when heartbeats\n     * are sent, so blocking for longer than the heartbeat interval might mean the application thread is not\n     * responsive to changes.\n     */\n    @Override\n    public long maximumTimeToWait(long currentTimeMs) {\n        return autoCommitState.map(ac -> ac.remainingMs(currentTimeMs)).orElse(Long.MAX_VALUE);\n    }\n\n    private static long findMinTime(final Collection<? extends RequestState> requests, final long currentTimeMs) {\n        return requests.stream()\n            .mapToLong(request -> request.remainingBackoffMs(currentTimeMs))\n            .min()\n            .orElse(Long.MAX_VALUE);\n    }\n\n    private KafkaException maybeWrapAsTimeoutException(Throwable t) {\n        if (t instanceof TimeoutException)\n            return (TimeoutException) t;\n        else\n            return new TimeoutException(t);\n    }\n\n    /**\n     * Generate a request to commit consumed offsets. Add the request to the queue of pending\n     * requests to be sent out on the next call to {@link #poll(long)}. If there are empty\n     * offsets to commit, no request will be generated and a completed future will be returned.\n     *\n     * @param requestState Commit request\n     * @return Future containing the offsets that were committed, or an error if the request\n     * failed.\n     */\n    private CompletableFuture<Map<TopicPartition, OffsetAndMetadata>> requestAutoCommit(final OffsetCommitRequestState requestState) {\n        AutoCommitState autocommit = autoCommitState.get();\n        CompletableFuture<Map<TopicPartition, OffsetAndMetadata>> result;\n        if (requestState.offsets.isEmpty()) {\n            result = CompletableFuture.completedFuture(Collections.emptyMap());\n        } else {\n            autocommit.setInflightCommitStatus(true);\n            OffsetCommitRequestState request = pendingRequests.addOffsetCommitRequest(requestState);\n            result = request.future;\n            result.whenComplete(autoCommitCallback(request.offsets));\n        }\n        return result;\n    }\n\n    /**\n     * If auto-commit is enabled, and the auto-commit interval has expired, this will generate and\n     * enqueue a request to commit all consumed offsets, and will reset the auto-commit timer to the\n     * interval. The request will be sent on the next call to {@link #poll(long)}.\n     * <p/>\n     * If the request completes with a retriable error, this will reset the auto-commit timer with\n     * the exponential backoff. If it fails with a non-retriable error, no action is taken, so\n     * the next commit will be generated when the interval expires.\n     * <p/>\n     * This will not generate a new commit request if a previous one hasn't received a response.\n     * In that case, the next auto-commit request will be sent on the next call to poll, after a\n     * response for the in-flight is received.\n     */\n    public void maybeAutoCommitAsync() {\n        if (autoCommitEnabled() && autoCommitState.get().shouldAutoCommit()) {\n            OffsetCommitRequestState requestState = createOffsetCommitRequest(\n                subscriptions.allConsumed(),\n                Long.MAX_VALUE);\n            CompletableFuture<Map<TopicPartition, OffsetAndMetadata>> result = requestAutoCommit(requestState);\n            // Reset timer to the interval (even if no request was generated), but ensure that if\n            // the request completes with a retriable error, the timer is reset to send the next\n            // auto-commit after the backoff expires.\n            resetAutoCommitTimer();\n            maybeResetTimerWithBackoff(result);\n        }\n    }\n\n    /**\n     * Reset auto-commit timer to retry with backoff if the future failed with a RetriableCommitFailedException.\n     */\n    private void maybeResetTimerWithBackoff(final CompletableFuture<Map<TopicPartition, OffsetAndMetadata>> result) {\n        result.whenComplete((offsets, error) -> {\n            if (error != null) {\n                if (error instanceof RetriableCommitFailedException) {\n                    log.debug(\"Asynchronous auto-commit of offsets {} failed due to retriable error.\", offsets, error);\n                    resetAutoCommitTimer(retryBackoffMs);\n                } else {\n                    log.debug(\"Asynchronous auto-commit of offsets {} failed: {}\", offsets, error.getMessage());\n                }\n            } else {\n                log.debug(\"Completed asynchronous auto-commit of offsets {}\", offsets);\n            }\n        });\n    }\n\n    /**\n     * Commit consumed offsets if auto-commit is enabled, regardless of the auto-commit interval.\n     * This is used for committing offsets before revoking partitions. This will retry committing\n     * the latest offsets until the request succeeds, fails with a fatal error, or the timeout\n     * expires. Note that:\n     * <ul>\n     *     <li>Considers {@link Errors#STALE_MEMBER_EPOCH} as a retriable error, and will retry it\n     *     including the latest member ID and epoch received from the broker.</li>\n     *     <li>Considers {@link Errors#UNKNOWN_TOPIC_OR_PARTITION} as a fatal error, and will not\n     *     retry it although the error extends RetriableException. The reason is that if a topic\n     *     or partition is deleted, revocation would not finish in time since the auto commit would keep retrying.</li>\n     * </ul>\n     *\n     * Also note that this will generate a commit request even if there is another one in-flight,\n     * generated by the auto-commit on the interval logic, to ensure that the latest offsets are\n     * committed before revoking partitions.\n     *\n     * @return Future that will complete when the offsets are successfully committed. It will\n     * complete exceptionally if the commit fails with a non-retriable error, or if the retry\n     * timeout expires.\n     */\n    public CompletableFuture<Void> maybeAutoCommitSyncBeforeRevocation(final long deadlineMs) {\n        if (!autoCommitEnabled()) {\n            return CompletableFuture.completedFuture(null);\n        }\n\n        CompletableFuture<Void> result = new CompletableFuture<>();\n        OffsetCommitRequestState requestState =\n            createOffsetCommitRequest(subscriptions.allConsumed(), deadlineMs);\n        autoCommitSyncBeforeRevocationWithRetries(requestState, result);\n        return result;\n    }\n\n    private void autoCommitSyncBeforeRevocationWithRetries(OffsetCommitRequestState requestAttempt,\n                                                           CompletableFuture<Void> result) {\n        CompletableFuture<Map<TopicPartition, OffsetAndMetadata>> commitAttempt = requestAutoCommit(requestAttempt);\n        commitAttempt.whenComplete((committedOffsets, error) -> {\n            if (error == null) {\n                result.complete(null);\n            } else {\n                if (error instanceof RetriableException || isStaleEpochErrorAndValidEpochAvailable(error)) {\n                    if (requestAttempt.isExpired()) {\n                        log.debug(\"Auto-commit sync before revocation timed out and won't be retried anymore\");\n                        result.completeExceptionally(maybeWrapAsTimeoutException(error));\n                    } else if (error instanceof UnknownTopicOrPartitionException) {\n                        log.debug(\"Auto-commit sync before revocation failed because topic or partition were deleted\");\n                        result.completeExceptionally(error);\n                    } else {\n                        // Make sure the auto-commit is retried with the latest offsets\n                        log.debug(\"Member {} will retry auto-commit of latest offsets after receiving retriable error {}\",\n                            memberInfo.memberId.orElse(\"undefined\"),\n                            error.getMessage());\n                        requestAttempt.offsets = subscriptions.allConsumed();\n                        requestAttempt.resetFuture();\n                        autoCommitSyncBeforeRevocationWithRetries(requestAttempt, result);\n                    }\n                } else {\n                    log.debug(\"Auto-commit sync before revocation failed with non-retriable error\", error);\n                    result.completeExceptionally(error);\n                }\n            }\n        });\n    }\n\n    /**\n     * Clear the inflight auto-commit flag and log auto-commit completion status.\n     */\n    private BiConsumer<? super Map<TopicPartition, OffsetAndMetadata>, ? super Throwable> autoCommitCallback(final Map<TopicPartition, OffsetAndMetadata> allConsumedOffsets) {\n        return (response, throwable) -> {\n            autoCommitState.ifPresent(autoCommitState -> autoCommitState.setInflightCommitStatus(false));\n            if (throwable == null) {\n                offsetCommitCallbackInvoker.enqueueInterceptorInvocation(allConsumedOffsets);\n                log.debug(\"Completed auto-commit of offsets {}\", allConsumedOffsets);\n            } else if (throwable instanceof RetriableCommitFailedException) {\n                log.debug(\"Auto-commit of offsets {} failed due to retriable error: {}\",\n                        allConsumedOffsets, throwable.getMessage());\n            } else {\n                log.warn(\"Auto-commit of offsets {} failed\", allConsumedOffsets, throwable);\n            }\n        };\n    }\n\n    /**\n     * Generate a request to commit offsets without retrying, even if it fails with a retriable\n     * error. The generated request will be added to the queue to be sent on the next call to\n     * {@link #poll(long)}.\n     *\n     * @param offsets Offsets to commit per partition.\n     * @return Future that will complete when a response is received, successfully or\n     * exceptionally depending on the response. If the request fails with a retriable error, the\n     * future will be completed with a {@link RetriableCommitFailedException}.\n     */\n    public CompletableFuture<Void> commitAsync(final Map<TopicPartition, OffsetAndMetadata> offsets) {\n        if (offsets.isEmpty()) {\n            log.debug(\"Skipping commit of empty offsets\");\n            return CompletableFuture.completedFuture(null);\n        }\n        OffsetCommitRequestState commitRequest = createOffsetCommitRequest(offsets, Long.MAX_VALUE);\n        pendingRequests.addOffsetCommitRequest(commitRequest);\n\n        CompletableFuture<Void> asyncCommitResult = new CompletableFuture<>();\n        commitRequest.future.whenComplete((committedOffsets, error) -> {\n            if (error != null) {\n                asyncCommitResult.completeExceptionally(commitAsyncExceptionForError(error));\n            } else {\n                asyncCommitResult.complete(null);\n            }\n        });\n        return asyncCommitResult;\n    }\n\n    /**\n     * Commit offsets, retrying on expected retriable errors while the retry timeout hasn't expired.\n     *\n     * @param offsets               Offsets to commit\n     * @param deadlineMs            Time until which the request will be retried if it fails with\n     *                              an expected retriable error.\n     * @return Future that will complete when a successful response\n     */\n    public CompletableFuture<Void> commitSync(final Map<TopicPartition, OffsetAndMetadata> offsets,\n                                              final long deadlineMs) {\n        CompletableFuture<Void> result = new CompletableFuture<>();\n        OffsetCommitRequestState requestState = createOffsetCommitRequest(offsets, deadlineMs);\n        commitSyncWithRetries(requestState, result);\n        return result;\n    }\n\n    private OffsetCommitRequestState createOffsetCommitRequest(final Map<TopicPartition, OffsetAndMetadata> offsets,\n                                                               final long deadlineMs) {\n        return jitter.isPresent() ?\n            new OffsetCommitRequestState(\n                offsets,\n                groupId,\n                groupInstanceId,\n                deadlineMs,\n                retryBackoffMs,\n                retryBackoffMaxMs,\n                jitter.getAsDouble(),\n                memberInfo) :\n            new OffsetCommitRequestState(\n                offsets,\n                groupId,\n                groupInstanceId,\n                deadlineMs,\n                retryBackoffMs,\n                retryBackoffMaxMs,\n                memberInfo);\n    }\n\n    private void commitSyncWithRetries(OffsetCommitRequestState requestAttempt,\n                                       CompletableFuture<Void> result) {\n        pendingRequests.addOffsetCommitRequest(requestAttempt);\n\n        // Retry the same commit request while it fails with RetriableException and the retry\n        // timeout hasn't expired.\n        requestAttempt.future.whenComplete((res, error) -> {\n            if (error == null) {\n                result.complete(null);\n            } else {\n                if (error instanceof RetriableException) {\n                    if (requestAttempt.isExpired()) {\n                        log.info(\"OffsetCommit timeout expired so it won't be retried anymore\");\n                        result.completeExceptionally(maybeWrapAsTimeoutException(error));\n                    } else {\n                        requestAttempt.resetFuture();\n                        commitSyncWithRetries(requestAttempt, result);\n                    }\n                } else {\n                    result.completeExceptionally(commitSyncExceptionForError(error));\n                }\n            }\n        });\n    }\n\n    private Throwable commitSyncExceptionForError(Throwable error) {\n        if (error instanceof StaleMemberEpochException) {\n            return new CommitFailedException(\"OffsetCommit failed with stale member epoch.\"\n                + Errors.STALE_MEMBER_EPOCH.message());\n        }\n        return error;\n    }\n\n    private Throwable commitAsyncExceptionForError(Throwable error) {\n        if (error instanceof RetriableException) {\n            return new RetriableCommitFailedException(error);\n        }\n        return error;\n    }\n\n    /**\n     * Enqueue a request to fetch committed offsets, that will be sent on the next call to {@link #poll(long)}.\n     *\n     * @param partitions       Partitions to fetch offsets for.\n     * @param deadlineMs       Time until which the request should be retried if it fails\n     *                         with expected retriable errors.\n     * @return Future that will complete when a successful response is received, or the request\n     * fails and cannot be retried. Note that the request is retried whenever it fails with\n     * retriable expected error and the retry time hasn't expired.\n     */\n    public CompletableFuture<Map<TopicPartition, OffsetAndMetadata>> fetchOffsets(\n        final Set<TopicPartition> partitions,\n        final long deadlineMs) {\n        if (partitions.isEmpty()) {\n            return CompletableFuture.completedFuture(Collections.emptyMap());\n        }\n        CompletableFuture<Map<TopicPartition, OffsetAndMetadata>> result = new CompletableFuture<>();\n        OffsetFetchRequestState request = createOffsetFetchRequest(partitions, deadlineMs);\n        fetchOffsetsWithRetries(request, result);\n        return result;\n    }\n\n    // Visible for testing\n    OffsetFetchRequestState createOffsetFetchRequest(final Set<TopicPartition> partitions,\n                                                             final long deadlineMs) {\n        return jitter.isPresent() ?\n            new OffsetFetchRequestState(\n                partitions,\n                retryBackoffMs,\n                retryBackoffMaxMs,\n                deadlineMs,\n                jitter.getAsDouble(),\n                memberInfo) :\n            new OffsetFetchRequestState(\n                partitions,\n                retryBackoffMs,\n                retryBackoffMaxMs,\n                deadlineMs,\n                memberInfo);\n    }\n\n    private void fetchOffsetsWithRetries(final OffsetFetchRequestState fetchRequest,\n                                         final CompletableFuture<Map<TopicPartition, OffsetAndMetadata>> result) {\n        CompletableFuture<Map<TopicPartition, OffsetAndMetadata>> currentResult = pendingRequests.addOffsetFetchRequest(fetchRequest);\n\n        // Retry the same fetch request while it fails with RetriableException and the retry timeout hasn't expired.\n        currentResult.whenComplete((res, error) -> {\n            boolean inflightRemoved = pendingRequests.inflightOffsetFetches.remove(fetchRequest);\n            if (!inflightRemoved) {\n                log.warn(\"A duplicated, inflight, request was identified, but unable to find it in the \" +\n                    \"outbound buffer:\" + fetchRequest);\n            }\n            if (error == null) {\n                result.complete(res);\n            } else {\n                if (error instanceof RetriableException || isStaleEpochErrorAndValidEpochAvailable(error)) {\n                    if (fetchRequest.isExpired()) {\n                        log.debug(\"OffsetFetch request for {} timed out and won't be retried anymore\", fetchRequest.requestedPartitions);\n                        result.completeExceptionally(maybeWrapAsTimeoutException(error));\n                    } else {\n                        fetchRequest.resetFuture();\n                        fetchOffsetsWithRetries(fetchRequest, result);\n                    }\n                } else\n                    result.completeExceptionally(error);\n            }\n        });\n    }\n\n    private boolean isStaleEpochErrorAndValidEpochAvailable(Throwable error) {\n        return error instanceof StaleMemberEpochException && memberInfo.memberEpoch.isPresent();\n    }\n\n    public void updateAutoCommitTimer(final long currentTimeMs) {\n        this.autoCommitState.ifPresent(t -> t.updateTimer(currentTimeMs));\n    }\n\n    // Visible for testing\n    Queue<OffsetCommitRequestState> unsentOffsetCommitRequests() {\n        return pendingRequests.unsentOffsetCommits;\n    }\n\n    private List<OffsetFetchRequestState> unsentOffsetFetchRequests() {\n        return pendingRequests.unsentOffsetFetches;\n    }\n\n    private void handleCoordinatorDisconnect(Throwable exception, long currentTimeMs) {\n        if (exception instanceof DisconnectException) {\n            coordinatorRequestManager.markCoordinatorUnknown(exception.getMessage(), currentTimeMs);\n        }\n    }\n\n    /**\n     * Update latest member ID and epoch used by the member.\n     *\n     * @param memberEpoch New member epoch received. To be included in the new request.\n     * @param memberId Current member ID. To be included in the new request.\n     */\n    @Override\n    public void onMemberEpochUpdated(Optional<Integer> memberEpoch, Optional<String> memberId) {\n        if (!memberEpoch.isPresent() && memberInfo.memberEpoch.isPresent()) {\n            log.info(\"Member {} won't include member id and epoch in following offset \" +\n                \"commit/fetch requests because it has left the group.\", memberInfo.memberId.orElse(\"unknown\"));\n        }\n        memberInfo.memberId = memberId;\n        memberInfo.memberEpoch = memberEpoch;\n    }\n\n    /**\n     * @return True if auto-commit is enabled as defined in the config {@link ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG}\n     */\n    public boolean autoCommitEnabled() {\n        return autoCommitState.isPresent();\n    }\n\n    /**\n     * Reset the auto-commit timer to the auto-commit interval, so that the next auto-commit is\n     * sent out on the interval starting from now. If auto-commit is not enabled this will\n     * perform no action.\n     */\n    public void resetAutoCommitTimer() {\n        autoCommitState.ifPresent(AutoCommitState::resetTimer);\n    }\n\n    /**\n     * Reset the auto-commit timer to the provided time (backoff), so that the next auto-commit is\n     * sent out then. If auto-commit is not enabled this will perform no action.\n     */\n    public void resetAutoCommitTimer(long retryBackoffMs) {\n        autoCommitState.ifPresent(s -> s.resetTimer(retryBackoffMs));\n    }\n\n    /**\n     * Drains the inflight offsetCommits during shutdown because we want to make sure all pending commits are sent\n     * before closing.\n     */\n    public NetworkClientDelegate.PollResult drainPendingOffsetCommitRequests() {\n        if (pendingRequests.unsentOffsetCommits.isEmpty())\n            return EMPTY;\n        List<NetworkClientDelegate.UnsentRequest> requests = pendingRequests.drainPendingCommits();\n        return new NetworkClientDelegate.PollResult(Long.MAX_VALUE, requests);\n    }\n\n    private class OffsetCommitRequestState extends RetriableRequestState {\n        private Map<TopicPartition, OffsetAndMetadata> offsets;\n        private final String groupId;\n        private final Optional<String> groupInstanceId;\n\n        /**\n         * Future containing the offsets that were committed. It completes when a response is\n         * received for the commit request.\n         */\n        private CompletableFuture<Map<TopicPartition, OffsetAndMetadata>> future;\n\n        OffsetCommitRequestState(final Map<TopicPartition, OffsetAndMetadata> offsets,\n                                 final String groupId,\n                                 final Optional<String> groupInstanceId,\n                                 final long deadlineMs,\n                                 final long retryBackoffMs,\n                                 final long retryBackoffMaxMs,\n                                 final MemberInfo memberInfo) {\n            super(logContext, CommitRequestManager.class.getSimpleName(), retryBackoffMs,\n                retryBackoffMaxMs, memberInfo, deadlineTimer(time, deadlineMs));\n            this.offsets = offsets;\n            this.groupId = groupId;\n            this.groupInstanceId = groupInstanceId;\n            this.future = new CompletableFuture<>();\n        }\n\n        // Visible for testing\n        OffsetCommitRequestState(final Map<TopicPartition, OffsetAndMetadata> offsets,\n                                 final String groupId,\n                                 final Optional<String> groupInstanceId,\n                                 final long deadlineMs,\n                                 final long retryBackoffMs,\n                                 final long retryBackoffMaxMs,\n                                 final double jitter,\n                                 final MemberInfo memberInfo) {\n            super(logContext, CommitRequestManager.class.getSimpleName(), retryBackoffMs, 2,\n                retryBackoffMaxMs, jitter, memberInfo, deadlineTimer(time, deadlineMs));\n            this.offsets = offsets;\n            this.groupId = groupId;\n            this.groupInstanceId = groupInstanceId;\n            this.future = new CompletableFuture<>();\n        }\n\n        public NetworkClientDelegate.UnsentRequest toUnsentRequest() {\n            Map<String, OffsetCommitRequestData.OffsetCommitRequestTopic> requestTopicDataMap = new HashMap<>();\n            for (Map.Entry<TopicPartition, OffsetAndMetadata> entry : offsets.entrySet()) {\n                TopicPartition topicPartition = entry.getKey();\n                OffsetAndMetadata offsetAndMetadata = entry.getValue();\n\n                OffsetCommitRequestData.OffsetCommitRequestTopic topic = requestTopicDataMap\n                    .getOrDefault(topicPartition.topic(),\n                        new OffsetCommitRequestData.OffsetCommitRequestTopic()\n                            .setName(topicPartition.topic())\n                    );\n\n                topic.partitions().add(new OffsetCommitRequestData.OffsetCommitRequestPartition()\n                    .setPartitionIndex(topicPartition.partition())\n                    .setCommittedOffset(offsetAndMetadata.offset())\n                    .setCommittedLeaderEpoch(offsetAndMetadata.leaderEpoch().orElse(RecordBatch.NO_PARTITION_LEADER_EPOCH))\n                    .setCommittedMetadata(offsetAndMetadata.metadata())\n                );\n                requestTopicDataMap.put(topicPartition.topic(), topic);\n            }\n\n            OffsetCommitRequestData data = new OffsetCommitRequestData()\n                    .setGroupId(this.groupId)\n                    .setGroupInstanceId(groupInstanceId.orElse(null))\n                    .setTopics(new ArrayList<>(requestTopicDataMap.values()));\n            if (memberInfo.memberId.isPresent()) {\n                data = data.setMemberId(memberInfo.memberId.get());\n            }\n            if (memberInfo.memberEpoch.isPresent()) {\n                data = data.setGenerationIdOrMemberEpoch(memberInfo.memberEpoch.get());\n                lastEpochSentOnCommit = memberInfo.memberEpoch;\n            } else {\n                lastEpochSentOnCommit = Optional.empty();\n            }\n\n            OffsetCommitRequest.Builder builder = new OffsetCommitRequest.Builder(data);\n\n            return buildRequestWithResponseHandling(builder);\n        }\n\n        /**\n         * Handle OffsetCommitResponse. This will complete the request future successfully if no\n         * errors are found in the response. If the response contains errors, this will:\n         *   - handle expected errors and fail the future with specific exceptions depending on the error\n         *   - fail the future with a non-recoverable KafkaException for all unexpected errors (even if retriable)\n         */\n        @Override\n        public void onResponse(final ClientResponse response) {\n            metricsManager.recordRequestLatency(response.requestLatencyMs());\n            long currentTimeMs = response.receivedTimeMs();\n            OffsetCommitResponse commitResponse = (OffsetCommitResponse) response.responseBody();\n            Set<String> unauthorizedTopics = new HashSet<>();\n            for (OffsetCommitResponseData.OffsetCommitResponseTopic topic : commitResponse.data().topics()) {\n                for (OffsetCommitResponseData.OffsetCommitResponsePartition partition : topic.partitions()) {\n                    TopicPartition tp = new TopicPartition(topic.name(), partition.partitionIndex());\n\n                    Errors error = Errors.forCode(partition.errorCode());\n                    if (error == Errors.NONE) {\n                        OffsetAndMetadata offsetAndMetadata = offsets.get(tp);\n                        long offset = offsetAndMetadata.offset();\n                        log.debug(\"OffsetCommit completed successfully for offset {} partition {}\", offset, tp);\n                        continue;\n                    }\n\n                    onFailedAttempt(currentTimeMs);\n                    if (error == Errors.GROUP_AUTHORIZATION_FAILED) {\n                        future.completeExceptionally(GroupAuthorizationException.forGroupId(groupId));\n                        return;\n                    } else if (error == Errors.COORDINATOR_NOT_AVAILABLE ||\n                        error == Errors.NOT_COORDINATOR ||\n                        error == Errors.REQUEST_TIMED_OUT) {\n                        coordinatorRequestManager.markCoordinatorUnknown(error.message(), currentTimeMs);\n                        future.completeExceptionally(error.exception());\n                        return;\n                    } else if (error == Errors.FENCED_INSTANCE_ID) {\n                        String fencedError = \"OffsetCommit failed due to group instance id fenced: \" + groupInstanceId;\n                        log.error(fencedError);\n                        future.completeExceptionally(new CommitFailedException(fencedError));\n                        return;\n                    } else if (error == Errors.OFFSET_METADATA_TOO_LARGE ||\n                        error == Errors.INVALID_COMMIT_OFFSET_SIZE) {\n                        future.completeExceptionally(error.exception());\n                        return;\n                    } else if (error == Errors.COORDINATOR_LOAD_IN_PROGRESS ||\n                        error == Errors.UNKNOWN_TOPIC_OR_PARTITION) {\n                        // just retry\n                        future.completeExceptionally(error.exception());\n                        return;\n                    } else if (error == Errors.UNKNOWN_MEMBER_ID) {\n                        log.error(\"OffsetCommit failed with {}\", error);\n                        future.completeExceptionally(new CommitFailedException(\"OffsetCommit \" +\n                            \"failed with unknown member ID. \" + error.message()));\n                        return;\n                    } else if (error == Errors.STALE_MEMBER_EPOCH) {\n                        log.error(\"OffsetCommit failed for member {} with stale member epoch error. Last epoch sent: {}\",\n                            memberInfo.memberId.orElse(\"undefined\"),\n                            lastEpochSentOnCommit.isPresent() ? lastEpochSentOnCommit.get() : \"undefined\");\n                        future.completeExceptionally(error.exception());\n                        return;\n                    } else if (error == Errors.TOPIC_AUTHORIZATION_FAILED) {\n                        // Collect all unauthorized topics before failing\n                        unauthorizedTopics.add(tp.topic());\n                    } else {\n                        // Fail with a non-retriable KafkaException for all unexpected errors\n                        // (even if they are retriable)\n                        future.completeExceptionally(new KafkaException(\"Unexpected error in commit: \" + error.message()));\n                        return;\n                    }\n                }\n            }\n\n            if (!unauthorizedTopics.isEmpty()) {\n                log.error(\"OffsetCommit failed due to not authorized to commit to topics {}\", unauthorizedTopics);\n                future.completeExceptionally(new TopicAuthorizationException(unauthorizedTopics));\n            } else {\n                future.complete(null);\n            }\n        }\n\n        @Override\n        String requestDescription() {\n            return \"OffsetCommit request for offsets \" + offsets;\n        }\n\n        @Override\n        CompletableFuture<?> future() {\n            return future;\n        }\n\n        void resetFuture() {\n            future = new CompletableFuture<>();\n        }\n\n        @Override\n        void removeRequest() {\n            if (!unsentOffsetCommitRequests().remove(this)) {\n                log.warn(\"OffsetCommit request to remove not found in the outbound buffer: {}\", this);\n            }\n        }\n    }\n\n    // Visible for testing\n    Optional<Integer> lastEpochSentOnCommit() {\n        return lastEpochSentOnCommit;\n    }\n\n    /**\n     * Represents a request that can be retried or aborted, based on member ID and epoch\n     * information.\n     */\n    abstract class RetriableRequestState extends TimedRequestState {\n\n        /**\n         * Member info (ID and epoch) to be included in the request if present.\n         */\n        final MemberInfo memberInfo;\n\n        RetriableRequestState(LogContext logContext, String owner, long retryBackoffMs,\n                              long retryBackoffMaxMs, MemberInfo memberInfo, Timer timer) {\n            super(logContext, owner, retryBackoffMs, retryBackoffMaxMs, timer);\n            this.memberInfo = memberInfo;\n        }\n\n        // Visible for testing\n        RetriableRequestState(LogContext logContext, String owner, long retryBackoffMs, int retryBackoffExpBase,\n                              long retryBackoffMaxMs, double jitter, MemberInfo memberInfo, Timer timer) {\n            super(logContext, owner, retryBackoffMs, retryBackoffExpBase, retryBackoffMaxMs, jitter, timer);\n            this.memberInfo = memberInfo;\n        }\n\n        /**\n         * @return String containing the request name and arguments, to be used for logging\n         * purposes.\n         */\n        abstract String requestDescription();\n\n        /**\n         * @return Future that will complete with the request response or failure.\n         */\n        abstract CompletableFuture<?> future();\n\n        /**\n         * Complete the request future with a TimeoutException if the request has been sent out\n         * at least once and the timeout has been reached.\n         */\n        void maybeExpire() {\n            if (numAttempts > 0 && isExpired()) {\n                removeRequest();\n                future().completeExceptionally(new TimeoutException(requestDescription() +\n                    \" could not complete before timeout expired.\"));\n            }\n        }\n\n        /**\n         * Build request with the given builder, including response handling logic.\n         */\n        NetworkClientDelegate.UnsentRequest buildRequestWithResponseHandling(final AbstractRequest.Builder<?> builder) {\n            NetworkClientDelegate.UnsentRequest request = new NetworkClientDelegate.UnsentRequest(\n                builder,\n                coordinatorRequestManager.coordinator()\n            );\n            request.whenComplete(\n                (response, throwable) -> {\n                    long completionTimeMs = request.handler().completionTimeMs();\n                    handleClientResponse(response, throwable, completionTimeMs);\n                });\n            return request;\n        }\n\n        private void handleClientResponse(final ClientResponse response,\n                                          final Throwable error,\n                                          final long requestCompletionTimeMs) {\n            try {\n                if (error == null) {\n                    onResponse(response);\n                } else {\n                    log.debug(\"{} completed with error\", requestDescription(), error);\n                    onFailedAttempt(requestCompletionTimeMs);\n                    handleCoordinatorDisconnect(error, requestCompletionTimeMs);\n                    future().completeExceptionally(error);\n                }\n            } catch (Throwable t) {\n                log.error(\"Unexpected error handling response for {}\", requestDescription(), t);\n                future().completeExceptionally(t);\n            }\n        }\n\n        @Override\n        public String toStringBase() {\n            return super.toStringBase() + \", \" + memberInfo;\n        }\n\n        abstract void onResponse(final ClientResponse response);\n\n        abstract void removeRequest();\n    }\n\n    class OffsetFetchRequestState extends RetriableRequestState {\n\n        /**\n         * Partitions to get committed offsets for.\n         */\n        public final Set<TopicPartition> requestedPartitions;\n\n        /**\n         * Future with the result of the request. This can be reset using {@link #resetFuture()}\n         * to get a new result when the request is retried.\n         */\n        private CompletableFuture<Map<TopicPartition, OffsetAndMetadata>> future;\n\n        public OffsetFetchRequestState(final Set<TopicPartition> partitions,\n                                       final long retryBackoffMs,\n                                       final long retryBackoffMaxMs,\n                                       final long deadlineMs,\n                                       final MemberInfo memberInfo) {\n            super(logContext, CommitRequestManager.class.getSimpleName(), retryBackoffMs,\n                retryBackoffMaxMs, memberInfo, deadlineTimer(time, deadlineMs));\n            this.requestedPartitions = partitions;\n            this.future = new CompletableFuture<>();\n        }\n\n        public OffsetFetchRequestState(final Set<TopicPartition> partitions,\n                                       final long retryBackoffMs,\n                                       final long retryBackoffMaxMs,\n                                       final long deadlineMs,\n                                       final double jitter,\n                                       final MemberInfo memberInfo) {\n            super(logContext, CommitRequestManager.class.getSimpleName(), retryBackoffMs, 2,\n                retryBackoffMaxMs, jitter, memberInfo, deadlineTimer(time, deadlineMs));\n            this.requestedPartitions = partitions;\n            this.future = new CompletableFuture<>();\n        }\n\n        public boolean sameRequest(final OffsetFetchRequestState request) {\n            return requestedPartitions.equals(request.requestedPartitions);\n        }\n\n        public NetworkClientDelegate.UnsentRequest toUnsentRequest() {\n\n            OffsetFetchRequest.Builder builder;\n            if (memberInfo.memberId.isPresent() && memberInfo.memberEpoch.isPresent()) {\n                builder = new OffsetFetchRequest.Builder(\n                        groupId,\n                        memberInfo.memberId.get(),\n                        memberInfo.memberEpoch.get(),\n                        true,\n                        new ArrayList<>(this.requestedPartitions),\n                        throwOnFetchStableOffsetUnsupported);\n            } else {\n                // Building request without passing member ID/epoch to leave the logic to choose\n                // default values when not present on the request builder.\n                builder = new OffsetFetchRequest.Builder(\n                        groupId,\n                        true,\n                        new ArrayList<>(this.requestedPartitions),\n                        throwOnFetchStableOffsetUnsupported);\n            }\n            return buildRequestWithResponseHandling(builder);\n        }\n\n        /**\n         * Handle OffsetFetch response, including successful and failed.\n         */\n        @Override\n        void onResponse(final ClientResponse response) {\n            long currentTimeMs = response.receivedTimeMs();\n            OffsetFetchResponse fetchResponse = (OffsetFetchResponse) response.responseBody();\n            Errors responseError = fetchResponse.groupLevelError(groupId);\n            if (responseError != Errors.NONE) {\n                onFailure(currentTimeMs, responseError);\n                return;\n            }\n            onSuccess(currentTimeMs, fetchResponse);\n        }\n\n        /**\n         * Handle failed responses. This will retry if the error is retriable, or complete the\n         * result future exceptionally in the case of non-recoverable or unexpected errors.\n         */\n        private void onFailure(final long currentTimeMs,\n                               final Errors responseError) {\n            log.debug(\"Offset fetch failed: {}\", responseError.message());\n            onFailedAttempt(currentTimeMs);\n            if (responseError == COORDINATOR_LOAD_IN_PROGRESS) {\n                future.completeExceptionally(responseError.exception());\n            } else if (responseError == Errors.UNKNOWN_MEMBER_ID) {\n                log.error(\"OffsetFetch failed with {} because the member is not part of the group\" +\n                    \" anymore.\", responseError);\n                future.completeExceptionally(responseError.exception());\n            } else if (responseError == Errors.STALE_MEMBER_EPOCH) {\n                log.error(\"OffsetFetch failed with {} and the consumer is not part \" +\n                    \"of the group anymore (it probably left the group, got fenced\" +\n                    \" or failed). The request cannot be retried and will fail.\", responseError);\n                future.completeExceptionally(responseError.exception());\n            } else if (responseError == Errors.NOT_COORDINATOR || responseError == Errors.COORDINATOR_NOT_AVAILABLE) {\n                // Re-discover the coordinator and retry\n                coordinatorRequestManager.markCoordinatorUnknown(\"error response \" + responseError.name(), currentTimeMs);\n                future.completeExceptionally(responseError.exception());\n            } else if (responseError == Errors.GROUP_AUTHORIZATION_FAILED) {\n                future.completeExceptionally(GroupAuthorizationException.forGroupId(groupId));\n            } else {\n                // Fail with a non-retriable KafkaException for all unexpected errors (even if\n                // they are retriable)\n                future.completeExceptionally(new KafkaException(\"Unexpected error in fetch offset response: \" + responseError.message()));\n            }\n        }\n\n        @Override\n        String requestDescription() {\n            return \"OffsetFetch request for partitions \" + requestedPartitions;\n        }\n\n        @Override\n        CompletableFuture<?> future() {\n            return future;\n        }\n\n        void resetFuture() {\n            future = new CompletableFuture<>();\n        }\n\n        @Override\n        void removeRequest() {\n            if (!unsentOffsetFetchRequests().remove(this)) {\n                log.warn(\"OffsetFetch request to remove not found in the outbound buffer: {}\", this);\n            }\n        }\n\n        /**\n         * Handle OffsetFetch response that has no group level errors. This will look for\n         * partition level errors and fail the future accordingly, also recording a failed request\n         * attempt. If no partition level errors are found, this will complete the future with the\n         * offsets contained in the response, and record a successful request attempt.\n         */\n        private void onSuccess(final long currentTimeMs,\n                               final OffsetFetchResponse response) {\n            Set<String> unauthorizedTopics = null;\n            Map<TopicPartition, OffsetFetchResponse.PartitionData> responseData =\n                    response.partitionDataMap(groupId);\n            Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>(responseData.size());\n            Set<TopicPartition> unstableTxnOffsetTopicPartitions = new HashSet<>();\n            for (Map.Entry<TopicPartition, OffsetFetchResponse.PartitionData> entry : responseData.entrySet()) {\n                TopicPartition tp = entry.getKey();\n                OffsetFetchResponse.PartitionData partitionData = entry.getValue();\n                if (partitionData.hasError()) {\n                    onFailedAttempt(currentTimeMs);\n                    Errors error = partitionData.error;\n                    log.debug(\"Failed to fetch offset for partition {}: {}\", tp, error.message());\n\n                    if (error == Errors.UNKNOWN_TOPIC_OR_PARTITION) {\n                        future.completeExceptionally(new KafkaException(\"Topic or Partition \" + tp + \" does not exist\"));\n                        return;\n                    } else if (error == Errors.TOPIC_AUTHORIZATION_FAILED) {\n                        if (unauthorizedTopics == null) {\n                            unauthorizedTopics = new HashSet<>();\n                        }\n                        unauthorizedTopics.add(tp.topic());\n                    } else if (error == Errors.UNSTABLE_OFFSET_COMMIT) {\n                        unstableTxnOffsetTopicPartitions.add(tp);\n                    } else {\n                        // Fail with a non-retriable KafkaException for all unexpected partition\n                        // errors (even if they are retriable)\n                        future.completeExceptionally(new KafkaException(\"Unexpected error in fetch offset \" +\n                                \"response for partition \" + tp + \": \" + error.message()));\n                        return;\n                    }\n                } else if (partitionData.offset >= 0) {\n                    // record the position with the offset (-1 indicates no committed offset to fetch);\n                    // if there's no committed offset, record as null\n                    offsets.put(tp, new OffsetAndMetadata(partitionData.offset, partitionData.leaderEpoch, partitionData.metadata));\n                } else {\n                    log.info(\"Found no committed offset for partition {}\", tp);\n                    offsets.put(tp, null);\n                }\n            }\n\n            if (unauthorizedTopics != null) {\n                future.completeExceptionally(new TopicAuthorizationException(unauthorizedTopics));\n            } else if (!unstableTxnOffsetTopicPartitions.isEmpty()) {\n                // TODO: Optimization question: Do we need to retry all partitions upon a single partition error?\n                log.info(\"The following partitions still have unstable offsets \" +\n                        \"which are not cleared on the broker side: {}\" +\n                        \", this could be either \" +\n                        \"transactional offsets waiting for completion, or \" +\n                        \"normal offsets waiting for replication after appending to local log\", unstableTxnOffsetTopicPartitions);\n                future.completeExceptionally(new UnstableOffsetCommitException(\"There are \" +\n                    \"unstable offsets for the requested topic partitions\"));\n            } else {\n                onSuccessfulAttempt(currentTimeMs);\n                future.complete(offsets);\n            }\n        }\n\n        private void chainFuture(\n            final CompletableFuture<Map<TopicPartition, OffsetAndMetadata>> otherFuture) {\n            this.future.whenComplete((r, t) -> {\n                if (t != null) {\n                    otherFuture.completeExceptionally(t);\n                } else {\n                    otherFuture.complete(r);\n                }\n            });\n        }\n\n        @Override\n        public String toStringBase() {\n            return super.toStringBase() +\n                    \", requestedPartitions=\" + requestedPartitions;\n        }\n    }\n\n    /**\n     * <p>This is used to stage the unsent {@link OffsetCommitRequestState} and {@link OffsetFetchRequestState}.\n     * <li>unsentOffsetCommits holds the offset commit requests that have not been sent out</>\n     * <li>unsentOffsetFetches holds the offset fetch requests that have not been sent out</li>\n     * <li>inflightOffsetFetches holds the offset fetch requests that have been sent out but not completed</>.\n     * <p>\n     * {@code addOffsetFetchRequest} dedupes the requests to avoid sending the same requests.\n     */\n\n    class PendingRequests {\n        // Queue is used to ensure the sequence of commit\n        Queue<OffsetCommitRequestState> unsentOffsetCommits = new LinkedList<>();\n        List<OffsetFetchRequestState> unsentOffsetFetches = new ArrayList<>();\n        List<OffsetFetchRequestState> inflightOffsetFetches = new ArrayList<>();\n\n        // Visible for testing\n        boolean hasUnsentRequests() {\n            return !unsentOffsetCommits.isEmpty() || !unsentOffsetFetches.isEmpty();\n        }\n\n        /**\n         * Add a commit request to the queue, so that it's sent out on the next call to\n         * {@link #poll(long)}. This is used from all commits (sync, async, auto-commit).\n         */\n        OffsetCommitRequestState addOffsetCommitRequest(OffsetCommitRequestState request) {\n            log.debug(\"Enqueuing OffsetCommit request for offsets: {}\", request.offsets);\n            unsentOffsetCommits.add(request);\n            return request;\n        }\n\n        /**\n         * <p>Adding an offset fetch request to the outgoing buffer.  If the same request was made, we chain the future\n         * to the existing one.\n         *\n         * <p>If the request is new, it invokes a callback to remove itself from the {@code inflightOffsetFetches}\n         * upon completion.\n         */\n        private CompletableFuture<Map<TopicPartition, OffsetAndMetadata>> addOffsetFetchRequest(final OffsetFetchRequestState request) {\n            Optional<OffsetFetchRequestState> dupe =\n                    unsentOffsetFetches.stream().filter(r -> r.sameRequest(request)).findAny();\n            Optional<OffsetFetchRequestState> inflight =\n                    inflightOffsetFetches.stream().filter(r -> r.sameRequest(request)).findAny();\n\n            if (dupe.isPresent() || inflight.isPresent()) {\n                log.debug(\"Duplicated unsent offset fetch request found for partitions: {}\", request.requestedPartitions);\n                dupe.orElseGet(inflight::get).chainFuture(request.future);\n            } else {\n                log.debug(\"Enqueuing offset fetch request for partitions: {}\", request.requestedPartitions);\n                this.unsentOffsetFetches.add(request);\n            }\n            return request.future;\n        }\n\n        /**\n         * Clear {@code unsentOffsetCommits} and moves all the sendable request in {@code\n         * unsentOffsetFetches} to the {@code inflightOffsetFetches} to bookkeep all the inflight\n         * requests. Note: Sendable requests are determined by their timer as we are expecting\n         * backoff on failed attempt. See {@link RequestState}.\n         */\n        List<NetworkClientDelegate.UnsentRequest> drain(final long currentTimeMs) {\n            // not ready to sent request\n            List<OffsetCommitRequestState> unreadyCommitRequests = unsentOffsetCommits.stream()\n                .filter(request -> !request.canSendRequest(currentTimeMs))\n                .collect(Collectors.toList());\n\n            failAndRemoveExpiredCommitRequests();\n\n            // Add all unsent offset commit requests to the unsentRequests list\n            List<NetworkClientDelegate.UnsentRequest> unsentRequests = unsentOffsetCommits.stream()\n                .filter(request -> request.canSendRequest(currentTimeMs))\n                .peek(request -> request.onSendAttempt(currentTimeMs))\n                .map(OffsetCommitRequestState::toUnsentRequest)\n                .collect(Collectors.toCollection(ArrayList::new));\n\n            // Partition the unsent offset fetch requests into sendable and non-sendable lists\n            Map<Boolean, List<OffsetFetchRequestState>> partitionedBySendability =\n                    unsentOffsetFetches.stream()\n                            .collect(Collectors.partitioningBy(request -> request.canSendRequest(currentTimeMs)));\n\n            failAndRemoveExpiredFetchRequests();\n\n            // Add all sendable offset fetch requests to the unsentRequests list and to the inflightOffsetFetches list\n            for (OffsetFetchRequestState request : partitionedBySendability.get(true)) {\n                request.onSendAttempt(currentTimeMs);\n                unsentRequests.add(request.toUnsentRequest());\n                inflightOffsetFetches.add(request);\n            }\n\n            // Clear the unsent offset commit and fetch lists and add all non-sendable offset fetch requests to the unsentOffsetFetches list\n            clearAll();\n            unsentOffsetFetches.addAll(partitionedBySendability.get(false));\n            unsentOffsetCommits.addAll(unreadyCommitRequests);\n\n            return Collections.unmodifiableList(unsentRequests);\n        }\n\n        /**\n         * Find the unsent commit requests that have expired, remove them and complete their\n         * futures with a TimeoutException.\n         */\n        private void failAndRemoveExpiredCommitRequests() {\n            Queue<OffsetCommitRequestState> requestsToPurge = new LinkedList<>(unsentOffsetCommits);\n            requestsToPurge.forEach(RetriableRequestState::maybeExpire);\n        }\n\n        /**\n         * Find the unsent fetch requests that have expired, remove them and complete their\n         * futures with a TimeoutException.\n         */\n        private void failAndRemoveExpiredFetchRequests() {\n            Queue<OffsetFetchRequestState> requestsToPurge = new LinkedList<>(unsentOffsetFetches);\n            requestsToPurge.forEach(RetriableRequestState::maybeExpire);\n        }\n\n        private void clearAll() {\n            unsentOffsetCommits.clear();\n            unsentOffsetFetches.clear();\n        }\n\n        private List<NetworkClientDelegate.UnsentRequest> drainPendingCommits() {\n            List<NetworkClientDelegate.UnsentRequest> res = unsentOffsetCommits.stream()\n                .map(OffsetCommitRequestState::toUnsentRequest)\n                .collect(Collectors.toCollection(ArrayList::new));\n            clearAll();\n            return res;\n        }\n    }\n\n    /**\n     * Encapsulates the state of auto-committing and manages the auto-commit timer.\n     */\n    private static class AutoCommitState {\n        private final Timer timer;\n        private final long autoCommitInterval;\n        private boolean hasInflightCommit;\n\n        private final Logger log;\n\n        public AutoCommitState(\n                final Time time,\n                final long autoCommitInterval,\n                final LogContext logContext) {\n            this.autoCommitInterval = autoCommitInterval;\n            this.timer = time.timer(autoCommitInterval);\n            this.hasInflightCommit = false;\n            this.log = logContext.logger(getClass());\n        }\n\n        public boolean shouldAutoCommit() {\n            if (!this.timer.isExpired()) {\n                return false;\n            }\n            if (this.hasInflightCommit) {\n                log.trace(\"Skipping auto-commit on the interval because a previous one is still in-flight.\");\n                return false;\n            }\n            return true;\n        }\n\n        public void resetTimer() {\n            this.timer.reset(autoCommitInterval);\n        }\n\n        public void resetTimer(long retryBackoffMs) {\n            this.timer.reset(retryBackoffMs);\n        }\n\n        public long remainingMs(final long currentTimeMs) {\n            this.timer.update(currentTimeMs);\n            return this.timer.remainingMs();\n        }\n\n        public void updateTimer(final long currentTimeMs) {\n            this.timer.update(currentTimeMs);\n        }\n\n        public void setInflightCommitStatus(final boolean inflightCommitStatus) {\n            this.hasInflightCommit = inflightCommitStatus;\n        }\n    }\n\n    static class MemberInfo {\n        Optional<String> memberId;\n        Optional<Integer> memberEpoch;\n\n        MemberInfo() {\n            this.memberId = Optional.empty();\n            this.memberEpoch = Optional.empty();\n        }\n\n        @Override\n        public String toString() {\n            return \"memberId=\" + memberId.orElse(\"undefined\") +\n                    \", memberEpoch=\" + (memberEpoch.isPresent() ? memberEpoch.get() : \"undefined\");\n        }\n    }\n}"
            },
            "candidatesTelemetryData": {
                "numberOfSuggestions": 0,
                "candidates": []
            },
            "iterationData": [],
            "targetClassMap": {}
        }
    }
]